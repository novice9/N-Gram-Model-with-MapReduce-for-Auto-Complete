thread-2: Start crawling
WebPage index: 00000
Akodon spegazzinii
Akodon spegazzinii , also known as Spegazzini's akodont [4] or Spegazzini's grass mouse , [1] is a rodent in the genus Akodon found in northwestern Argentina . It occurs in grassland and forest at 400 to 3,500 m (1,300 to 11,500 ft) above sea level. After the species was first named in 1897, several other names were given to various populations now included in A. spegazzinii . They are now all recognized as part of a single, widespread and variable species. Akodon spegazzinii is related to Akodon boliviensis and other members of the A. boliviensis species group . It reproduces year-round. Because it is widely distributed and common, Akodon spegazzinii is listed as " Least Concern " on the IUCN Red List .
Akodon spegazzinii is medium in size for the A. boliviensis species group. The coloration of its upperparts varies considerably, from light to dark and from yellowish to reddish brown. The underparts are yellow-brown to gray. The eyes are surrounded by a ring of yellow fur. The skull contains an hourglass-shaped interorbital region (between the eyes) and various features of the skull distinguish the species from its close allies. Head and body length is 93 to 196 mm (3.7 to 7.7 in) and body mass is 13.0 to 38.0 g (0.46 to 1.34 oz). Its karyotype has 2n = 40 and FN = 40. [4]

Taxonomy
Akodon spegazzinii was first described in 1897 from Salta Province by Oldfield Thomas on the basis of a collection made in late 1896 or early 1897 by mycologist Carlos Luigi Spegazzini , after whom the species was named. [5] Four years later, Joel Asaph Allen named Akodon tucumanensis from Tucumán Province , comparing it to various species now synonymized under Abrothrix olivaceus . [6] Thomas named an additional species, Akodon alterus , from La Rioja Province in 1919, and considered it closely related to A. spegazzinii . [7] A fourth species, Akodon leucolimnaeus , was described by Ángel Cabrera from Catamarca Province in 1926, but after 1932 it was associated with Akodon lactens (now Necromys lactens ) as a subspecies . [8]
In 1961, Cabrera listed both spegazzinii and tucumanensis as subspecies of Akodon boliviensis , with alterus as a full synonym of A. boliviensis tucumanensis . [4] In 1990, Philip Myers and colleagues reviewed the Akodon boliviensis species group. They provisionally kept Akodon spegazzinii as a species separate from A. boliviensis , with tucumanensis as a subspecies, and suggested that alterus was likely related to spegazzinii and tucumanensis . [9] Subsequently, the treatment of these species in systematic works became variable. A 1992 paper suggested that alterus and tucumanensis were, at best, very similar to each other, [10] but in 1997, Michael Mares and colleagues listed each of the three as distinct species in a compendium of the mammals of Catamarca, citing differences in habitat and fur coloration. [11] They were followed by Mónica Díaz and Rubén Bárquez in 2007, among others. [12] In 2000 Díaz and colleagues listed alterus and tucumanensis as subspecies of spegazzinii in a review of the mammals of Salta. [13] Guy Musser and Michael Carleton, in the 2005 third edition of Mammal Species of the World , also considered the three to represent the same species, [4] as did Ulyses Pardiñas and colleagues in a 2006 review of Argentinean Akodontini . [14] Meanwhile, Carlos Galliari and Pardiñas had recognized Akodon leucolimnaeus as a true Akodon , not a Necromys , in 1995. Although associated with the Akodon boliviensis group, its precise status remained unclear. [15] The common name "Catamarca akodont" was proposed for this species. [8]
In 1980, Julio Contreras and María Rosi [16] identified an Akodon from the province of Mendoza as Akodon varius neocenus (now Akodon neocenus ), but the following year, they identified it as a new species, named Akodon minoprioi in a presentation at a scientific meeting. This name was never formally validated. [17] In 2000, Janet Braun and colleagues formally named this species Akodon oenos and allied it to the Akodon varius species group. The specific name , oenos , is Greek for "wine" and refers to the animal's occurrence in the wine-producing region of Mendoza. [18] The common names "Monte akodont" [19] and "wine grass mouse" [20] were proposed for this species.
In 2010, Pablo Jayat and colleagues reviewed the members of the Akodon boliviensis species group, including A. spegazzinii , in Argentina. They could not find clear differentiation in either morphological or molecular characters between animals belonging to A. alterus , A. leucolimnaeus , A. spegazzinii , and A. tucumanensis , and consequently concluded that they all represent a single species. [10] Although genetic variation is relatively high within A. spegazzinii , there is no clear geographic structure among haplotypes from different regions. [21] The next year, Ulyses Pardiñas and colleagues concluded that A. oenos , which had formerly, and incorrectly, been placed in the A. varius species group, was in fact another synonym of A. spegazzinii . [16] The proliferation of scientific names for this one species occurred because of the terseness of the original description of A. spegazzinii , [22] and a lack of large samples and of appreciation of the substantial variation occurring within A. spegazzinii . [10]
According to phylogenetic analysis of sequences from the mitochondrial cytochrome b gene, Akodon spegazzinii is most closely related to A. boliviensis and more distantly to other members of the A. boliviensis species group, including Akodon polopi and Akodon sylvanus . [23] The boliviensis group is part of the highly diverse genus Akodon and thereby of the tribe Akodontini , which includes about 90 species of South American rodents. Akodontini is one of several tribes within the subfamily Sigmodontinae and the family Cricetidae , which includes hundreds of mainly small rodents distributed chiefly in Eurasia and the Americas. [24]

Description
The species is intermediate in size for the Akodon boliviensis species group; it is smaller than A. polopi and A. sylvanus , but larger than A. boliviensis and A. caenosus . [25] The more distantly related A. budini and A. simulator , which occur in the same area, are larger. [26] Akodon spegazzinii is variable in coloration, ranging from light to dark and from reddish to yellowish brown. [22] In general, animals in wetter, lower-lying areas are darker, and those in open, dry environments are paler. There is also variation within populations, and sometimes young mice are darker and lactating females are more reddish. [21] The formerly recognized species Akodon tucumanensis corresponds to the dark, low-altitude populations, while A. leucolimnaeus and A. alterus represent more reddish, high-altitude animals. [27]
The coloration of the upperparts is generally uniform, with some scattered darker hairs. There is a yellow ring around the eyes, [22] which is more prominent than in A. sylvanus . [28] The underparts are not strongly demarcated from the upperparts in color and are yellow-brown to gray. There are some scattered white hairs on the chin. [22] Although this white spot is better developed than in A. sylvanus , A. spegazzinii lacks the conspicuous white spot seen in A. simulator . [28] The color of the feet ranges from white and yellow-brown to gray. Ungual tufts of hairs cover the claws; these hairs are grayish-brown at the bases and whitish at the tips. The amount of hair on the tail is variable, but it is dark brown above and white to yellow-brown below. [22] High-altitude animals tend to have hairier ears and tails. [21]
In the skull, the front part (rostrum) is large, [21] but not as long as in A. budini . [28] The skull is more robust than in the very similar Akodon boliviensis , [29] but less so than in A. simulator . [28] The hourglass-shaped interorbital region (between the eyes) [21] is narrower than in A. caenosus [30] and not as squared as in A. polopi . [31] Members of the Akodon varius group, with which Akodon oenos was formerly associated, tend to have much broader interorbital regions. [32] The braincase is somewhat inflated and bears well-developed temporal and lambdoid crests [22] relative to the situation in A. caenosus ; [30] Akodon polopi has even better developed crests. [31]
Although the zygomatic plates (plates of bone at the side of the skull) are variable, their size is generally intermediate for the Akodon boliviensis group and their front margin ranges from straight to a little concave. [21] The zygomatic notches, projections at the front of the plates, are better developed than in A. caenosus and A. sylvanus . [33] The incisive foramina (openings in the front part of the palate ) are long, sometimes extending between the first upper molars . [22] Tiny posterolateral palatal pits are located at the back of the palate. [21] The back margin of the palate is squared to rounded, with a spine in the middle ( medial process ) sometimes present. The opening behind the palate, the mesopterygoid fossa , is of intermediate width, [22] being narrower than in A. sylvanus , A. simulator , and A. budini but broader than in A. caenosus . [34]
The masseteric crests (crests on the outer sides of the mandibles ) reach their front ends below the front border of the first molars. Usually, the capsular process (a projection at the back of the mandible housing the root of the lower incisor ) is well-developed. [21] The enamel of the upper incisors is yellowish-orange [21] and the incisors are orthodont (with their cutting edge perpendicular to the plane of the toothrow) or slightly opisthodont (with the cutting edge inclined backwards). [35] In contrast, Akodon simulator has more proodont incisors (with the cutting edge oriented forwards) [28] and Akodon neocenus has more opisthodont incisors. [32] The molars are more hypsodont (high-crowned) than in A. caenosus , [30] but are unlike the very hypsodont molars of A. budini . [28] The molar rows are relatively longer than in A. polopi . [31] There are 13 or 14 thoracic (chest), 7 or 8 lumbar (abdomen), and 23 or 26 caudal (tail) vertebrae. [21]
The karyotype includes 40 chromosomes , with a fundamental number of 40 major chromosomal arms [21] and resembles that of other members of the A. boliviensis group. [36] Head and body length is 93 to 196 mm (3.7 to 7.7 in), averaging 158 mm (6.2 in); tail length is 46 to 83 mm (1.8 to 3.3 in), averaging 66 mm (2.6 in); hindfoot length is 18 to 25 mm (0.71 to 0.98 in), averaging 23 mm (0.91 in); ear length is 12 to 21 mm (0.47 to 0.83 in), averaging 14 mm (0.55 in); and body mass is 13.0 to 38.0 g (0.46 to 1.34 oz), averaging 21.6 g (0.76 oz). [25] Like other members of the Akodon boliviensis group, individuals of Akodon spegazzinii continue to grow in adulthood. [37]

Distribution and ecology
Akodon spegazzinii is found in northwestern Argentina, in the provinces of Salta, Catamarca, Tucumán, La Rioja, and Mendoza, at altitudes of 400 to 3,500 m (1,300 to 11,500 ft). [28] Although its main distribution is in the northern provinces of Salta, Tucumán, and Catamarca, there are also scattered records from the more southern provinces of La Rioja and Mendoza, where it is likely restricted to patches of wet habitat. [38] Akodon alterus has been reported from Jujuy, but this record was likely based on misidentified specimens of A. boliviensis , [39] and records of Akodon spegazzinii from Jujuy were based on misidentified A. sylvanus . [40] Akodon spegazzinii is known from a paleontological site in Tucumán Province dated to the latest Pleistocene ( Lujanian ); it is among the most common species there. [41] The species occurs in the Yungas forest as well as the drier Monte Desert and Puna , where it is found only along streams. In the cloud grasslands of the higher portions of the Yungas, it is the dominant species of sigmodontine rodent. [42]
Although reproduction occurs around the year, there is a peak during the summer (November to April). Molting mostly occurs during autumn and winter (April to August). [42] At one locality in Mendoza, Akodon spegazzinii occurs at an estimated density of 21 individuals per hectare (8.5 per acre) and has a home range size of about 300 m 2 (3200 sq ft). [43] A number of sigmodontines have been recorded as occurring with A. spegazzinii , including A. caenosus , A. simulator , Neotomys ebriosus , Abrothrix illuteus , Reithrodon auritus , Andinomys edax , and various species of Eligmodontia , Necromys , Calomys , Oligoryzomys , Oxymycterus , and Phyllotis . [42] The tick Ixodes sigelos has been recorded on A. spegazzinii in Tucumán. [44] In addition, the mites Androlaelaps fahrenholzi , Androlaelaps rotundus , and Eulaelaps stabularis [45] and the flea Cleopsylla townsendii [46] are known from the species.

Conservation status
Akodon spegazzinii is listed as " Least Concern " on the IUCN Red List in view of its wide distribution and apparently stable population; in addition, it occurs in several protected areas . [1] Both Akodon oenos and Akodon leucolimnaeus are listed as " Data Deficient " with a trend of declining populations; they are said to be threatened by agricultural development. [47]
WebPage index: 00001
Akodon boliviensis
Akodon boliviensis , also known as the Bolivian grass mouse [1] or Bolivian akodont , [2] is a species of rodent in the family Cricetidae . It is found in the Andes from southeastern Peru through Bolivia into northwestern Argentina . [3]
WebPage index: 00002
Flying Eagle cent
The Flying Eagle cent is a one- cent piece struck by the Mint of the United States as a pattern coin in 1856, and for circulation in 1857 and 1858. The coin was designed by Mint Chief Engraver James B. Longacre , with the eagle in flight based on the work of Longacre's predecessor, Christian Gobrecht .
By the early 1850s, the large cent (about the size of a half dollar ) being issued by the Mint was becoming both unpopular in commerce and expensive to coin. After experimenting with various sizes and compositions, the Mint decided on an alloy of 88% copper and 12% nickel for a new, smaller cent. After the Mint produced patterns with an 1856 date and gave them to legislators and officials, Congress formally authorized the new piece in February 1857.
The new cent was issued in exchange for the worn Spanish colonial silver coin that had circulated in the U.S. until then, as well as for its larger predecessor. So many cents were issued that they choked commercial channels, especially as they were not legal tender and no one had to take them. The eagle design did not strike well, and was replaced in 1859 by Longacre's Indian Head cent .

Inception
The cent was the first official United States coin to be struck at the Philadelphia Mint in 1793. [1] These pieces, today known as large cents , were made of pure copper and were about the size of a half dollar . [2] They were struck every year, except 1815 due to a shortage of metal, but were slow to become established in commerce. Worn Spanish colonial silver pieces were then commonly used as money throughout the United States. At the time, both gold and silver were legal tender there, but copper coins were not; the federal government would not redeem them or take them in payment of taxes . [3]
The Mint then struck silver or gold in response to deposits by those holding bullion , and made little profit from those transactions. By the 1840s, profits, or seignorage , from monetizing copper into cents helped fund the Mint. In 1849, copper prices rose sharply, causing the Department of the Treasury to investigate possible alternatives to the large one-cent pieces. [4] The cent was unpopular in trade; as it was not a legal tender, nobody had to take it, and banks and merchants often refused it. [5] The cent was disliked for its large size as well. In 1837, the eccentric New York chemist Lewis Feuchtwanger had experimented with a smaller cent size in making model coins as part of a plan to sell his alloy (similar to base-metal German silver ) to the government for use in coinage. His pieces circulated as hard times tokens in the recession years of the late 1830s and early 1840s. [5]
By 1850, it was no longer profitable for the Mint to strike cents, and on May 14, New York Senator Daniel S. Dickinson introduced legislation for a cent made out of billon , copper with a small amount of silver. At the time, it was widely felt that coins should contain a large proportion of their face value in metal. The coin would be annular; that is, it would have a hole in the middle. The Mint struck experimental pieces, and found that it was difficult to eject such pieces from the presses where they were struck, and that it was expensive to recover the silver from the alloy. Provisions for a smaller cent were dropped from the legislation that gave congressional approval for the three-cent piece in 1851. [6] Numismatic historian Walter Breen suggested that one factor in rejecting the holed coins was that they reminded many of Chinese cash coins with their minimal purchasing value. [5] A drop in copper prices in 1851 and early 1852 made the matter of a smaller cent less urgent at the Department of the Treasury, which supervised Mint activities. [4]
Copper prices resurged in late 1852 and into 1853 past the $0.40 per pound that the Mint viewed as the break-even point for cent manufacture after considering the cost of production; 1 pound (0.45 kg) of copper made 42⅔ large cents. In 1853, patterns using a base-metal alloy were struck using a quarter eagle obverse die, about the size of a dime . [7] Some of the proposed alloys contained the metal nickel . [5] Also considered for use in the cent was "French bronze" (95% copper with the remainder tin and zinc ) [a] and various varieties of German silver. In his 1854 annual report, Mint Director James Ross Snowden advocated the issue of small, bronze cents, as well as the elimination of the half cent , which he described as useless in commerce. [8] A number of pattern cents were struck in 1854 and 1855. These featured various designs, including several depictions of Liberty and two adaptations of work by the late Mint chief engraver Christian Gobrecht : one showing a seated Liberty, which Gobrecht had placed on the silver coins in the 1830s, and another of a flying eagle, which Gobrecht had created based upon a sketch by Titian Peale . [9]

Preparation
In early 1856, Snowden proposed legislation to allow him to issue a smaller cent, but leaving the size and metallic composition up to him and Secretary of the Treasury James Guthrie . Under the plan, the new piece would be legal tender, up to ten cents. It would be issued in exchange for the old Spanish silver still circulating in the United States. In the exchange, the Spanish silver would be given full value (12½ cents per real , or bit ) when normally such pieces traded at about a 20% discount due to wear. The loss the government would take on the trade would be paid for by the seignorage on the base-metal pieces. The new cents would also be issued for the old cents, and in exchange for the same value in half cents—that denomination was to be discontinued. [8] The bill was introduced in the Senate on March 25, 1856. The old cent weighed 168 grains (10.9 g); on April 16, the bill was amended to provide for a cent of at least 95% copper weighing at least 96 grains (6.2 g) and passed the Senate in that form. [10] [11]
While the legislation was being considered, Mint Melter and Refiner James Curtis Booth was conducting experiments on alloys that might be appropriate for the new cent. [8] In July 1856, Snowden wrote to Guthrie, proposing an alloy of 88% copper and 12% nickel as ideal and suggesting amendments to the pending bill that would accomplish this. Booth also wrote to Guthrie to boost the alloy; [12] both men proposed a weight of 72 grains (4.7 g) as convenient as 80 cents would equal a troy pound (373 g), although the avoirdupois pound (454 g) was more commonly used for base metals. [13]
The Mint's chief engraver, James B. Longacre , was instructed to prepare designs for pattern coins. [13] Initially, Longacre worked with Liberty head designs such as were common at the time, but Snowden asked that a flying eagle design be prepared. This occurred as Booth's experiments continued; the first cent patterns with the flying eagle design were about the size of a quarter. [14] To promote the new alloy, the Mint had 50 half cents struck in it, and had them sent to Washington for Treasury officials to show to officials and congressmen. [5] In early November 1856, Longacre prepared dies in what would prove to be the final design, depicting a flying eagle on the obverse and a wreathed denomination on the reverse, in the size sought by Booth. [14]
The Mint struck at least several hundred patterns using Longacre's flying eagle design in the proposed composition. In an effort to secure public acceptance of the new pieces, these were distributed to various congressmen and other officials, initially in November 1856. Two hundred were sent to the House Committee on Coinage, Weights and Measures , while four were given to President Franklin Pierce . At least 634 specimens were distributed, and possibly several thousand; extra were available on request. This was the origin of the highly collectable 1856 Flying Eagle cent, which is considered by numismatists as part of the Flying Eagle series although it was actually a pattern or transition piece, not an official coin, as congressional approval had not yet been granted. Additional 1856 small cents were later struck by Snowden for illicit sale, and to exchange for pieces the Mint sought for its coin collection . [14] [15]
In December 1856, Snowden wrote to Missouri Representative John S. Phelps , hoping for progress with the legislation, and stating that he was already "pressed on all hands, and from every quarter, for the new cent—in fact, the public are very anxious for its issue". [16] When the legislation, amended to include the weight and alloy the Mint had decided on, was debated in the House of Representatives on December 24, it was opposed by Tennessee Congressman George Washington Jones over the legal tender provision; Jones felt that under the Constitution's Contract Clause , only gold and silver should be made legal tender. Phelps defended the bill on the ground that Congress had the constitutional power to regulate the value of money, but when the bill was brought back up to be considered on January 14, 1857, the legal tender provision had been removed. This time, the bill was opposed by New York Congressman Thomas R. Whitney , who objected to a provision in the bill that legalized the Mint's practice of designing and striking medals commissioned by the public, feeling that the government should not compete with private medallists. The provision was removed, and the bill passed the following day. The House version was then considered by the Senate, which debated it on February 4, and passed it with a further amendment allowing the redemption of the Spanish coins for a minimum of two years. The House agreed to this on February 18, and President Pierce signed the bill on the 21st. [16] The act made foreign gold and silver coins no longer legal tender, but Spanish dollars were redeemable at their nominal value for two years in exchange for the new copper-nickel cents. The half cent was abolished. [17] The new pieces would be the same size (19 mm), though somewhat heavier, than cents are today. [18]
In anticipation of the success of the legislation, most of the 333,456 large cents struck in 1857 never left the Philadelphia Mint, and were later melted. [19] Snowden purchased a new set of rollers and other equipment so that the Mint could produce its own cent planchets , the first time it had done so in over 50 years. [20] Although the legislation was still a day from final passage, Snowden recommended Longacre's designs to Guthrie on February 20. Guthrie approved them on the 24th, though he requested that the edge of the coin be made less sharp; Snowden promised to comply. Flying Eagle cents were struck beginning in April 1857 and were held pending official release. [21] The Mint stored the pieces pending accumulation of a sufficient supply; in mid-May, Snowden notified Philadelphia newspapers that distribution would begin on May 25. [14]

Design
Longacre's obverse of an eagle in flight is based on that of the Gobrecht dollar, struck in small quantities from 1836 to 1839. Although Gobrecht's model is not known with certainty, some sources state that the bird in flight was based on Peter the eagle , a tame bird fed by Mint workers in the early 1830s until it was caught up in machinery and killed. The bird was stuffed, and is still displayed at the Philadelphia Mint. [22]
Despite its derivative nature, Longacre's eagle has been widely admired. According to art historian Cornelius Vermeule in his book on U.S. coins, the flying eagle motif, when used in the 1830s, was "the first numismatic bird that could be said to derive from nature rather than from colonial carving or heraldry". [23] Vermeule described the Flying Eagle cent's replacement, the Indian Head cent , as "far less attractive to the eye than the Peale-Gobrecht flying eagle and its variants". [24] Sculptor Augustus Saint-Gaudens , when commissioned in 1905 to provide new designs for American coinage, sought to return a flying eagle design to the cent, writing to President Theodore Roosevelt , "I am using a flying eagle, a modification of the device which was used on the cent of 1857. I had not seen that coin for many years, and was so impressed by it that I thought if carried out with some modifications, nothing better could be done. It is by all odds the best design on any American coin." [25] Saint-Gaudens did return the flying eagle to American coinage, but his design was used for the reverse of the double eagle rather than the cent. [25]
The wreath on the reverse is also derivative, having been previously used on Longacre's Type II gold dollar of 1854, and the three-dollar piece of the same year. [24] It is composed of leaves of wheat, corn, cotton and tobacco, thus including produce associated with both the North and the South. The cotton leaves are sometimes said to be maple leaves; the two types are not dissimilar, and maple leaves are more widely known than cotton leaves. An ear of corn is also visible. [22]

Release, production, and collecting
The Philadelphia Mint released the new cents to the public on May 25, 1857. In anticipation of large popular demand, Mint authorities built a temporary wooden structure in the courtyard of the Philadelphia facility. On the morning of the date of release, hundreds of people queued, one line for those exchanging Spanish silver for cents, the other for those bringing in old copper cents and half cents. From 9 am, clerks paid out cents for the old pieces; outside the Mint precincts, early purchasers sold the new cents at a premium. Snowden wrote to Guthrie, "the demand for them is enormous … we had on hand this morning $30,000 worth, that is three million pieces. Nearly all of this amount will be paid out today." [20] The 1856 specimen became publicly known about the time of issuance, and had the public checking their pocket change; 1856 small cents sold for as much as $2 by 1859. [26] The public interest in the new cents set off a coin collecting boom: in addition to seeking the rare 1856 cent, some tried to collect sets of large cents back to 1793, and found they would have to pay a premium for the rarer dates. [14]
The Mint had trouble striking the new design. This was due to the hard copper-nickel alloy and the fact that the eagle on one side of the piece was directly opposite parts of the reverse design; efforts to bring out the design more fully led to increased die breakage. Many Flying Eagle cents show weaknesses, especially at the eagle's head and tail, which are opposite the wreath. [27] [28] In 1857, Snowden suggested the replacement of the eagle with a head of Christopher Columbus . Longacre replied that as there had been objections to proposals to place George Washington on the coinage, there would also be opposition to a Columbus design. [29] Despite the difficulties, the 17,450,000 Flying Eagle cents struck at Philadelphia in 1857 constituted the greatest production of a single coin in a year at a U.S. mint to that time. [30]
In 1858, the Mint tried to alleviate the breakage problem using a new version of the cent with a shallower relief. This attempt led to the major variety of the series, as coins of the revised version have smaller letters in the inscriptions than those struck earlier. The two varieties are about equally common, and were probably struck side by side for some period as the Mint used up older dies. Efforts to conserve dies were the probable cause of another variety, the 1858/7, as 1857-dated dies were overstruck to allow them to be used in the new year. [31]
The Mint prepared pattern coins with a much smaller eagle in 1858, which struck well, but which officials disliked. [29] Snowden directed Longacre to prepare various patterns that he could select from for a new piece to replace the Flying Eagle cent as of January 1, 1859. The Mint produced between 60 and 100 sets of twelve patterns showing various designs; these were circulated to officials and also were quietly sold by the Mint over the next several years. Longacre's design showing Liberty wearing an Indian-style headress was adopted, with a wreath with lower relief for the reverse of the Indian Head cent, solving the metal flow issues. [32] On November 4, 1858, Snowden wrote to the Treasury Department, stating that the Flying Eagle cent had proved "not very acceptable to the general population" as they felt the bird was not true to life, and that the Native American design would "giv[e] it the character of America". [33]
By September 1857, the volume of Spanish silver coming to the Mint had been so large that Snowden gave up the idea of being able to pay for it just with cents, authorizing payment with gold and silver coins. On March 3, 1859, the redemption of the foreign pieces was extended for an additional two years. As commerce was choked with the new cents, Congress repealed this provision in July 1860, though Snowden continued the practice for more than a year without authorization from Congress. Bankers Magazine for October 1861 reported the end of the exchange, and quoted the Philadelphia Press : "the large issue of the new nickel cents has rendered them almost as much of a nuisance as the old Spanish currency." [34] According to Breen, "the foreign silver coins had been legal tender, receivable for all kinds of payments including postage stamps and some taxes; the nickel cents were not. They quickly filled shopkeepers' cashboxes to the exclusion of almost everything else; they began to be legally refused in trade." [35] The glut was ended by the hoarding of all federal coinage in the wake of the economic upset caused by the Civil War . [36] [37]
After the war, the hoarded Flying Eagle cents re-entered circulation. Many remained there only a few years, being pulled out from among the new bronze cents in Treasury Department redemption programs in the 1860s and 1870s—thirteen million copper-nickel cents were retired by exchange for other base-metal coinage. By the 1880s, it was a rarity in circulation. [38] [39] The 2018 edition of R.S. Yeoman 's A Guide Book of United States Coins lists the 1857, 1858 large letters, and 1858 small letters each at $30 in G-4 Good condition , the next to lowest collectable grade (AG-3). The 1856 is $6,500 in that grade, rising to $20,000 in uncirculated MS-63. The 1858/7 starts at $75 in G-4, rising to $11,000 in MS-63. [40] An 1856 cent in MS-66 condition sold at auction in January 2004 for $172,500. [30]
WebPage index: 00003
Pipe organ
The pipe organ is a musical instrument that produces sound by driving pressurized air (called wind ) through organ pipes selected via a keyboard . Because each pipe produces a single pitch, the pipes are provided in sets called ranks , each of which has a common timbre and volume throughout the keyboard compass. Most organs have multiple ranks of pipes of differing timbre, pitch, and volume that the player can employ singly or in combination through the use of controls called stops .
A pipe organ has one or more keyboards played by the hands (called manuals ), and a pedalboard played by the feet; each keyboard has its own group of stops. The keyboard(s), pedalboard, and stops are housed in the organ's console . The organ's continuous supply of wind allows it to sustain notes for as long as the corresponding keys are pressed, unlike the piano and harpsichord whose sound begins to dissipate immediately after it is played. The smallest portable pipe organs may have only one or two dozen pipes and one manual; the largest may have over 20,000 pipes and seven manuals. [2] A list of some of the most notable and largest pipe organs in the world can be viewed at List of pipe organs .
The origins of the pipe organ can be traced back to the water organ in Ancient Greece , in the 3rd century BC, [3] in which the wind supply was created with water pressure. By the 6th or 7th century AD, bellows were used to supply organs with wind. [3] Beginning in the 12th century, the organ began to evolve into a complex instrument capable of producing different timbres . A pipe organ with "great leaden pipes" was sent to the West by the Byzantine emperor Constantine V as a gift to Pepin the Short , King of the Franks , in 757. Pepin's son Charlemagne requested a similar organ for his chapel in Aachen in 812, beginning the pipe organ's establishment in Western church music. [4] By the 17th century, most of the sounds available on the modern classical organ had been developed. [5] From that time, the pipe organ was the most complex man-made device [6] - a distinction it retained until it was displaced by the telephone exchange in the late 19th century. [7]
Pipe organs are installed in churches, synagogues, concert halls, schools, and other public buildings. They are used in the performance of classical music , sacred music , secular music , and popular music . In the early 20th century, pipe organs were installed in theaters to accompany the screening of films during the silent movie era; in municipal auditoria, where orchestral transcriptions were popular; and in the homes of the wealthy. [8] The beginning of the 21st century has seen a resurgence in installations in concert halls. The organ boasts a substantial repertoire , which spans over 500 years. [9]

History and development

Antiquity
The organ is one of the oldest instruments still used in European classical music that has commonly been credited as having derived from Greece. Its earliest predecessors were built in Ancient Greece in the 3rd century BC. The word organ is derived from the Greek όργανον (organon), [12] a generic term for an instrument or a tool, [13] via the Latin organum , an instrument similar to a portative organ used in ancient Roman circus games.
The Greek engineer Ctesibius of Alexandria is credited with inventing the organ in the 3rd century BC. He devised an instrument called the hydraulis , which delivered a wind supply maintained through water pressure to a set of pipes. [14] The hydraulis was played in the arenas of the Roman Empire . The pumps and water regulators of the hydraulis were replaced by an inflated leather bag in the 2nd century AD, [14] and true bellows began to appear in the Eastern Roman Empire in the 6th or 7th century AD. [3] Some 400 pieces of a hydraulis from the year 228 AD have been revealed during the 1931 archaeological excavations in the former Roman town Aquincum , province of Pannonia (present day Budapest ), which was used as a music instrument by the Aquincum fire dormitory; a modern replica produces an enjoyable sound.
The 9th century Persian geographer Ibn Khurradadhbih (d. 911), in his lexicographical discussion of instruments, cited the urghun (organ) as one of the typical instruments of the Eastern Roman (Byzantine) Empire . [15] It was often used in the Hippodrome in the imperial capital of Constantinople . The first Western pipe organ with "great leaden pipes" was sent from Constantinople to the West by the Byzantine emperor Constantine V as a gift to Pepin the Short King of the Franks in 757. Pepin's son Charlemagne requested a similar organ for his chapel in Aachen in 812, beginning its establishment in Western church music. [16]

Medieval
Portable organs (the portative and the positive organ ) were invented in the Middle Ages. Towards the middle of the 13th century, the portatives represented in the miniatures of illuminated manuscripts appear to have real keyboards with balanced keys, as in the Cantigas de Santa Maria . [18] Its portability made the portative useful for the accompaniment of both sacred and secular music in a variety of settings.
Large organs such as the one installed in 1361 in Halberstadt , Germany , [19] the first documented permanent organ installation, likely prompted Guillaume de Machaut to describe the organ as "the king of instruments", a characterization still frequently applied. [20] The Halberstadt organ was the first instrument to use a chromatic key layout across its three manuals and pedalboard, although the keys were wider than on modern instruments. [21] It had twenty bellows operated by ten men, and the wind pressure was so high that the player had to use the full strength of his arm to hold down a key. [19]
Until the mid-15th century, organs had no stop controls. Each manual controlled ranks at multiple pitches, known as the Blockwerk . [22] Around 1450, controls were designed that allowed the ranks of the Blockwerk to be played individually. These devices were the forerunners of modern stop actions. [23] The higher-pitched ranks of the Blockwerk remained grouped together under a single stop control; these stops developed into mixtures . [24]

Renaissance and Baroque periods
During the Renaissance and Baroque periods, the organ's tonal colors became more varied. Organ builders fashioned stops that imitated various instruments, such as the krummhorn and the viola da gamba . The Baroque period is often thought of as organ building's "golden age," as virtually every important refinement was brought to a culminating art. [26] Builders such as Arp Schnitger , Jasper Johannsen, Zacharias Hildebrandt and Gottfried Silbermann constructed instruments that were in themselves artistic masterpieces, displaying both exquisite craftsmanship and beautiful sound. These organs featured well-balanced mechanical key actions, giving the organist precise control over the pipe speech. Schnitger's organs featured particularly distinctive reed timbres and large Pedal and Rückpositiv divisions. [26]
Different national styles of organ building began to develop, often due to changing political climates. [27] In the Netherlands, the organ became a large instrument with several divisions, doubled ranks, and mounted cornets. The organs of northern Germany also had more divisions, and independent pedal divisions became increasingly common. [27] The divisions of the organ became visibly discernible from the case design. 20th-century musicologists labelled this the Werkprinzip . [28]
In France, as in Italy, Spain and Portugal, organs were primarily designed to play alternatim verses rather than accompany congregational singing . The French Classical Organ , became remarkably consistent throughout France over the course of the Baroque era, more so than any other style of organ building in history, and standardized registrations developed. [29] [30] It was elaborately described by Dom Bédos de Celles in his treatise L'art du facteur d'orgues ( The Art of Organ Building ). [31] For example, in France, the organ at Notre-Dame's (St. Etienne, Loire) was built by Joseph and Claude-Ignace Callinet in 1837, at a time when their career was at its apex.
In England, many pipe organs were taken out of churches during the English Reformation of the 16th century and the Commonwealth period. Often these were relocated to private homes. At the Restoration , organ builders such as Renatus Harris and "Father" Bernard Smith brought new organ-building ideas from continental Europe. English organs evolved from small one- or two-manual instruments into three or more divisions disposed in the French manner with grander reeds and mixtures. [32] The Echo division began to be enclosed in the early 18th century, and in 1712 Abraham Jordan claimed his "swelling organ" at St Magnus-the-Martyr to be a new invention. [29] The swell box and the independent pedal division appeared in English organs beginning in the 18th century. [32] [33]

Romantic period
During the Romantic period, the organ became more symphonic, capable of creating a gradual crescendo. New technologies and the work of organ builders such as Eberhard Friedrich Walcker , Aristide Cavaillé-Coll , and Henry Willis made it possible to build larger organs with more stops, more variation in sound and timbre, and more divisions. [32] Enclosed divisions became common, and registration aids were developed to make it easier for the organist to manage the great number of stops. The desire for louder, grander organs required that the stops be voiced on a higher wind pressure than before. As a result, a greater force was required to overcome the wind pressure and depress the keys. To solve this problem, Cavaillé-Coll configured the English " Barker lever " to assist in operating the key action. [34]
Organ builders began to lean towards specifications with fewer mixtures and high-pitched stops. They preferred to use more 8′ and 16′ stops in their specifications and wider pipe scales. [35] These practices created a warmer, richer sound than was common in the 18th century. Organs began to be built in concert halls (such as the organ at the Palais du Trocadéro in Paris ), and composers such as Camille Saint-Saëns and Gustav Mahler used the organ in their orchestral works.

Modern development
The development of pneumatic and electro-pneumatic key actions in the late 19th century made it possible to locate the console independently of the pipes, greatly expanding the possibilities in organ design. Electric stop actions were also developed, which allowed sophisticated combination actions to be created. [36]
In the mid-20th century, organ builders began to build historically inspired instruments modelled on Baroque organs. They returned to building mechanical key actions, voicing with lower wind pressures and thinner pipe scales, and designing specifications with more mixture stops. [37] This became known as the Organ reform movement .
In the late 20th century, organ builders began to incorporate digital components into their key, stop, and combination actions. Besides making these mechanisms simpler and more reliable, this also makes it possible to record and play back an organist’s performance via the MIDI protocol. [38] In addition, some organ builders have incorporated digital stops into their pipe organs.
The electronic organ developed throughout the 20th century. Some pipe organs were replaced by digital organs because of their lower purchase price, smaller physical size, and minimal maintenance requirements. In the early 1970s, Rodgers Instruments pioneered the hybrid organ, an electronic instrument that incorporates real pipes; other builders such as Allen Organs and Johannus Orgelbouw have since built hybrid organs.

Construction
A pipe organ contains one or more sets of pipes, a wind system, and one or more keyboards. The pipes produce sound when pressurized air produced by the wind system passes through them. An action connects the keyboards to the pipes. Stops allow the organist to control which ranks of pipes sound at a given time. The organist operates the stops and the keyboards from the console .

Pipes
Organ pipes are made from either wood or metal [39] and produce sound ("speak") when air under pressure ("wind") is directed through them. [40] As one pipe produces a single pitch , multiple pipes are necessary to accommodate the musical scale . The greater the length of the pipe, the lower its resulting pitch will be. [41] The timbre and volume of the sound produced by a pipe depends on the volume of air delivered to the pipe and the manner in which it is constructed and voiced, the latter adjusted by the builder to produce the desired tone and volume. Hence a pipe's volume cannot be readily changed while playing. [41]
Organ pipes are divided into flue pipes and reed pipes according to their design and timbre. Flue pipes produce sound by forcing air through a fipple , like that of a recorder , whereas reed pipes produce sound via a beating reed , like that of a clarinet or saxophone . [42]
Pipes are arranged by timbre and pitch into ranks. A rank is a row of pipes mounted vertically onto a windchest . [43] The stop mechanism admits air to each rank. For a given pipe to sound, the stop governing the pipe's rank must be engaged, and the key corresponding to its pitch must be depressed. Ranks of pipes are organized into groups called divisions. Each division generally is played from its own keyboard and conceptually comprises an individual instrument within the organ. [44]

Action
An organ contains two actions, or systems of moving parts. When a key is depressed, the key action admits wind into a pipe. The stop action allows the organist to control which ranks are engaged. An action may be mechanical, pneumatic, or electrical (or some combination of these, such as electro-pneumatic action). [45] The key action is independent of the stop action, allowing an organ to combine a mechanical key action along with an electric stop action.
A key action which physically connects the keys and the windchests is a mechanical or tracker action . Connection is achieved through a series of rods called trackers. When the organist depresses a key, the corresponding tracker pulls open its pallet, allowing wind to enter the pipe. [46]
In a mechanical stop action, each stop control operates a valve for a whole rank of pipes. When the organist selects a stop, the valve allows wind to reach the selected rank. [43] This control was at first a draw stop knob , which the organist selects by pulling (or drawing) toward himself/herself. This is the origin of the idiom " to pull out all the stops ". [47] More modern stop selectors, used for electric actions, are tilting tablets or rocker tabs.
Tracker action has been used from antiquity to modern times. Despite the extra effort needed in playing, many organists [ who? ] prefer tracker action because of a feel and a control of the pipe valve operation. Before the pallet opens, wind pressure augments tension of the pallet spring, but once the pallet opens, only the spring tension is felt at the key. This provides a "breakaway" feel.
A later development was the tubular-pneumatic action , which uses changes of pressure within lead tubing to operate pneumatic valves throughout the instrument. This allowed a lighter touch, and more flexibility in the location of the console, within a 50-foot (15-m) limit. This type of construction was used in the late 19th century to early 20th century, and has had only rare application since the 1920s. [48]
A more recent development is the electric action which uses low voltage DC to control the key and/or stop mechanisms. Electricity may control the action indirectly through air pressure valves (pneumatics), in which case the action is electro-pneumatic . In such actions, an electromagnet attracts a small pilot valve which lets wind go to a bellows ("pneumatic") which opens the pallet. When electricity operates the action directly without the assistance of pneumatics, it is commonly referred to as direct electric action . [48] In this type, the electromagnet's armature carries a disc pallet.
When electrical wiring alone is used to connect the console to the windchest, electric actions allow the console to be separated at any practical distance from the rest of the organ, and to be movable. [49] Electric stop actions can be controlled at the console by stop knobs, by pivoted tilting tablets, or rocker tabs. These are simple switches, like wall switches for room lights. Some may include electromagnets for setting or resetting when combinations are selected.
The most modern actions are primarily electronic, which connect the console and windchests via narrow data cables instead of the larger multiconductor cables of electric actions. Boxes containing small embedded computers in the console and near the windchests translate console commands into fast serial data for the cable, and back into electrical commands at the windchest[s].

Wind system
The wind system consists of the parts that produce, store, and deliver wind to the pipes. Pipe organ wind pressures are on the order of 0.10 psi (0.69 kPa). Organ builders often measure organ wind using a U-tube manometer containing water, so commonly give its magnitude as the difference in water levels in the two legs of the manometer, rather than in units of pressure. The difference in water level is proportional to the difference in pressure between the wind being measured and the atmosphere. [50] The 0.10 psi above would register as 2.75 inches of water (70 mmAq ). An Italian organ from the Renaissance period may be on only 2.2 inches (56 mm), [51] while (in the extreme) solo stops in some large 20th-century organs may require up to 50 inches (1,300 mm). In isolated, extreme cases, some stops have been voiced on 100 inches (2,500 mm). [52]
Playing the organ before electricity required at least one person to operate the bellows . When signaled by the organist, a calcant would operate a set of bellows, supplying the organ with air. [53] Because calcants were expensive, organists would usually practise on other instruments such as the clavichord or harpsichord . [54] By the mid-19th-century bellows were also being operated by water engines, [55] steam engines or gasoline engines. [56] [57] [58] Starting in the 1860s bellows were gradually replaced by wind turbines which were later directly connected to electrical motors. [59] This made it possible for organists to practice regularly on the organ. Most organs, both new and historic, have electric blowers, although others can still be operated manually. [60] The wind supplied is stored in one or more regulators to maintain a constant pressure in the windchests until the action allows it to flow into the pipes. [61]

Stops
Each stop usually controls one rank of pipes, although mixtures and undulating stops (such as the Voix céleste ) control multiple ranks. [62] The name of the stop reflects not only the stop's timbre and construction, but also the style of the organ in which it resides. For example, the names on an organ built in the north German Baroque style generally will be derived from the German language, while the names of similar stops on an organ in the French Romantic style will usually be French. Most countries tend to use only their own languages for stop nomenclature. English-speaking nations as well as Japan are more receptive to foreign nomenclature. Stop names are not standardized: two otherwise identical stops from different organs may have different names. [63]
To facilitate a large range of timbres, organ stops exist at different pitch levels. A stop that sounds at unison pitch when a key is depressed is referred to as being at 8′ (pronounced "eight-foot") pitch. This refers to the length of the lowest-sounding pipe in that rank, which is approximately eight feet. For the same reason, a stop that sounds an octave higher is at 4′ pitch, and one that sounds two octaves higher is at 2′ pitch. Likewise, a stop that sounds an octave lower than unison pitch is at 16′ pitch, and one that sounds two octaves lower is at 32′ pitch. [62] Stops of different pitch levels are designed to be played simultaneously.
The label on a stop knob or rocker tab indicates the stop’s name and its pitch in feet. Stops that control multiple ranks display a Roman numeral indicating the number of ranks present, instead of its pitch. [64] Thus, a stop labelled "Open Diapason 8′ " is a single-rank diapason stop sounding at 8′ pitch. A stop labelled "Mixture V" is a five-rank mixture.
Sometimes, a single rank of pipes may be able to be controlled by several stops, allowing the rank to be played at multiple pitches or on multiple manuals. Such a rank is said to be unified or borrowed . For example, an 8′ Diapason rank may also be made available as a 4′ Octave. When both of these stops are selected and a key (for example, c′) [65] is pressed, two pipes of the same rank will sound: the pipe normally corresponding to the key played (c′), and the pipe one octave above that (c′′). Because the 8′ rank does not have enough pipes to sound the top octave of the keyboard at 4′ pitch, it is common for an extra octave of pipes used only for the borrowed 4′ stop to be added. In this case, the full rank of pipes (now an extended rank ) is one octave longer than the keyboard. [66]
Special unpitched stops also appear in some organs. Among these are the Zimbelstern (a wheel of rotating bells), the nightingale (a pipe submerged in a small pool of water, creating the sound of a bird warbling when wind is admitted), [67] and the effet d'orage ("thunder effect", a device that sounds the lowest bass pipes simultaneously). Standard orchestral percussion instruments such as the drum , chimes , celesta , and harp have also been imitated in organ building. [68]

Console
The controls available to the organist, including the keyboards , couplers , expression pedals , stops, and registration aids are accessed from the console. [70] The console is either built into the organ case or detached from it.

Keyboards
Keyboards played by the hands are known as manuals (from the Latin manus , meaning "hand"). The keyboard played by the feet is a pedalboard . Every organ has at least one manual (most have two or more), and most have a pedalboard. Each keyboard is named for a particular division of the organ (a group of ranks) and generally controls only the stops from that division. The range of the keyboards has varied widely across time and between countries. Most current specifications call for two or more manuals with sixty-one notes (five octaves, from C to c″″) and a pedalboard with thirty or thirty-two notes (two and a half octaves, from C to f′ or g′). [65] [71]

Couplers
A coupler allows the stops of one division to be played from the keyboard of another division. For example, a coupler labelled "Swell to Great" allows the stops drawn in the Swell division to be played on the Great manual. This coupler is a unison coupler, because it causes the pipes of the Swell division to sound at the same pitch as the keys played on the Great manual. Coupling allows stops from different divisions to be combined to create various tonal effects. It also allows every stop of the organ to be played simultaneously from one manual. [72]
Octave couplers , which add the pipes an octave above (super-octave) or below (sub-octave) each note that is played, may operate on one division only (for example, the Swell super octave, which adds the octave above what is being played on the Swell to itself), or act as a coupler to another keyboard (for example, the Swell super-octave to Great, which adds to the Great manual the ranks of the Swell division an octave above what is being played). [72]
In addition, larger organs may use unison off couplers, which prevent the stops pulled in a particular division from sounding at their normal pitch. These can be used in combination with octave couplers to create innovative aural effects, and can also be used to rearrange the order of the manuals to make specific pieces easier to play. [72]

Enclosure and expression pedals
Enclosure refers to a system that allows for the control of volume without requiring the addition or subtraction of stops. In a two-manual organ with Great and Swell divisions, the Swell will be enclosed. In larger organs, parts or all of the Choir and Solo divisions may also be enclosed. [74] The pipes of an enclosed division are placed in a chamber generally called the swell box . At least one side of the box is constructed from horizontal or vertical palettes known as swell shades , which operate in a similar way to Venetian blinds ; their position can be adjusted from the console. When the swell shades are open, more sound is heard than when they are closed. [74] Sometimes the shades are exposed, but they are often concealed behind a row of facade-pipes or a grill.
The most common method of controlling the louvers is the balanced swell pedal . This device is usually placed above the centre of the pedalboard and is configured to rotate away from the organist from a near-vertical position (in which the shades are closed) to a near-horizontal position (in which the shades are open). [75] An organ may also have a similar-looking crescendo pedal , found alongside any expression pedals. Pressing the crescendo pedal forward cumulatively activates the stops of the organ, starting with the softest and ending with the loudest; pressing it backwards reverses this process. [76]

Combination action
Organ stops can be combined in countless permutations, resulting in a great variety of sounds. A combination action can be used to switch instantly from one combination of stops (called a registration ) to another. Combination actions feature small buttons called pistons that can be pressed by the organist, generally located beneath the keys of each manual (thumb pistons) or above the pedalboard (toe pistons). [77] The pistons may be divisional (affecting only a single division) or general (affecting all the divisions), and are either preset by the organ builder or can be altered by the organist. Modern combination actions operate via computer memory, and can store several channels of registrations. [78]

Casing
The pipes, action, and wind system are almost always contained in a case, the design of which also may incorporate the console. The case blends the organ's sound and aids in projecting it into the room. [79] The case often is designed to complement the building's architectural style and it may contain ornamental carvings and other decorations. The visible portion of the case, called the façade , will most often contain pipes, which may be either sounding pipes or dummy pipes solely for decoration. The façade pipes may be plain, burnished , gilded , or painted [80] and are usually referred to as (en) montre within the context of the French organ school . [81] [82]
Organ cases occasionally feature a few ranks of pipes protruding horizontally from the case in the manner of a row of trumpets . These are referred to as pipes en chamade and are particularly common in organs of the Iberian peninsula and large 20th-century instruments. [83]
Many organs, particularly those built in the early 20th century, are contained in one or more rooms called organ chambers. Because sound does not project from a chamber into the room as clearly as from a freestanding organ case, enchambered organs may sound muffled and distant. [84] For this reason, some modern builders, particularly those building instruments specializing in polyphony rather than Romantic compositions, avoid this unless the architecture of the room makes it necessary.

Tuning and regulation
The goal of tuning a pipe organ is to adjust the pitch of each pipe so that they all sound in tune with each other. How the pitch of each pipe is adjusted depends on the type and construction of that pipe.
Regulation adjusts the action so that all pipes sound correctly. If the regulation is wrongly set, the keys may be at different heights, some pipes may sound when the keys are not pressed (a "cipher"), or pipes may not sound when a key is pressed. Tracker action, for example in the organ of Cradley Heath Baptist Church , includes adjustment nuts on the wire ends of the wooden trackers, which have the effect of changing the effective length of each tracker.

Repertoire
The main development of organ repertoire has progressed along with that of the organ itself, leading to distinctive national styles of composition. Because organs are commonly found in churches and synagogues, the organ repertoire includes a large amount of sacred music , which is accompanimental (choral anthems , congregational hymns , liturgical elements, etc.) as well as solo in nature ( chorale preludes , hymn versets designed for alternatim use, etc.). [8] The organ's secular repertoire includes preludes , fugues , sonatas , organ symphonies, suites, and transcriptions of orchestral works.
Although most countries whose music falls into the Western tradition have contributed to the organ repertoire, France and Germany in particular have produced exceptionally large amounts of organ music. There is also an extensive repertoire from the Netherlands, England, and the United States.
Before the Baroque era, keyboard music generally was not written for one instrument or another, but rather was written to be played on any keyboard instrument. For this reason, much of the organ's repertoire through the Renaissance period is the same as that of the harpsichord . Pre-Renaissance keyboard music is found in compiled manuscripts that may include compositions from a variety of regions. The oldest of these sources is the Robertsbridge Codex , dating from about 1360. [86] The Buxheimer Orgelbuch, which dates from about 1470 and was compiled in Germany, includes intabulations of vocal music by the English composer John Dunstaple . [87] The earliest Italian organ music is found in the Faenza Codex , dating from 1420. [88]
In the Renaissance period, Dutch composers such as Jan Pieterszoon Sweelinck composed both fantasias and psalm settings. Sweelinck in particular developed a rich collection of keyboard figuration that influenced subsequent composers. [89] The Italian composer Claudio Merulo wrote in the typical Italian genres of the toccata , the canzona , and the ricercar . [90] In Spain, the works of Antonio de Cabezón began the most prolific period of Spanish organ composition, [91] which culminated with Juan Cabanilles .
Early Baroque organ music in Germany was highly contrapuntal . Sacred organ music was based on chorales: composers such as Samuel Scheidt and Heinrich Scheidemann wrote chorale preludes, chorale fantasias , and chorale motets . [91] Towards the end of the Baroque era, the chorale prelude and the partita became mixed, forming the chorale partita . [92] This genre was developed by Georg Böhm , Johann Pachelbel , and Dieterich Buxtehude . The primary type of free-form piece in this period was the praeludium , as exemplified in the works of Matthias Weckmann , Nicolaus Bruhns , Böhm, and Buxtehude. [93] The organ music of Johann Sebastian Bach fused characteristics of every national tradition and historical style in his large-scale preludes and fugues and chorale-based works. [94] Towards the end of the Baroque era, George Frideric Handel composed the first organ concertos . [95]
In France, organ music developed during the Baroque era through the music of Jean Titelouze , François Couperin , and Nicolas de Grigny . [97] Because the French organ of the 17th and early 18th centuries was very standardized, a conventional set of registrations developed for its repertoire. The music of French composers (and Italian composers such as Girolamo Frescobaldi ) was written for use during the Mass . Very little secular organ music was composed in France and Italy during the Baroque period; the written repertoire is almost exclusively intended for liturgical use. [98] In England, composers such as John Blow and John Stanley wrote multi-sectional free works for liturgical use called voluntaries through the 19th century. [99] [100]
Organ music was seldom written in the Classical era, as composers preferred the piano with its ability to create dynamics. [101] In Germany, the six sonatas op. 65 of Felix Mendelssohn (published 1845) marked the beginning of a renewed interest in composing for the organ. Inspired by the newly built Cavaillé-Coll organs, the French organist-composers César Franck , Alexandre Guilmant and Charles-Marie Widor led organ music into the symphonic realm. [101] The development of symphonic organ music continued with Louis Vierne and Charles Tournemire . Widor and Vierne wrote large-scale, multi-movement works called organ symphonies that exploited the full possibilities of the symphonic organ. [102] Max Reger and Sigfrid Karg-Elert 's symphonic works made use of the abilities of the large Romantic organs being built in Germany at the time. [101]
In the 19th and 20th centuries, organ builders began to build instruments in concert halls and other large secular venues, allowing the organ to be used as part of an orchestra, as in Saint-Saëns' Symphony No. 3 . [101] Frequently the organ is given a soloistic part, such as in Joseph Jongen 's Symphonie Concertante for Organ & Orchestra , Francis Poulenc 's Concerto for Organ, Strings and Tympani , and Frigyes Hidas' Organ Concerto.
Other composers who have used the organ prominently in orchestral music include Gustav Holst , Richard Strauss , Ottorino Respighi , Gustav Mahler , Anton Bruckner , and Ralph Vaughan Williams . [103] Because these concert hall instruments could approximate the sounds of symphony orchestras, transcriptions of orchestral works found a place in the organ repertoire. [104] As silent films became popular, theatre organs were installed in theatres to provide accompaniment for the films. [101]
In the 20th-century symphonic repertoire, both sacred and secular, [105] continued to progress through the music of Marcel Dupré , Maurice Duruflé , and Herbert Howells . [101] Other composers, such as Olivier Messiaen , György Ligeti , Jehan Alain , Jean Langlais , Gerd Zacher , and Petr Eben , wrote post-tonal organ music. [101] Messiaen's music in particular redefined many of the traditional notions of organ registration and technique. [106]

See also

Notes
WebPage index: 00004
Dutch East India Company
The United East India Company or the United East Indian Company , also known as the United East Indies Company ( Dutch : Vereenigde Oost-Indische Compagnie ; or Verenigde Oostindische Compagnie in modern spelling; VOC ), referred to by the British as the Dutch East India Company , [2] or sometimes known as the Dutch East Indies Company , was originally established as a chartered company in 1602, when the Dutch government granted it a 21-year monopoly on the Dutch spice trade . It is often considered to be the world's first truly transnational corporation [note 1] [3] and the first company in history to issue bonds and shares of stock to the general public. [note 2] In other words, the VOC was officially the first publicly traded company [note 3] of the world, because it was the first company to be ever actually listed on an official stock exchange . [note 4] [6] As the first historical model of the quasi-fictional concept of the megacorporation , the VOC possessed quasi-governmental powers, including the ability to wage war, imprison and execute convicts, [7] negotiate treaties, strike its own coins , and establish colonies. [8]
The company was also considered by many to be the very first major and the most successful corporation in history. [note 5] [9] [10] [11] [12] [13] Statistically, the VOC eclipsed all of its rivals in international trade for almost 200 years of existence. [14] [15] Between 1602 and 1796 the VOC sent almost a million Europeans to work in the Asia trade on 4,785 ships, and netted for their efforts more than 2.5 million tons of Asian trade goods. By contrast, the rest of Europe combined sent only 882,412 people from 1500 to 1795, and the fleet of the English (later British ) East India Company , the VOC's nearest competitor, was a distant second to its total traffic with 2,690 ships and a mere one-fifth the tonnage of goods carried by the VOC. The VOC enjoyed huge profits from its spice monopoly through most of the 17th century. [16]
Having been set up in 1602, to profit from the Malukan spice trade, in 1619 the VOC established a capital in the port city of Jayakarta , changing the name to Batavia (now Jakarta ). Over the next two centuries the Company acquired additional ports as trading bases and safeguarded their interests by taking over surrounding territory. [17] It remained an important trading concern and paid an 18% annual dividend for almost 200 years. [18]
Due to structural changes, the Fourth Anglo-Dutch War and French invasion of the Netherlands, the company was nationalised in 1800, [19] its possessions and the debt being taken over by the government of the Dutch Batavian Republic . The VOC's territories became the Dutch East Indies and were expanded over the course of the 19th century to include the whole of the Indonesian archipelago, which would later become the modern Republic of Indonesia .

Company name, logo and flag
Around the world and especially in English-speaking countries, the VOC is widely known as the "Dutch East India Company". The name ‘Dutch East India Company’ is used to make a distinction with the [British] East India Company (EIC) and other East Indian companies (such as the Danish East India Company , French East India Company , Portuguese East India Company , and the Swedish East India Company ). The abbreviation " VOC " stands for " V ereenigde O ost-Indische C ompagnie" or " V erenigde O ostindische C ompagnie" in Dutch , literally meaning "United East-Indian Company", "United East-India Company", or "United East-Indies Company".
The VOC monogram was possibly the first globally-recognized corporate logo . [9] The logo of the VOC consisted of a large capital 'V' with an O on the left and a C on the right leg. It appeared on various corporate items, such as cannon and coins. The first letter of the hometown of the chamber conducting the operation was placed on top (see figure for example of the Amsterdam chamber logo). The adaptability, elegance, flexibility, simplicity, symbolism, and symmetry were considered notable characteristics of the VOC's well-designed monogram-logo, those ensured its success at a time when the concept of the corporate identity was virtually unknown. [20] An Australian vintner has used the VOC logo since the late 20th century, having re-registered the company's name for the purpose. [21]
The flag of the company was orange, white, and blue (see Dutch flag ), with the company logo embroidered on it.

History

Origins
Before the Dutch Revolt , Antwerp had played an important role as a distribution centre in northern Europe . However, after 1591 the Portuguese used an international syndicate of the German Fuggers and Welsers , and Spanish and Italian firms, that used Hamburg as its northern staple port to distribute their goods, thereby cutting Dutch merchants out of the trade.
At the same time, the Portuguese trade system was unable to increase supply to satisfy growing demand, in particular the demand for pepper. Demand for spices was relatively inelastic , and therefore each lag in the supply of pepper caused a sharp rise in pepper prices.
In addition, as the Portuguese crown had been united in a personal union with the Spanish crown in 1580, with which the Dutch Republic was at war, the Portuguese Empire became an appropriate target for Dutch military incursions. These three factors motivated Dutch merchants to enter the intercontinental spice trade themselves. Further, a number of Dutchmen like Jan Huyghen van Linschoten and Cornelis de Houtman obtained first hand knowledge of the "secret" Portuguese trade routes and practices, thereby providing opportunity. [22]
The stage was thus set for Houtman's 1595 four-ship exploratory expedition to Banten , the main pepper port of West Java, where they clashed with both the Portuguese and indigenous Indonesians. Houtman's expedition then sailed east along the north coast of Java , losing twelve crew to a Javanese attack at Sidayu and killing a local ruler in Madura . Half the crew were lost before the expedition made it back to the Netherlands the following year, but with enough spices to make a considerable profit. [23]
In 1598, an increasing number of fleets were sent out by competing merchant groups from around the Netherlands. Some fleets were lost, but most were successful, with some voyages producing high profits. In March 1599, a fleet of eight ships under Jacob van Neck was the first Dutch fleet to reach the 'Spice Islands' of Maluku, the source of pepper, cutting out the Javanese middlemen. The ships returned to Europe in 1599 and 1600 and the expedition made a 400 percent profit. [23]
In 1600, the Dutch joined forces with the Muslim Hituese on Ambon Island in an anti-Portuguese alliance, in return for which the Dutch were given the sole right to purchase spices from Hitu. [24] Dutch control of Ambon was achieved when the Portuguese surrendered their fort in Ambon to the Dutch-Hituese alliance. In 1613, the Dutch expelled the Portuguese from their Solor fort, but a subsequent Portuguese attack led to a second change of hands; following this second reoccupation, the Dutch once again captured Solor, in 1636. [24]
East of Solor, on the island of Timor, Dutch advances were halted by an autonomous and powerful group of Portuguese Eurasians called the Topasses . They remained in control of the Sandalwood trade and their resistance lasted throughout the 17th and 18th century, causing Portuguese Timor to remain under the Portuguese sphere of control. [25] [26]

Formation, rise and fall

Formative years
At the time, it was customary for a company to be set up only for the duration of a single voyage, and to be liquidated upon the return of the fleet. Investment in these expeditions was a very high-risk venture, not only because of the usual dangers of piracy, disease and shipwreck, but also because the interplay of inelastic demand and relatively elastic supply [27] of spices could make prices tumble at just the wrong moment, thereby ruining prospects of profitability. To manage such risk the forming of a cartel to control supply would seem logical. The English had been the first to adopt this approach, by bundling their resources into a monopoly enterprise, the English East India Company in 1600, thereby threatening their Dutch competitors with ruin. [28]
In 1602, the Dutch government followed suit, sponsoring the creation of a single "United East Indies Company" that was also granted monopoly over the Asian trade. With a capital of 6,440,200 guilders , [29] the charter of the new company empowered it to build forts, maintain armies, and conclude treaties with Asian rulers. It provided for a venture that would continue for 21 years, with a financial accounting only at the end of each decade. [28]
In February 1603, the Company seized the Santa Catarina (ship) off Singapore, a 1500-ton Portuguese merchant carrack . [30] She was such a rich prize that her sale proceeds increased the capital of the VOC by more than 50%. [31]
In that same year the first permanent Dutch trading post in Indonesia was established in Banten , West Java and in 1611, another was established at Jayakarta (later "Batavia" and then "Jakarta"). [32] In 1610, the VOC established the post of Governor General to more firmly control their affairs in Asia. To advise and control the risk of despotic Governors General, a Council of the Indies ( Raad van Indië ) was created. The Governor General effectively became the main administrator of the VOC's activities in Asia, although the Heeren XVII , a body of 17 shareholders representing different chambers, continued to officially have overall control. [24]
VOC headquarters were located in Ambon during the tenures of the first three Governors General (1610–1619), but it was not a satisfactory location. Although it was at the centre of the spice production areas, it was far from the Asian trade routes and other VOC areas of activity ranging from Africa to India to Japan. [33] [34] A location in the west of the archipelago was thus sought; the Straits of Malacca were strategic, but had become dangerous following the Portuguese conquest and the first permanent VOC settlement in Banten was controlled by a powerful local ruler and subject to stiff competition from Chinese and English traders. [24]
In 1604, a second English East India Company voyage commanded by Sir Henry Middleton reached the islands of Ternate , Tidore , Ambon and Banda ; in Banda, they encountered severe VOC hostility, which saw the beginning of Anglo-Dutch competition for access to spices. [32] From 1611 to 1617, the English established trading posts at Sukadana (southwest Kalimantan ), Makassar , Jayakarta and Jepara in Java , and Aceh, Pariaman and Jambi in Sumatra which threatened Dutch ambitions for a monopoly on East Indies trade. [32]
Diplomatic agreements in Europe in 1620 ushered in a period of co-operation between the Dutch and the English over the spice trade. [32] This ended with a notorious, but disputed incident, known as the ' Amboyna massacre ', where ten Englishmen were arrested, tried and beheaded for conspiracy against the Dutch government. [35] Although this caused outrage in Europe and a diplomatic crisis, the English quietly withdrew from most of their Indonesian activities (except trading in Banten) and focused on other Asian interests.

Growth
In 1619, Jan Pieterszoon Coen was appointed Governor-General of the VOC. He saw the possibility of the VOC becoming an Asian power, both political and economic. On 30 May 1619, Coen, backed by a force of nineteen ships, stormed Jayakarta, driving out the Banten forces; and from the ashes established Batavia as the VOC headquarters. In the 1620s almost the entire native population of the Banda Islands was driven away, starved to death, or killed in an attempt to replace them with Dutch plantations. [36] These plantations were used to grow cloves and nutmeg for export. Coen hoped to settle large numbers of Dutch colonists in the East Indies, but implementation of this policy never materialised, mainly because very few Dutch were willing to emigrate to Asia. [37]
Another of Coen's ventures was more successful. A major problem in the European trade with Asia at the time was that the Europeans could offer few goods that Asian consumers wanted, except silver and gold. European traders therefore had to pay for spices with the precious metals, which were in short supply in Europe, except for Spain and Portugal. The Dutch and English had to obtain it by creating a trade surplus with other European countries. Coen discovered the obvious solution for the problem: to start an intra-Asiatic trade system, whose profits could be used to finance the spice trade with Europe. In the long run this obviated the need for exports of precious metals from Europe, though at first it required the formation of a large trading-capital fund in the Indies. The VOC reinvested a large share of its profits to this end in the period up to 1630. [38]
The VOC traded throughout Asia. Ships coming into Batavia from the Netherlands carried supplies for VOC settlements in Asia. Silver and copper from Japan were used to trade with India and China for silk, cotton, porcelain, and textiles. These products were either traded within Asia for the coveted spices or brought back to Europe. The VOC was also instrumental in introducing European ideas and technology to Asia. The Company supported Christian missionaries and traded modern technology with China and Japan. A more peaceful VOC trade post on Dejima , an artificial island off the coast of Nagasaki , was for more than two hundred years the only place where Europeans were permitted to trade with Japan. [39] When the VOC tried to military force Ming dynasty China to open up to Dutch trade, the Chinese defeated the Dutch in a war over the Penghu islands from 1623–1624 and forced the VOC to abandon Penghu for Taiwan. The Chinese defeated the VOC again at the Battle of Liaoluo Bay in 1633.
The Vietnamese Nguyen Lords defeated the VOC in a 1643 battle during the Trịnh–Nguyễn War , blowing up a Dutch ship. The Cambodians defeated the VOC in a war from 1643–44 on the Mekong River .
In 1640, the VOC obtained the port of Galle , Ceylon , from the Portuguese and broke the latter's monopoly of the cinnamon trade. In 1658, Gerard Pietersz. Hulft laid siege to Colombo , which was captured with the help of King Rajasinghe II of Kandy . By 1659, the Portuguese had been expelled from the coastal regions, which were then occupied by the VOC, securing for it the monopoly over cinnamon. To prevent the Portuguese or the English from ever recapturing Sri Lanka, the VOC went on to conquer the entire Malabar Coast from the Portuguese, almost entirely driving them from the west coast of India. When news of a peace agreement between Portugal and the Netherlands reached Asia in 1663, Goa was the only remaining Portuguese city on the west coast. [40]
In 1652, Jan van Riebeeck established an outpost at the Cape of Good Hope (the southwestern tip of Africa, now Cape Town , South Africa) to re-supply VOC ships on their journey to East Asia. This post later became a full-fledged colony, the Cape Colony , when more Dutch and other Europeans started to settle there.
VOC trading posts were also established in Persia , Bengal , Malacca , Siam , Canton [ verification needed ] , Formosa (now Taiwan), as well as the Malabar and Coromandel coasts in India. In 1662, however, Koxinga expelled the Dutch from Taiwan [42] ( see History of Taiwan ).
In 1663, the VOC signed "Painan Treaty" with several local lords in the Painan area that were revolting against the Aceh Sultanate . The treaty resolved the VOC to build a trading post in the area and eventually monopolise the trade there, especially the gold trade. [43]
By 1669, the VOC was the richest private company the world had ever seen, with over 150 merchant ships, 40 warships, 50,000 employees, a private army of 10,000 soldiers, and a dividend payment of 40% on the original investment. [44]
Many of the VOC employees inter-mixed with the indigenous peoples and expanded the population of Indos in pre-colonial history [45] [46]

Reorientation
Around 1670, two events caused the growth of VOC trade to stall. In the first place, the highly profitable trade with Japan started to decline. The loss of the outpost on Formosa to Koxinga in the 1662 Siege of Fort Zeelandia and related internal turmoil in China (where the Ming dynasty was being replaced with the Qing dynasty ) brought an end to the silk trade after 1666. Though the VOC substituted Bengali for Chinese silk other forces affected the supply of Japanese silver and gold. The shogunate enacted a number of measures to limit the export of these precious metals, in the process limiting VOC opportunities for trade, and severely worsening the terms of trade. Therefore, Japan ceased to function as the lynchpin of the intra-Asiatic trade of the VOC by 1685. [47]
Even more importantly, the Third Anglo-Dutch War temporarily interrupted VOC trade with Europe. This caused a spike in the price of pepper, which enticed the English East India Company (EIC) to aggressively enter this market in the years after 1672. Previously, one of the tenets of the VOC pricing policy was to slightly over-supply the pepper market, so as to depress prices below the level where interlopers were encouraged to enter the market (instead of striving for short-term profit maximisation). The wisdom of such a policy was illustrated when a fierce price war with the EIC ensued, as that company flooded the market with new supplies from India. In this struggle for market share, the VOC (which had much larger financial resources) could wait out the EIC. Indeed, by 1683, the latter came close to bankruptcy; its share price plummeted from 600 to 250; and its president Josiah Child was temporarily forced from office. [48]
However, the writing was on the wall. Other companies, like the French East India Company and the Danish East India Company also started to make inroads on the Dutch system. The VOC therefore closed the heretofore flourishing open pepper emporium of Bantam by a treaty of 1684 with the Sultan. Also, on the Coromandel Coast , it moved its chief stronghold from Pulicat to Negapatnam , so as to secure a monopoly on the pepper trade at the detriment of the French and the Danes. [49] However, the importance of these traditional commodities in the Asian-European trade was diminishing rapidly at the time. The military outlays that the VOC needed to make to enhance its monopoly were not justified by the increased profits of this declining trade. [50]
Nevertheless, this lesson was slow to sink in and at first the VOC made the strategic decision to improve its military position on the Malabar Coast (hoping thereby to curtail English influence in the area, and end the drain on its resources from the cost of the Malabar garrisons) by using force to compel the Zamorin of Calicut to submit to Dutch domination. In 1710, the Zamorin was made to sign a treaty with the VOC undertaking to trade exclusively with the VOC and expel other European traders. For a brief time, this appeared to improve the Company's prospects. However, in 1715, with EIC encouragement, the Zamorin renounced the treaty. Though a Dutch army managed to suppress this insurrection temporarily, the Zamorin continued to trade with the English and the French, which led to an appreciable upsurge in English and French traffic. The VOC decided in 1721 that it was no longer worth the trouble to try to dominate the Malabar pepper and spice trade. A strategic decision was taken to scale down the Dutch military presence and in effect yield the area to EIC influence. [51]
The 1741 Battle of Colachel by Nair warriors of Travancore under Raja Marthanda Varma defeated the Dutch. The Dutch commander Captain Eustachius De Lannoy was captured. Marthanda Varma agreed to spare the Dutch captain's life on condition that he joined his army and trained his soldiers on modern lines. This defeat in the Travancore-Dutch War is considered the earliest example of an organised Asian power overcoming European military technology and tactics; and it signalled the decline of Dutch power in India. [52]
The attempt to continue as before as a low volume-high profit business enterprise with its core business in the spice trade had therefore failed. The Company had however already (reluctantly) followed the example of its European competitors in diversifying into other Asian commodities, like tea, coffee, cotton, textiles, and sugar. These commodities provided a lower profit margin and therefore required a larger sales volume to generate the same amount of revenue. This structural change in the commodity composition of the VOC's trade started in the early 1680s, after the temporary collapse of the EIC around 1683 offered an excellent opportunity to enter these markets. The actual cause for the change lies, however, in two structural features of this new era.
In the first place, there was a revolutionary change in the tastes affecting European demand for Asian textiles, and coffee and tea, around the turn of the 18th century. Secondly, a new era of an abundant supply of capital at low interest rates suddenly opened around this time. The second factor enabled the Company to easily finance its expansion in the new areas of commerce. [53] Between the 1680s and 1720s, the VOC was therefore able to equip and man an appreciable expansion of its fleet, and acquire a large amount of precious metals to finance the purchase of large amounts of Asian commodities, for shipment to Europe. The overall effect was to approximately double the size of the company. [54]
The tonnage of the returning ships rose by 125 percent in this period. However, the Company's revenues from the sale of goods landed in Europe rose by only 78 percent. This reflects the basic change in the VOC's circumstances that had occurred: it now operated in new markets for goods with an elastic demand, in which it had to compete on an equal footing with other suppliers. This made for low profit margins. [55] Unfortunately, the business information systems of the time made this difficult to discern for the managers of the company, which may partly explain the mistakes they made from hindsight. This lack of information might have been counteracted (as in earlier times in the VOC's history) by the business acumen of the directors. Unfortunately by this time these were almost exclusively recruited from the political regent class, which had long since lost its close relationship with merchant circles. [56]
Low profit margins in themselves do not explain the deterioration of revenues. To a large extent the costs of the operation of the VOC had a "fixed" character (military establishments; maintenance of the fleet and such). Profit levels might therefore have been maintained if the increase in the scale of trading operations that in fact took place, had resulted in economies of scale . However, though larger ships transported the growing volume of goods, labour productivity did not go up sufficiently to realise these. In general the Company's overhead rose in step with the growth in trade volume; declining gross margins translated directly into a decline in profitability of the invested capital. The era of expansion was one of "profitless growth". [57]
Concretely: "[t]he long-term average annual profit in the VOC's 1630–70 'Golden Age' was 2.1 million guilders, of which just under half was distributed as dividends and the remainder reinvested. The long-term average annual profit in the 'Expansion Age' (1680–1730) was 2.0 million guilders, of which three-quarters was distributed as dividend and one-quarter reinvested. In the earlier period, profits averaged 18 percent of total revenues; in the latter period, 10 percent. The annual return of invested capital in the earlier period stood at approximately 6 percent; in the latter period, 3.4 percent." [57]
Nevertheless, in the eyes of investors the VOC did not do too badly. The share price hovered consistently around the 400 mark from the mid-1680s (excepting a hiccup around the Glorious Revolution in 1688), and they reached an all-time high of around 642 in the 1720s. VOC shares then yielded a return of 3.5 percent, only slightly less than the yield on Dutch government bonds. [58]

Decline and fall
However, from there on the fortunes of the VOC started to decline. Five major problems, not all of equal weight, can be used to explain its decline in the next fifty years to 1780. [59]
Despite of all this, the VOC in 1780 remained an enormous operation. Its capital in the Republic, consisting of ships and goods in inventory, totalled 28 million guilders; its capital in Asia, consisting of the liquid trading fund and goods en route to Europe, totalled 46 million guilders. Total capital, net of outstanding debt, stood at 62 million guilders. The prospects of the company at this time therefore need not have been hopeless, had one of the many plans to reform it been taken successfully in hand. However, then the Fourth Anglo-Dutch War intervened. British attacks in Europe and Asia reduced the VOC fleet by half; removed valuable cargo from its control; and devastated its remaining power in Asia. The direct losses of the VOC can be calculated at 43 million guilders. Loans to keep the company operating reduced its net assets to zero. [61]
From 1720 on, the market for sugar from Indonesia declined as the competition from cheap sugar from Brazil increased. European markets became saturated. Dozens of Chinese sugar traders went bankrupt which led to massive unemployment, which in turn led to gangs of unemployed coolies . The Dutch government in Batavia did not adequately respond to these problems. In 1740, rumours of deportation of the gangs from the Batavia area led to widespread rioting. The Dutch military searched houses of Chinese in Batavia for weapons. When a house accidentally burnt down, military and impoverished citizens started slaughtering and pillaging the Chinese community. [62] This massacre of the Chinese was deemed sufficiently serious for the board of the VOC to start an official investigation into the Government of the Dutch East Indies for the first time in its history.
After the Fourth Anglo-Dutch War, the VOC was a financial wreck, and after vain attempts by the provincial States of Holland and Zeeland to reorganise it, was nationalised on 1 March 1796 [63] by the new Batavian Republic . Its charter was renewed several times, but allowed to expire on 31 December 1799. [63] Most of the possessions of the former VOC were subsequently occupied by Great Britain during the Napoleonic wars , but after the new United Kingdom of the Netherlands was created by the Congress of Vienna , some of these were restored to this successor state of the old Dutch Republic by the Anglo-Dutch Treaty of 1814 .

Organizational structure
The VOC is generally considered to be the world's first truly transnational corporation and it was also the first multinational enterprise to issue shares of stock to the public. Some historians such as Timothy Brook and Russell Shorto consider the VOC as the pioneering corporation in the first wave of the corporate globalization era. [9] [10] The VOC was the first multinational corporation to operate officially in different continents such as Europe , Asia and Africa . While the VOC mainly operated in what later became the Dutch East Indies (modern Indonesia), the company also had important operations elsewhere. It employed people from different continents and origins in the same functions and working environments. Although it was a Dutch company its employees included not only people from the Netherlands, but also many from Germany and from other countries as well. Besides the diverse north-west European workforce recruited by the VOC in the Dutch Republic , the VOC made extensive use of local Asian labour markets. As a result, the personnel of the various VOC offices in Asia consisted of European and Asian employees. Asian or Eurasian workers might be employed as sailors, soldiers, writers, carpenters, smiths, or as simple unskilled workers. [64] At the height of its existence the VOC had 25,000 employees worked in Asia and 11,000 were en route. [65] Also, while most of its shareholders were Dutch, about a quarter of the initial shareholders were Zuid-Nederlanders (people from an area that includes modern Belgium and Luxembourg ) and there were also a few dozen Germans. [66]
The VOC had two types of shareholders: the participanten , who could be seen as non-managing members, and the 76 bewindhebbers (later reduced to 60) who acted as managing directors. This was the usual set-up for Dutch joint-stock companies at the time. The innovation in the case of the VOC was, that the liability of not just the participanten , but also of the bewindhebbers was limited to the paid-in capital (usually, bewindhebbers had unlimited liability). The VOC therefore was a limited liability company . Also, the capital would be permanent during the lifetime of the company. As a consequence, investors that wished to liquidate their interest in the interim could only do this by selling their share to others on the Amsterdam Stock Exchange . [67] Confusion of confusions , a 1688 dialogue by the Sephardi Jew Joseph de la Vega analysed the workings of this one-stock exchange.
The VOC consisted of six Chambers ( Kamers ) in port cities: Amsterdam , Delft , Rotterdam , Enkhuizen , Middelburg and Hoorn . Delegates of these chambers convened as the Heeren XVII (the Lords Seventeen). They were selected from the bewindhebber -class of shareholders. [68]
Of the Heeren XVII , eight delegates were from the Chamber of Amsterdam (one short of a majority on its own), four from the Chamber of Zeeland, and one from each of the smaller Chambers, while the seventeenth seat was alternatively from the Chamber of Middelburg-Zeeland or rotated among the five small Chambers. Amsterdam had thereby the decisive voice. The Zeelanders in particular had misgivings about this arrangement at the beginning. The fear was not unfounded, because in practice it meant Amsterdam stipulated what happened.
The six chambers raised the start-up capital of the Dutch East India Company:
The raising of capital in Rotterdam did not go so smoothly. A considerable part originated from inhabitants of Dordrecht . Although it did not raise as much capital as Amsterdam or Middelburg-Zeeland, Enkhuizen had the largest input in the share capital of the VOC. Under the first 358 shareholders, there were many small entrepreneurs, who dared to take the risk . The minimum investment in the VOC was 3,000 guilders, which priced the Company's stock within the means of many merchants. [69]
Among the early shareholders of the VOC, immigrants played an important role. Under the 1,143 tenderers were 39 Germans and no fewer than 301 from the Southern Netherlands (roughly present Belgium and Luxembourg, then under Habsburg rule), of whom Isaac le Maire was the largest subscriber with ƒ85,000. VOC's total capitalisation was ten times that of its British rival.
The Heeren XVII (Lords Seventeen) met alternately 6 years in Amsterdam and 2 years in Middelburg-Zeeland. They defined the VOC's general policy and divided the tasks among the Chambers. The Chambers carried out all the necessary work, built their own ships and warehouses and traded the merchandise. The Heeren XVII sent the ships' masters off with extensive instructions on the route to be navigated, prevailing winds, currents, shoals and landmarks. The VOC also produced its own charts .
In the context of the Dutch-Portuguese War the company established its headquarters in Batavia, Java (now Jakarta , Indonesia ). Other colonial outposts were also established in the East Indies , such as on the Maluku Islands , which include the Banda Islands , where the VOC forcibly maintained a monopoly over nutmeg and mace . Methods used to maintain the monopoly involved extortion and the violent suppression of the native population, including mass murder . [70] In addition, VOC representatives sometimes used the tactic of burning spice trees to force indigenous populations to grow other crops, thus artificially cutting the supply of spices like nutmeg and cloves. [71]

VOC outposts
Organization and leadership structures were varied as necessary in the various VOC outposts:
Opperhoofd is a Dutch word (pl. Opperhoofden ), which literally means 'supreme chief'. In this VOC context, the word is a gubernatorial title, comparable to the English Chief factor , for the chief executive officer of a Dutch factory in the sense of trading post, as led by a factor , i.e. agent.

Council of Justice in Batavia
The Council of Justice in Batavia was the appellate court for all the other VOC Company posts in the VOC empire.

Shareholder activism at the VOC and the beginnings of modern corporate governance
The seventeenth-century Dutch businessmen, especially the VOC investors, were possibly the history's first recorded investors to actually consider the corporate governance 's problems. [72] [73] Isaac Le Maire , who often known as the history's first recorded short seller , was also a sizeable shareholder of the VOC. In 1609, he complained of the VOC's shoddy corporate governance. On 24 January 1609, Le Maire filed a petition against the VOC, marking the first recorded expression of shareholder activism . In what is the first recorded corporate governance dispute, Le Maire formally charged that the VOC's board of directors (the Heeren XVII) sought to "retain another’s money for longer or use it ways other than the latter wishes" and petitioned for the liquidation of the VOC in accordance with standard business practice. [74] [75] [76] Initially the largest single shareholder in the VOC and a bewindhebber sitting on the board of governors, Le Maire apparently attempted to divert the firm's profits to himself by undertaking 14 expeditions under his own accounts instead of those of the company. Since his large shareholdings were not accompanied by greater voting power, Le Maire was soon ousted by other governors in 1605 on charges of embezzlement, and was forced to sign an agreement not to compete with the VOC. Having retained stock in the company following this incident, in 1609 Le Maire would become the author of what is celebrated as "first recorded expression of shareholder advocacy at a publicly traded company". [77] [78] [79]
In 1622, the history's first recorded shareholder revolt also happened among the VOC investors who complained that the company account books had been "smeared with bacon" so that they might be "eaten by dogs." The investors demanded a "reeckeninge," a proper financial audit. [80] The 1622 campaign by the shareholders of the VOC is a testimony of genesis of corporate social responsibility (CSR) in which shareholders staged protests by distributing pamphlets and complaining about management self enrichment and secrecy. [81]

Main trading posts, settlements and colonies

Asia

Indonesia

Indian subcontinent

Japan

Taiwan

Malaysia

Thailand

Vietnam

Africa

Mauritius

South Africa

Main competitors

Conflicts and wars involving the VOC

Historical roles and legacy
With regard to the VOC's importance in global corporate history, Australian journalist Hugh Edwards (1970) writes, "The Vereenigde Oost-Indische Compagnie was the most powerful single commercial concern the world has ever known. General Motors , British Tobacco , Ford , The Shell Company , Mitsubishi , Standard Oil – any of the other giant holdings of today are on the level of village bootmakers compared with the might and power and influence once wielded by the VOC." [82] [83] In his book Amsterdam: A History of the World's Most Liberal City (2013), Russell Shorto summarizes the VOC's greatness: "Like the oceans it mastered, the VOC had a scope that is hard to fathom. One could craft a defensible argument that no company in history has had such an impact on the world. Its surviving archives—in Cape Town , Colombo, Chennai , Jakarta , and The Hague —have been measured (by a consortium applying for a UNESCO grant to preserve them) in kilometers. In innumerable ways the VOC both expanded the world and brought its far-flung regions together. It introduced Europe to Asia and Africa, and vice versa (while its sister multinational, the West India Company , set New York City in motion and colonized Brazil and the Caribbean Islands). It pioneered globalization and invented what might be the first modern bureaucracy. It advanced cartography and shipbuilding. It fostered disease, slavery, and exploitation on a scale never before imaged." [10]
The company was arguably the first megacorporation the world has ever seen, possessing quasi-governmental powers, including the ability to wage war, imprison and execute convicts, negotiate treaties, coin money and establish colonies. Many economic and political historians consider the VOC as the most valuable, powerful and influential corporation in the world history. [9] [10] [11]
The VOC existed for almost 200 years from its founding in 1602, when the States-General of the Netherlands granted it a 21-year monopoly over Dutch operations in Asia until its demise in 1796. During those two centuries (between 1602 and 1796), the VOC sent almost a million Europeans to work in the Asia trade on 4,785 ships, and netted for their efforts more than 2.5 million tons of Asian trade goods. By contrast, the rest of Europe combined sent only 882,412 people from 1500 to 1795, and the fleet of the English (later British ) East India Company , the VOC's nearest competitor, was a distant second to its total traffic with 2,690 ships and a mere one-fifth the tonnage of goods carried by the VOC. The VOC enjoyed huge profits from its spice monopoly through most of the 17th century. [84]
The company was also considered by many to be the very first major and the greatest corporation in history. [note 6] [86] The VOC's territories were even larger than some countries. By 1669, the VOC was the richest company the world had ever seen, with over 150 merchant ships, 40 warships, 50,000 employees, a private army of 10,000 soldiers, and a dividend payment of 40% on the original investment. [87] [88] [89]

Institutional innovations and contributions to business history
The VOC played a crucial role in the rise of corporate globalization , corporate governance , corporate identity , corporate social responsibility , corporate finance , modern entrepreneurship , and financial capitalism . [92] [93] [11] During its golden age, the company made some fundamental institutional innovations in economic and financial history . These financially revolutionary innovations allowed a single company (like the VOC) to mobilize financial resources from a large number of investors and create ventures at a scale that had previously only been possible for monarchs. [94] In the words of Canadian historian and sinologist Timothy Brook , "the Dutch East India Company—the VOC, as it is known—is to corporate capitalism what Benjamin Franklin 's kite is to electronics : the beginning of something momentous that could not have been predicted at the time." [9] The birth of the VOC is often considered to be the official beginning of the corporate globalization era with the rise of modern corporations as consistently combined political-economic-environmental- cultural forces that significantly affect human lives in every corner of the world today. [9] [95] As the world's first publicly traded company and first listed company (the first company to be ever listed on an official stock exchange), the VOC was the first company to actually issue stock and bonds to the general public. Considered by many experts to be the world's first truly (modern) multinational corporation , [96] the VOC was also the first permanently organized limited-liability joint-stock company , with a permanent capital base. [note 7] [98] The VOC shareholders were the pioneers in laying the basis for modern corporate governance and corporate finance . The VOC is often considered as the precursor of modern corporations, if not the first truly modern corporation. [99] It was the VOC that invented the idea of investing in the company rather than in a specific venture governed by the company. With its pioneering features such as corporate identity (first globally-recognized corporate logo ), entrepreneurial spirit, legal personhood , transnational (multinational) operational structure, high stable profitability, permanent capital (fixed capital stock), [100] [101] freely transferable shares and tradable securities , separation of ownership and management , and limited liability for both shareholders and managers, the VOC is generally considered a major institutional breakthrough [102] and the model for the large-scale business enterprises that now dominate the global economy . [103]
The VOC was the driving force behind the rise of Amsterdam as the first modern model of (global) international financial centres [note 8] that now dominate the global financial system . During the 17th century and most of the 18th century, Amsterdam had been the most influential and powerful financial centre of the world. [105] [106] [107] [108] [109] The VOC also played a major role in the creation of the world's first fully functioning financial market , [110] with the birth of a fully fledged capital market . [111] The Dutch were also the first to effectively use a fully-fledged capital market (including the bond market and the stock market) to finance companies (such as the VOC and the WIC ). It was in the 17th-century Dutch Republic that the global securities market began to take on its modern form. And it was in Amsterdam that the important institutional innovations such as publicly traded companies, transnational corporations, capital markets (including bond markets and stock markets), central banking system, investment banking system, and investment funds ( mutual funds ) were systematically operated for the first time in history. In 1602 the VOC established an exchange in Amsterdam where VOC stock and bonds could be traded in a secondary market . The VOC undertook the world's first recorded IPO in the same year. The Amsterdam Stock Exchange ( Amsterdamsche Beurs or Beurs van Hendrick de Keyser in Dutch) was also the world's first fully-fledged stock exchange. While the Italian city-states produced the first transferable government bonds, they didn't develop the other ingredient necessary to produce a fully fledged capital market: corporate shareholders. The Dutch East India Company (VOC) became the first company to offer shares of stock . The dividend averaged around 18% of capital over the course of the company's 200-year existence. The establishment of Amsterdam Stock Exchange (1602) by the VOC, has long been recognized as the origin of modern stock exchanges that specialize in creating and sustaining secondary markets in the securities issued by corporations. Dutch investors were the first to trade their shares at a regular stock exchange. The process of buying and selling these shares of stock in the VOC became the basis of the first official stock market in history. [112] [90] It was in the Dutch Republic that the early techniques of stock-market manipulation were developed. The Dutch pioneered stock futures , stock options , short selling , bear raids , debt-equity swaps, and other speculative instruments . [113] Amsterdam businessman Joseph de la Vega 's Confusion of Confusions (1688) was the earliest book about stock trading .

Impacts on economic and social history of the Netherlands
The VOC was a major force behind the financial revolution [note 9] [116] [117] and economic miracle [118] [119] [120] of the Dutch Republic in the 17th-century. During their Golden Age , the Dutch Republic (or the Northern Netherlands), as the resource-poor and obscure cousins of the more urbanized Southern Netherlands , rose to become the world's leading economic and financial superpower. [note 10] [123] [124] [125] [126] [127] The VOC's shipyards also contributed greatly to the Dutch domination of global shipbuilding and shipping industries during the 1600s. [note 11] "By seventeenth century standards," as Richard Unger affirms, Dutch shipbuilding "was a massive industry and larger than any shipbuilding industry which had preceded it." [130] By the 1670s the size of the Dutch merchant fleet probably exceeded the combined fleets of England, France, Spain, Portugal, and Germany. [131] Until the mid-1700s, the economic system of the Dutch Republic (including its financial system) was the most advanced and sophisticated ever seen in history. [132] However, in a typical multicultural society of the Netherlands , the VOC's history (and especially its dark side) has always been a potential source of controversy. In 2006 when the Dutch Prime Minister Jan Pieter Balkenende referred to the pioneering entrepreneurial spirit and work ethics of the Dutch people and Dutch Republic in their Golden Age , he coined the term "VOC mentality" ( VOC-mentaliteit in Dutch). [note 12] It unleashed a wave of criticism, since such romantic views about the Dutch Golden Age ignores the inherent historical associations with colonialism, exploitation and violence. Balkenende later stressed that "it had not been his intention to refer to that at all". [134] But in spite of criticisms, the "VOC-mentality", as a characteristic of the selective historical perspective on the Dutch Golden Age, has been considered a key feature of Dutch cultural policy for many years. [134]

Influences on Dutch Golden Age art
From 1609 the VOC had a trading post in Japan ( Hirado, Nagasaki ), which used local paper for its own administration. However, the paper was also traded to the VOC's other trading posts and even the Dutch Republic. Many impressions of the Dutch Golden Age artist Rembrandt 's prints were done on Japanese paper. From about 1647 Rembrandt sought increasingly to introduce variation into his prints by using different sorts of paper, and printed most of his plates regularly on Japanese paper. He also used the paper for his drawings . The Japanese paper types – which was actually imported from Japan by the VOC – attracted Rembrandt with its warm, yellowish colour. [135] They are often smooth and shiny, whilst Western paper has a more rough and matt surface. [136] Moreover, the VOC's imported Chinese porcelain wares are often depicted in many Dutch Golden Age genre paintings , especially in Jan Vermeer 's paintings. [9]

VOC world as a knowledge network in the Dutch maritime world-system
During the Dutch Golden Age , the Dutch – using their expertise in doing business, cartography, shipbuilding, seafaring and navigation – traveled to the far corners of the world, leaving their language embedded in the names of many places . Dutch exploratory voyages revealed largely unknown landmasses to the civilized world and put their names on the world map . The Dutch came to dominate the map making and map printing industry by virtue of their own travels, trade ventures, and widespread commercial networks. [137] As Dutch ships reached into the unknown corners of the globe, Dutch cartographers incorporated new geographical discoveries into their work. Instead of using the information themselves secretly, they published it, so the maps multiplied freely. For almost 200 years, the Dutch dominated world trade . [138] Dutch ships carried goods, but they also opened up opportunities for the exchange of knowledge. [139] The commercial networks of the Dutch trading companies, i.e. the VOC and West India Company (WIC), provided an infrastructure which was accessible to people with a scholarly interest in the exotic world. [140] [141] [142] [143] The VOC's bookkeeper Hendrick Hamel was the first known European/Westerner to experience first-hand and write about Joseon -era Korea . [note 13] In his report (published in the Dutch Republic) in 1666 Hendrick Hamel described his adventures on the Korean Peninsula and gave the first accurate description of daily life of Koreans to the western world. [144] [145] [146] The VOC trade post on Dejima , an artificial island off the coast of Nagasaki , was for more than two hundred years the only place where Europeans were permitted to trade with Japan. Rangaku (literally 'Dutch Learning', and by extension 'Western Learning') is a body of knowledge developed by Japan through its contacts with the Dutch enclave of Dejima , which allowed Japan to keep abreast of Western technology and medicine in the period when the country was closed to foreigners, 1641–1853, because of the Tokugawa shogunate 's policy of national isolation ( sakoku ).

Roles in the global economy and international relations
Considered by many political and economic historians as an autonomous quasi-state within another state, [note 14] the VOC, for almost 200 years of its existence, was a key (non-state) player in Asia-related trade and politics. More than just a pure multinational trading company, the VOC – although privately financed – was the international arm of the Dutch Republic government . The company was much an unofficial representative of the States General of the United Provinces in foreign relations of the Dutch Republic with many states, especially Dutch-Asian relations.
The VOC had seminal influences on the history of some countries and territories such as New York ( New Netherland ), [149] Indonesia , Australia , New Zealand , South Africa , Taiwan and Japan . [150]

Contributions in the Age of Exploration
See also: Maritime history of the Dutch East India Company ; Dutch Republic in the Age of Discovery ; Cartography and geography in the Dutch Republic ; Golden Age of Dutch cartography / Golden Age of Netherlandish cartography
The Dutch East India Company (VOC) was also a major force behind the Golden Age of Dutch exploration and discovery (ca. 1590s–1720s). The VOC-funded exploratory voyages such as those led by Willem Janszoon ( Duyfken ), Henry Hudson ( Halve Maen ) and Abel Tasman revealed largely unknown landmasses to the civilized world. Also, in the Golden Age of Netherlandish cartography [note 15] (ca. 1570s–1670s), the VOC's navigators and cartographers [note 16] helped shape modern geographical and cartographic (like world map ) knowledge as we know them today.

Halve Maen'
In 1609, English sea captain and explorer Henry Hudson was hired by the VOC émigrés running the VOC located in Amsterdam [151] to find a north-east passage to Asia, sailing around Scandinavia and Russia. He was turned back by the ice of the Arctic in his second attempt, so he sailed west to seek a north-west passage rather than return home. He ended up exploring the waters off the east coast of North America aboard the vlieboot Halve Maen . His first landfall was at Newfoundland and the second at Cape Cod .
Hudson believed that the passage to the Pacific Ocean was between the St. Lawrence River and Chesapeake Bay , so he sailed south to the Bay then turned northward, traveling close along the shore. He first discovered Delaware Bay and began to sail upriver looking for the passage. This effort was foiled by sandy shoals, and the Halve Maen continued north. After passing Sandy Hook , Hudson and his crew entered the narrows into the Upper New York Bay. (Unbeknownst to Hudson, the narrows had already been discovered in 1524 by explorer Giovanni da Verrazzano ; today, the bridge spanning them is named after him. [152] ) Hudson believed that he had found the continental water route, so he sailed up the major river which later bore his name: the Hudson . He found the water too shallow to proceed several days later, at the site of present-day Troy, New York . [153]
Upon returning to the Netherlands, Hudson reported that he had found a fertile land and an amicable people willing to engage his crew in small-scale bartering of furs, trinkets, clothes, and small manufactured goods. His report was first published in 1611 by Emanuel Van Meteren , an Antwerp émigré and the Dutch Consul at London. [151] This stimulated interest [154] in exploiting this new trade resource, and it was the catalyst for Dutch merchant-traders to fund more expeditions.
In 1611–12, the Admiralty of Amsterdam sent two covert expeditions to find a passage to China with the yachts Craen and Vos , captained by Jan Cornelisz Mey and Symon Willemsz Cat, respectively. In four voyages made between 1611 and 1614, the area between present-day Maryland and Massachusetts was explored, surveyed, and charted by Adriaen Block , Hendrick Christiaensen , and Cornelius Jacobsen Mey . The results of these explorations, surveys, and charts made from 1609 through 1614 were consolidated in Block's map, which used the name New Netherland for the first time.

Dutch exploration and mapping of Australasia
In terms of world history of geography and exploration, the VOC can be credited with putting most of Australia's coast (then Hollandia Nova and other names) on the world map, between 1606 and 1756. [155] While Australia's territory (originally known as New Holland ) never became an actual Dutch settlement or colony, [156] Dutch navigators were the first to undisputedly explore and map Australian coastline. In the 17th century, the VOC's navigators and explorers charted almost three-quarters of Australia's coastline, except its east coast. The Dutch ship, Duyfken , led by Willem Janszoon , made the first documented European landing in Australia in 1606. [157] Although a theory of Portuguese discovery in the 1520s exists, it lacks definitive evidence. [158] [159] [160] Precedence of discovery has also been claimed for China , [161] France , [162] Spain , [163] India , [164] and even Phoenicia . [165]
Hendrik Brouwer 's discovery of Brouwer Route that sailing east from the Cape of Good Hope until land was sighted, and then sailing north along the west coast of Australia was a much quicker route than around the coast of the Indian Ocean made Dutch landfalls on the west coast inevitable. The first such landfall was in 1616, when Dirk Hartog landed at Cape Inscription on what is now known as Dirk Hartog Island , off the coast of Western Australia, and left behind an inscription on a pewter plate . In 1697 the Dutch captain Willem de Vlamingh landed on the island and discovered Hartog's plate . He replaced it with one of his own, which included a copy of Hartog's inscription, and took the original plate home to Amsterdam , where it is still kept in the Rijksmuseum Amsterdam .
In 1627, the VOC's explorers François Thijssen and Pieter Nuyts discovered the south coast of Australia and charted about 1,800 kilometres (1,100 mi) of it between Cape Leeuwin and the Nuyts Archipelago . [166] [167] François Thijssen , captain of the ship 't Gulden Zeepaert (The Golden Seahorse), sailed to the east as far as Ceduna in South Australia . The first known ship to have visited the area is the Leeuwin ("Lioness"), a Dutch vessel that charted some of the nearby coastline in 1622. The log of the Leeuwin has been lost, so very little is known of the voyage. However, the land discovered by the Leeuwin was recorded on a 1627 map by Hessel Gerritsz : Caert van't Landt van d'Eendracht ("Chart of the Land of Eendracht"), which appears to show the coast between present-day Hamelin Bay and Point D’Entrecasteaux. Part of Thijssen's map shows the islands St Francis and St Peter, now known collectively with their respective groups as the Nuyts Archipelago . Thijssen's observations were included as soon as 1628 by the VOC cartographer Hessel Gerritsz in a chart of the Indies and New Holland. This voyage defined most of the southern coast of Australia and discouraged the notion that " New Holland " as it was then known, was linked to Antarctica.
In 1642, Abel Tasman sailed from Mauritius and on 24 November, sighted Tasmania . He named Tasmania Anthoonij van Diemenslandt (Anglicised as Van Diemen's Land ), after Anthony van Diemen , the VOC's Governor General, who had commissioned his voyage. [168] [169] [170] It was officially renamed Tasmania in honour of its first European discoverer on 1 January 1856. [171]
In 1642, during the same expedition, Tasman's crew discovered and charted New Zealand 's coastline. They were the first Europeans known to reach New Zealand. Tasman anchored at the northern end of the South Island in Golden Bay (he named it Murderers' Bay ) in December 1642 and sailed northward to Tonga following a clash with local Māori. Tasman sketched sections of the two main islands' west coasts. Tasman called them Staten Landt , after the States General of the Netherlands , and that name appeared on his first maps of the country. In 1645 Dutch cartographers changed the name to Nova Zeelandia in Latin, from Nieuw Zeeland , after the Dutch province of Zeeland . It was subsequently Anglicised as New Zealand by James Cook . Various claims have been made that New Zealand was reached by other non- Polynesian voyagers before Tasman, but these are not widely accepted.

Criticisms
In spite of the VOC's historical successes and contributions, the company has long been criticized for its quasi-absolute commercial monopoly , colonialism , exploitation (including use of slave labour ), slave trade , use of violence, environmental destruction (including deforestation ), and overly bureaucratic in organizational structure. [10]

Cultural depictions of people and things associated with the VOC

Non-fiction works relating to the history of the VOC

Places and things named after the VOC and its people
For the full list of places explored, mapped, and named by people of the VOC, see List of place names of Dutch origin .

VOC's important heritage sites

Populated places established by people of the VOC
Populated places (including cities, towns and villages) established/founded [note 20] by people of the Dutch East India Company (VOC).

VOC scholars
Some of the notable VOC scholars including Charles Ralph Boxer , Leonard Blussé , Femme Gaastra , Günter Schilder, Oscar Gelderblom, Joost Jonker, and Om Prakash .

VOC archives and records
The VOC's operations (trading posts and colonies) produced not only warehouses packed with spices, coffee, tea, textiles, porcelain and silk, but also shiploads of documents. Data on political, economic, cultural, religious, and social conditions spread over an enormous area circulated between the VOC establishments, the administrative centre of the trade in Batavia (modern-day Jakarta ), and the board of directors (the Heeren XVII/Gentlemen Seventeen) in the Dutch Republic. [174] The VOC records are included in UNESCO 's Memory of the World Register . [175]

VOC coins

Notable VOC ships

See also
Other trade companies of the age of the sail
Governors General of the Dutch East India Company

Notes
WebPage index: 00005
Snohomish County Executive
The Snohomish County Executive is the head of the executive branch of Snohomish County, Washington . The position is subject to four-year terms (with a term limit of 3) and is a partisan office. [1]

History
County voters approved the adoption of a home-rule charter for Snohomish County on November 6, 1979, creating the position of a county executive and a five-member county council . Prior to the adoption, the county government was led by three commissioners elected at-large . [2] [3] The new position took effect on May 1, 1980, with Willis Tucker elected as the first executive. [4]

List of executives

List of elections

See also
WebPage index: 00006
Gone with the Wind (novel)
Gone with the Wind is a novel by American writer Margaret Mitchell , first published in 1936. The story is set in Clayton County and Atlanta , both in Georgia , during the American Civil War and Reconstruction Era . It depicts the struggles of young Scarlett O'Hara, the spoiled daughter of a well-to-do plantation owner, who must use every means at her disposal to claw her way out of poverty following the destructive Sherman's March to the Sea . This historical novel features a Bildungsroman or coming-of-age story, with the title taken from a poem written by Ernest Dowson .
Gone with the Wind was popular with American readers from the outset and was the top American fiction bestseller in the year it was published and in 1937. As of 2014, a Harris poll found it to be the second favorite book of American readers, just behind the Bible. More than 30 million copies have been printed worldwide.
Written from the perspective of the slaveholder, Gone with the Wind is Southern plantation fiction. Its portrayal of slavery and African Americans has been considered controversial, especially by succeeding generations, as well as its use of a racial epithet and ethnic slurs common to the period. However, the novel has become a reference point for subsequent writers about the South, both black and white. Scholars at American universities refer to it in their writings, interpret and study it. The novel has been absorbed into American popular culture.
Mitchell received the Pulitzer Prize for Fiction for the book in 1937. It was adapted into a 1939 American film . Gone with the Wind is the only novel by Mitchell published during her lifetime. [2]
Mitchell used color symbolism, especially the colors red and green, which frequently are associated with Scarlett O'Hara. Mitchell identified the primary theme as survival. She left the ending speculative for the reader, however. She was often asked what became of her lovers, Rhett and Scarlett. She replied, "For all I know, Rhett may have found someone else who was less difficult." [3] Two sequels authorized by Mitchell's estate were published more than a half century later. A parody was also produced.

Biographical background and publication
Born in 1900 in Atlanta , Georgia , Margaret Mitchell was a Southerner and writer throughout her life. She grew up hearing stories about the American Civil War and the Reconstruction from her tyrannical Irish-American grandmother, who had endured its suffering. Her forceful and intellectual mother was a suffragist who fought for the rights of women to vote.
As a young woman, Mitchell found love with an army lieutenant who was killed in World War I, and she would carry his memory for the remainder of her life. After studying at Smith College for a year, during which time her mother died from the Spanish flu , Mitchell returned to Atlanta. She married, but her husband was an abusive bootlegger. Mitchell took a job writing feature articles for the Atlanta Journal at a time when Atlanta debutantes of her class did not work. After divorcing her first husband, she married again, this time to a man who shared her interest in writing and literature.
Margaret Mitchell began writing Gone with the Wind in 1926 to pass the time while recovering from a slow-healing auto-crash injury. [3] In April 1935, Harold Latham of Macmillan , an editor looking for new fiction, read her manuscript and saw that it could be a best-seller. After Latham had agreed to publish the book, Mitchell worked for another six months checking the historical references and rewriting the opening chapter several times. [4] Mitchell and her husband John Marsh, a copy editor by trade, edited the final version of the novel. Mitchell wrote the book's final moments first and then wrote the events that led up to these. [5] Gone with the Wind was published in June 1936.

Title
The author tentatively titled the novel Tomorrow is Another Day , from its last line. [6] Other proposed titles included Bugles Sang True , Not in Our Stars , and Tote the Weary Load . [4] The title Mitchell finally chose is from the first line of the third stanza of the poem "Non Sum Qualis Eram Bonae sub Regno Cynarae" by Ernest Dowson :
Scarlett O'Hara uses the title phrase when she wonders to herself if her home on a plantation called " Tara " is still standing, or if it had "gone with the wind which had swept through Georgia." [8] In a general sense, the title is a metaphor for the departure of a way of life in the South prior to the Civil War. When taken in the context of Dowson's poem about "Cynara," the phrase "gone with the wind" alludes to erotic loss. [9] The poem expresses the regrets of someone who has lost his passionate feelings for his "old passion," Cynara. [10] Dowson's Cynara, a name that comes from the Greek word for artichoke, represents a lost love. [11]
The title was printed throughout the 1,037 pages of the book as shown here: Gone with the Wind, using lower-case letters for the words with and the . An upper-case W for the word with appeared in the title printed on the dust jacket where the words GONE, WITH and WIND were in capital letters in brown ink against a yellow background ( GONE WITH the WIND ), giving the title a billboard-like presentation. The title was printed in all capitals, partially italicized, on two lines in blue ink: GONE (first line), WITH THE WIND (second line), on the hardcover, which was "Confederate gray". [12]

Plot summary
Gone with the Wind takes place in the southern United States in the state of Georgia during the American Civil War (1861–1865) and the Reconstruction Era (1865–1877). The novel unfolds against the backdrop of rebellion wherein seven southern states initially, Georgia among them, have declared their secession from the United States (the "Union") and formed the Confederate States of America (the "Confederacy"), after Abraham Lincoln was elected president. The Union refuses to accept secession and no compromise is found as war approaches.

Part I
The novel opens in April 1861 at " Tara ," a plantation owned by Gerald O'Hara, an Irish immigrant who has become a successful planter, and his wife, Ellen Robillard O'Hara, from a coastal aristocratic family of French descent. Their 16-year-old daughter, Scarlett, is not beautiful, but men seldom realized it once they were caught up in her charm. [13] It was the day before the men were called to war, Fort Sumter having been fired on two days earlier.
There are brief but vivid descriptions of the South as it began and grew, with backgrounds of the main characters: the stylish and highbrow French, the gentlemanly English, the forced-to-flee and looked-down-upon Irish. Scarlett learns that one of her many beaux, Ashley Wilkes, will soon be engaged to his cousin, Melanie Hamilton. She is heart-stricken. The next day at the Wilkeses' barbecue at Twelve Oaks, Scarlett tells Ashley she loves him, and he admits he cares for her. [14] However, he knows he would not be happy if married to her because of their personality differences. She loses her temper at him, and he silently takes it.
Rhett Butler , who has a reputation as a rogue, had been alone in the library when Ashley and Scarlett entered and felt it wiser to stay unseen during the argument. Rhett applauds Scarlett for the "unladylike" spirit she displayed with Ashley. Infuriated and humiliated, she tells Rhett, "You aren't fit to wipe his boots!" [14]
After rejoining the other party guests, she learns that war has been declared and the men are going to enlist. Seeking revenge, Scarlett accepts a marriage proposal from Melanie's brother, Charles Hamilton. They marry two weeks later. Charles dies of pneumonia following the measles two months after the war begins. As a young widow, Scarlet gives birth to her first child, Wade Hampton Hamilton, named after his father's general. [15] She is bound by custom to wear black and avoid conversation with young men. Scarlett feels restricted by these conventions and bitterly misses her life as a young, unmarried woman.

Part II
Aunt Pittypat is living with Melanie in Atlanta and invites Scarlett to stay with them. In Atlanta, Scarlett's spirits revive, and she is busy with hospital work and sewing circles for the Confederate Army. Scarlett encounters Rhett Butler again at a benefit dance, where he is dressed like a dandy . [16] Although Rhett believes the war is a lost cause, he is blockade running for profit. The men must bid for a dance with a lady, and Rhett bids "one hundred fifty dollars-in gold" [16] for a dance with Scarlett. They waltz to the tune of "When This Cruel War is Over," and Scarlett sings the words. [16] [17] [18]
Others at the dance were shocked that Rhett would bid for a widow and that she would accept the dance while still wearing black (or widow's weeds). Melanie defends her, arguing she is supporting the cause for which Melanie's husband, Ashley, is fighting.
At Christmas (1863), Ashley is granted a furlough from the army. Melanie becomes pregnant with their first child.

Part III
The war is going badly for the Confederacy. By September 1864 Atlanta is besieged from three sides. [19] The city becomes desperate and hundreds of wounded Confederate soldiers pour in. Melanie goes into labor with only the inexperienced Scarlett to assist, as all the doctors are attending the soldiers. Prissy, a young slave, cries out in despair and fear, "De Yankees is comin!" [20] In the chaos, Scarlett, left to fend for herself, cries for the comfort and safety of her mother and Tara. The tattered Confederate States Army sets flame to Atlanta and abandons it to the Union Army .
Melanie gives birth to a boy, Beau, with Scarlett's assistance. Scarlett then finds Rhett and begs him to take herself, Wade, Melanie, Beau, and Prissy to Tara. Rhett laughs at the idea but steals an emaciated horse and a small wagon , and they follow the retreating army out of Atlanta.
Part way to Tara, Rhett has a change of heart and abandons Scarlett to enlist in the army (he later recounts that when they learned he had attended West Point , they put him in the artillery, which may have saved his life). Scarlett then makes her way to Tara, where she is welcomed on the steps by her father, Gerald. Things have drastically changed: Scarlett's mother is dead, her father has lost his mind with grief, her sisters are sick with typhoid fever , the field slaves left after Emancipation, the Yankees have burned all the cotton, and there is no food in the house. Scarlett avows that she and her family will survive and never be hungry again.
The long tiring struggle for survival begins that has Scarlett working in the fields. There are hungry people to feed and little food. There is the ever-present threat of the Yankees who steal and burn. At one point, a Yankee soldier trespasses on Tara, and it is implied that he would steal from the house and possibly rape Scarlett and Melanie. Scarlett kills him with Charles's pistol, and sees that Melanie had also prepared to fight him with a sword.
A long post-war succession of Confederate soldiers returning home stop at Tara to find food and rest. Eventually Ashley returns from the war, with his idealistic view of the world shattered.

Part IV
Life at Tara slowly begins to recover when new taxes are levied on the plantation. Scarlett knows only one man with enough money to help her: Rhett Butler. She looks for him in Atlanta only to learn he is in jail. Rhett refuses to give money to Scarlett, and leaving the jailhouse in fury, she runs into Frank Kennedy, who runs a store in Atlanta and is betrothed to Scarlett's sister, Suellen. Realizing Frank also has money, Scarlett hatches a plot and tells Frank that Suellen will not marry him. Frank succumbs to Scarlett's charms and he marries her two weeks later knowing he has done "something romantic and exciting for the first time in his life." [21] Always wanting her to be happy and radiant, Frank gives Scarlett the money to pay the taxes.
While Frank has a cold and is pampered by Aunt Pittypat, Scarlett goes over the accounts at Frank's store and finds that many owe him money. Scarlett is now terrified about the taxes and decides money, a lot of it, is needed. She takes control of the store, and her business practices leave many Atlantans resentful of her. With a loan from Rhett she buys a sawmill and runs it herself, all scandalous conduct. To Frank's relief, Scarlett learns she is pregnant, which curtails her "unladylike" activities for a while. She convinces Ashley to come to Atlanta and manages the mill, all the while still in love with him. At Melanie's urging, Ashley takes the job. Melanie becomes the center of Atlanta society, and Scarlett gives birth to Ella Lorena: "Ella for her grandmother Ellen, and Lorena because it was the most fashionable name of the day for girls." [22]
Georgia is under martial law , and life has taken on a new and more frightening tone. For protection, Scarlett keeps Frank's pistol tucked in the upholstery of the buggy. Her trips alone to and from the mill take her past a shantytown where criminal elements live. While on her way home one evening, she is accosted by two men who try to rob her, but she escapes with the help of Big Sam, the former Negro foreman from Tara. Attempting to avenge his wife, Frank and the Ku Klux Klan raid the shantytown whereupon Frank is shot dead. Scarlett is a widow again.
To keep the raiders from being arrested, Rhett puts on a charade. He walks into the Wilkeses' home with Hugh Elsing and Ashley, singing and pretending to be drunk. Yankee officers outside question Rhett, and he says he and the other men had been at Belle Watling's brothel that evening, a story Belle later confirms to the officers. The men are indebted to Rhett, and his Scallawag reputation among them improves a notch, but the men's wives, except Melanie, are livid at owing their husbands' lives to Belle Watling.
Frank Kennedy lies in a casket in the quiet stillness of the parlor in Aunt Pittypat's home. Scarlett is remorseful. She is swigging brandy from Aunt Pitty's swoon bottle when Rhett comes to call. She tells him tearfully, "I'm afraid I'll die and go to hell." He says, "Maybe there isn't a hell." [23] Before she can cry any further, he asks her to marry him, saying, "I always intended having you, one way or another." [23] She says she doesn't love him and doesn't want to be married again. However, he kisses her passionately, and in the heat of the moment she agrees to marry him. One year later, Scarlett and Rhett announce their engagement, which becomes the talk of the town.

Part V
Mr. and Mrs. Butler honeymoon in New Orleans , spending lavishly. Upon returning to Atlanta, they stay in the bridal suite at the National Hotel while their new home on Peachtree Street is being built. Scarlett chooses a modern Swiss chalet style home like the one she saw in Harper's Weekly , with red wallpaper, thick red carpet, and black walnut furniture. Rhett describes it as an "architectural horror". [24] Shortly after they move into their new home, the sardonic jabs between them turn into full-blown quarrels. Scarlett wonders why Rhett married her. Then "with real hate in her eyes", [24] she tells Rhett she will have a baby, which she does not want.
Wade is seven years old in 1869 when his half-sister, Eugenie Victoria , named after two queens, is born. She has blue eyes like Gerald O'Hara, and Melanie nicknames her, "Bonnie Blue," in reference to the Bonnie Blue Flag of the Confederacy.
When Scarlett is feeling well again, she makes a trip to the mill and talks to Ashley, who is alone in the office. In their conversation, she comes away believing Ashley still loves her and is jealous of her intimate relations with Rhett, which excites her. She returns home and tells Rhett she does not want more children. From then on, they sleep separately, and when Bonnie is two years old, she sleeps in a little bed beside Rhett (with the light on all night because she is afraid of the dark). Rhett turns his attention toward Bonnie, dotes on her, spoils her, and worries about her reputation when she is older.
Melanie is giving a surprise birthday party for Ashley. Scarlett goes to the mill to keep Ashley there until party time, a rare opportunity for her to see him alone. When she sees him, she feels "sixteen again, a little breathless and excited." [25] Ashley tells her how pretty she looks, and they reminisce about the days when they were young and talk about their lives now. Suddenly Scarlett's eyes fill with tears, and Ashley holds her head against his chest. Ashley sees his sister, India Wilkes, standing in the doorway. Before the party has even begun, a rumor of an affair between Ashley and Scarlett spreads, and Rhett and Melanie hear it. Melanie refuses to accept any criticism of her sister-in-law, and India Wilkes is banished from the Wilkeses' home for it, causing a rift in the family.
Rhett, more drunk than Scarlett has ever seen him, returns home from the party long after Scarlett. His eyes are bloodshot, and his mood is dark and violent. He enjoins Scarlett to drink with him. Not wanting him to know she is fearful of him, she throws back a drink and gets up from her chair to go back to her bedroom. He stops her and pins her shoulders to the wall. She tells him he is jealous of Ashley, and Rhett accuses her of "crying for the moon" [26] over Ashley. He tells her they could have been happy together saying, "for I loved you and I know you." [26] He then takes her in his arms and carries her up the stairs to her bedroom, where it is strongly implied that he rapes her—or, possibly, that they have consensual sex following the argument.
Next morning Rhett leaves for Charleston and New Orleans with Bonnie. Scarlett finds herself missing him, but she is still unsure if Rhett loves her, having said it while drunk. She learns she is pregnant with her fourth child.
When Rhett returns, Scarlett waits for him at the top of the stairs. She wonders if Rhett will kiss her, but to her irritation, he does not. He says she looks pale. She says it's because she is pregnant. He sarcastically asks if the father is Ashley. She calls Rhett a cad and tells him no woman would want his baby. He says, "Cheer up, maybe you'll have a miscarriage." [27] She lunges at him, but he dodges, and she tumbles backwards down the stairs. She is seriously ill for the first time in her life, having lost her child and broken her ribs. Rhett is remorseful, believing he has killed her. Sobbing and drunk, he buries his head in Melanie's lap and confesses he had been a jealous cad.
Scarlett, who is thin and pale, goes to Tara, taking Wade and Ella with her, to regain her strength and vitality from "the green cotton fields of home." [28] When she returns healthy to Atlanta, she sells the mills to Ashley. She finds Rhett's attitude has noticeably changed. He is sober, kinder, polite—and seemingly disinterested. Though she misses the old Rhett at times, Scarlett is content to leave well enough alone.
Bonnie is four years old in 1873. Spirited and willful, she has her father wrapped around her finger and giving into her every demand. Even Scarlett is jealous of the attention Bonnie gets. Rhett rides his horse around town with Bonnie in front of him, but Mammy insists it is not fitting for a girl to ride a horse with her dress flying up. Rhett heeds her words and buys Bonnie a Shetland pony , whom she names "Mr. Butler," and teaches her to ride sidesaddle . Then Rhett pays a boy named Wash twenty-five cents to teach Mr. Butler to jump over wood bars. When Mr. Butler is able to get his fat legs over a one-foot bar, Rhett puts Bonnie on the pony, and soon Mr. Butler is leaping bars and Aunt Melly's rose bushes.
Wearing her blue velvet riding habit with a red feather in her black hat, Bonnie pleads with her father to raise the bar to one and a half feet. He gives in, warning her not to come crying if she falls. Bonnie yells to her mother, "Watch me take this one!" [29] The pony gallops towards the wood bar, but trips over it. Bonnie breaks her neck in the fall, and dies.
In the dark days and months following Bonnie's death, Rhett is often drunk and disheveled, while Scarlett, though deeply bereaved also, seems to hold up under the strain. With the untimely death of Melanie Wilkes a short time later, Rhett decides he only wants the calm dignity of the genial South he once knew in his youth and leaves Atlanta to find it. Meanwhile, Scarlett dreams of love that has eluded her for so long. However, she still has Tara and knows she can win Rhett back, because "tomorrow is another day." [30]

Structure

Coming-of-age story
Margaret Mitchell arranged Gone with the Wind chronologically, basing it on the life and experiences of the main character, Scarlett O'Hara, as she grew from adolescence into adulthood. During the time span of the novel, from 1861 to 1873, Scarlett ages from sixteen to twenty-eight years. This is a type of Bildungsroman , [31] a novel concerned with the moral and psychological growth of the protagonist from youth to adulthood ( coming-of-age story ). Scarlett's development is affected by the events of her time. [31] Mitchell used a smooth linear narrative structure . The novel is known for its exceptional "readability". [32] The plot is rich with vivid characters .

Genre
Gone with the Wind is often placed in the literary subgenre of the historical romance novel. [33] Pamela Regis has argued that is more appropriately classified as a historical novel , as it does not contain all of the elements of the romance genre. [34] The novel has also been described as an early classic of the erotic historical genre, because it is thought to contain some degree of pornography. [35]

Plot discussion

Slavery
Slavery in Gone with the Wind is a backdrop to a story that is essentially about other things. [37] Southern plantation fiction (also known as Anti-Tom literature , in reference to reactions to Harriet Beecher Stowe's anti-slavery novel, Uncle Tom's Cabin of 1852) from the mid-19th century, culminating in Gone With the Wind , is written from the perspective and values of the slaveholder and tends to present slaves as docile and happy. [38]

Caste system
The characters in the novel are organized into two basic groups along class lines: the white planter class, such as Scarlett and Ashley, and the black house servant class. The slaves depicted in Gone with the Wind are primarily loyal house servants, such as Mammy, Pork, Prissy, and Uncle Peter. [39] House servants are the highest "caste" of slaves in Mitchell's caste system. [40] They choose to stay with their masters after the Emancipation Proclamation of 1863 and subsequent Thirteenth Amendment of 1865 sets them free. Of the servants who stayed at Tara, Scarlett thinks, "There were qualities of loyalty and tirelessness and love in them that no strain could break, no money could buy." [41]
The field slaves make up the lower class in Mitchell's caste system. [40] [42] The field slaves from the Tara plantation and the foreman, Big Sam, are taken away by Confederate soldiers to dig ditches [19] and never return to the plantation. Mitchell wrote that other field slaves were "loyal" and "refused to avail themselves of the new freedom", [40] but the novel has no field slaves who stay on the plantation to work after they have been emancipated.
American William Wells Brown escaped from slavery and published his memoir, or slave narrative , in 1847. He wrote of the disparity in conditions between the house servant and the field hand:

Faithful and devoted slave
Although the novel is more than 1,000 pages long, the character of Mammy never considers what her life might be like away from Tara. [44] She recognizes her freedom to come and go as she pleases, saying, "Ah is free, Miss Scarlett. You kain sen' me nowhar Ah doan wanter go," but Mammy remains duty-bound to "Miss Ellen's chile." [23] (No other name for Mammy is noted in the novel.)
Eighteen years before the publication of Gone with the Wind , an article titled, "The Old Black Mammy," written in the Confederate Veteran in 1918, discussed the romanticized view of the mammy character that had persisted in Southern literature:
Micki McElya, in her book Clinging to Mammy , suggests the myth of the faithful slave, in the figure of Mammy, lingered because white Americans wished to live in a world in which African Americans were not angry over the injustice of slavery. [47]
The best-selling anti-slavery novel from the 19th century is Uncle Tom's Cabin by Harriet Beecher Stowe , published in 1852. Uncle Tom's Cabin is mentioned briefly in Gone with the Wind as being accepted by the Yankees as "revelation second only to the Bible". [41] The enduring interest of both Uncle Tom's Cabin and Gone with the Wind has resulted in lingering stereotypes of 19th-century African-American slaves. [48] Gone with the Wind has become a reference point for subsequent writers about the South, both black and white alike. [49]

Southern belle
The southern belle is an archetype for a young woman of the antebellum American South upper class. The southern belle was believed to be physically attractive but, more importantly, personally charming with sophisticated social skills. She is subject to the correct code of female behavior. [51] The novel's heroine, Scarlett O'Hara, charming though not beautiful, is a classic southern belle.
For young Scarlett, the ideal southern belle is represented by her mother, Ellen O'Hara. In "A Study in Scarlett", published in The New Yorker , Claudia Roth Pierpont wrote:
However, Scarlett is not always willing to conform. Kathryn Lee Seidel, in her book, The Southern Belle in the American Novel , wrote:
The figure of a pampered southern belle, Scarlett lives through an extreme reversal of fortune and wealth, and survives to rebuild Tara and her self-esteem. [54] Her bad belle traits, Scarlett's deceitfulness, shrewdness, manipulation, and superficiality, in contrast to Melanie's good belle traits, trust, self-sacrifice, and loyalty, enable her to survive in the post-war South, and pursue her main interest, to make enough money to survive and prosper. [55] Although Scarlett was "born" around 1845, she is portrayed to appeal to modern-day readers for her passionate and independent spirit, determination and obstinate refusal to feel defeated. [56]

Historical background
Marriage was supposed to be the goal of all southern belles, as women's status was largely determined by that of their husbands. All social and educational pursuits were directed towards it. Despite the Civil War and loss of a generation of eligible men, young ladies were still expected to marry. [57] By law and Southern social convention, household heads were adult, white propertied males, and all white women and all African Americans were thought to require protection and guidance because they lacked the capacity for reason and self-control. [58]
The Atlanta Historical Society has produced a number of Gone with the Wind exhibits, among them a 1994 exhibit titled, "Disputed Territories: Gone with the Wind and Southern Myths". The exhibit asked, "Was Scarlett a Lady?", finding that historically most women of the period were not involved in business activities as Scarlett was during Reconstruction, when she ran a sawmill. White women performed traditional jobs such as teaching and sewing, and generally disliked work outside the home. [59]
During the Civil War, Southern women played a major role as volunteer nurses working in makeshift hospitals. Many were middle- and upper class women who had never worked for wages or seen the inside of a hospital. One such nurse was Ada W. Bacot, a young widow who had lost two children. Bacot came from a wealthy South Carolina plantation family that owned 87 slaves. [60]
In the fall of 1862, Confederate laws were changed to permit women to be employed in hospitals as members of the Confederate Medical Department. [61] Twenty-seven-year-old nurse Kate Cumming from Mobile, Alabama, described the primitive hospital conditions in her journal:

Battles
The Civil War came to an end on April 26, 1865 when Confederate General Johnston surrendered his armies in the Carolinas Campaign to Union General Sherman . Several battles are mentioned or depicted in Gone with the Wind .

Early and mid war years

Atlanta Campaign
The Atlanta Campaign (May–September 1864) took place in northwest Georgia and the area around Atlanta.
Confederate General Johnston fights and retreats from Dalton (May 7–13) [19] to Resaca (May 13–15) to Kennesaw Mountain (June 27). Union General Sherman suffers heavy losses to the entrenched Confederate army. Unable to pass through Kennesaw, Sherman swings his men around to the Chattahoochee River where the Confederate army is waiting on the opposite side of the river. Once again, General Sherman flanks the Confederate army, forcing Johnston to retreat to Peachtree Creek (July 20), five miles northeast of Atlanta.

March to the Sea
The Savannah Campaign was conducted in Georgia during November and December 1864.

President Lincoln's murder
Although Abraham Lincoln is mentioned in the novel fourteen times, no reference is made to his assassination on April 14, 1865.

Beau ideal
Ashley Wilkes is the beau ideal of Southern manhood. A planter by inheritance, Ashley knew the Confederate cause had died at the conclusion of the American Civil War . [67] Ashley's name signifies paleness. His "pallid skin literalizes the idea of Confederate death." [68]
He contemplates leaving Georgia for New York City. Had he gone North, he would have joined numerous other Confederate carpetbaggers there. [67] Ashley, embittered by war, tells Scarlett he has been "in a state of suspended animation" since the surrender. [69] He feels he is not "shouldering a man's burden" at Tara and believes he is "much less than a man—much less, indeed, than a woman". [69]
A "young girl's dream of the Perfect Knight", [70] Ashley is like a young girl himself. [71] With his "poet's eye", [72] Ashley has a "feminine sensitivity". [73] Scarlett is angered by the "slur of effeminacy flung at Ashley" when her father tells her the Wilkes family was "born queer". [74] (Mitchell's use of the word "queer" is for its sexual connotation because queer , in the 1930s, was associated with homosexuality.) [75] Ashley's effeminacy is associated with his appearance, his lack of forcefulness, and sexual impotency. [76] He rides, plays poker, and drinks like "proper men", but his heart is not in it, Gerald claims. [74] [77] The embodiment of castration, Ashley wears the head of Medusa on his cravat pin. [74] [75]
Scarlett's love interest, Ashley Wilkes, lacks manliness, and her husbands—the "calf-like" [14] Charles Hamilton, and the "old-maid in britches", [14] Frank Kennedy—are unmanly as well. Mitchell is critiquing masculinity in southern society since Reconstruction. [78] Even Rhett Butler, the well-groomed dandy, [79] is effeminate or "gay-coded." [80] Charles, Frank and Ashley represent the impotence of the post-war white South. [68] Its power and influence has been diminished.

Scallawag
The word "scallawag" is defined as a loafer, a vagabond, or a rogue. [81] Scallawag had a special meaning after the Civil War as an epithet for a white Southerner who accepted and supported Republican reforms. [82] Mitchell defines Scallawags as "Southerners who had turned Republican very profitably." [83] Rhett Butler is accused of being a "damned Scallawag." [84] In addition to scallawags, Mitchell portrays other types of scoundrels in the novel: Yankees, carpetbaggers , Republicans, prostitutes, and overseers. In the early years of the Civil War, Rhett is called a "scoundrel" for his "selfish gains" profiteering as a blockade-runner. [85]
As a scallawag, Rhett is despised. He is the "dark, mysterious, and slightly malevolent hero loose in the world". [86] Literary scholars have identified elements of Mitchell's first husband, Berrien "Red" Upshaw, in the character of Rhett. [86] Another sees the image of Italian actor Rudolph Valentino , whom Margaret Mitchell interviewed as a young reporter for The Atlanta Journal . [87] [88] Fictional hero Rhett Butler has a "swarthy face, flashing teeth and dark alert eyes". [89] He is a "scamp, blackguard, without scruple or honor." [89]

Dark sexuality
The most passionate and virile character in the novel is Rhett, with whom Margaret Mitchell associates "dark sexuality" and the "black devil." [91] Mitchell's romantic hero is colored—portrayed in blacks and browns. Rhett's symbolic dark image is placed within the context of two other images: the mythic black rapist and the dark-skinned Arab sheik played by screen idol Rudolph Valentino in the film, The Sheik (1921). By portraying Rhett in this manner, Mitchell plays upon racial anxieties and sexual fantasies of the South. [92] Rhett's demons are prostitutes and liquor, as demonstrated by his intimacy with Belle Watling, in whose brothel he often resides, [86] and his bouts of drunkenness. The "black beast rapist" is associated with liquor. [93] Rhett is a "terrifying faceless black bulk" when he appears before Scarlett in a drunken jealous rage on the night of Ashley's party. He shows Scarlett his "large brown hands" and says, "I could tear you to pieces with them." [26] [93]
With Rhett's "swarthy face" [89] juxtaposed against Scarlett's "magnolia-white skin," [13] the two white protagonists are a metaphor for an interracial couple. Their romance crosses racial boundaries in a paradox to the usual white man and black or multi-racial woman, upending racial stereotypes. [94]
Rhett and Scarlett's bedroom scene (Chapter 54) is often read as a rape that was meant to suggest Reconstruction fear of black-on-white rape in the South. [91] Since the late 20th century, critics have suggested the book was built around rape fantasies. [95] [96] In one interpretation of the scene, the "dandified dangerous lover" carries Scarlett up the stairs into her first encounter with the erotic. [97] In another interpretation, a marital rape occurs. [98]

Characters

Main characters

Minor characters

Scarlett's immediate family

Tara

Clayton County

Atlanta

Robillard family

Themes

Survival

Color symbolism
Mitchell's use of color in the novel is symbolic and open to interpretation. Red, green, and a variety of hues of each of these colors, are the predominant palette of colors related to Scarlett. [118] She is also linked to white by the color of her skin. Symbolically, red and green have been broadly defined to mean "vitality" (red) and "rebirth" (green). [118] Mitchell interwove the two colors into her description of the Tara plantation: "red fields with springing green cotton". [15] The red fields are "blood-colored after rains". [13] The whitewashed brick plantation house is virtually nondescript by comparison to the plantation fields and sits like an island in a sea of red. [13] In springtime, the lawn around the plantation house turns emerald green. [50]
For the Irish and others, green in the novel represents Mitchell's commemoration of her "Green Irish heritage." Gerald O'Hara pridefully sings "The Wearin 'o the Green". [107] [119] Scarlett's green-coded Irish identity is the strength that ensures she will thrive post-war. [120] Rhett likens Scarlett's strength to the mythological figure Antaeus , who stays strong only when he is in contact with his Mother Earth. [28] Scarlett's mythical mother is Tara. [121]
Scarlett is not all green; her name suggests the "erotically-charged color" red. The only openly "scarlet woman" in the novel is the red-headed Belle Watling, [122] whose hair is "too red to be true". [114] Mammy is reluctant to reveal her red petticoat to Rhett; [24] nevertheless, she has sexual knowledge akin to Belle Watling. [123] Scarlett, whom Mitchell pits against the war, "prostitutes" herself to pay the taxes on Tara. By her name, Scarlett evokes emotions and images of the color scarlet: "blood, passion, anger, sexuality, madness". [124]

Reception

Reviews
The sales of Margaret Mitchell's novel in the summer of 1936, deep in the Great Depression and at the virtually unprecedented high price of three dollars, reached about one million by the end of December. [32] The book was a bestseller by the time reviews began to appear in national magazines. [5] Herschel Brickell, a critic for the New York Evening Post , lauded Mitchell for the way she "tosses out the window all the thousands of technical tricks our novelists have been playing with for the past twenty years." [125]
Ralph Thompson, a book reviewer for The New York Times , was critical of the length of the novel, and wrote in June 1936:

Racial, ethnicity and social issues
Gone with the Wind has been criticized for its stereotypical and derogatory portrayal of African Americans in the 19th century South . [128] Former field hands during the early days of Reconstruction are described behaving "as creatures of small intelligence might naturally be expected to do. Like monkeys or small children turned loose among treasured objects whose value is beyond their comprehension, they ran wild—either from perverse pleasure in destruction or simply because of their ignorance." [40]
Commenting on this passage of the novel, Jabari Asim , author of The N Word: Who Can Say It, Who Shouldn't, and Why , says it is, "one of the more charitable passages in Gone With the Wind , Margaret Mitchell hesitated to blame black 'insolence' [40] during Reconstruction solely on 'mean niggers', [40] of which, she said, there were few even in slavery days." [129]
Critics say that Mitchell downplayed the violent role of the Ku Klux Klan and their abuse of freedmen. Author Pat Conroy , in his preface to a later edition of the novel, describes Mitchell's portrayal of the Ku Klux Klan as having "the same romanticized role it had in The Birth of a Nation and appears to be a benign combination of the Elks Club and a men's equestrian society". [130]
Regarding the historical inaccuracies of the novel, historian Richard N. Current points out:
In Gone with the Wind , Mitchell explores some complexities in racial issues. Scarlett was asked by a Yankee woman for advice on who to appoint as a nurse for her children; Scarlett suggested a "darky", much to the disgust of the Yankee woman who was seeking an Irish maid, a "Bridget". [41] African Americans and Irish Americans are treated "in precisely the same way" in Gone with the Wind , writes David O'Connell in his 1996 book, The Irish Roots of Margaret Mitchell's Gone With the Wind . Ethnic slurs on the Irish and Irish stereotypes pervade the novel, O'Connell claims, and Scarlett is not an exception to the terminology. [132] Irish scholar Geraldine Higgins notes that Jonas Wilkerson labels Scarlett: "you highflying, bogtrotting Irish". [133] Higgins says that, as the Irish American O'Haras were slaveholders and African Americans were held in bondage, the two ethnic groups are not equivalent in the ethnic hierarchy of the novel. [134]
The novel has been criticized for promoting plantation values. Mitchell biographer Marianne Walker, author of Margaret Mitchell and John Marsh: The Love Story Behind Gone with the Wind , believes that those who attack the book on these grounds have not read it. She said that the popular 1939 film "promotes a false notion of the Old South". Mitchell was not involved in the screeplay or film production. [135]
James Loewen , author of Lies My Teacher Told Me: Everything Your American History Textbook Got Wrong , says this novel should be taught in schools. [128] In 1984, an alderman in Waukegan, Illinois , challenged the book's inclusion on the reading list of the Waukegan School District on the grounds of "racism" and "unacceptable language." He objected to the frequent use of the racial slur nigger . He also objected to several other books: The Nigger of the 'Narcissus' , Uncle Tom's Cabin , and Adventures of Huckleberry Finn for the same reason. [136]

Awards and recognition
In 1937, Margaret Mitchell received the Pulitzer Prize for Fiction for Gone with the Wind and the second annual National Book Award from the American Booksellers Association. [137] It is ranked as the second favorite book by American readers, just behind the Bible, according to a 2008 Harris Poll . [138] The poll found the novel has its strongest following among women, those aged 44 or more, both Southerners and Midwesterners, both whites and Hispanics, and those who have not attended college. In a 2014 Harris poll, Mitchell's novel ranked again as second, after the Bible. [139] The novel is on the list of best-selling books . As of 2010, more than 30 million copies have been printed in the United States and abroad. [140] More than 24 editions of Gone with the Wind have been issued in China. [140] TIME magazine critics, Lev Grossman and Richard Lacayo , included the novel on their list of the 100 best English-language novels from 1923 to the present (2005). [141] [142] In 2003 the book was listed at number 21 on the BBC's The Big Read poll of the UK's "best-loved novel." [143]

Adaptations
Gone with the Wind has been adapted several times for stage and screen:

In popular culture
Gone with the Wind has appeared in many places and forms in popular culture :

Books, television and more

Collectibles
A wide array of collectibles related to both the novel and film are available for purchase, especially on auction websites. Items include vintage lamps, figurines, matchbooks, cookbooks, collector plates and various editions of the novel.
On June 30, 1986, the 50th anniversary of the day Gone with the Wind went on sale, the U.S. Post Office issued a 1-cent stamp showing an image of Margaret Mitchell. The stamp was designed by Ronald Adair and was part of the U.S. Postal Service's Great Americans series . [161]
On September 10, 1998, the U.S. Post Office issued a 32-cents stamp as part of its Celebrate the Century series recalling various important events in the 20th century. The stamp, designed by Howard Paine, displays the book with its original dust jacket , a white Magnolia blossom, and a hilt placed against a background of green velvet. [161]
To commemorate the 75th anniversary (2011) of the publication of Gone with the Wind in 1936, Scribner published a paperback edition featuring the book's original jacket art. [162]

The Windies
The Windies are ardent Gone with the Wind fans who follow all the latest news and events surrounding the book and film. They gather periodically in costumes from the film or dressed as Margaret Mitchell. Atlanta, Georgia is their mecca. [163]

Legacy
One story of a the legacy of Gone with the Wind is that people worldwide incorrectly think it was the "true story" of the Old South and how it was changed by the American Civil War and Reconstruction. The film adaptation of the novel "amplified this effect." [164] The plantation legend was "burned" into the mind of the public through Mitchell's vivid prose. [165] Moreover, her fictional account of the war and its aftermath has influenced how the world has viewed the city of Atlanta for successive generations. [166]
Some readers of the novel have seen the film first and read the novel afterward. One difference between the film and the novel is the staircase scene, in which Rhett carries Scarlett up the stairs. In the film, Scarlett weakly struggles and does not scream as Rhett starts up the stairs. In the novel, "he hurt her and she cried out, muffled, frightened." [26] [98]
Earlier in the novel, in an intended rape at Shantytown (Chapter 44), Scarlett is attacked by a black man who rips open her dress while a white man grabs hold of the horse's bridle. She is rescued by another black man, Big Sam. [91] In the film, she is attacked by a white man, while a black man grabs the horse's bridle.
The Library of Congress began a multiyear "Celebration of the Book" in July 2012 with an exhibition on Books That Shaped America , and an initial list of 88 books by American authors that have influenced American lives. Gone with the Wind was included in the Library's list. Librarian of Congress, James H. Billington said:
Among books on the list considered to be the Great American Novel were Moby-Dick , Adventures of Huckleberry Finn , The Great Gatsby , The Grapes of Wrath , The Catcher in the Rye , Invisible Man , and To Kill a Mockingbird .
Throughout the world, the novel appeals due to its universal themes: war, love, death, racial conflict, class, gender and generation, which speak especially to women. [168] In North Korea, readers relate to the novel's theme of survival, finding it to be "the most compelling message of the novel". [169] The China Post claimed that Mitchell's first choice of a title for the book was "Tote Your Heavy Bag." [170] Margaret Mitchell's personal collection of nearly 70 foreign language translations of her novel was given to the Atlanta Public Library after her death. [171]
On August 16, 2012, the Archdiocese of Atlanta announced that it had been bequeathed a 50% stake in the trademarks and literary rights to Gone With the Wind from the estate of Margaret Mitchell's deceased nephew, Joseph Mitchell . One of Mitchell's biographers, Darden Asbury Pyron, stated that Margaret Mitchell had "an intense relationship" with her mother, who was a Roman Catholic.
Margaret Mitchell had separated from the Catholic Church. [172]

Original manuscript
Although some of Mitchell's papers and documents related to the writing of Gone with the Wind were burned after her death, many documents, including assorted draft chapters, were preserved. [173] The last four chapters of the novel are held by the Pequot Library of Southport, Connecticut . [174]

Publication and reprintings (1936-USA)
The first printing of 10,000 copies contains the original publication date: "Published May, 1936". After the book was chosen as the Book-of-the-Month's selection for July, publication was delayed until June 30. The second printing of 25,000 copies (and subsequent printings) contains the release date: "Published June, 1936." The third printing of 15,000 copies was made in June 1936. Additionally, 50,000 indistinguishable copies were printed for the Book-of-the-Month Club July selection. Gone with the Wind was officially released to the American public on June 30, 1936. [175] A summary table of printings for 1936 is shown below:

Sequels and prequels
Although Mitchell refused to write a sequel to Gone with the Wind , Mitchell's estate authorized Alexandra Ripley to write a sequel, which was titled Scarlett . [178] The book was subsequently adapted into a television mini-series in 1994. [179] A second sequel was authorized by Mitchell's estate titled Rhett Butler's People , by Donald McCaig . [180] The novel parallels Gone With the Wind from Rhett Butler's perspective. In 2010, Mitchell's estate authorized McCaig to write a prequel, which follows the life of the house servant Mammy, whom McCaig names "Ruth". The novel, Ruth's Journey , was released in 2014. [181]
The copyright holders of Gone with the Wind attempted to suppress publication of The Wind Done Gone by Alice Randall , [182] which retold the story from the perspective of the slaves. A federal appeals court denied the plaintiffs an injunction ( Suntrust v. Houghton Mifflin ) against publication on the basis that the book was parody and therefore protected by the First Amendment . The parties subsequently settled out of court and the book went on to become a New York Times Best Seller .
A book sequel unauthorized by the copyright holders, The Winds of Tara by Katherine Pinotti, [183] was blocked from publication in the United States. The novel was republished in Australia, avoiding U.S. copyright restrictions.
Numerous unauthorized sequels to Gone with the Wind have been published in Russia, mostly under the pseudonym Yuliya Hilpatrik, a cover for a consortium of writers. The New York Times states that most of these have a "Slavic" flavor. [184]
Several sequels were written in Hungarian under the pseudonym Audrey D. Milland or Audrey Dee Milland, by at least four different authors (who are named in the colophon as translators to make the book seem a translation from the English original, a procedure common in the 1990s but prohibited by law since then). The first one picks up where Ripley's Scarlett ended, the next one is about Scarlett's daughter Cat. Other books include a prequel trilogy about Scarlett's grandmother Solange and a three-part miniseries of a supposed illegitimate daughter of Carreen. [185]

See also
WebPage index: 00007
Marawi
Marawi ( Maranaoan : Inged san Marawi , Filipino : Lungsod Islamiko ng Marawi ), often referred to as Marawi City , is the capital city of the province of Lanao del Sur on the island of Mindanao in the Philippines . According to the 2015 census, it has a population of 201,785. [3]
The people of Marawi are called the Maranaos and speak the Maranao language . They are named after Lake Lanao , which is called Meranau in the language, upon whose shores Marawi lies. The city is also called the Summer Capital of the South because of its higher elevation and cooler climate [4] — a nickname it shares with Malaybalay , which legally holds the title.

History
Marawi was founded as Dansalan in 1639 by the Spaniards led by Francisco Atienza who came from Iligan and were attempting to conquer the entire Lake Lanao area. However, it was abandoned later the same year when thousands of Maranao warriors invested the then-fortifying settlement, pressing the Spaniards hard and thus they returned to Iligan, having failed in their quest. [5] The Spaniards only returned to the area when they began the conquest of the Sultanate of Maguindanao in late 19th century, only to be abandoned once again when the Americans came there in 1900. It served as the capital of the undivided Lanao province from 1907 to 1940. Dansalan in Meranaw is a place where ships berth - a port of entry.
A Tribal leader of Marawi before Spanish colonization was "Datu Buadi Sa Kayo". He collected taxes in his era.
According to the late well-known Meranaw scholar Dr. Mamitua Saber, Marawi City got its charter in 1940. The granting of a charter to the old Dansalan municipality was jointly conceived by the Philippine Commonwealth President Manuel L. Quezon and Assemblyman, later Senator, Tomas L. Cabili . The changing of the official name from Dansalan to Marawi was through Congressional amendment of the Charter in 1956 sponsored by Sen. Domocao Alonto ( nl ) . This is embodied by Republic Act No. 1552 dated June 16, 1956.
The renaming of the city as "Islamic City of Marawi" was proposed by Parliamentary Bill No. 261 in the defunct Batasang Pambansa , the former parliament of the Philippines during the Marcos regime, reportedly to attract funds from the Middle East . [4]

2017 clashes
On May 23, 2017, a pro- Islamic State group called Maute group attacked the city. The battle is ongoing as of May 25. CNN reports that the militants have about 500 men. [6] Philippine President Duterte has declared Martial Law on the island of Mindanao, where the fighting is taking place. [7]

Traditional Royal Sultans of Marawi City

Geography
Marawi City has a total land area of 8,755 hectares (21,630 acres). [1] It is located on the shores of Lake Lanao and straddles the area where the Agus River starts. It is bounded to the north by the municipalities of Kapai and Saguiaran ; to the south by Lake Lanao; to the east by the municipalities of Bubong and Ditsaan-Ramain ; and to the west by the municipalities of Marantao and Saguiaran. [4]

Topography
Mountains, rolling hills, valleys and a large placid lake dominate the city's landscape. Angoyao Hills ( Brgy. Sogod) served as natural viewpoint over the water of the Lake Lanao. Signal Hill (Brgy. Matampay), Arumpac Hill (Brgy. Saduc) and Mt. Mupo (Brgy. Guimba) are considered beautiful but mysterious. Mt. Mupo, located within the Sacred Mountain National Park , is known for its untouched trees and beautiful, perfect cone. [ citation needed ]

Barangays
Marawi City is politically subdivided into 96 barangays . [8]

Mayors after People Power Revolution 1986

Vice Mayors after People Power Revolution 1986

Prominent families involved in local politics

Climate
Due to its high elevation, with the elevation along Lake Lanao at around 2,300 feet (700 m), [2] Marawi's weather is cool (in tropical terms) and pleasant, distinguished by an even distribution of rainfall throughout the year.

Demographics

Language
Maranao or Meranaw is widely spoken in Marawi City, however, local inhabitants can also speak Filipino , Cebuano , English and Arabic .

Religion
Marawi City is predominantly Muslim city which accounts for 99.6% of the population. [4] Sharia criminal law exists but without stoning, amputations, flagellations or other Islamic punishments as they are against the law of the Philippines. The distribution of alcoholic products and gambling is forbidden and women must cover their heads, though non-Muslims are exempted from this rule. Other than sharia law in personal matters, these laws are not applicable elsewhere in Lanao del Sur.

Economy
The economy of Marawi City is largely based on agriculture, trading and exporting. Most industries in the city are agriculture-oriented. They include rice and corn farming, hollow blocks manufacturing, goldsmithing , and saw milling . Small and cottage-size enterprises are engaged in garment making, mat and malong weaving, wood carving , brassware making, web development , and blacksmithing . [4]
Apart from that, Marawi City is home to NPC - Agus 1 Hydro Electric Power Plant and the first of the six cascading Agus Hydro Power Plants.

Points of interest

Architecture
The feeling of the unique natural setting of the Maranaos in Marawi City is manifested by the presence of many large Torogans , an antique royal high-roofed houses with carvings designed by the Meranau, and Amai Sambitory old Buildings in Barrio Naga in front of Tuaka Laput, Marawi City.

Education
Marawi is home to the main campus of Mindanao State University , the biggest state university in Mindanao and next to University of the Philippines. Other institutions/colleges are well established in the city and are as follows:
Other notable secondary schools are:
TESDA is also stationed in Marawi City which caters to technical training of students for the province.

Neighbor Cities and Sister Cities

See also
WebPage index: 00008
2017 Indian Premier League
The 2017 season of the Indian Premier League , also known as IPL 10 , was the tenth edition of the IPL, a professional Twenty20 cricket league established by the BCCI in 2007. The tournament featured the eight teams that played in the previous season . The 2017 season started on 5 April 2017 and finished on 21 May 2017, with Hyderabad hosting the opening match and the final. Mumbai Indians won by 1 run against Rising Pune Supergiant in the final , winning their third title. [1] Krunal Pandya of Mumbai Indians was declared man of the match.
Sunrisers Hyderabad captain David Warner won the Orange Cap for the leading run-scorer of the tournament with 641 runs. Bhuvneshwar Kumar , also of Sunrisers Hyderabad, was awarded the Purple Cap for finishing as the leading wicket-taker of the tournament with 26 wickets. Rising Pune Supergiant's Ben Stokes was named Most Valuable Player, also known as Man of the Series, while Basil Thampi of Gujarat Lions was named the Emerging Player of the Tournament.

Format
Eight teams were contesting the season. The schedule for the tournament was published on 15 February 2017. [2] The league stage, consisting of 56 matches, took place between 5 April and 14 May 2017. The top four teams qualified for the play-offs, with the final held in Hyderabad on 21 May. [1]

Venues
Ten venues were selected to host the matches. The opening match and the final were played at the Rajiv Gandhi International Cricket Stadium in Hyderabad. [2]

Personnel changes
The retention lists for the season were announced in December 2016. [3] On 3 February, the BCCI announced that the player auction would be held on 20 February 2017 in Bangalore with a total of 799 players registered for it. [4] On 14 February, the IPL Desk released a list of 351 players. [5] Out of the 351 players shortlisted, 66 players were sold at the 2017 IPL Auction. [6] [7] [8]

Opening ceremonies
Unlike the previous IPL seasons each of which had a single opening ceremony, the 2017 season featured opening ceremonies at each venue before the start of the first match at the venue. [9] [10] The ceremonies included performances by Amy Jackson (at Hyderabad); [11] Shalmali Kholgade and Riteish Deshmukh (at Pune); [12] Bhoomi Trivedi , Sachin-Jigar and Tiger Shroff (at Rajkot); [13] Harshdeep Kaur and Disha Patani (at Indore); [14] Benny Dayal and Kriti Sanon (at Bangalore); [15] Sushant Singh Rajput and Malaika Arora (at Mumbai); [16] Shillong Chamber Choir , Monali Thakur and Shraddha Kapoor (at Kolkata); [17] Raftaar , Yami Gautam and Guru Randhawa (at Delhi). [18]

Teams and standings
Source: ESPNcricinfo [19]

Match summary

League progression in IPL

League stage

Match results

Playoffs

Preliminary

Final

Statistics

Most runs

Most wickets

See also
WebPage index: 00009
President of Iran
The President of Iran ( Persian : رییس‌جمهور ایران Rayis Jomhur Irān ) is the head of government of the Islamic Republic of Iran . The President is the highest popularly elected official in Iran, although the President carries out the decrees, and answers to the Supreme Leader of Iran , who functions as the country's head of state . [2] [3] Unlike the executive in other countries, the President of Iran does not have full control over anything, as these are ultimately under the control of the Supreme Leader . [4] [5] Chapter IX of the Constitution of the Islamic Republic of Iran sets forth the qualifications for presidential candidates. The procedures for presidential election and all other elections in Iran are outlined by the Supreme Leader. [6] [7] The President functions as the executive of the decrees and wishes of the Supreme Leader. These include signing treaties and other agreements with foreign countries and international organizations, with Supreme Leader's approval; administering national planning, budget, and state employment affairs, as decreed by the Supreme Leader. [8] [9] [10] [11] [12] [13] [14] [15] [16] The President also appoints the ministers, subject to the approval of Parliament , and the Supreme Leader who can dismiss or reinstate any of the ministers at any time, regardless of the president or parliament's decision. [17] [18] [19]
As such, the current long-time Supreme Leader Ali Khamenei , who has been ruling for nearly three decades, has been issuing decrees and making the final decisions on economy, environment, foreign policy, national planning such as population growth, and everything else in Iran. [8] [9] [11] [12] [13] [20] [21] Khamenei also makes the final decisions on the amount of transparency in elections in Iran, [22] and has fired and reinstated Presidential cabinet appointments. [17] [18]
The President of Iran is elected for a four-year term by the direct vote of the people and may not serve for more than two consecutive terms or more than 8 years.
The current President of Iran is Hassan Rouhani , who assumed office on August 3, 2013, after the 2013 Iranian presidential election . He succeeded Mahmoud Ahmadinejad , who served 8 years in office from 2005 to 2013. Rouhani won re-election in the 2017 presidential election .

Background
After the Iranian Revolution of 1979 and referendum to create the Islamic Republic on March 29 and 30, the new government needed to craft a new constitution. Ayatollah Ruhollah Khomeini , ordered an election for the Assembly of Experts , the body tasked with writing the constitution. [23] The assembly presented the constitution on October 24, 1979, and Supreme Leader Ruhollah Khomeini and Prime Minister Mehdi Bazargan approved it.
The 1979 Constitution designated the Supreme Leader of Iran as the head of state and the President and Prime Minister as the heads of government. The post of Prime Minister was abolished in 1989.
The first Iranian presidential election was held on January 25, 1980 and resulted in the election of Abulhassan Banisadr with 76% of the votes. Banisadr was impeached on June 22, 1981 by Parliament . Until the early election on July 24, 1981 , the duties of the President were undertaken by the Provisional Presidential Council. Mohammad-Ali Rajai was elected President on July 24, 1981 and took office on August 2. Rajai was in office for less than one month because he and his prime minister were both assassinated. Once again a Provisional Presidential Council filled the office until October 13, 1981 when Ali Khamenei was elected president.
The election on August 3, 2005 resulted in a victory for Mahmoud Ahmadinejad . The election on June 12, 2009 was reported by government authorities as a victory for Mahmoud Ahmadinejad, the incumbent candidate, although this is greatly disputed by supporters of rival candidates, who noted the statistical anomalies in voting reports and large-scale overvoting in the officially announced tallies. [24]
Ali Khamenei , Akbar Hashemi Rafsanjani , Mohammad Khatami , Mahmoud Ahmadinejad and Hassan Rouhani were each elected president for two terms.

Qualifications and election
The procedures for presidential election and all other elections in Iran are outlined by the Supreme Leader. [25] The President of Iran is elected for a four-year term in a national election by universal adult suffrage for everyone of at least 18 years of age. [26] Candidates for the presidency must be approved by the Council of Guardians , a twelve-member body consisting of six clerics (selected by Iran's Supreme Leader ) and six lawyers (proposed by the Supreme Leader-appointed head of Iran's judicial system, and voted in by the Parliament). [27] According to the Constitution of Iran candidates for the presidency must possess the following qualifications:
Within these guidelines the Council vetoes candidates who are deemed unacceptable. The approval process is considered to be a check on the president's power, and usually amounts to a small number of candidates being approved. In the 1997 election , for example, only four out of 238 presidential candidates were approved by the council. Western observers have routinely criticized the approvals process as a way for the Council and Supreme Leader to ensure that only conservative and like-minded Islamic fundamentalists can win office. However, the council rejects the criticism, citing approval of so-called reformists in previous elections. The council rejects most of the candidates stating that they are not "a well-known political figure", a requirement by the current law.
The President must be elected with a simple majority of the popular vote. If no candidate receives a majority in the first round, a runoff election is held between the top two candidates.

Presidential council
According to the Iranian constitution , when the President dies or is impeached, a special provisional Presidential Council temporarily rules in his place until an election can be held. The President automatically becomes the Head of the Supreme National Security Council and the Head of the Supreme Council of Cultural Revolution .

Powers and responsibilities
Unlike the executive in other countries, the President of Iran does not have full control over anything, as these are ultimately under the control of the Supreme Leader . [30] [31] The President functions as the executive of the decrees and wishes of the Supreme Leader. [8] [9] [11] [12] [13] [32] [33] [34] [35] The President also appoints the ministers, subject to the approval of Parliament , and the Supreme Leader who can dismiss or reinstate any of the ministers at any time, regardless of the president or parliament's decision. [17] [18] [36]
The President's duties include the following, subject to supervision and approval by the Supreme Leader:

Oath of office

Commentary on the presidency in constitution
Unlike the executive in other countries, the President of Iran does not have full control over anything, as these are ultimately under the control of the Supreme Leader . [37] [38] The President functions as the executive of the decrees and wishes of the Supreme Leader. [8] [9] [11] [39] [12] [13] [40] [41] [42]
The President also appoints the ministers, subject to the approval of Parliament , and the Supreme Leader who can dismiss or reinstate any of the ministers at any time, regardless of the president or parliament's decision. [17] [18] [43]
The procedures for presidential election and all other elections in Iran are outlined by the Supreme Leader. [44] [45]
TIME Magazine noted that presidential elections in Iran change nothing as Supreme Leader Khamenei — and not the President — wields the ultimate power. [46] Tallha Abdulrazaq, an Iraqi researcher at the ​ University of Exeter 's​ ​Strategy​ ​and​ ​Security​ ​Institute, stated that Khamenei, the longtime Supreme Leader of Iran, always uses the president as a kind of a buffer zone between him and the people. “Anything that goes right, Khamenei then can say 'I am the wise leader who put this guy in charge and he made the right policy decisions.' Anything that goes wrong, he can say ‘we should get rid of this guy. He is not good for the country, he is not good for you.’" [47]

Latest election

Living former presidents

See also
WebPage index: 00010
WannaCry ransomware attack
The WannaCry ransomware attack was a worldwide cyberattack by the WannaCry [a] ransomware cryptoworm , which targeted computers running the Microsoft Windows operating system by encrypting data and demanding ransom payments in the Bitcoin cryptocurrency . [8]
The attack started on Friday, 12 May 2017, [9] and within a day was reported to have infected more than 230,000 computers in over 150 countries. [10] [11] Parts of Britain's National Health Service (NHS), Spain's Telefónica , FedEx and Deutsche Bahn were hit, along with many other countries and companies worldwide. [12] [13] [14]
WannaCry spreads across local networks and the Internet [15] to systems that have not been updated with recent security updates , to directly infect any exposed systems. [6] [16] A "critical" patch had been issued by Microsoft on 14 March 2017 to remove the underlying vulnerability for supported systems, nearly two months before the attack, [17] but many organizations had not yet applied it. [18] Those still running exposed older, unsupported operating systems such as Windows XP and Windows Server 2003 , were initially at particular risk but the day after the outbreak Microsoft took the unusual step of releasing updates for these operating systems too. [4] [19] Almost all victims were running Windows 7 . [20]
Much of the attention and comment around the event was occasioned by the fact that the U.S. National Security Agency (NSA) had discovered the vulnerability in the past, but instead of informing Microsoft, had built the EternalBlue exploit for their own offensive work . [21] [22] It was only when the existence of this was revealed by The Shadow Brokers that Microsoft became aware of the issue, and could produce a security update. [23]
Shortly after the attack began, a web security researcher who blogs as "MalwareTech" discovered an effective kill switch by registering a domain name he found in the code of the ransomware. This greatly slowed the spread of the infection, effectively halting the initial outbreak on Monday, 15 May 2017, but new versions have since been detected that lack the kill switch. [24] [25] [26] [27] Researchers have also found ways to recover data from infected machines under some circumstances. [20]
Within four days of the initial outbreak, security experts were saying that most organizations had applied updates, and that new infections had slowed to a trickle. [28]

WannaCry malware
WannaCry [a] is the ransomware computer worm that targets computers running Microsoft Windows . [29] Initially, the worm uses the EternalBlue exploit to enter a computer, taking advantage of a vulnerability in Microsoft 's implementation of the Server Message Block (SMB) protocol. It installs DoublePulsar , a backdoor implant tool, which then transfers and runs the WannaCry ransomware package.
Several organizations have released detailed technical writeups of the malware, including Microsoft, [30] Cisco, [15] Malwarebytes, [31] and McAfee. [32]
The "payload" works in the same fashion as most modern ransomware: it finds and encrypts a range of data files, then displays a "ransom note" informing the user and demanding a payment in bitcoin . [33] It is considered a network worm because it also includes a "transport" mechanism to automatically spread itself. This transport code scans for vulnerable systems, then uses the EternalBlue exploit to gain access, and the DoublePulsar tool to install and execute a copy of itself. [15]

"Kill switch"
The software contained a URL that, when discovered and registered by a security researcher to track activity from infected machines, was found to act as a " kill switch " that shuts down the software, stopping the spread of the ransomware. The researcher speculated that this had been included in the software as a mechanism to prevent it being run on quarantined machines so that it is harder for anti-virus researchers to investigate the software; he observed that some sandbox environments will respond to all queries with traffic in order to trick the software into thinking that it is still able to access the internet, so the software queried an "intentionally unregistered domain" to verify it was receiving traffic that it should not. [34] He also noted that it was not an unprecedented technique, having been observed in the Necurs trojan . [34]
On 19 May it was reported that hackers were trying to use a Mirai botnet variant to effect a distributed attack on WannaCry's kill-switch domain with the intention of knocking it offline. [35] On 22 May @MalwareTechBlog protected the domain by switching to a cached version of the site, capable of dealing with much higher traffic loads than the live site. [36]

EternalBlue
The network infection vector, EternalBlue , was released by the hacker group called The Shadow Brokers on 14 April 2017, [23] along with other tools apparently leaked from Equation Group , which is widely believed to be part of the United States National Security Agency . [37] [38]
EternalBlue exploits vulnerability MS17-010 [17] in Microsoft 's implementation of the Server Message Block (SMB) protocol. [29] This Windows vulnerability was not a zero-day flaw, but one for which Microsoft had released a "critical" advisory, along with a security patch to fix the vulnerability two months before, on 14 March 2017. [17] The patch was to the Server Message Block (SMB) protocol used by Windows, [39] [40] and fixed several versions of the Microsoft Windows operating system, including Windows Vista onwards (with the exception of Windows 8 ), as well as server and embedded versions such as Windows Server 2008 onwards and Windows Embedded POSReady 2009 respectively, but not the older unsupported Windows XP and Windows Server 2003 . [17] The day after the WannaCry outbreak Microsoft released updates for these too. [4] [19]
Windows 10 did not have the vulnerability. [41]

DoublePulsar
DoublePulsar is a backdoor tool, also released by The Shadow Brokers on 14 April 2017, [23] Starting from 21 April 2017, security researchers reported that computers with the DoublePulsar backdoor installed were in the tens of thousands. [42] By 25 April, reports estimated the number of infected computers to be up to several hundred thousands, with numbers increasing exponentially every day. [43] [44] The WannaCry code can take advantage of any existing DoublePulsar infection, or installs it itself. [15] [45] [46]

Attribution
Cybersecurity companies Kaspersky Lab and Symantec have both said the code has some similarities with that previously used by the Lazarus Group [47] (believed to have carried out the cyberattack on Sony Pictures in 2014 and a Bangladesh bank heist in 2016—and linked to North Korea ). [47] However, this could also be either simple re-use of code by another group, [48] or an attempt to shift blame—as in a cyber false flag operation. [47] North Korea itself denies being responsible for the cyberattack. [49] [50]

Cyberattack
On 12 May 2017 WannaCry began affecting computers worldwide, [52] with evidence pointing to an initial infection in Asia at 7:44am UTC. [9] [53] The initial infection was likely through an exposed vulnerable SMB port, [54] rather than email phishing as initially assumed. [9]
When executed, the malware first checks the " kill switch " domain name; [b] if it is not found, then the ransomware encrypts the computer's data, [55] [33] [56] then attempts to exploit the SMB vulnerability to spread out to random computers on the Internet, [31] and "laterally" to computers on the same network. [32] As with other modern ransomware, the payload displays a message informing the user that files have been encrypted, and demands a payment of around $300 in bitcoin within three days, or $600 within seven days. [33] [57]
Organizations that had not installed Microsoft's security update were affected by the attack. [39] Those still running the older Windows XP [58] were at particularly high risk because no security patches had been released since April 2014 (with the exception of one emergency patch released in May 2014). [4] [59] However, the day after the outbreak Microsoft released an emergency security patch for Windows XP. [4] As of May 2017, less than 0.1 percent of the affected computers were running Windows XP. [60]
A Kaspersky Labs study reports that 98 percent of the affected computers were running Windows 7. [60]
According to Wired , affected systems will also have had the DoublePulsar backdoor installed; this will also need to be removed when systems are decrypted. [7]
Three hardcoded bitcoin addresses, or "wallets", are used to receive the payments of victims. As with all such wallets, their transactions and balances are publicly accessible even though the wallet owners remain unknown. [61] As of 25 May 2017, at 7:40 UTC, a total of 302 payments totaling $126,742.48 (49.60319 BTC) had been transferred. [62]

Defensive response
Several hours after the initial release of the ransomware on 12 May 2017, while trying to establish the size of the attack, a researcher known by the name MalwareTech [63] [34] accidentally discovered what amounted to a " kill switch " hardcoded in the malware. [64] [65] [66] Registering a domain name for a DNS sinkhole stopped the attack spreading as a worm, because the ransomware only encrypted the computer's files if it was unable to connect to that domain, which all computers infected with WannaCry before the website's registration had been unable to do. While this did not help already infected systems, it severely slowed the spread of the initial infection and gave time for defensive measures to be deployed worldwide, particularly in North America and Asia, which had not been attacked to the same extent as elsewhere. [67] [68] [69] [70]
Microsoft released a statement recommending users install update MS17-010 to protect themselves against the attack. [4] In an unusual move, the company also made security patches available to the general public for several out-of-support versions of Windows, including Windows XP , Windows 8 and Windows Server 2003 . [4]
On 16 May 2017, researchers from University College London and Boston University reported that their PayBreak system could defeat WannaCry and several other families of ransomware. [71] [72] On 19 May 2017, a group of French security researchers reported that they had found a way to unlock the program without paying the ransom under some circumstances. [73] Several other tools have been released to help protecting against the WannaCry malware, among which WannaSmile, [74] that disables the SMBv1 protocol which implementations embed the flaw and WannaPatch [ citation needed ] that detects if a system is vulnerable and, if so, automates the downloading of the needed patch.
Within four days of the initial outbreak, security experts were saying that most organizations had applied updates, and that new infections had slowed to a trickle. [28]
A flaw in the encryption used by the WannaCry malware has been used to create a tool called "WannaKey" which can, in some cases, decrypt a WannaCry infected Windows XP PC's files. It works by pulling traces of a private key from the memory of an infected Windows XP computer, but its creator, Adrien Guinet, cautions that "the trick fails if the malware or any other process happened to overwrite the lingering decryption key, or if the computer rebooted any time after infection". Guinet recommends users leave the computer untouched until they can run his program. [75] This tool was later reused by other researchers for a new tool "wanakiwi" that also works for Windows Server 2003 and Windows 7. [73]

Advice on ransom
Experts advised against paying the ransom due to no early reports of people getting their data back after payment and as high revenues would encourage more of such campaigns. [76] [77] [78]

Impact
The ransomware campaign was unprecedented in scale according to Europol , [10] which estimates that around 200,000 computers were infected across 150 countries. According to Kaspersky Lab , the four most affected countries were Russia , Ukraine , India and Taiwan . [79]
The attack affected many National Health Service hospitals in England and Scotland, [80] and up to 70,000 devices – including computers, MRI scanners , blood-storage refrigerators and theatre equipment – may have been affected. [81] On 12 May, some NHS services had to turn away non-critical emergencies, and some ambulances were diverted. [82] [83] In 2016, thousands of computers in 42 separate NHS trusts in England were reported to be still running Windows XP. [58] NHS hospitals in Wales and Northern Ireland were unaffected by the attack. [84] [82]
Nissan Motor Manufacturing UK in Tyne and Wear , England, halted production after the ransomware infected some of their systems. Renault also stopped production at several sites in an attempt to stop the spread of the ransomware. [85] [86]
The attack's impact is said to be relatively low compared to other potential attacks of the same type and could have been much worse had a security expert, who was independently researching the malware, not discovered that a kill-switch had been built in by its creators [87] [88] or if it had been specifically targeted on highly critical infrastructure , like nuclear power plants , dams or railway systems. [89] [90]

EternalRocks
Via a honeypot mechanism, Security researcher Miroslav Stampar detected a new malware named " EternalRocks " that uses seven leaked NSA hacking tools and leaves Windows machines vulnerable for future attacks that may occur at any time. When installed, the worm names itself WannaCry in attempt to evade security experts. [91] [92] [93] [94]

Reactions
A number of experts highlighted the NSA 's non-disclosure of the underlying vulnerability, and their loss of control over the EternalBlue attack tool that exploited it. Edward Snowden said that if the NSA had " privately disclosed the flaw used to attack hospitals when they found it, not when they lost it, [the attack] may not have happened". [95] British cybersecurity expert Graham Cluley also sees "some culpability on the part of the U.S. intelligence services". According to him and others "they could have done something ages ago to get this problem fixed, and they didn't do it". He also said that despite obvious uses for such tools to spy on people of interest , they have a duty to protect their countries' citizens. [96] Others have also commented that this attack shows that the practice of intelligence agencies to stockpile exploits for offensive purposes rather than disclosing them for defensive purposes may be problematic. [88] Microsoft president and chief legal officer Brad Smith wrote, "Repeatedly, exploits in the hands of governments have leaked into the public domain and caused widespread damage. An equivalent scenario with conventional weapons would be the U.S. military having some of its Tomahawk missiles stolen." [97] [98] [99] Russian President Vladimir Putin placed the responsibility of the attack on U.S. intelligence services, for having created EternalBlue. [100] On 17 May United States bipartisan lawmakers introduced the PATCH Act [101] that aims to have exploits reviewed by an independent board to "balance the need to disclose vulnerabilities with other national security interests while increasing transparency and accountability to maintain public trust in the process". [102]
A cybersecurity researcher, working in loose collaboration with UK's National Cyber Security Centre , [103] [104] researched the malware and discovered a "kill switch" . [34] Later globally dispersed security researchers collaborated online to develop open source tools [105] [106] that allow for decryption without payment under some circumstances. [73] Snowden states that when "[NSA]-enabled ransomware eats the Internet, help comes from researchers, not spy agencies" and asks why this is the case. [107] [108] [104]
Other experts also used the publicity around the attack as a chance to reiterate the value and importance of having good, regular and secure backups , good cybersecurity including isolating critical systems, using appropriate software, and having the latest security patches installed. [109] Adam Segal , director of the digital and cyberspace policy program at the Council on Foreign Relations , stated that "the patching and updating systems are broken, basically, in the private sector and in government agencies". [88] In addition, Segal said that governments' apparent inability to secure vulnerabilities "opens a lot of questions about backdoors and access to encryption that the government argues it needs from the private sector for security". [88] Arne Schönbohm , President of Germany's Federal Office for Information Security (BSI), stated that "the current attacks show how vulnerable our digital society is. It's a wake-up call for companies to finally take IT security [seriously]". [40]
The effects of the attack also had political implications; in the UK the impact on the NHS quickly became political, with claims that the effects were exacerbated by Government under-funding of the NHS, in particular the refusal to pay extra to keep protecting outdated Windows XP systems from such attacks. [110] Home Secretary Amber Rudd refused to say whether patient data had been backed up , and Shadow Health Secretary Jon Ashworth accused Health Secretary Jeremy Hunt of refusing to act on a critical note from Microsoft, the National Cyber Security Centre (NCSC) and the National Crime Agency that had been received two months previously. [111] Others argued that hardware and software vendors often fail to account for future security flaws, selling systems that − due to their technical design and market incentives − eventually won't be able to properly receive and apply patches. [112]

Affected organizations
The following is an alphabetical list of organisations confirmed to have been affected:

See also

Notes
WebPage index: 00011
Wayne Walker
Wayne Harrison Walker (September 30, 1936 – May 19, 2017) was an American professional football player and sports broadcaster . He played fifteen seasons with the Detroit Lions of the National Football League , as a linebacker and placekicker . Walker played in 200 regular season games, the second most for a defensive player at the time. [1] He played in three Pro Bowls and was thrice selected as a first-team All-NFL player. After the 1972 season, he retired as a player and was a sports broadcaster for CBS and the sports director for KPIX-TV in San Francisco from 1974 to 1994. Walker was a weekend sportscaster during the off-season during his later years as a Detroit Lion.

Early years
Born and raised in Boise, Idaho , Walker graduated from Boise High School in 1954. [2] As a teen, he played American Legion baseball against hall of famer Harmon Killebrew of Payette ; [3] Walker passed on an offer to play minor league baseball to attend college. [4]

College football
Walker played college football at the University of Idaho in Moscow , then a member of the Pacific Coast Conference , as a center and middle linebacker for the Vandals under head coach Skip Stahley . [5] [6] Walker's teammate (and road roommate) at Idaho was Jerry Kramer . Both Walker and Kramer went on to play in the NFL, and both had their numbers retired at Idaho. [6] [7]
As a senior in 1957 , Walker was a team captain and was selected by the United Press as a second-team center on the All-Pacific Coast team . [8] In the East-West Shrine Game at San Francisco in late December, [9] he played on both sides of the ball and had five tackles , three assists, two interceptions , and blocked a kick ; he was voted the outstanding defensive player of the game. [5] [10] [11] He also played in the College All-Star Game in mid-August 1958, helping the pro rookies defeat the Detroit Lions , his new team, 35–19. [12] [13]

Professional football
Walker was selected by the Detroit Lions in the fourth round of the 1958 NFL draft , 45th overall, in December 1957 , weeks before Detroit won the NFL title , their third of the decade. [2] He played for the Lions for 15 years from 1958 to 1972. [2] Walker appeared in 200 games for the Lions, a franchise record that was later broken by placekicker Jason Hanson . [14] [15] He also scored 345 points, which ranked third in Lions history at the time of his retirement (currently ninth). [16] As a placekicker, Walker converted 53 of 131 field goal attempts for a 40.5% success rate, [2] the lowest field goal percentage in NFL history. [17] On extra points, he converted 172 of 175 attempts for a 98.3% success rate. [2]

Broadcasting career
After his retirement from the NFL, Walker was the sports director for KPIX-TV , the CBS affiliate in San Francisco for twenty years, from 1974 to 1994, where he succeeded Barry Tompkins . He was also a sports commentator for the San Francisco 49ers ' radio broadcasts for over twenty years and a commentator on Oakland Athletics baseball broadcasts [18] from 1976 to 1980 and 1985 ; he teamed up with fellow southern Idahoan Harmon Killebrew in 1979 , but the struggling A's lost 108 games. Walker was also a color commentator on regional NFL games for several years on CBS , working many games with Tom Brookshier , who moved from color commentary to play-by-play beginning in 1981 . [19]

Later years
Walker retired from broadcasting in 1999 and he and his wife Sylvia resided in the Boise area since 1994. [20] In 1994, he began hosting Incredible Idaho, a half-hour outdoor show on Boise's NBC affiliate, KTVB-TV . [21]
Diagnosed with throat cancer in June 2007, Walker lost 60 pounds (27 kg) after chemotherapy and radiation treatment. As of 2009 he was healthy again and had regained most of the lost weight. [20] In October 2015, Walker announced that he was suffering from Parkinson's disease , possibly as a result of the many concussions he suffered during his playing days. [22] [23] He died on May 19, 2017, from complications from Parkinson's disease. [24] [25] [26]
In December 1999, Walker was ninth on the Sports Illustrated list of greatest sports figures from Idaho. [27]
WebPage index: 00012
1644
1644 ( MDCXLIV ) was a leap year starting on Friday ( dominical letter CB) of the Gregorian calendar and a leap year starting on Monday ( dominical letter GF) of the Julian calendar , the 1644th year of the Common Era (CE) and Anno Domini (AD) designations, the 644th year of the 2nd millennium , the 44th year of the 17th century , and the 5th year of the 1640s decade. As of the start of 1644, the Gregorian calendar was 10 days ahead of the Julian calendar, which remained in localized use until 1923.

Events

January–June

July–December

Date unknown

Births

January–March

April–June

July–September

October–December

Deaths
WebPage index: 00013
Of Plymouth Plantation
Written over a period of years by William Bradford , the leader of the Plymouth Colony in Massachusetts , Of Plymouth Plantation is regarded as the most authoritative account of the Pilgrims and the early years of the colony they founded. Written between 1630 and 1651, the journal describes the story of the Pilgrims from 1608, when they settled in the Dutch Republic on the European mainland through the 1620 Mayflower voyage to the New World, until the year 1647. The book ends with a list, written in 1651, of Mayflower passengers and what happened to them.

Naming
The document has carried many names. At the top of the original text is Of Plim̃oth Plantation , [a] but newer prints of the text utilize the modern spelling, " Plymouth." The text of Bradford's journal is often called the History of Plymouth Plantation . In Wilberforce's text it is cited as History of the Plantation of Plymouth . [1] It is also sometimes called William Bradford's Journal . A version published by the Commonwealth of Massachusetts (after the return of the manuscript from England in 1897) is titled Bradford's History "Of Plimoth Plantation" while labeled The Bradford History on the spine. [2] It has also been called The Mayflower , although it is not a ship's log and was written after the events. [2]

Bradford's material
Bradford, along with Edward Winslow and others, contributed material to George Morton, who merged everything into a letter published in London in 1622, Mourt's Relation , [3] which was primarily a journal of the colonists' first years at Plymouth.
Bradford’s history is a blend of fact and interpretation. The Bradford journal records not only the events of the first 30 years but also the reactions of the colonists. The Bradford journal is regarded by historians as the preeminent work of 17th century America. It is Bradford’s simple yet vivid story, as told in his journal, that has made the Pilgrims the much-loved "spiritual ancestors of all Americans" (Samuel Eliot Morison).
Bradford apparently never made an effort to publish the manuscript during his lifetime. He did intend for it to be preserved and read by others, writing at the end of chapter 6:
Bradford, like all writers of his time, uses a variety of spelling. A rule code for spelling was unknown then and dictionaries were uncommon. Consistency in spelling was not a virtue, and even important state papers might reflect regional speech. In addition, there were a number of particular customs used, as for example the long s (ſ), which was used when the letter s was doubled or used initially. Bradford also uses common abbreviations such as wt for with, and yt for that.
It should also be noted that the "y" was not the y as used in present English, but instead was a thorn or thet , pronounced as th is today. The word ye was not pronounced as yee , but rather was pronounced as the is today.

History of the manuscript
After the death of Thomas Prence , who succeeded Bradford as Governor of Plymouth Colony, the manuscript was left in the tower of the Old South Meeting House in Boston. During the Revolutionary War , British troops occupied the church and the manuscript was lost for another century. After quotes from the missing book appeared in Samuel Wilberforce 's A History of the Protestant Episcopal Church in America , in the 1850s it was discovered in the Bishop of London's library at Fulham Palace , [2] and was published in print in 1856. Formal proposals to return the manuscript were not successful until the 1897 initiative of the Hon. George Frisbie Hoar , United States Senator from Massachusetts, supported by the Pilgrim Society , the American Antiquarian Society , and the New England Society of New York .
When Bishop of London Frederick Temple learned of the importance of the book, he thought it should be returned. Because the book was being held by the Church of England, approval from the Archbishop of Canterbury was needed in order to return it. By the time the formal request from Hoar's group reached England, the Archbishop was Frederick Temple. The bishop's Consistorial and Episcopal Court of London observed that although how the book got there was not known, the marriage and birth registry in the back of the book should have been deposited with the Church, that this library was the proper place for it, thus the book was a church document and the Diocese of London had proper control of it. The court went on to observe that when the Colony declared independence in 1776, the Diocese of London was no longer the proper place, because London's registry was no longer the proper repository for such a registry. The bishop's court ordered that a photographic copy of the document be made for the court, and the original be delivered to the Governor of Massachusetts. [2]
The Bradford journal was presented to the Governor of the Commonwealth of Massachusetts during a joint session of the legislature on May 26, 1897. It is on deposit in the State Library of Massachusetts in the State House in Boston. [5] In June 1897 the state legislature ordered publication of the history with copies of the documents associated with the return. [2] In 1912, the Massachusetts Historical Society published a "final" authorized version of the text.
Early in the 16th century, rag-based paper replaced parchment book pages. Both parchment and rag paper are very durable. Documents from the 17th century usually outlast those written on the highly acidic 19th and 20th century wood pulp-based paper. William Bradford's manuscript journal is a vellum-bound volume measuring 11 1 ⁄ 2 by 7 3 ⁄ 4 inches (292 × 197 mm). There are 270 pages, numbered (sometimes inaccurately) by Bradford. The ink is slightly faded and has turned brown with age, but it is still completely legible. The pages are somewhat foxed , but otherwise the almost 400-year-old document is in remarkably good condition. Page 243 is missing, with a note from Prence that it was missing when he got the document. [2]

From the journal
(Describing the Pilgrims' safe arrival at Cape Cod aboard the Mayflower)

Notes

See also
WebPage index: 00014
World War II
World War II (often abbreviated to WWII or WW2 ), also known as the Second World War , was a global war that lasted from 1939 to 1945, although related conflicts began earlier. It involved the vast majority of the world's countries —including all of the great powers —eventually forming two opposing military alliances : the Allies and the Axis . It was the most widespread war in history, and directly involved more than 100 million people from over 30 countries. In a state of " total war ", the major participants threw their entire economic, industrial, and scientific capabilities behind the war effort , erasing the distinction between civilian and military resources. Marked by mass deaths of civilians, including the Holocaust (in which approximately 11 million people were killed) [1] [2] and the strategic bombing of industrial and population centres (in which approximately one million were killed, and which included the atomic bombings of Hiroshima and Nagasaki ), [3] it resulted in an estimated 50 million to 85 million fatalities . These made World War II the deadliest conflict in human history . [4]
The Empire of Japan aimed to dominate Asia and the Pacific and was already at war with the Republic of China in 1937, [5] but the world war is generally said to have begun on 1 September 1939 [6] with the invasion of Poland by Nazi Germany and subsequent declarations of war on Germany by France and the United Kingdom . Supplied by the Soviet Union , from late 1939 to early 1941, in a series of campaigns and treaties , Germany conquered or controlled much of continental Europe, and formed the Axis alliance with Italy and Japan . Under the Molotov–Ribbentrop Pact of August 1939, Germany and the Soviet Union partitioned and annexed territories of their European neighbours, Poland , Finland , Romania and the Baltic states . The war continued primarily between the European Axis powers and the coalition of the United Kingdom and the British Commonwealth , with campaigns including the North Africa and East Africa campaigns, the aerial Battle of Britain , the Blitz bombing campaign , the Balkan Campaign as well as the long-running Battle of the Atlantic . On 22 June 1941, the European Axis powers launched an invasion of the Soviet Union , opening the largest land theatre of war in history , which trapped the major part of the Axis' military forces into a war of attrition . In December 1941, Japan attacked the United States and European colonies in the Pacific Ocean, and quickly conquered much of the Western Pacific.
The Axis advance halted in 1942 when Japan lost the critical Battle of Midway , near Hawaii , and Germany was defeated in North Africa and then, decisively, at Stalingrad in the Soviet Union. In 1943, with a series of German defeats on the Eastern Front , the Allied invasion of Sicily and the Allied invasion of Italy which brought about Italian surrender, and Allied victories in the Pacific, the Axis lost the initiative and undertook strategic retreat on all fronts. In 1944, the Western Allies invaded German-occupied France , while the Soviet Union regained all of its territorial losses and invaded Germany and its allies. During 1944 and 1945 the Japanese suffered major reverses in mainland Asia in South Central China and Burma , while the Allies crippled the Japanese Navy and captured key Western Pacific islands.
The war in Europe concluded with an invasion of Germany by the Western Allies and the Soviet Union, culminating in the capture of Berlin by Soviet troops and the subsequent German unconditional surrender on 8 May 1945 . Following the Potsdam Declaration by the Allies on 26 July 1945 and the refusal of Japan to surrender under its terms, the United States dropped atomic bombs on the Japanese cities of Hiroshima and Nagasaki on 6 August and 9 August respectively. With an invasion of the Japanese archipelago imminent, the possibility of additional atomic bombings, and the Soviet Union's declaration of war on Japan and invasion of Manchuria , Japan surrendered on 15 August 1945. Thus ended the war in Asia, cementing the total victory of the Allies.
World War II altered the political alignment and social structure of the world. The United Nations (UN) was established to foster international co-operation and prevent future conflicts. The victorious great powers —the United States, the Soviet Union, China, the United Kingdom, and France—became the permanent members of the United Nations Security Council . [7] The Soviet Union and the United States emerged as rival superpowers , setting the stage for the Cold War , which lasted for the next 46 years. Meanwhile, the influence of European great powers waned, while the decolonisation of Asia and Africa began. Most countries whose industries had been damaged moved towards economic recovery . Political integration, especially in Europe , emerged as an effort to end pre-war enmities and to create a common identity. [8]

Chronology
The start of the war in Europe is generally held to be 1 September 1939, [9] [10] beginning with the German invasion of Poland ; Britain and France declared war on Germany two days later. The dates for the beginning of war in the Pacific include the start of the Second Sino-Japanese War on 7 July 1937, [11] [12] or even the Japanese invasion of Manchuria on 19 September 1931. [13] [14]
Others follow the British historian A. J. P. Taylor , who held that the Sino-Japanese War and war in Europe and its colonies occurred simultaneously and the two wars merged in 1941. This article uses the conventional dating. Other starting dates sometimes used for World War II include the Italian invasion of Abyssinia on 3 October 1935. [15] The British historian Antony Beevor views the beginning of World War II as the Battles of Khalkhin Gol fought between Japan and the forces of Mongolia and the Soviet Union from May to September 1939. [16]
The exact date of the war's end is also not universally agreed upon. It was generally accepted at the time that the war ended with the armistice of 14 August 1945 ( V-J Day ), rather than the formal surrender of Japan (2 September 1945). A peace treaty with Japan was signed in 1951 to formally tie up any loose ends such as compensation to be paid to Allied prisoners of war who had been victims of atrocities. [17] A treaty regarding Germany's future allowed the reunification of East and West Germany to take place in 1990 and resolved other post-World War II issues. [18]

Background

Europe
World War I had radically altered the political European map, with the defeat of the Central Powers —including Austria-Hungary , Germany , Bulgaria and the Ottoman Empire —and the 1917 Bolshevik seizure of power in Russia , which eventually led to the founding of the Soviet Union . Meanwhile, the victorious Allies of World War I , such as France, Belgium, Italy, Greece and Romania, gained territory, and new nation-states were created out of the collapse of Austria-Hungary and the Ottoman and Russian Empires .
To prevent a future world war, the League of Nations was created during the 1919 Paris Peace Conference . The organisation's primary goals were to prevent armed conflict through collective security , military and naval disarmament , and settling international disputes through peaceful negotiations and arbitration.
Despite strong pacifist sentiment after World War I , [19] its aftermath still caused irredentist and revanchist nationalism in several European states. These sentiments were especially marked in Germany because of the significant territorial, colonial, and financial losses incurred by the Treaty of Versailles . Under the treaty, Germany lost around 13 per cent of its home territory and all of its overseas possessions , while German annexation of other states was prohibited, reparations were imposed, and limits were placed on the size and capability of the country's armed forces . [20]
The German Empire was dissolved in the German Revolution of 1918–1919 , and a democratic government, later known as the Weimar Republic , was created. The interwar period saw strife between supporters of the new republic and hardline opponents on both the right and left . Italy, as an Entente ally, had made some post-war territorial gains; however, Italian nationalists were angered that the promises made by Britain and France to secure Italian entrance into the war were not fulfilled in the peace settlement. From 1922 to 1925, the Fascist movement led by Benito Mussolini seized power in Italy with a nationalist, totalitarian , and class collaborationist agenda that abolished representative democracy, repressed socialist, left-wing and liberal forces, and pursued an aggressive expansionist foreign policy aimed at making Italy a world power , promising the creation of a " New Roman Empire ". [21]
Adolf Hitler, after an unsuccessful attempt to overthrow the German government in 1923, eventually became the Chancellor of Germany in 1933 . He abolished democracy, espousing a radical, racially motivated revision of the world order , and soon began a massive rearmament campaign . [22] It was at this time that political scientists began to predict that a second Great War might take place. [23] [ page needed ] Meanwhile, France, to secure its alliance, allowed Italy a free hand in Ethiopia , which Italy desired as a colonial possession. The situation was aggravated in early 1935 when the Territory of the Saar Basin was legally reunited with Germany and Hitler repudiated the Treaty of Versailles, accelerated his rearmament programme, and introduced conscription . [24]
To contain Germany, the United Kingdom, France and Italy formed the Stresa Front in April 1935; however, that June, the United Kingdom made an independent naval agreement with Germany, easing prior restrictions. The Soviet Union, concerned by Germany's goals of capturing vast areas of Eastern Europe , drafted a treaty of mutual assistance with France. Before taking effect though, the Franco-Soviet pact was required to go through the bureaucracy of the League of Nations, which rendered it essentially toothless. [25] The United States, concerned with events in Europe and Asia, passed the Neutrality Act in August of the same year. [26]
Hitler defied the Versailles and Locarno treaties by remilitarising the Rhineland in March 1936, encountering little opposition. [27] In October 1936, Germany and Italy formed the Rome–Berlin Axis . A month later, Germany and Japan signed the Anti-Comintern Pact , which Italy would join in the following year.

Asia
The Kuomintang (KMT) party in China launched a unification campaign against regional warlords and nominally unified China in the mid-1920s, but was soon embroiled in a civil war against its former Chinese Communist Party allies. [28] In 1931, an increasingly militaristic Empire of Japan , which had long sought influence in China [29] as the first step of what its government saw as the country's right to rule Asia , used the Mukden Incident as a pretext to launch an invasion of Manchuria and establish the puppet state of Manchukuo . [30]
Too weak to resist Japan, China appealed to the League of Nations for help. Japan withdrew from the League of Nations after being condemned for its incursion into Manchuria. The two nations then fought several battles, in Shanghai , Rehe and Hebei , until the Tanggu Truce was signed in 1933. Thereafter, Chinese volunteer forces continued the resistance to Japanese aggression in Manchuria , and Chahar and Suiyuan . [31] After the 1936 Xi'an Incident , the Kuomintang and communist forces agreed on a ceasefire to present a united front to oppose Japan. [32]

Pre-war events

Italian invasion of Ethiopia (1935)
The Second Italo–Abyssinian War was a brief colonial war that began in October 1935 and ended in May 1936. The war began with the invasion of the Ethiopian Empire (also known as Abyssinia ) by the armed forces of the Kingdom of Italy ( Regno d'Italia ), which was launched from Italian Somaliland and Eritrea . [33] The war resulted in the military occupation of Ethiopia and its annexation into the newly created colony of Italian East Africa ( Africa Orientale Italiana , or AOI); in addition, it exposed the weakness of the League of Nations as a force to preserve peace. Both Italy and Ethiopia were member nations, but the League did nothing when the former clearly violated the League's Article X . [34] Germany was the only major European nation to support the invasion. Italy subsequently dropped its objections to Germany's goal of absorbing Austria . [35]

Spanish Civil War (1936–39)
When civil war broke out in Spain, Hitler and Mussolini lent military support to the Nationalist rebels , led by General Francisco Franco . The Soviet Union supported the existing government, the Spanish Republic . Over 30,000 foreign volunteers, known as the International Brigades , also fought against the Nationalists. Both Germany and the USSR used this proxy war as an opportunity to test in combat their most advanced weapons and tactics. The Nationalists won the civil war in April 1939; Franco, now dictator, bargained with both sides during the Second World War, but never concluded any major agreements. He did send volunteers to fight on the Eastern Front under German command but Spain remained neutral and did not allow either side to use its territory. [36] [ page needed ]

Japanese invasion of China (1937)
In July 1937, Japan captured the former Chinese imperial capital of Beijing after instigating the Marco Polo Bridge Incident , which culminated in the Japanese campaign to invade all of China. [37] The Soviets quickly signed a non-aggression pact with China to lend materiel support, effectively ending China's prior co-operation with Germany . Generalissimo Chiang Kai-shek deployed his best army to defend Shanghai , but, after three months of fighting, Shanghai fell. The Japanese continued to push the Chinese forces back, capturing the capital Nanking in December 1937. After the fall of Nanking, tens of thousands if not hundreds of thousands of Chinese civilians and disarmed combatants were murdered by the Japanese . [38] [39]
In March 1938, Nationalist Chinese forces won their first major victory at Taierzhuang but then the city of Xuzhou was taken by Japanese in May. [40] In June 1938, Chinese forces stalled the Japanese advance by flooding the Yellow River ; this manoeuvre bought time for the Chinese to prepare their defences at Wuhan , but the city was taken by October. [41] Japanese military victories did not bring about the collapse of Chinese resistance that Japan had hoped to achieve; instead the Chinese government relocated inland to Chongqing and continued the war. [42] [43]

Soviet–Japanese border conflicts
In the mid-to-late 1930s, Japanese forces in Manchukuo had sporadic border clashes with the Soviet Union and Mongolia. The Japanese doctrine of Hokushin-ron , which emphasised Japan's expansion northward, was favoured by the Imperial Army during this time. With the Japanese defeat at Khalkin Gol in 1939, the ongoing Second Sino-Japanese War [44] and ally Nazi Germany pursuing neutrality with the Soviets, this policy would prove difficult to maintain. Japan and the Soviet Union eventually signed a Neutrality Pact in April 1941, and Japan adopted the doctrine of Nanshin-ron , promoted by the Navy, which took its focus southward, eventually leading to its war with the United States and the Western Allies. [45] [46]

European occupations and agreements
In Europe, Germany and Italy were becoming more aggressive. In March 1938, Germany annexed Austria , again provoking little response from other European powers. [47] Encouraged, Hitler began pressing German claims on the Sudetenland , an area of Czechoslovakia with a predominantly ethnic German population; and soon Britain and France followed the counsel of British Prime Minister Neville Chamberlain and conceded this territory to Germany in the Munich Agreement , which was made against the wishes of the Czechoslovak government, in exchange for a promise of no further territorial demands. [48] Soon afterwards, Germany and Italy forced Czechoslovakia to cede additional territory to Hungary and Poland annexed Czechoslovakia's Zaolzie region. [49]
Although all of Germany's stated demands had been satisfied by the agreement, privately Hitler was furious that British interference had prevented him from seizing all of Czechoslovakia in one operation. In subsequent speeches Hitler attacked British and Jewish "war-mongers" and in January 1939 secretly ordered a major build-up of the German navy to challenge British naval supremacy. In March 1939, Germany invaded the remainder of Czechoslovakia and subsequently split it into the German Protectorate of Bohemia and Moravia and a pro-German client state , the Slovak Republic . [50] Hitler also delivered an ultimatum to Lithuania, forcing the concession of the Klaipėda Region .
Greatly alarmed and with Hitler making further demands on the Free City of Danzig , Britain and France guaranteed their support for Polish independence ; when Italy conquered Albania in April 1939, the same guarantee was extended to Romania and Greece . [51] Shortly after the Franco -British pledge to Poland, Germany and Italy formalised their own alliance with the Pact of Steel . [52] Hitler accused Britain and Poland of trying to "encircle" Germany and renounced the Anglo-German Naval Agreement and the German–Polish Non-Aggression Pact .
In August 1939, Germany and the Soviet Union signed the Molotov–Ribbentrop Pact , [53] a non-aggression treaty with a secret protocol. The parties gave each other rights to "spheres of influence" (western Poland and Lithuania for Germany; eastern Poland , Finland, Estonia , Latvia and Bessarabia for the USSR). It also raised the question of continuing Polish independence. [54] The agreement was crucial to Hitler because it assured that Germany would not have to face the prospect of a two-front war, as it had in World War I, after it defeated Poland.
The situation reached a general crisis in late August as German troops continued to mobilise against the Polish border. In a private meeting with the Italian foreign minister, Count Ciano , Hitler asserted that Poland was a "doubtful neutral" that needed to either yield to his demands or be "liquidated" to prevent it from drawing off German troops in the future "unavoidable" war with the Western democracies. He did not believe Britain or France would intervene in the conflict. [55] On 23 August Hitler ordered the attack to proceed on 26 August, but upon hearing that Britain had concluded a formal mutual assistance pact with Poland and that Italy would maintain neutrality, he decided to delay it. [56]
In response to British requests for direct negotiations to avoid war, Germany made demands on Poland, which only served as a pretext to worsen relations. [57] On 29 August, Hitler demanded that a Polish plenipotentiary immediately travel to Berlin to negotiate the handover of Danzig , and to allow a plebiscite in the Polish Corridor in which the German minority would vote on secession. [57] The Poles refused to comply with the German demands and on the night of 30–31 August in a violent meeting with the British ambassador Neville Henderson , Ribbentrop declared that Germany considered its claims rejected. [58]

Course of the war

War breaks out in Europe (1939–40)
On 1 September 1939, Germany invaded Poland under the false pretext that the Poles had carried out a series of sabotage operations against German targets near the border. [59] Two days later, on 3 September, after a British ultimatum to Germany to cease military operations was ignored, Britain and France, followed by the fully independent Dominions [60] of the British Commonwealth [61] — Australia (3 September), Canada (10 September), New Zealand (3 September), and South Africa (6 September)—declared war on Germany. However, initially the alliance provided limited direct military support to Poland, consisting of a cautious, half-hearted French probe into the Saarland . [62] The Western Allies also began a naval blockade of Germany , which aimed to damage the country's economy and war effort. [63] Germany responded by ordering U-boat warfare against Allied merchant and warships, which was to later escalate into the Battle of the Atlantic .
On 17 September 1939, after signing a cease-fire with Japan , the Soviets invaded Poland from the east. [64] The Polish army was defeated and Warsaw surrendered to the Germans on 27 September, with final pockets of resistance surrendering on 6 October. Poland's territory was divided between Germany and the Soviet Union , with Lithuania and Slovakia also receiving small shares. After the defeat of Poland's armed forces, the Polish resistance established an Underground State and a partisan Home Army . [65] About 100,000 Polish military personnel were evacuated to Romania and the Baltic countries; many of these soldiers later fought against the Germans in other theatres of the war. [66] Poland's Enigma codebreakers were also evacuated to France. [67]
On 6 October, Hitler made a public peace overture to Britain and France, but said that the future of Poland was to be determined exclusively by Germany and the Soviet Union. Chamberlain rejected this on 12 October, saying "Past experience has shown that no reliance can be placed upon the promises of the present German Government." [58] After this rejection Hitler ordered an immediate offensive against France, [68] but bad weather forced repeated postponements until the spring of 1940. [69] [70] [71]
After signing the German–Soviet Treaty of Friendship, Cooperation and Demarcation , the Soviet Union forced the Baltic countries —Estonia, Latvia and Lithuania—to allow it to station Soviet troops in their countries under pacts of "mutual assistance" . [72] [73] [74] Finland rejected territorial demands, prompting a Soviet invasion in November 1939. [75] The resulting Winter War ended in March 1940 with Finnish concessions . [76] Britain and France, treating the Soviet attack on Finland as tantamount to its entering the war on the side of the Germans, responded to the Soviet invasion by supporting the USSR's expulsion from the League of Nations. [74]
In June 1940, the Soviet Union forcibly annexed Estonia, Latvia and Lithuania , [73] and the disputed Romanian regions of Bessarabia, Northern Bukovina and Hertza . Meanwhile, Nazi-Soviet political rapprochement and economic co-operation [77] [78] gradually stalled, [79] [ page needed ] [80] and both states began preparations for war. [81]

Western Europe (1940–41)
In April 1940, Germany invaded Denmark and Norway to protect shipments of iron ore from Sweden , which the Allies were attempting to cut off by unilaterally mining neutral Norwegian waters. [82] Denmark capitulated after a few hours, and despite Allied support , during which the important harbour of Narvik temporarily was recaptured from the Germans, Norway was conquered within two months. [83] British discontent over the Norwegian campaign led to the replacement of the British Prime Minister, Neville Chamberlain , with Winston Churchill on 10 May 1940. [84]
Germany launched an offensive against France and, adhering to the Manstein Plan also attacked the neutral nations of Belgium , the Netherlands , and Luxembourg on 10 May 1940. [85] That same day British forces landed in Iceland and the Faroes to preempt a possible German invasion of the islands. [86] The U.S., in close co-operation with the Danish envoy to Washington D.C. , agreed to protect Greenland , laying the political framework for the formal establishment of bases in April 1941. The Netherlands and Belgium were overrun using blitzkrieg tactics in a few days and weeks, respectively. [87] The French-fortified Maginot Line and the main body of the Allied forces which had moved into Belgium were circumvented by a flanking movement through the thickly wooded Ardennes region, [88] mistakenly perceived by Allied planners as an impenetrable natural barrier against armoured vehicles. [89] [90] As a result, the bulk of the Allied armies found themselves trapped in an encirclement and were beaten. The majority were taken prisoner, whilst over 300,000, mostly British and French, were evacuated from the continent at Dunkirk by early June, although abandoning almost all of their equipment. [91]
On 10 June, Italy invaded France , declaring war on both France and the United Kingdom. [92] Paris fell to the Germans on 14 June and eight days later France signed an armistice with Germany and was soon divided into German and Italian occupation zones , [93] and an unoccupied rump state under the Vichy Regime , which, though officially neutral, was generally aligned with Germany. France kept its fleet but the British feared the Germans would seize it, so on 3 July, the British attacked it . [94]
The Battle of Britain [95] began in early July with Luftwaffe attacks on shipping and harbours . [96] On 19 July, Hitler again publicly offered to end the war, saying he had no desire to destroy the British Empire . The United Kingdom rejected this ultimatum. [97] The main German air superiority campaign started in August but failed to defeat RAF Fighter Command , and a proposed invasion was postponed indefinitely on 17 September. The German strategic bombing offensive intensified as night attacks on London and other cities in the Blitz , but largely failed to disrupt the British war effort. [96]
Using newly captured French ports, the German Navy enjoyed success against an over-extended Royal Navy , using U-boats against British shipping in the Atlantic . [98] The British scored a significant victory on 27 May 1941 by sinking the German battleship Bismarck . [99] Perhaps most importantly, during the Battle of Britain the Royal Air Force had successfully resisted the Luftwaffe's assault, and the German bombing campaign largely ended in May 1941. [100]
Throughout this period, the neutral United States took measures to assist China and the Western Allies. In November 1939, the American Neutrality Act was amended to allow "cash and carry" purchases by the Allies. [101] In 1940, following the German capture of Paris, the size of the United States Navy was significantly increased . In September, the United States further agreed to a trade of American destroyers for British bases . [102] Still, a large majority of the American public continued to oppose any direct military intervention into the conflict well into 1941. [103]
Although Roosevelt had promised to keep the United States out of the war, he nevertheless took concrete steps to prepare for war. In December 1940 he accused Hitler of planning world conquest and ruled out negotiations as useless, calling for the US to become an " arsenal of democracy " and promoted the passage of Lend-Lease aid to support the British war effort. [97] In January 1941 secret high level staff talks with the British began for the purposes of determining how to defeat Germany should the US enter the war. They decided on a number of offensive policies, including an air offensive, the "early elimination" of Italy, raids, support of resistance groups, and the capture of positions to launch an offensive against Germany. [104]
At the end of September 1940, the Tripartite Pact united Japan, Italy and Germany to formalise the Axis Powers . The Tripartite Pact stipulated that any country, with the exception of the Soviet Union, not in the war which attacked any Axis Power would be forced to go to war against all three. [105] The Axis expanded in November 1940 when Hungary, Slovakia and Romania joined the Tripartite Pact. [106] Romania would make a major contribution (as did Hungary ) to the Axis war against the USSR, partially to recapture territory ceded to the USSR , partially to pursue its leader Ion Antonescu 's desire to combat communism. [107]

Mediterranean (1940–41)
Italy began operations in the Mediterranean, initiating a siege of Malta in June, conquering British Somaliland in August, and making an incursion into British-held Egypt in September 1940. In October 1940, Italy started the Greco-Italian War because of Mussolini's jealousy of Hitler's success but within days was repulsed and pushed back into Albania, where a stalemate soon occurred. [108] The United Kingdom responded to Greek requests for assistance by sending troops to Crete and providing air support to Greece. Hitler decided that when the weather improved he would take action against Greece to assist the Italians and prevent the British from gaining a foothold in the Balkans, to strike against the British naval dominance of the Mediterranean, and to secure his hold on Romanian oil. [109]
In December 1940, British Commonwealth forces began counter-offensives against Italian forces in Egypt and Italian East Africa . [110] The offensive in North Africa was highly successful and by early February 1941 Italy had lost control of eastern Libya and large numbers of Italian troops had been taken prisoner. The Italian Navy also suffered significant defeats, with the Royal Navy putting three Italian battleships out of commission by a carrier attack at Taranto , and neutralising several more warships at the Battle of Cape Matapan . [111]
The Germans soon intervened to assist Italy. Hitler sent German forces to Libya in February, and by the end of March they had launched an offensive which drove back the Commonwealth forces which had been weakened to support Greece. [112] In under a month, Commonwealth forces were pushed back into Egypt with the exception of the besieged port of Tobruk . [113] The Commonwealth attempted to dislodge Axis forces in May and again in June , but failed on both occasions. [114]
By late March 1941, following Bulgaria 's signing of the Tripartite Pact , the Germans were in position to intervene in Greece. Plans were changed, however, because of developments in neighbouring Yugoslavia . The Yugoslav government had signed the Tripartite Pact on 25 March, only to be overthrown two days later by a British-encouraged coup . Hitler viewed the new regime as hostile and immediately decided to eliminate it. On 6 April Germany simultaneously invaded both Yugoslavia and Greece , making rapid progress and forcing both nations to surrender within the month. The British were driven from the Balkans after Germany conquered the Greek island of Crete by the end of May. [115] Although the Axis victory was swift, bitter partisan warfare subsequently broke out against the Axis occupation of Yugoslavia , which continued until the end of the war.
The Allies did have some successes during this time. In the Middle East, Commonwealth forces first quashed an uprising in Iraq which had been supported by German aircraft from bases within Vichy-controlled Syria , [116] then, with the assistance of the Free French , invaded Syria and Lebanon to prevent further such occurrences. [117]

Axis attack on the USSR (1941)
With the situation in Europe and Asia relatively stable, Germany, Japan, and the Soviet Union made preparations. With the Soviets wary of mounting tensions with Germany and the Japanese planning to take advantage of the European War by seizing resource-rich European possessions in Southeast Asia , the two powers signed the Soviet–Japanese Neutrality Pact in April 1941. [118] By contrast, the Germans were steadily making preparations for an attack on the Soviet Union, massing forces on the Soviet border. [119]
Hitler believed that Britain's refusal to end the war was based on the hope that the United States and the Soviet Union would enter the war against Germany sooner or later. [120] He therefore decided to try to strengthen Germany's relations with the Soviets, or failing that, to attack and eliminate them as a factor. In November 1940, negotiations took place to determine if the Soviet Union would join the Tripartite Pact. The Soviets showed some interest, but asked for concessions from Finland, Bulgaria, Turkey, and Japan that Germany considered unacceptable. On 18 December 1940, Hitler issued the directive to prepare for an invasion of the Soviet Union.
On 22 June 1941, Germany, supported by Italy and Romania, invaded the Soviet Union in Operation Barbarossa , with Germany accusing the Soviets of plotting against them. They were joined shortly by Finland and Hungary. [121] The primary targets of this surprise offensive [122] [ page needed ] were the Baltic region , Moscow and Ukraine , with the ultimate goal of ending the 1941 campaign near the Arkhangelsk-Astrakhan line , from the Caspian to the White Seas . Hitler's objectives were to eliminate the Soviet Union as a military power, exterminate Communism, generate Lebensraum ("living space") [123] by dispossessing the native population [124] [ page needed ] and guarantee access to the strategic resources needed to defeat Germany's remaining rivals. [125] [ page needed ]
Although the Red Army was preparing for strategic counter-offensives before the war, [126] [ page needed ] Barbarossa forced the Soviet supreme command to adopt a strategic defence . During the summer, the Axis made significant gains into Soviet territory, inflicting immense losses in both personnel and materiel. By the middle of August, however, the German Army High Command decided to suspend the offensive of a considerably depleted Army Group Centre , and to divert the 2nd Panzer Group to reinforce troops advancing towards central Ukraine and Leningrad. [127] [ page needed ] The Kiev offensive was overwhelmingly successful, resulting in encirclement and elimination of four Soviet armies, and made possible further advance into Crimea and industrially developed Eastern Ukraine (the First Battle of Kharkov ). [128]
The diversion of three quarters of the Axis troops and the majority of their air forces from France and the central Mediterranean to the Eastern Front [129] prompted Britain to reconsider its grand strategy . [130] [ page needed ] In July, the UK and the Soviet Union formed a military alliance against Germany [131] The British and Soviets invaded Iran to secure the Persian Corridor and Iran's oil fields . [132] In August, the United Kingdom and the United States jointly issued the Atlantic Charter . [133]
By October Axis operational objectives in Ukraine and the Baltic region were achieved, with only the sieges of Leningrad [134] [ page needed ] and Sevastopol continuing. [135] A major offensive against Moscow was renewed; after two months of fierce battles in increasingly harsh weather the German army almost reached the outer suburbs of Moscow, where the exhausted troops [136] were forced to suspend their offensive. [137] Large territorial gains were made by Axis forces, but their campaign had failed to achieve its main objectives: two key cities remained in Soviet hands, the Soviet capability to resist was not broken, and the Soviet Union retained a considerable part of its military potential. The blitzkrieg phase of the war in Europe had ended. [138] [ page needed ]
By early December, freshly mobilised reserves [139] [ page needed ] allowed the Soviets to achieve numerical parity with Axis troops. [140] This, as well as intelligence data which established that a minimal number of Soviet troops in the East would be sufficient to deter any attack by the Japanese Kwantung Army , [141] [ page needed ] allowed the Soviets to begin a massive counter-offensive that started on 5 December all along the front and pushed German troops 100–250 kilometres (62–155 mi) west. [142]

War breaks out in the Pacific (1941)
In 1939, the United States had renounced its trade treaty with Japan; and, beginning with an aviation gasoline ban in July 1940, Japan became subject to increasing economic pressure. [97] During this time, Japan launched its first attack against Changsha , a strategically important Chinese city, but was repulsed by late September. [143] Despite several offensives by both sides, the war between China and Japan was stalemated by 1940. To increase pressure on China by blocking supply routes, and to better position Japanese forces in the event of a war with the Western powers, Japan invaded and occupied northern Indochina . [144] Afterwards, the United States embargoed iron, steel and mechanical parts against Japan. [145] Other sanctions soon followed.
In August of that year, Chinese communists launched an offensive in Central China ; in retaliation, Japan instituted harsh measures in occupied areas to reduce human and material resources for the communists. [146] Continued antipathy between Chinese communist and nationalist forces culminated in armed clashes in January 1941 , effectively ending their co-operation. [147] In March, the Japanese 11th army attacked the headquarters of the Chinese 19th army but was repulsed during Battle of Shanggao . [148] In September, Japan attempted to take the city of Changsha again and clashed with Chinese nationalist forces. [149]
German successes in Europe encouraged Japan to increase pressure on European governments in Southeast Asia . The Dutch government agreed to provide Japan some oil supplies from the Dutch East Indies , but negotiations for additional access to their resources ended in failure in June 1941. [150] In July 1941 Japan sent troops to southern Indochina, thus threatening British and Dutch possessions in the Far East. The United States, United Kingdom and other Western governments reacted to this move with a freeze on Japanese assets and a total oil embargo. [151] [152]
Since early 1941 the United States and Japan had been engaged in negotiations in an attempt to improve their strained relations and end the war in China. During these negotiations Japan advanced a number of proposals which were dismissed by the Americans as inadequate. [153] At the same time the US, Britain, and the Netherlands engaged in secret discussions for the joint defence of their territories, in the event of a Japanese attack against any of them. [154] Roosevelt reinforced the Philippines (an American protectorate scheduled for independence in 1946) and warned Japan that the US would react to Japanese attacks against any "neighboring countries". [154]
Frustrated at the lack of progress and feeling the pinch of the American-British-Dutch sanctions, Japan prepared for war, as IJA General Hideki Tojo became Imperial Japan's Prime Minister on 17 October. On 20 November it presented an interim proposal as its final offer. It called for the end of American aid to China and for the supply of oil and other resources to Japan. In exchange they promised not to launch any attacks in Southeast Asia and to withdraw their forces from their threatening positions in southern Indochina. [153] The American counter-proposal of 26 November required that Japan evacuate all of China without conditions and conclude non-aggression pacts with all Pacific powers. [155] That meant Japan was essentially forced to choose between abandoning its ambitions in China, or seizing the natural resources it needed in the Dutch East Indies by force; [156] the Japanese military did not consider the former an option, and many officers considered the oil embargo an unspoken declaration of war. [157]
Japan planned to rapidly seize European colonies in Asia to create a large defensive perimeter stretching into the Central Pacific; the Japanese would then be free to exploit the resources of Southeast Asia while exhausting the over-stretched Allies by fighting a defensive war. [158] To prevent American intervention while securing the perimeter it was further planned to neutralise the United States Pacific Fleet and the American military presence in the Philippines from the outset. [159] On 7 December 1941 (8 December in Asian time zones), Japan attacked British and American holdings with near-simultaneous offensives against Southeast Asia and the Central Pacific . [160] These included an attack on the American fleet at Pearl Harbor , the Philippines , landings in Thailand and Malaya [160] and the battle of Hong Kong .
These attacks led the United States, Britain , China, Australia and several other states to formally declare war on Japan, whereas the Soviet Union, being heavily involved in large-scale hostilities with European Axis countries, maintained its neutrality agreement with Japan. [161] Germany, followed by the other Axis states, declared war on the United States [162] in solidarity with Japan, citing as justification the American attacks on German war vessels that had been ordered by Roosevelt. [121] [163]

Axis advance stalls (1942–43)
In January 1942, the Allied Big Four [164] —the United States, Britain, the Soviet Union and China—and 22 smaller or exiled governments issued the Declaration by United Nations , thereby affirming the Atlantic Charter , [165] and agreeing to not to sign a separate peace with the Axis powers.
During 1942, Allied officials debated on the appropriate grand strategy to pursue. All agreed that defeating Germany was the primary objective. The Americans favoured a straightforward, large-scale attack on Germany through France. The Soviets were also demanding a second front. The British, on the other hand, argued that military operations should target peripheral areas to wear out German strength, leading to increasing demoralisation, and bolster resistance forces. Germany itself would be subject to a heavy bombing campaign. An offensive against Germany would then be launched primarily by Allied armour without using large-scale armies. [166] Eventually, the British persuaded the Americans that a landing in France was infeasible in 1942 and they should instead focus on driving the Axis out of North Africa. [167]
At the Casablanca Conference in early 1943, the Allies reiterated the statements issued in the 1942 Declaration by the United Nations, and demanded the unconditional surrender of their enemies. The British and Americans agreed to continue to press the initiative in the Mediterranean by invading Sicily to fully secure the Mediterranean supply routes. [168] Although the British argued for further operations in the Balkans to bring Turkey into the war, in May 1943, the Americans extracted a British commitment to limit Allied operations in the Mediterranean to an invasion of the Italian mainland and to invade France in 1944. [169]

Pacific (1942–43)
By the end of April 1942, Japan and its ally Thailand had almost fully conquered Burma , Malaya , the Dutch East Indies , Singapore , and Rabaul , inflicting severe losses on Allied troops and taking a large number of prisoners. [170] Despite stubborn resistance by Filipino and US forces, the Philippine Commonwealth was eventually captured in May 1942, forcing its government into exile. [171] On 16 April, in Burma, 7,000 British soldiers were encircled by the Japanese 33rd Division during the Battle of Yenangyaung and rescued by the Chinese 38th Division. [172] Japanese forces also achieved naval victories in the South China Sea , Java Sea and Indian Ocean , [173] and bombed the Allied naval base at Darwin , Australia. In January 1942, the only Allied success against Japan was a Chinese victory at Changsha . [174] These easy victories over unprepared US and European opponents left Japan overconfident, as well as overextended. [175]
In early May 1942, Japan initiated operations to capture Port Moresby by amphibious assault and thus sever communications and supply lines between the United States and Australia. The planned invasion was thwarted when an Allied task force, centred on two American fleet carriers, fought Japanese naval forces to a draw in the Battle of the Coral Sea . [176] Japan's next plan, motivated by the earlier Doolittle Raid , was to seize Midway Atoll and lure American carriers into battle to be eliminated; as a diversion, Japan would also send forces to occupy the Aleutian Islands in Alaska. [177] In mid-May, Japan started the Zhejiang-Jiangxi Campaign in China, with the goal of inflicting retribution on the Chinese who aided the surviving American airmen in the Doolittle Raid by destroying air bases and fighting against the Chinese 23rd and 32nd Army Groups. [178] [179] In early June, Japan put its operations into action but the Americans, having broken Japanese naval codes in late May, were fully aware of plans and order of battle, and used this knowledge to achieve a decisive victory at Midway over the Imperial Japanese Navy . [180]
With its capacity for aggressive action greatly diminished as a result of the Midway battle, Japan chose to focus on a belated attempt to capture Port Moresby by an overland campaign in the Territory of Papua . [181] The Americans planned a counter-attack against Japanese positions in the southern Solomon Islands , primarily Guadalcanal , as a first step towards capturing Rabaul , the main Japanese base in Southeast Asia. [182]
Both plans started in July, but by mid-September, the Battle for Guadalcanal took priority for the Japanese, and troops in New Guinea were ordered to withdraw from the Port Moresby area to the northern part of the island , where they faced Australian and United States troops in the Battle of Buna-Gona . [183] Guadalcanal soon became a focal point for both sides with heavy commitments of troops and ships in the battle for Guadalcanal. By the start of 1943, the Japanese were defeated on the island and withdrew their troops . [184] In Burma, Commonwealth forces mounted two operations. The first, an offensive into the Arakan region in late 1942, went disastrously, forcing a retreat back to India by May 1943. [185] The second was the insertion of irregular forces behind Japanese front-lines in February which, by the end of April, had achieved mixed results. [186]

Eastern Front (1942–43)
Despite considerable losses, in early 1942 Germany and its allies stopped a major Soviet offensive in central and southern Russia, keeping most territorial gains they had achieved during the previous year. [187] In May the Germans defeated Soviet offensives in the Kerch Peninsula and at Kharkov , [188] and then launched their main summer offensive against southern Russia in June 1942, to seize the oil fields of the Caucasus and occupy Kuban steppe , while maintaining positions on the northern and central areas of the front. The Germans split Army Group South into two groups: Army Group A advanced to the lower Don River and struck south-east to the Caucasus, while Army Group B headed towards the Volga River . The Soviets decided to make their stand at Stalingrad on the Volga. [189]
By mid-November, the Germans had nearly taken Stalingrad in bitter street fighting when the Soviets began their second winter counter-offensive, starting with an encirclement of German forces at Stalingrad [190] and an assault on the Rzhev salient near Moscow , though the latter failed disastrously. [191] By early February 1943, the German Army had taken tremendous losses; German troops at Stalingrad had been forced to surrender, [192] and the front-line had been pushed back beyond its position before the summer offensive. In mid-February, after the Soviet push had tapered off, the Germans launched another attack on Kharkov , creating a salient in their front line around the Russian city of Kursk . [193]

Western Europe/Atlantic & Mediterranean (1942–43)
Exploiting poor American naval command decisions, the German navy ravaged Allied shipping off the American Atlantic coast. [194] By November 1941, Commonwealth forces had launched a counter-offensive, Operation Crusader , in North Africa, and reclaimed all the gains the Germans and Italians had made. [195] In North Africa, the Germans launched an offensive in January, pushing the British back to positions at the Gazala Line by early February, [196] followed by a temporary lull in combat which Germany used to prepare for their upcoming offensives. [197] Concerns the Japanese might use bases in Vichy-held Madagascar caused the British to invade the island in early May 1942. [198] An Axis offensive in Libya forced an Allied retreat deep inside Egypt until Axis forces were stopped at El Alamein . [199] On the Continent, raids of Allied commandos on strategic targets, culminating in the disastrous Dieppe Raid , [200] demonstrated the Western Allies' inability to launch an invasion of continental Europe without much better preparation, equipment, and operational security. [201] [ page needed ]
In August 1942, the Allies succeeded in repelling a second attack against El Alamein [202] and, at a high cost, managed to deliver desperately needed supplies to the besieged Malta . [203] A few months later, the Allies commenced an attack of their own in Egypt, dislodging the Axis forces and beginning a drive west across Libya. [204] This attack was followed up shortly after by Anglo-American landings in French North Africa , which resulted in the region joining the Allies. [205] Hitler responded to the French colony's defection by ordering the occupation of Vichy France ; [205] although Vichy forces did not resist this violation of the armistice, they managed to scuttle their fleet to prevent its capture by German forces. [205] [206] The now pincered Axis forces in Africa withdrew into Tunisia , which was conquered by the Allies in May 1943. [205] [207]
In early 1943 the British and Americans began the Combined Bomber Offensive , a strategic bombing campaign against Germany. The goals were to disrupt the German war economy, reduce German morale, and " de-house " the civilian population. [208]

Allies gain momentum (1943–44)
After the Guadalcanal Campaign, the Allies initiated several operations against Japan in the Pacific. In May 1943, Canadian and U.S. forces were sent to eliminate Japanese forces from the Aleutians . [209] Soon after, the U.S., with support from Australian and New Zealand forces, began major operations to isolate Rabaul by capturing surrounding islands , and breach the Japanese Central Pacific perimeter at the Gilbert and Marshall Islands . [210] By the end of March 1944, the Allies had completed both of these objectives, and had also neutralised the major Japanese base at Truk in the Caroline Islands . In April, the Allies launched an operation to retake Western New Guinea . [211] In the Soviet Union, both the Germans and the Soviets spent the spring and early summer of 1943 preparing for large offensives in central Russia. On 4 July 1943, Germany attacked Soviet forces around the Kursk Bulge . Within a week, German forces had exhausted themselves against the Soviets' deeply echeloned and well-constructed defences [212] and, for the first time in the war, Hitler cancelled the operation before it had achieved tactical or operational success. [213] This decision was partially affected by the Western Allies' invasion of Sicily launched on 9 July which, combined with previous Italian failures, resulted in the ousting and arrest of Mussolini later that month. [214] Also, in July 1943 the British firebombed Hamburg killing over 40,000 people. [215]
On 12 July 1943, the Soviets launched their own counter-offensives , thereby dispelling any chance of German victory or even stalemate in the east. The Soviet victory at Kursk marked the end of German superiority, [216] giving the Soviet Union the initiative on the Eastern Front. [217] [218] The Germans tried to stabilise their eastern front along the hastily fortified Panther–Wotan line , but the Soviets broke through it at Smolensk and by the Lower Dnieper Offensives . [219]
On 3 September 1943, the Western Allies invaded the Italian mainland , following Italy's armistice with the Allies . [220] Germany responded by disarming Italian forces, seizing military control of Italian areas, [221] and creating a series of defensive lines. [222] German special forces then rescued Mussolini , who then soon established a new client state in German occupied Italy named the Italian Social Republic , [223] causing an Italian civil war . The Western Allies fought through several lines until reaching the main German defensive line in mid-November. [224]
German operations in the Atlantic also suffered. By May 1943, as Allied counter-measures became increasingly effective , the resulting sizeable German submarine losses forced a temporary halt of the German Atlantic naval campaign. [225] In November 1943, Franklin D. Roosevelt and Winston Churchill met with Chiang Kai-shek in Cairo and then with Joseph Stalin in Tehran . [226] The former conference determined the post-war return of Japanese territory [227] and the military planning for the Burma Campaign , [228] while the latter included agreement that the Western Allies would invade Europe in 1944 and that the Soviet Union would declare war on Japan within three months of Germany's defeat. [229]
From November 1943, during the seven-week Battle of Changde , the Chinese forced Japan to fight a costly war of attrition, while awaiting Allied relief. [230] [231] [232] In January 1944, the Allies launched a series of attacks in Italy against the line at Monte Cassino and tried to outflank it with landings at Anzio . [233] By the end of January, a major Soviet offensive expelled German forces from the Leningrad region , [234] ending the longest and most lethal siege in history .
The following Soviet offensive was halted on the pre-war Estonian border by the German Army Group North aided by Estonians hoping to re-establish national independence . This delay slowed subsequent Soviet operations in the Baltic Sea region. [235] By late May 1944, the Soviets had liberated Crimea , largely expelled Axis forces from Ukraine, and made incursions into Romania , which were repulsed by the Axis troops. [236] The Allied offensives in Italy had succeeded and, at the expense of allowing several German divisions to retreat, on 4 June, Rome was captured. [237]
The Allies had mixed success in mainland Asia. In March 1944, the Japanese launched the first of two invasions, an operation against British positions in Assam, India , [238] and soon besieged Commonwealth positions at Imphal and Kohima . [239] In May 1944, British forces mounted a counter-offensive that drove Japanese troops back to Burma, [239] and Chinese forces that had invaded northern Burma in late 1943 besieged Japanese troops in Myitkyina . [240] The second Japanese invasion of China aimed to destroy China's main fighting forces, secure railways between Japanese-held territory and capture Allied airfields. [241] By June, the Japanese had conquered the province of Henan and begun a new attack on Changsha in the Hunan province. [242]

Allies close in (1944)
On 6 June 1944 (known as D-Day ), after three years of Soviet pressure, [243] the Western Allies invaded northern France . After reassigning several Allied divisions from Italy, they also attacked southern France . [244] These landings were successful, and led to the defeat of the German Army units in France. Paris was liberated by the local resistance assisted by the Free French Forces , both led by General Charles de Gaulle , on 25 August [245] and the Western Allies continued to push back German forces in western Europe during the latter part of the year. An attempt to advance into northern Germany spearheaded by a major airborne operation in the Netherlands failed. [246] After that, the Western Allies slowly pushed into Germany, but failed to cross the Ruhr river in a large offensive. In Italy, Allied advance also slowed due to the last major German defensive line . [247]
On 22 June, the Soviets launched a strategic offensive in Belarus (" Operation Bagration ") that destroyed the German Army Group Centre almost completely. [248] Soon after that another Soviet strategic offensive forced German troops from Western Ukraine and Eastern Poland. The Soviet advance prompted resistance forces in Poland to initiate several uprisings against the German occupation. However, the largest of these in Warsaw , where German soldiers massacred 200,000 civilians, and a national uprising in Slovakia , did not receive Soviet support and were subsequently suppressed by the Germans. [249] The Red Army's strategic offensive in eastern Romania cut off and destroyed the considerable German troops there and triggered a successful coup d'état in Romania and in Bulgaria , followed by those countries' shift to the Allied side. [250]
In September 1944, Soviet troops advanced into Yugoslavia and forced the rapid withdrawal of German Army Groups E and F in Greece , Albania and Yugoslavia to rescue them from being cut off. [251] By this point, the Communist-led Partisans under Marshal Josip Broz Tito , who had led an increasingly successful guerrilla campaign against the occupation since 1941, controlled much of the territory of Yugoslavia and engaged in delaying efforts against German forces further south. In northern Serbia , the Red Army , with limited support from Bulgarian forces, assisted the Partisans in a joint liberation of the capital city of Belgrade on 20 October. A few days later, the Soviets launched a massive assault against German-occupied Hungary that lasted until the fall of Budapest in February 1945. [252] Unlike impressive Soviet victories in the Balkans, bitter Finnish resistance to the Soviet offensive in the Karelian Isthmus denied the Soviets occupation of Finland and led to a Soviet-Finnish armistice on relatively mild conditions, [253] [254] although Finland was forced to fight their former allies .
By the start of July 1944, Commonwealth forces in Southeast Asia had repelled the Japanese sieges in Assam, pushing the Japanese back to the Chindwin River [255] while the Chinese captured Myitkyina. In China, the Japanese had more successes, having finally captured Changsha in mid-June and the city of Hengyang by early August. [256] Soon after, they invaded the province of Guangxi, winning major engagements against Chinese forces at Guilin and Liuzhou by the end of November [257] and successfully linking up their forces in China and Indochina by mid-December. [258]
In the Pacific, US forces continued to press back the Japanese perimeter. In mid-June 1944, they began their offensive against the Mariana and Palau islands , and decisively defeated Japanese forces in the Battle of the Philippine Sea . These defeats led to the resignation of the Japanese Prime Minister, Hideki Tojo , and provided the United States with air bases to launch intensive heavy bomber attacks on the Japanese home islands. In late October, American forces invaded the Filipino island of Leyte ; soon after, Allied naval forces scored another large victory in the Battle of Leyte Gulf , one of the largest naval battles in history. [259]

Axis collapse, Allied victory (1944–45)
On 16 December 1944, Germany made a last attempt on the Western Front by using most of its remaining reserves to launch a massive counter-offensive in the Ardennes to split the Western Allies, encircle large portions of Western Allied troops and capture their primary supply port at Antwerp to prompt a political settlement. [260] By January, the offensive had been repulsed with no strategic objectives fulfilled. [260] In Italy, the Western Allies remained stalemated at the German defensive line. In mid-January 1945, the Soviets and Poles attacked in Poland, pushing from the Vistula to the Oder river in Germany, and overran East Prussia . [261] On 4 February, US, British, and Soviet leaders met for the Yalta Conference . They agreed on the occupation of post-war Germany, and on when the Soviet Union would join the war against Japan. [262]
In February, the Soviets entered Silesia and Pomerania , while Western Allies entered western Germany and closed to the Rhine river. By March, the Western Allies crossed the Rhine north and south of the Ruhr , encircling the German Army Group B , [263] while the Soviets advanced to Vienna . In early April, the Western Allies finally pushed forward in Italy and swept across western Germany, while Soviet and Polish forces stormed Berlin in late April. American and Soviet forces met at the Elbe river on 25 April. On 30 April 1945, the Reichstag was captured, signalling the military defeat of Nazi Germany. [264]
Several changes in leadership occurred during this period. On 12 April, President Roosevelt died and was succeeded by Harry S. Truman . Benito Mussolini was killed by Italian partisans on 28 April. [265] Two days later, Hitler committed suicide , and was succeeded by Grand Admiral Karl Dönitz . [266]
German forces surrendered in Italy on 29 April. Total and unconditional surrender was signed on 7 May , to be effective by the end of 8 May . [267] German Army Group Centre resisted in Prague until 11 May. [268]
In the Pacific theatre, American forces accompanied by the forces of the Philippine Commonwealth advanced in the Philippines , clearing Leyte by the end of April 1945. They landed on Luzon in January 1945 and recaptured Manila in March following a battle which reduced the city to ruins. Fighting continued on Luzon, Mindanao , and other islands of the Philippines until the end of the war . [269] Meanwhile, the United States Army Air Forces (USAAF) were destroying strategic and populated cities and towns in Japan in an effort to destroy Japanese war industry and civilian morale. On the night of 9–10 March, USAAF B-29 bombers struck Tokyo with thousands of incendiary bombs , which killed 100,000 civilians and destroyed 16 square miles (41 km 2 ) within a few hours. Over the next five months, the USAAF firebombed a total of 67 Japanese cities , killing 393,000 civilians and destroying 65% of built-up areas. [270]
In May 1945, Australian troops landed in Borneo , over-running the oilfields there. British, American, and Chinese forces defeated the Japanese in northern Burma in March, and the British pushed on to reach Rangoon by 3 May. [271] Chinese forces started to counterattack in Battle of West Hunan that occurred between 6 April and 7 June 1945. American naval and amphibious forces also moved towards Japan, taking Iwo Jima by March, and Okinawa by the end of June. [272] At the same time, American submarines cut off Japanese imports, drastically reducing Japan's ability to supply its overseas forces. [273]
On 11 July, Allied leaders met in Potsdam, Germany . They confirmed earlier agreements about Germany, [274] and reiterated the demand for unconditional surrender of all Japanese forces by Japan, specifically stating that "the alternative for Japan is prompt and utter destruction". [275] During this conference, the United Kingdom held its general election , and Clement Attlee replaced Churchill as Prime Minister. [276]
The Allies called for unconditional Japanese surrender in the Potsdam Declaration of 27 July, but the Japanese government rejected the call. In early August, the USAAF dropped atomic bombs on the Japanese cities of Hiroshima and Nagasaki . The Allies justified the atomic bombings as a military necessity to avoid invading the Japanese home islands which would cost the lives of between 250,000–500,000 Allied servicemen and millions of Japanese troops and civilians. [277] Between the two bombings, the Soviets, pursuant to the Yalta agreement, invaded Japanese-held Manchuria , and quickly defeated the Kwantung Army , which was the largest Japanese fighting force. [278] [279] The Red Army also captured Sakhalin Island and the Kuril Islands . On 15 August 1945, Japan surrendered , with the surrender documents finally signed on the deck of the American battleship USS Missouri on 2 September 1945, ending the war. [280]

Aftermath
The Allies established occupation administrations in Austria and Germany . The former became a neutral state, non-aligned with any political bloc. The latter was divided into western and eastern occupation zones controlled by the Western Allies and the USSR, accordingly. A denazification programme in Germany led to the prosecution of Nazi war criminals and the removal of ex-Nazis from power, although this policy moved towards amnesty and re-integration of ex-Nazis into West German society. [281]
Germany lost a quarter of its pre-war (1937) territory. Among the eastern territories, Silesia , Neumark and most of Pomerania were taken over by Poland, East Prussia was divided between Poland and the USSR, followed by the expulsion of the 9 million Germans from these provinces, as well as the expulsion of 3 million Germans from the Sudetenland in Czechoslovakia to Germany. By the 1950s, every fifth West German was a refugee from the east. The Soviet Union also took over the Polish provinces east of the Curzon line , from which 2 million Poles were expelled ; [282] north-east Romania, [283] [284] parts of eastern Finland, [285] and the three Baltic states were also incorporated into the USSR. [286] [287]
In an effort to maintain peace, [288] the Allies formed the United Nations , which officially came into existence on 24 October 1945, [289] and adopted the Universal Declaration of Human Rights in 1948, as a common standard for all member nations. [290] The great powers that were the victors of the war—the United States, Soviet Union, China, Britain, and France—formed the permanent members of the UN's Security Council . [7] The five permanent members remain so to the present, although there have been two seat changes, between the Republic of China and the People's Republic of China in 1971, and between the Soviet Union and its successor state , the Russian Federation , following the dissolution of the Soviet Union . The alliance between the Western Allies and the Soviet Union had begun to deteriorate even before the war was over. [291]
Germany had been de facto divided, and two independent states, the Federal Republic of Germany and the German Democratic Republic [292] were created within the borders of Allied and Soviet occupation zones, accordingly. The rest of Europe was also divided into Western and Soviet spheres of influence . [293] Most eastern and central European countries fell into the Soviet sphere , which led to establishment of Communist-led regimes, with full or partial support of the Soviet occupation authorities. As a result, Poland , Hungary , East Germany , [294] Czechoslovakia , Romania , and Albania [295] became Soviet satellite states . Communist Yugoslavia conducted a fully independent policy, causing tension with the USSR . [296]
Post-war division of the world was formalised by two international military alliances, the United States-led NATO and the Soviet-led Warsaw Pact ; [297] the long period of political tensions and military competition between them, the Cold War , would be accompanied by an unprecedented arms race and proxy wars . [298]
In Asia, the United States led the occupation of Japan and administrated Japan's former islands in the Western Pacific , while the Soviets annexed Sakhalin and the Kuril Islands . [299] Korea , formerly under Japanese rule , was divided and occupied by the Soviet Union in the North and the US in the South between 1945 and 1948. Separate republics emerged on both sides of the 38th parallel in 1948, each claiming to be the legitimate government for all of Korea, which led ultimately to the Korean War . [300]
In China, nationalist and communist forces resumed the civil war in June 1946. Communist forces were victorious and established the People's Republic of China on the mainland, while nationalist forces retreated to Taiwan in 1949. [301] In the Middle East, the Arab rejection of the United Nations Partition Plan for Palestine and the creation of Israel marked the escalation of the Arab–Israeli conflict . While European powers attempted to retain some or all of their colonial empires , their losses of prestige and resources during the war rendered this unsuccessful, leading to decolonisation . [302] [303]
The global economy suffered heavily from the war, although participating nations were affected differently. The US emerged much richer than any other nation; it had a baby boom and by 1950 its gross domestic product per person was much higher than that of any of the other powers and it dominated the world economy. [304] The UK and US pursued a policy of industrial disarmament in Western Germany in the years 1945–1948. [305] Because of international trade interdependencies this led to European economic stagnation and delayed European recovery for several years. [306] [307]
Recovery began with the mid-1948 currency reform in Western Germany , and was sped up by the liberalisation of European economic policy that the Marshall Plan (1948–1951) both directly and indirectly caused. [308] [309] The post-1948 West German recovery has been called the German economic miracle . [310] Italy also experienced an economic boom [311] and the French economy rebounded . [312] By contrast, the United Kingdom was in a state of economic ruin, [313] and although it received a quarter of the total Marshall Plan assistance, more than any other European country, [314] continued relative economic decline for decades. [315]
The Soviet Union, despite enormous human and material losses, also experienced rapid increase in production in the immediate post-war era. [316] Japan experienced incredibly rapid economic growth, becoming one of the most powerful economies in the world by the 1980s. [317] China returned to its pre-war industrial production by 1952. [318]

Impact

Casualties and war crimes
Estimates for the total number of casualties in the war vary, because many deaths went unrecorded. Most suggest that some 60 million people died in the war, including about 20 million military personnel and 40 million civilians. [319] [320] [321] Many of the civilians died because of deliberate genocide , massacres , mass-bombings , disease , and starvation .
The Soviet Union lost around 27 million people during the war, [322] including 8.7 million military and 19 million civilian deaths. The largest portion of military dead were 5.7 million ethnic Russians , followed by 1.3 million ethnic Ukrainians . [323] A quarter of the people in the Soviet Union were wounded or killed. [324] Germany sustained 5.3 million military losses, mostly on the Eastern Front and during the final battles in Germany. [325]
Of the total number of deaths in World War II, approximately 85 per cent—mostly Soviet and Chinese—were on the Allied side and 15 per cent were on the Axis side. Many of these deaths were caused by war crimes committed by German and Japanese forces in occupied territories. An estimated 11 [326] to 17 million [327] civilians died either as a direct or as an indirect result of Nazi ideological policies, including the systematic genocide of around 6 million Jews during the Holocaust , along with a further 5 to 6 million ethnic Poles and other Slavs (including Ukrainians and Belarusians ) [328] — Roma , homosexuals , and other ethnic and minority groups. [327] Hundreds of thousands (varying estimates) of ethnic Serbs , along with gypsies and Jews, were murdered by the Axis-aligned Croatian Ustaše in Yugoslavia , [329] and retribution-related killings were committed just after the war ended.
In Asia and the Pacific, between 3 million and more than 10 million civilians, mostly Chinese (estimated at 7.5 million [330] ), were killed by the Japanese occupation forces. [331] The best-known Japanese atrocity was the Nanking Massacre , in which fifty to three hundred thousand Chinese civilians were raped and murdered. [332] Mitsuyoshi Himeta reported that 2.7 million casualties occurred during the Sankō Sakusen . General Yasuji Okamura implemented the policy in Heipei and Shantung . [333]
Axis forces employed biological and chemical weapons . The Imperial Japanese Army used a variety of such weapons during its invasion and occupation of China ( see Unit 731 ) [334] [335] and in early conflicts against the Soviets . [336] Both the Germans and Japanese tested such weapons against civilians [337] and, sometimes on prisoners of war . [338]
The Soviet Union was responsible for the Katyn massacre of 22,000 Polish officers, [339] and the imprisonment or execution of thousands of political prisoners by the NKVD , [340] in the Baltic states , and eastern Poland annexed by the Red Army.
The mass-bombing of cities in Europe and Asia has often been called a war crime. However, no positive or specific customary international humanitarian law with respect to aerial warfare existed before or during World War II. [341]

Concentration camps, slave labour, and genocide
The German government led by Adolf Hitler and the Nazi Party was responsible for the Holocaust , the killing of approximately 6 million Jews, as well as 2.7 million ethnic Poles , [342] and 4 million others who were deemed " unworthy of life " (including the disabled and mentally ill , Soviet prisoners of war , homosexuals , Freemasons , Jehovah's Witnesses , and Romani ) as part of a programme of deliberate extermination. About 12 million, most of whom were Eastern Europeans , were employed in the German war economy as forced labourers . [343]
In addition to Nazi concentration camps , the Soviet gulags ( labour camps ) led to the death of citizens of occupied countries such as Poland, Lithuania, Latvia, and Estonia, as well as German prisoners of war (POWs) and even Soviet citizens who had been or were thought to be supporters of the Nazis. [344] Sixty per cent of Soviet POWs of the Germans died during the war. [345] Richard Overy gives the number of 5.7 million Soviet POWs. Of those, 57 per cent died or were killed, a total of 3.6 million. [346] Soviet ex-POWs and repatriated civilians were treated with great suspicion as potential Nazi collaborators, and some of them were sent to the Gulag upon being checked by the NKVD. [347]
Japanese prisoner-of-war camps , many of which were used as labour camps, also had high death rates. The International Military Tribunal for the Far East found the death rate of Western prisoners was 27.1 per cent (for American POWs, 37 per cent), [348] seven times that of POWs under the Germans and Italians. [349] While 37,583 prisoners from the UK, 28,500 from the Netherlands, and 14,473 from the United States were released after the surrender of Japan , the number of Chinese released was only 56. [350]
According to historian Zhifen Ju, at least five million Chinese civilians from northern China and Manchukuo were enslaved between 1935 and 1941 by the East Asia Development Board , or Kōain , for work in mines and war industries. After 1942, the number reached 10 million. [351] The US Library of Congress estimates that in Java , between 4 and 10 million rōmusha (Japanese: "manual labourers"), were forced to work by the Japanese military. About 270,000 of these Javanese labourers were sent to other Japanese-held areas in South East Asia, and only 52,000 were repatriated to Java. [352]
On 19 February 1942, Roosevelt signed Executive Order 9066 , interning about 100,000 Japanese living on the West Coast. Canada had a similar programme. [353] [354] In addition, 14,000 German and Italian citizens who had been assessed as being security risks were also interned. [355]
In accordance with the Allied agreement made at the Yalta Conference millions of POWs and civilians were used as forced labour by the Soviet Union . [356] In Hungary's case, Hungarians were forced to work for the Soviet Union until 1955. [357]

Occupation
In Europe, occupation came under two forms. In Western, Northern, and Central Europe (France, Norway, Denmark, the Low Countries, and the annexed portions of Czechoslovakia ) Germany established economic policies through which it collected roughly 69.5 billion reichmarks (27.8 billion US Dollars) by the end of the war, this figure does not include the sizeable plunder of industrial products, military equipment, raw materials and other goods. [358] Thus, the income from occupied nations was over 40 per cent of the income Germany collected from taxation, a figure which increased to nearly 40 per cent of total German income as the war went on. [359]
In the East, the much hoped for bounties of Lebensraum were never attained as fluctuating front-lines and Soviet scorched earth policies denied resources to the German invaders. [360] Unlike in the West, the Nazi racial policy encouraged extreme brutality against what it considered to be the " inferior people " of Slavic descent; most German advances were thus followed by mass executions . [361] Although resistance groups formed in most occupied territories, they did not significantly hamper German operations in either the East [362] or the West [363] until late 1943.
In Asia, Japan termed nations under its occupation as being part of the Greater East Asia Co-Prosperity Sphere , essentially a Japanese hegemony which it claimed was for purposes of liberating colonised peoples. [364] Although Japanese forces were originally welcomed as liberators from European domination in some territories, their excessive brutality turned local public opinion against them within weeks. [365] During Japan's initial conquest it captured 4,000,000 barrels (640,000 m 3 ) of oil (~5.5×10 5 tonnes) left behind by retreating Allied forces, and by 1943 was able to get production in the Dutch East Indies up to 50 million barrels (~6.8 × 10 ^ 6 t), 76 per cent of its 1940 output rate. [365]

Home fronts and production
In Europe, before the outbreak of the war, the Allies had significant advantages in both population and economics. In 1938, the Western Allies (United Kingdom, France, Poland and British Dominions) had a 30 per cent larger population and a 30 per cent higher gross domestic product than the European Axis powers (Germany and Italy); if colonies are included, it then gives the Allies more than a 5:1 advantage in population and nearly 2:1 advantage in GDP. [366] In Asia at the same time, China had roughly six times the population of Japan, but only an 89 per cent higher GDP; this is reduced to three times the population and only a 38 per cent higher GDP if Japanese colonies are included. [366]
Though the Allies' economic and population advantages were largely mitigated during the initial rapid blitzkrieg attacks of Germany and Japan, they became the decisive factor by 1942, after the United States and Soviet Union joined the Allies, as the war largely settled into one of attrition . [367] While the Allies' ability to out-produce the Axis is often attributed to the Allies having more access to natural resources, other factors, such as Germany and Japan's reluctance to employ women in the labour force , [368] Allied strategic bombing , [369] and Germany's late shift to a war economy [370] contributed significantly. Additionally, neither Germany nor Japan planned to fight a protracted war, and were not equipped to do so. [371] To improve their production, Germany and Japan used millions of slave labourers ; [372] Germany used about 12 million people, mostly from Eastern Europe, [343] while Japan used more than 18 million people in Far East Asia. [351] [352]

Advances in technology and warfare
Aircraft were used for reconnaissance , as fighters , bombers , and ground-support , and each role was advanced considerably. Innovation included airlift (the capability to quickly move limited high-priority supplies, equipment, and personnel); [373] and of strategic bombing (the bombing of enemy industrial and population centres to destroy the enemy's ability to wage war). [374] Anti-aircraft weaponry also advanced, including defences such as radar and surface-to-air artillery, such as the German 88 mm gun . The use of the jet aircraft was pioneered and, though late introduction meant it had little impact, it led to jets becoming standard in air forces worldwide. [375]
Advances were made in nearly every aspect of naval warfare , most notably with aircraft carriers and submarines. Although aeronautical warfare had relatively little success at the start of the war, actions at Taranto , Pearl Harbor , and the Coral Sea established the carrier as the dominant capital ship in place of the battleship. [376] [377] [378]
In the Atlantic, escort carriers proved to be a vital part of Allied convoys, increasing the effective protection radius and helping to close the Mid-Atlantic gap . [379] Carriers were also more economical than battleships because of the relatively low cost of aircraft [380] and their not requiring to be as heavily armoured. [381] Submarines, which had proved to be an effective weapon during the First World War , [382] were anticipated by all sides to be important in the second. The British focused development on anti-submarine weaponry and tactics, such as sonar and convoys, while Germany focused on improving its offensive capability, with designs such as the Type VII submarine and wolfpack tactics. [383] Gradually, improving Allied technologies such as the Leigh light , hedgehog , squid , and homing torpedoes proved victorious.
Land warfare changed from the static front lines of World War I to increased mobility and combined arms . The tank , which had been used predominantly for infantry support in the First World War, had evolved into the primary weapon. [384] In the late 1930s, tank design was considerably more advanced than it had been during World War I, [385] and advances continued throughout the war with increases in speed, armour and firepower.
At the start of the war, most commanders thought enemy tanks should be met by tanks with superior specifications. [386] This idea was challenged by the poor performance of the relatively light early tank guns against armour, and German doctrine of avoiding tank-versus-tank combat. This, along with Germany's use of combined arms, were among the key elements of their highly successful blitzkrieg tactics across Poland and France. [384] Many means of destroying tanks , including indirect artillery , anti-tank guns (both towed and self-propelled ), mines , short-ranged infantry antitank weapons, and other tanks were utilised. [386] Even with large-scale mechanisation, infantry remained the backbone of all forces, [387] and throughout the war, most infantry were equipped similarly to World War I. [388]
The portable machine gun spread, a notable example being the German MG34 , and various submachine guns which were suited to close combat in urban and jungle settings. [388] The assault rifle , a late war development incorporating many features of the rifle and submachine gun, became the standard postwar infantry weapon for most armed forces. [389] [390]
Most major belligerents attempted to solve the problems of complexity and security involved in using large codebooks for cryptography by designing ciphering machines, the most well known being the German Enigma machine . [391] Development of SIGINT ( sig nals int elligence) and cryptanalysis enabled the countering process of decryption. Notable examples were the Allied decryption of Japanese naval codes [392] and British Ultra , a pioneering method for decoding Enigma benefiting from information given to Britain by the Polish Cipher Bureau , which had been decoding early versions of Enigma before the war. [393] Another aspect of military intelligence was the use of deception , which the Allies used to great effect, such as in operations Mincemeat and Bodyguard . [392] [394] Other technological and engineering feats achieved during, or as a result of, the war include the world's first programmable computers ( Z3 , Colossus , and ENIAC ), guided missiles and modern rockets , the Manhattan Project 's development of nuclear weapons , operations research and the development of artificial harbours and oil pipelines under the English Channel .

See also
See also List of World War II documentary films

Notes

Citations
WebPage index: 00015
Thrust reversal
Thrust reversal , also called reverse thrust , is the temporary diversion of an aircraft engine 's thrust so that it is directed forward, rather than backwards. Reverse thrust acts against the forward travel of the aircraft, providing deceleration . Thrust reverser systems are featured on many jet aircraft to help slow down just after touch-down, reducing wear on the brakes and enabling shorter landing distances. Such devices affect the aircraft significantly and are considered important for safe operations by airlines . There have been accidents involving thrust reversal systems such as Lauda Air Flight 004 .
Reverse thrust is also available on many propeller-driven aircraft through reversing the controllable-pitch propellers to a negative angle. The equivalent concept for a ship is called astern propulsion .

Principle and uses
A landing roll consists of touchdown, bringing the aircraft to taxi speed, and eventually to a complete stop. However, most commercial jet engines continue to produce thrust in the forward direction, even when idle, acting against the deceleration of the aircraft. [1] The brakes of the landing gear of most modern aircraft are sufficient in normal circumstances to stop the aircraft by themselves, but for safety purposes, and to reduce the stress on the brakes, [2] another deceleration method is needed. In scenarios involving bad weather, where factors like snow or rain on the runway reduce the effectiveness of the brakes, and in emergencies like rejected takeoffs , this need is more pronounced. [3]
A simple and effective method is to reverse the direction of the exhaust stream of the jet engine and use the power of the engine itself to decelerate. Ideally, the reversed exhaust stream would be directed straight forward. [4] However, for aerodynamic reasons, this is not possible, and a 135° angle is taken, resulting in less effectiveness than would otherwise be possible. Thrust reversal can also be used in flight to reduce airspeed, though this is not common with modern aircraft. [5] There are three common types of thrust reversing systems used on jet engines: the target, clam-shell, and cold stream systems. Some propeller-driven aircraft equipped with variable-pitch propellers can reverse thrust by changing the pitch of their propeller blades. Most commercial jetliners have such devices, and it also has applications in military aviation. [4]

Types of thrust reversal systems
Small aircraft typically do not have thrust reversal systems, except in specialized applications. On the other hand, large aircraft (those weighing more than 12,500 lb) almost always have the ability to reverse thrust. [ citation needed ] Reciprocating engine , turboprop and jet aircraft can all be designed to include thrust reversal systems.

Propeller-driven aircraft
Propeller-driven aircraft generate reverse thrust by changing the angle of their controllable-pitch propellers so that the propellers direct their thrust forward. This reverse thrust feature became available with the development of controllable-pitch propellers, which change the angle of the propeller blades to make efficient use of engine power over a wide range of conditions. Single-engine aircraft tend not to have reverse thrust. However, single-engine turboprop aircraft such as the PAC P-750 XSTOL , [6] Cessna 208 Caravan , and Pilatus PC-6 Porter do have this feature available. [ citation needed ]
One special application of reverse thrust comes in its use on multi-engine seaplanes and flying boats . These aircraft, when landing on water, have no conventional braking method and must rely on slaloming and/or reverse thrust, as well as the drag of the water in order to slow or stop. In addition, reverse thrust is often necessary for maneuvering on the water, where it is used to make tight turns or even propel the aircraft in reverse, maneuvers which may prove necessary for leaving a dock or beach. [ citation needed ]

Jet aircraft
On aircraft using jet engines, thrust reversal is accomplished by causing the jet blast to flow forward. The engine does not run or rotate in reverse; instead, thrust reversing devices are used to block the blast and redirect it forward. High bypass ratio engines usually reverse thrust by changing the direction of only the fan airflow, since the majority of thrust is generated by this section, as opposed to the core. There are three jet engine thrust reversal systems in common use: [5]

Target type
The target thrust reverser uses a pair of hydraulically -operated 'bucket' type doors to reverse the hot gas stream. For forward thrust, these doors form the propelling nozzle of the engine. In the original implementation of this system on the Boeing 707 , [7] and still common today, two reverser buckets were hinged so when deployed they block the rearward flow of the exhaust and redirect it with a forward component. This type of reverser is visible at the rear of the engine during deployment. [5]

Clam-shell type
The clam-shell door, or cascade, system is pneumatically operated. When activated, the doors rotate to open the ducts and close the normal exit, causing the thrust to be directed forward. [5] The cascade thrust reverser is commonly used on turbofan engines. On turbojet engines, this system would be less effective than the target system, as the cascade system only makes use of the fan airflow and does not affect the main engine core, which continues to produce thrust. [1]

Cold stream type
In addition to the two types used on turbojet and low-bypass turbofan engines, a third type of thrust reverser is found on some high-bypass turbofan engines. Doors in the bypass duct are used to redirect the air that is accelerated by the engine's fan section but does not pass through the combustion chamber (called bypass air) such that it provides reverse thrust. [3] The cold stream reverser system is activated by an air motor. During normal operation, the reverse thrust vanes are blocked. On selection, the system folds the doors to block off the cold stream final nozzle and redirect this airflow to the cascade vanes. [5] This system can redirect both the exhaust flow of the fan and of the core. [4]
The cold stream system is known for structural integrity, reliability, and versatility. During thrust reverser activation, a sleeve mounted around the perimeter of the aircraft engine nacelle moves aft to expose cascade vanes which act to redirect the engine fan flow. This thrust reverser system can be heavy and difficult to integrate into nacelles housing large engines. [8]

Operation
In most cockpit setups, reverse thrust is set when the thrust levers are on idle by pulling them further back. [1] Reverse thrust is typically applied immediately after touchdown, often along with spoilers , to improve deceleration early in the landing roll when residual aerodynamic lift and high speed limit the effectiveness of the brakes located on the landing gear. Reverse thrust is always selected manually, either using levers attached to the thrust levers or moving the thrust levers into a reverse thrust 'gate'.
The early deceleration provided by reverse thrust can reduce landing roll by a quarter or more. [4] Regulations dictate, however, that an aircraft must be able to land on a runway without the use of thrust reversal in order to be certified to land there as part of scheduled airline service.
Once the aircraft's speed has slowed, reverse thrust is shut down to prevent the reversed airflow from throwing debris in front of the engine intakes where it can be ingested, causing foreign object damage . If circumstances require it, reverse thrust can be used all the way to a stop, or even to provide thrust to push the aircraft backward, though aircraft tugs or towbars are more commonly used for that purpose. When reverse thrust is used to push an aircraft back from the gate, the maneuver is called a powerback . Some manufacturers warn against the use of this procedure during icy conditions as using reverse thrust on snow- or slush-covered ground can cause slush, water, and runway deicers to become airborne and adhere to wing surfaces. [9]
If the full power of reverse thrust is not desirable, thrust reverse can be operated with the throttle set at less than full power, even down to idle power, which reduces stress and wear on engine components. Reverse thrust is sometimes selected on idling engines to eliminate residual thrust, in particular in icy or slick conditions, or when the engines' jet blast could cause damage. [ citation needed ]

In-flight operation
Some aircraft, notably some Russian and Soviet aircraft , are able to safely use reverse thrust in flight, though the majority of these are propeller-driven. Many commercial aircraft, however, cannot. In-flight use of reverse thrust has several advantages. It allows for rapid deceleration, enabling quick changes of speed. It also prevents the speed build-up normally associated with steep dives, allowing for rapid loss of altitude , which can be especially useful in hostile environments such as combat zones, and when making steep approaches to land. [ citation needed ]
The Douglas DC-8 series of airliners has been certified for in-flight reverse thrust since service entry in 1959. Safe and effective for facilitating quick descents at acceptable speeds, it nonetheless produced significant aircraft buffeting, so actual use was less common on passenger flights and more common on cargo and ferry flights, where passenger comfort is not a concern. [10]
The Hawker Siddeley Trident , a 120- to 180-seat airliner, was capable of descending at up to 10,000 ft/min (3,050 m/min) by use of reverse thrust, though this capability was rarely used.
The Concorde supersonic airliner could use reverse thrust in the air to increase the rate of descent. Only the inboard engines were used, and the engines were placed in reverse idle only in subsonic flight and when the aircraft was below 30,000 ft in altitude. This would increase the rate of descent to around 10,000 ft/min. [ citation needed ]
The Boeing C-17 Globemaster III is one of the few modern aircraft that uses reverse thrust in flight. The Boeing-manufactured aircraft is capable of in-flight deployment of reverse thrust on all four engines to facilitate steep tactical descents up to 15,000 ft/min (4,600 m/min) into combat environments (a descent rate of just over 170 mph, or 274 km/h). The Lockheed C-5 Galaxy , introduced in 1969, also has in-flight reverse capability, although on the inboard engines only. [11]
The Saab 37 Viggen (retired in November 2005) also had the ability to use reverse thrust both before landing, to shorten the needed runway, and taxiing after landing, allowing many Swedish roads to double as wartime runways .
The Shuttle Training Aircraft , a highly modified Grumman Gulfstream II , used reverse thrust in flight to help simulate Space Shuttle aerodynamics so astronauts could practice landings. A similar technique was employed on a modified Tupolev Tu-154 which simulated the Russian Buran space shuttle. [ citation needed ]

Effectiveness
The amount of thrust and power generated are proportional to the speed of the aircraft, making reverse thrust more effective at high speeds. [2] For maximum effectiveness, it should be applied quickly after touchdown. [1] If activated at low speeds, foreign object damage is possible. There is some danger of an aircraft with thrust reversers applied momentarily leaving the ground again due to both the effect of the reverse thrust and the nose-up pitch effect from the spoilers . For aircraft susceptible to such an occurrence, pilots must take care to achieve a firm position on the ground before applying reverse thrust. [2] If applied before the nose-wheel is in contact with the ground, there is a chance of asymmetric deployment causing an uncontrollable yaw towards the side of higher thrust, as steering the aircraft with the nose wheel is the only way to maintain control of the direction of travel in this situation. [1]
Reverse thrust mode is used only for a fraction of aircraft operating time but affects it greatly in terms of design , weight, maintenance , performance, and cost. Penalties are significant but necessary since it provides stopping force for added safety margins, directional control during landing rolls, and aids in rejected take-offs and ground operations on contaminated runways where normal braking effectiveness is diminished. Airlines consider thrust reverser systems a vital part of reaching a maximum level of aircraft operating safety . [8]

Thrust reversal-related accidents and incidents
In-flight deployment of reverse thrust has directly contributed to the crashes of several transport-type aircraft:

See also
WebPage index: 00016
May 27
May 27 is the 147th day of the year (148th in leap years ) in the Gregorian calendar . There are 218 days remaining until the end of the year. This date is slightly more likely to fall on a Wednesday, Friday or Sunday (58 in 400 years each) than on Monday or Tuesday (57), and slightly less likely to occur on a Thursday or Saturday (56).

Events

Births

Deaths

Holidays and observances

External links
WebPage index: 00017
Billboard (magazine)
Billboard (stylized as billboard ) is an American entertainment media brand owned by the Hollywood Reporter-Billboard Media Group, a division of Eldridge Industries . It publishes pieces involving news, video, opinion, reviews, events and style. It is also known for its music charts , including the Billboard Hot 100 and Billboard 200 , tracking the most popular singles and albums in different genres. It also hosts events, owns a publishing firm, and operates several TV shows. Billboard was founded in 1894 by William Donaldson and James Hennegan as a trade publication for bill posters. Donaldson later acquired Hennegen's interest in 1900 for $500.
In the 1900s, it covered the entertainment industry, such as circuses, fairs and burlesque shows. It also created a mail service for travelling entertainers. Billboard began focusing more on the music industry as the jukebox , phonograph and radio became commonplace. Many topics it covered were spun-off into different magazines, including Amusement Business in 1961 to cover outdoor entertainment so that it could focus on music. After Donaldson died in 1925, Billboard was passed down to his children and Hennegan's children, until it was sold to private investors in 1985, and has since been owned by various parties.

History

Early history
The first issue of Billboard was published in Cincinnati, Ohio, on November 1, 1894 by William Donaldson and James Hennegan. [2] [3] Initially, it covered the advertising and bill posting industry and was called Billboard Advertising . [4] [5] [a] At the time, billboards, posters and paper advertisements placed in public spaces were the primary means of advertising. [5] Donaldson handled editorial and advertising, while Hennegan, who owned Hennegan Printing Co., managed magazine production. The first issues were just eight pages long. [6] The paper had columns like "The Bill Room Gossip" and "The Indefatigable and Tireless Industry of the Bill Poster." [2] A department for agricultural fairs was established in 1896. [7] The title was changed to The Billboard in 1897. [8]
After a brief departure over editorial differences, Donaldson purchased Hennegan's interest in the business in 1900 for $500, to save it from bankruptcy. [6] [9] That May, Donaldson changed it from a monthly to a weekly paper with a greater emphasis on breaking news. He improved editorial quality and opened new offices in New York, Chicago, San Francisco, London and Paris. [8] [9] He also re-focused the magazine on outdoor entertainment like fairs, carnivals, circuses, vaudeville and burlesque shows. [2] [8] A section devoted to circuses was introduced in 1900, followed by more prominent coverage of outdoor events in 1901. [7] Billboard also covered topics including regulation, a lack of professionalism, economics and new shows. It had a "stage gossip" column covering the private lives of entertainers, a "tent show" section covering traveling shows and a sub-section called "Freaks to order." [2] According to The Seattle Times , Donaldson also published news articles "attacking censorship, praising productions exhibiting 'good taste' and fighting yellow journalism." [10]
As railroads became more developed, Billboard set up a mail forwarding system for traveling entertainers. The location of an entertainer was tracked in the paper's Routes Ahead column, then Billboard would receive mail on the star's behalf and publish a notice in its "Letter-Box" column that it has mail for them. [2] This service was first introduced in 1904. It became one of Billboard ' s largest sources of profit [10] and celebrity connections. [2] By 1914, there were 42,000 people using the service. [6] It was also used as the official address of traveling entertainers for draft letters during World War I . [11] In the 1960s, when it was discontinued, Billboard was still processing 1,500 letters per week. [10]
In 1920, Donaldson made a then-controversial move by hiring an African-American journalist James Albert Jackson to write a weekly column devoted to African-American performers. [2] According to The Business of Culture: Strategic Perspectives on Entertainment and Media , the column identified discrimination against black performers and helped validate their careers. [2] Jackson was the first black critic at a national magazine with a predominantly white audience. According to his grandson, Donaldson also established a policy against identifying performers by their race. [10] Donaldson died in 1925. [2]

Focus on music
Billboard ' s editorial changed focus as technology in recording and playback developed. It covered "marvels of modern technology" like the phonograph , record players and wireless radios. [2] It began covering coin-operated entertainment machines in 1899 and created a dedicated section for them called "Amusement Machines" in March 1932. [9] Billboard began covering the motion picture industry in 1907, [7] but ended up focusing on music due to competition from Variety . [12] It created a Billboard radio broadcasting station in the 1920s. [8]
The jukebox industry continued to grow through the Great Depression and advertised heavily in Billboard . [8] :262 This led to even more editorial focus on music. [8] The proliferation of the phonograph and radio also contributed to its growing music emphasis. [8] Billboard published the first music hit parade on January 4, 1936, [13] and introduced a "Record Buying Guide" in January 1939. [9] In 1940, it introduced "Chart Line", which tracks the best-selling records. This was followed by a chart for jukebox records in 1944 called Music BoxMachine charts. [8] [9] By the 1940s, Billboard was more of a music industry specialist publication. [4] The number of charts it published grew after World War II , due to a growing variety of music interests and genres. It had eight charts by 1987, covering different genres and formats, [9] and 28 charts by 1994. [10]
By 1943, it had about 100 employees. [7] The magazine's offices moved to Brighton, Ohio in 1946, then to New York City in 1948. [10] A five-column tabloid format was adopted in November 1950 and coated paper was first used in Billboard ' s print issues in January 1963, allowing for photojournalism. [9] Billboard Publications Inc. acquired a monthly trade magazine for candy and cigarette machine vendors called Vend and, in the 1950s, acquired an advertising trade publication called Tide . [8] By 1969, Billboard Publications Inc. owned eleven trade and consumer publications, a publisher called Guptill Publications, a set of self-study cassette tapes and four television franchises. It also acquired Photo Weekly that year. [8]
Over time, the subjects Billboard still covered outside of music were spun-off into separate publications. Funspot magazine was created in 1957 to cover amusement parks and Amusement Business was created in 1961 to cover outdoor entertainment. In January 1961, Billboard was renamed to Billboard Music Week [5] [8] to emphasize its new exclusive interest in music. [12] Two years later, it was renamed to just Billboard . [8] [9] According to The New Business Journalism , by 1984, Billboard Publications was a "prosperous" conglomerate of trade magazines and Billboard had become the "undisputed leader" in music industry news. [4] In the early 1990s, Billboard introduced Billboard Airplay Monitors , a publication for disc jockeys and music programmers. [5] By the end of the 1990s, Billboard dubbed itself the "bible" of the recording industry. [5]

Changes in ownership
Billboard struggled after its founder William Donaldson died in 1925 and within three years was once again heading towards bankruptcy . [8] Donaldson's son-in-law Roger Littleford took over in 1928 and "nursed the publication back to health." [8] [11] His sons, Bill and Roger, became co-publishers in 1946 [11] and inherited the publication in the late 1970s after Roger Littleford's death. [8] They sold it to private investors in 1985 for an estimated $40 million. [14] The investors cut costs and acquired a trade publication for the Broadway theatre industry called Backstage . [8]
In 1987, Billboard was sold again to Affiliated Publications for $100 million. [14] Billboard Publications Inc. became a subsidiary of Affiliated Publications called BPI Communications. [8] As BPI Communications, it acquired The Hollywood Reporter , Adweek , Marketing Week and Mediaweek . It purchased Broadcast Data Systems , which is a high-tech firm for tracking music airtime. [8] Private investors from Boston Ventures and BPI executives re-purchased a two-thirds interest in Billboard Publications for $100 million and more acquisitions followed. In 1993, it created a division called Billboard Music Group for music-related publications. [8]
In 1994, Billboard Publications was sold to a Dutch media conglomerate, Verenigde Nederlandse Uitgeverijen (VNU), for $220 million. [15] [b] VNU acquired the Clio Awards in advertising and the National Research Group in 1997, as well as Editor & Publisher in 1999. In July 2000, it paid $650 million for the publisher Miller Freeman. BPI was combined with other entities in VNU in 2000 to form Bill Communications Inc. By time CEO Gerald Hobbs retired in 2003, VNU had grown substantially larger, but it had a large amount of debt from the acquisitions. An attempted $7 billion acquisition of IMS Health in 2005 prompted protests from shareholders that halted the deal. It eventually agreed to an $11 billion takeover bid from investors in 2006. [8]
VNU then changed its name to Nielsen in 2007, the namesake of a company it acquired for $2.5 billion in 1999. [17] [18] New CEO Robert Krakoff divested some of the previously owned publications, restructured the organization, and planned some acquisitions before dying suddenly in 2007 to be replaced by Greg Farrar. [8]
Nielsen owned Billboard until 2009, when it was one of eight publications sold to e5 Global Media Holdings. e5 was formed by investment firms Pluribus Capital Management and Guggenheim Partners for the purpose of the acquisition. [19] [20] The following year, the new parent company was renamed to Prometheus Global Media. [21] Three years later, Guggenheim Partners acquired Pluribus' share of Prometheus and became the sole owner of Billboard . [22] [23]
In December 2015, Guggenheim Digital Media spun out several media brands, including Billboard , to its own executive, Todd Boehly. [24] [25] The assets operate under the Hollywood Reporter-Billboard Media Group, a unit of the holding company Eldridge Industries . [26]

1990s–present
Timothy White was appointed Editor in Chief in 1991, a position he held until his unexpected death in 2002. White wrote a weekly column promoting music with "artistic merit," while criticizing music with violent or misogynistic themes. [27] He reworked the publication's music charts. [27] Rather than rely on data from music retailers, new charts used data from store checkout scanners obtained from Nielsen SoundScan . [8] He also wrote in-depth profiles on musicians. [28] The website, Billboard.com, was launched in 1995. [14] Keith Girard replaced White before being fired in May 2004. He and a female employee filed a $29 million lawsuit alleging Billboard fired them unfairly with an intent to damage their reputations. [29] The lawsuit claimed they experienced sexual harassment, a hostile work environment and a financially motivated lack of editorial integrity. [29] [30] Email evidence suggested human resources were given special instructions to watch minority employees. [30] The case was settled out-of-court in 2006 for a non-disclosed sum. [31]
In the 2000s, economic decline in the music industry dramatically reduced readership and advertising from Billboard ' s traditional audience. [29] [32] Circulation declined from 40,000 in circulation in the 1990s to less than 17,000 by 2014. [31] The publication's staff and ownership were also undergoing frequent changes. [30] In 2005 Billboard expanded its editorial outside the music industry into other areas of digital and mobile entertainment. [14]
Bill Werde was named editorial director in 2008, [33] and was followed by Janice Min in January 2014, who is also responsible for editorial content at The Hollywood Reporter . [33] The magazine has since been making changes to make it more of a general interest music news source, as opposed to solely an industry trade. It started covering more celebrity, fashion, and gossip. [31] [32] [34] Min hired Tony Gervino as the publication's editor, which was different than Billboard's historical appointments, in that he did not have a background in the music industry. [34] Tony Gervino was appointed Editor in Chief in April 2014. [35] An item on NPR covered a leaked version of Billboard ' s annual survey, which it said had more gossip and focused on less professional topics than prior surveys. For example, it polled readers on a lawsuit pop-star Kesha filed against her producer alleging sexual abuse. [31]
Gervino was let go in May 2016. A note from Min to the editorial staff indicated that Senior Vice President of Digital Content Mike Bruno would serve as the head of editorial moving forward. [36]

News publishing
Billboard publishes a news website and weekly magazine that cover music, video and home entertainment. Most of the articles are written by staff writers, while some are written by industry experts. [9] It covers news, gossip, opinion, [2] and music reviews, but its "most enduring and influential creation" is the Billboard charts . [5] The charts track music sales, radio airtime and other data about the most popular songs and albums. [5] The Billboard Hot 100 chart of the top-selling songs was introduced in 1955. Since then, the Billboard 200 , which tracks the top-selling albums, has become more popular as an indicator of commercial success. [2] Billboard has also published books in collaboration with Watson-Guptill and a radio and television series called American Top Forty, based on Billboard charts. [9] A daily Billboard Bulletin was introduced in February 1997 [5] and Billboard hosts about 20 industry events each year. [1]
Billboard is considered one of the most reputable sources of music industry news. [10] [32] It has a print circulation of 17,000 and an online readership of 1.2 million unique monthly views. The website includes the Billboard Charts, news separated by music genre, videos, and a separate website. It also compiles lists, hosts a fashion website called Pret-a-Reporter, and publishes eight different newsletters. The print magazine's regular sections include: [1]

Archives

See also

Notes
WebPage index: 00018
Independent music
Independent music (often shortened to indie music or indie ) is music produced independently from major commercial record labels or their subsidiaries, a process that may include an autonomous, do-it-yourself approach to recording and publishing. The term indie is sometimes also used to describe a genre (such as indie rock and indie pop ); as a genre term, "indie" may include music that is not independently produced, and most independent music artists do not fall into a single, defined musical style or genre, and usually create music that can be categorized into other genres. [ not verified in body ]

Record labels
Independent labels have a long history of promoting developments in popular music, stretching back to the post-war period in the United States, with labels such as Sun Records , King Records , and Stax . [1]
In the United Kingdom during the 1950s and 1960s, the major record companies had so much power that independent labels struggled to become established. Several British producers and artists launched independent labels as outlets for their work and artists they liked, but the majority failed as commercial ventures and were swallowed up by the majors. [1]
In the United States, independent labels and distributors often banded together to form organizations to promote trade and parity within the industry. The National Academy of Recording Arts and Sciences (NARAS), famous as the organization behind the Grammy Awards , began in the 1950s as an organization of 25 independent record labels including Herald , Ember , and Atlantic Records . The 1970s saw the founding of the National Association of Independent Record Distributors (NAIRD), which became A2IM in 2004. Smaller organizations also existed including the Independent Music Association (IMA), founded by Don Kulak in the late 1980s. At its zenith, it had 1,000 independent labels on its member rosters. The 1990s brought Affiliated Independent Record Companies (AIRCO), whose most notable member was upstart punk - thrash rock [ disambiguation needed ] label Mystic Records , and The Independent Music Retailer's Association (IMRA), a short-lived organization founded by Mark Wilkins and Don Kulak. The latter is most notable for a lawsuit involving co-op money it filed on behalf of its member Digital Distributors in conjunction with Warehouse Record Stores. [2] The adjudication of the case grossed $178,000,000 from the distribution arms of major labels. The proceeds were distributed amongst all plaintiffs.
During the punk rock era, the number of independent labels grew. [1] The UK Indie Chart was first compiled in 1980, and independent distribution became better organized from the late 1970s onwards. [3] From the late 1970s into the 1980s, certain UK independent labels (such as Rough Trade , Cherry Red , Factory , Glass , Cheree Records and Creation ) came to contribute something in terms of aesthetic identity to the acts whose records they released.
In the late 1980s, Seattle-based Sub Pop Records was at the center of the grunge scene. In the late 1990s and into the 2000s as the advent of MP3 files and digital download sites such as Apple's iTunes Store changed the recording industry, an indie neo-soul scene soon emerged from the urban underground soul scenes of London, New York, Philadelphia, Chicago and Los Angeles, primarily due to commercial radio and the major labels ' biased focus on the marketing, promotion & airplay of pop and hip hop music during this period. Independent labels such as Dome Record and Expansion Records in the U.K. and Burger , Wiener , and Ubiquity Records in the U.S. and a plethora of others around the world continue to release independent bands and music.

Going major versus staying independent
Many acts choose to go from an independent label to a major label if given the opportunity, as major labels have considerably more power and financial means to promote and distribute product, thus increasing the chances of greater success. [ citation needed ] Some acts, however, may choose not to go to a major label if given the opportunity, as independence generally offers more freedom. [ citation needed ]
Similarly, others may become independent label acts after having already experienced recording on a major label. Bradley Joseph asked to be let go from his major label deal with Narada/Virgin Records and subsequently became an independent artist. He says, "As an independent, business is a prime concern and can take over if not controlled. [4] A lot of musicians don't learn the business. You just have to be well-rounded in both areas. You have to understand publishing. You have to understand how you make money, what's in demand, and what helps you make the most out of your talent. [5] But some artists just want to be involved in the music and don't like the added problems or have the personality to work with both". Joseph suggests newer artists read and study both courses and pick one that best suits their own needs and wants. [4]
A successful independent label with a strong musical reputation can be very appealing to a major label. Major labels look at independent labels to stay current with the ever-changing music scene. [6]
If an act moves to a major label from an independent, they are awarded greater opportunity for success, but it does not guarantee success. About one in ten albums released by major labels make a profit for the label. [7] Some artists have recorded for independent record companies for their entire careers and have had solid careers. Independent labels tend to be more open creatively, however, an independent label that is creatively productive is not necessarily financially lucrative. Independent labels are often operations of one, two, or only half a dozen people, with almost no outside assistance and run out of tiny offices. [8] This lack of resources can make it difficult for a band to make revenue from sales. It can also be more difficult for the indie label to get its artists' music played on radio stations around the country when compared to the pull of a major label. A testament to this fact could be that since 1991, there have only been twelve independent label albums that have reached the number one spot on the US Billboard 200 Album Chart. There have, however, been dozens of independent albums that have reached the top 40 of the US Album Chart.
Some major labels have created an opportunity for independent artists to be featured on a distribution/marketing CD project with no strings attached in an effort to help boost awareness of the Independent Music community [ citation needed ] .
The difference among various independent labels lies with distribution; this is probably the most important aspect of running a label. Examples are:
Independent label that signs and distributes its own acts . These independent labels find and sign their own acts; then the label manufactures, distributes, and promotes its own product.
Independent label distributed by a major label . These independent labels are similar to the type mentioned above in that they find and sign their own acts, but they have a separate contract with a major label to handle manufacturing, distribution, and/or promotion. The major label has no control over the independent label, simply an agreement to distribute its product. Either the independent or the major can terminate the pact at the end of the contractual agreement if they so choose. The independent provides for its own financial stability, and has no outside monetary assistance from a major label. -If signing to an independent label, this type of venture probably affords the better benefit. This is because the act's contract is actually with the independent label, which may offer more creative control, yet the act is having its album distributed by a major label, which also has an interest in seeing the album become successful.
Independent label owned by a major label . Some major labels have started independent labels or purchased an existing independent label outright, and have these labels use, or continue to use, independent distribution for their product. The reason for this is because independents usually are on the cutting edge of new sounds and potential hit artists, and signs acts and releases albums for less money than would have otherwise been spent if the acts were signed directly to the major label. One benefit of this scenario is that if the act eventually proves successful enough on this type of independent , and is seeking a major label deal, it may see its subsequent albums released directly on the major-label owner of its independent label. The moniker "independent" is sometimes associated with these major-label owned independent labels because they use independent distributors to distribute their albums instead of their affiliated major-label distribution system. However, these labels are not true independents, the differences being: a) these independent labels can seek the financial backing of their major-label owner should they ever fall on hard financial times. b) the major-label owner can sign acts itself, and then place acts on its independent label if it chooses, even though the independent label signs acts itself. c) the major-label owner can potentially steal away any act from its independent label at any time and bring that act directly to the major-label owner, regardless of if the act is still under contract to the independent label. d) the major-label owner could completely shut down the independent label entirely or sale it off for financial reasons or for restructuring of the overall conglomerate. None of these are circumstances that pertain to true independent labels like those in the first two examples. A record label needs more than independent distribution to qualify as an independent label, otherwise it is an arm of a major label. [9]
It can be very difficult for independent bands to sign to a record label that may not be familiar with their specific style. It can take years of dedicated effort, self-promotion, and rejections before landing a contract with either an independent or major record label. Bands that are ready to go this route need to be sure they are prepared both in terms of the music they offer as well as their realistic expectations for success. [10]

Major label contracts
Most major label artists earn a 10–16% royalty rate. [11] However, before a band is able to receive any of their royalties, they must clear their label for all of their debts, known as recoupable expenses. These expenses arise from the cost of such things as album packaging and artwork, tour support, and video production. An additional part of the recoupable expenses are the artist's advance . An advance is like a loan . It allows the artist to have money to live and record with until their record is released. However, before they can gain any royalties, the advance must be paid back in full to the record label. Since only the most successful artists recoup production and marketing costs, an unsuccessful artist's debt may carry over to their next album, meaning that they see little to no royalties.
Major label advances are generally much larger than independent labels can offer. If an independent label is able to offer an advance, it will likely fall in the $5,000–$100,000 range. [ citation needed ] On the other hand, major labels are able to offer artists advances in the range of $150,000–$500,000. Some smaller independent labels offer no advance at all; just recording cost, album packaging, and artwork, which is also recoupable. If an artist gets no advance at all, they owe their record company less money, thus allowing them to start receiving royalty checks earlier; that is, if sales warrant any royalty checks at all. However, since the record label typically recoups so many different costs, it's actually to the artist's advantage to get the largest advance possible because they may not see any royalties checks for quite some time; again, that is, if sales warrant any royalties checks at all. Another advantage of getting an advance; the advance money the artist owes the label is only recoupable through the artist's royalties, not through a return of the advance itself. [12]
In a record contract, options are agreed upon between the record company and the act. Options allow the label to request additional albums from the act if they so choose. Major labels tend to ask for more options in a contract than independents. For instance, a contract may state "one album, with an option for four". This would mean a total of five possible albums. This means that if the first album was recorded and released by the label and was profitable, the label is going to pick up its option for a second album. The act, therefore, must deliver a second album to the label. If that album is successful, the label will pick up its option for a third album; and so on and so on, depending on how many options are stated in the contract. Picking up the option for another album lies strictly with the label, not the act. The label can pick up as many options as it wants, up to the amount stated in the contract, it does not have to pick up all the options. That means, although a contract may state it has an option for four albums, the label does not have to pick up all four of these options. The reason for this is, say the act's first album is successful and the label picks up an option for a second album, but that second album fails miserably. The label could decide it is not about to spend more money on another album, and not pick up any more options and drop the act from its roster. Another ploy the label could utilize is to pick up an option for another album, even after a failed album had been released. If the label doesn't like the finished product of the new album the act has recorded, the label may not release that album, and then pick up its option for yet another album! The label then may not release that album as well! But the money spent for recording these unreleased albums may still be recoupable from the albums that have already been released. Because the act is under contract with the label, it cannot record music for another record label without permission. This scenario could potentially tie an act down to a label for years, even though the label has no intentions of releasing any more product from this act, in a career that guarantees no success, and if so, typically only sees a few prime years of prosperity. Some acts consider this unfair because the label has the right to not distribute an artist's work, yet legally keep them bound and prevent them from recording elsewhere. In effect, the label could continue to demand more albums through the options clause until it deems one commercially or artistically acceptable. Record labels also effectively own the product recorded (released or not) by an act during the duration of their contract with the label. [13]
Options are only beneficial to the record label. The fewer options allowed in the contract, the better for the act. [ citation needed ] An example: if an act's first and/or second album is successful, but there are no more options left, the label will re-sign the act all over again anyway. This time it will most likely be with a much better royalty rate and more creative freedom than the previous contract stated. Or, the act can decide to move to another label altogether, one that is offering a better royalty rate or creative freedom. However, when the label holds a clause for lots of options for additional albums, it has the advantage. Besides the scenario in the above paragraph of the label requesting albums it may not release and preventing the act from recording elsewhere, the exact opposite could happen instead. The act could release a blockbuster album on their very first release. The label will surely pick up its options for future albums and distribute them, but the act will continue to see all its royalty checks and recoupable expenses calculated under the same contract it signed many years ago. When its contract is finally up (with all those options), the act may have declined considerably in music popularity and may not have the same bargaining position that it had so many years ago when it released that blockbuster album. Had there been fewer options on the initial contract, the act could have negotiated a new and better contract while in its prime.

Independent label contracts
Independent label contracts typically resemble contracts offered by major labels because they have similar legal liabilities to define before representing an artist. There are differences, however, usually with regards to less advances, lower studio costs, lower royalties, but fewer album options. Due to financial constraints, independents typically spend much less on marketing and promotion than major labels. But with lower royalties rates typically paid to artists and lower production and promotion costs, independent labels generally can turn a profit off lower volumes of sales than a major label can.
Although not common, [14] there have been instances of profit-sharing deals with independent labels in which an act can get as much as 40–50% of the net profits. In this type of contract, the net gain after all expenses have been taken out are divided between the label and artist by a negotiated percentage. However, deals in this form can take longer for an artist to gain any profits, if at all, since all expenses – such as recording, manufacturing, publicity and marketing, music videos, etc., are also taken into account. Only if an independent artist becomes vastly popular are deals of this type more advantageous.
Independent labels rely heavily on personal networking, or "word of mouth", to expose their acts. [15] Independent labels tend to avoid high budget marketing tactics, which usually does not fall in the budget of an independent label. This of course contributes to the overall lower production cost, and may help the artist to receive royalties sooner, if warranted. Major labels tend to watch indie label artists and gauge their success, and may offer to sign acts from independents when their contract is up. The major may also request to buy the contract of the act from the independent label before the contract is up, giving the independent label a hefty financial payment if they choose to sell the contract.

Competition between independent and traditional publishing
Independent music sales volume is difficult to track, but in 2010 independent retailer CD Baby claimed to have sold over 5 million CDs during its lifetime. [16] CD Baby no longer reports its number of CDs sold, but in 2010 claimed to have paid a total of $107 million to artists over its lifetime and currently claims that this figure is now over $200 million. [17]
Apple has announced that they have sold over 16 billion songs through their iTunes service. [18] Most of this is “mainstream” music, and doesn't reflect access by new content producers to the market, but it does indicate significant competition with traditional CD sales.
Whether the sales from non-traditional sources come mostly from tapping into an expanding market or from siphoning sales away from traditional CD distribution is difficult to assess in the face of the RIAA's claim that music piracy causes 12.5 billion dollars damage to the US economy annually. [19]

See also
WebPage index: 00019
Family Guy (season 4)
The fourth season of the animated comedy series Family Guy aired on Fox from May 1, 2005, to May 21, 2006, and consisted of thirty episodes, making it the longest season to date. The first half of the season is included within the volume 3 DVD box set, which was released on November 29, 2005, and the second half is included within the volume 4 DVD box set, which was released on November 14, 2006. Volume 4 was split into seasons 4 and 5 in regions outside the United States, leading to confusion over season numbers between U.S., Australian, and UK consumers. The last three episodes of season 4 were the basis for the movie known as Stewie Griffin: The Untold Story , and are edited for content; Fox does not include these episodes in the official episode count.
Family Guy had been canceled in 2002 due to low ratings, but was revived by Fox after reruns on Adult Swim became the network's most-watched program, and more than three million DVDs of the show were sold. " North by North Quahog " was the first episode to air following the series' revival.

Production
The show was first canceled after the 1999–2000 season, but following a last-minute reprieve, it returned for a third season in 2001. [1] In 2002, Family Guy was canceled after three seasons due to low ratings . [2] Fox tried to sell rights for reruns of the show, but it was hard to find networks that were interested; Cartoon Network eventually bought the rights, "[...] basically for free", according to the president of 20th Century Fox Television Production. [3] When the reruns were shown on Cartoon Network's Adult Swim in 2002, Family Guy became the channel's most-watched show with an average 1.9 million viewers per episode. [4] Following this, the show's first season was released on DVD in April 2003. [2] The DVD set sold 2.2 million copies, [5] making it the best-selling television DVD of 2003 [6] and the second highest-selling television DVD ever, behind the first season of Comedy Central 's Chappelle's Show . [7] The season 2 DVD release also sold more than 1 million copies. [4] The show's popularity in both DVD sales and reruns rekindled Fox's interest. [2] They ordered 35 new episodes in 2004, marking the first revival of a television show based on DVD sales. [7] [8] Gail Berman said cancelling the show was one of her most difficult decisions, and she was therefore happy it would return. [3] The network also began production of a film based on the show. [6]
"North by North Quahog" was the first episode to be broadcast after the show's cancellation. It was written by Seth MacFarlane and directed by Peter Shin . [9] MacFarlane believed the show's three-year hiatus was beneficial because animated shows do not normally have hiatuses, and towards the end of their seasons "... you see a lot more sex jokes and (bodily function) jokes and signs of a fatigued staff that their brains are just fried". [10] With "North by North Quahog", the writing staff tried to keep the show "... exactly as it was" before its cancellation, and did not "... have the desire to make it any slicker " than it already was. [10] Walter Murphy , who had composed music for the show before its cancellation, returned to compose the music for "North by North Quahog". Murphy and the orchestra recorded an arrangement of Bernard Herrmann 's score from North by Northwest , a film referenced multiple times in the episode. [11]
Fox had ordered five episode scripts at the end of the third season; these episodes had been written but not produced. One of these scripts was adapted into "North by North Quahog". The original script featured Star Wars character Boba Fett , and later actor, writer and producer Aaron Spelling , but the release of the iconic film The Passion of the Christ inspired the writers to incorporate Mel Gibson into the episode. Multiple endings were written, including one in which Death comes for Gibson. During production, an episode of South Park was released entitled " The Passion of the Jew " that also featured Gibson as a prominent character. This gave the Family Guy writers pause, fearing accusations "[...] that we had ripped them off." [12]

Episodes

Reception

Ratings
This season received high Nielsen ratings ; "North by North Quahog", the premiere episode was broadcast as part of an animated television night on Fox, alongside two episodes of The Simpsons and the pilot episode of American Dad! . [40] The episode was watched by 11.85 million viewers, [13] the show's highest ratings since the airing of the first season episode " Brian: Portrait of a Dog ". [41] Its ratings also surpassed the ratings of both episodes of The Simpsons and American Dad! . [13] Season four's three-part finale was watched by 8.2 million viewers, [42] bringing the season average to 7.9 million viewers per episode. [43]

Awards and nominations
This season was nominated for a number of awards. In 2005, the Academy of Television Arts & Sciences nominated "North by North Quahog" for a Primetime Emmy Award for Outstanding Animated Program (for Programming Less Than One Hour) . [44] It nominated " PTV " in the same category one year later. [45] Neither of the episodes won the award, as South Park received the award in 2005 [46] and The Simpsons was the eventual recipient of the award in 2006. [47] Peter Shin and Dan Povenmire were both nominated for an Annie Award in the Best Directing in an Animated Television Production category, for directing "North by North Quahog" and "PTV" respectively; Shin eventually won the award. [48] MacFarlane won the Annie Award for Best Voice-over Performance for providing the voice of Stewie in " Brian the Bachelor ". [48] At the Annie Awards the following year, John Viener was nominated in the category Writing in an Animated Television Production, for writing "Untitled Griffin Family History", but lost the award to Ian Maxtone-Graham , who wrote the episode of The Simpsons titled " The Seemingly Neverending Story ". [49] The editors of the episode " Blind Ambition " won the Motion Picture Sound Editors Golden Reel Award for Best Sound Editing in Television Animated. [50]

Critical reception
Season 4 received positive reviews from critics. Reviewing the season premiere, Mark McGuire of The Times Union wrote: "... the first minute or so of the resurrected Family Guy ranks among the funniest 60 seconds I've seen so far this season." [51] The Pitt News reviewer John Nigro felt that the show had not lost its steam while it was on hiatus, and was surprised that the show had been canceled because of its "wildly extravagant shock factor". [52] Nigro cited " Breaking Out Is Hard to Do ", " Petarded " and " Perfect Castaway " as the season's best episodes. [52] In 2007, BBC Three named the episode "PTV" "The Best Episode...So Far". [53] The episode has also been praised by Maureen Ryan of the Chicago Tribune , who called it " Family Guy ' s most rebellious outing yet". [54] The Boston Globe critic Matthew Gilbert felt Family Guy ' s fourth season was as "crankily irreverent as ever". [55]
Fewer critics responded negatively to the season; Seattle Post-Intelligencer critic Melanie McFarland reacted very bitterly, stating "Three years off the air has not made the 'Family Guy' team that much more creative". [56] Critics of both PopMatters and IGN criticized the first few episodes but felt the show regained its humor after " Don't Make Me Over "; [57] [58] IGN's Mike Drucker commented "At that point, we get some amazingly creative humor. It's almost like MacFarlane and gang decided they had thanked their fans enough and could return to what made the show successful in the first place." [57] Media watchdog group the Parents Television Council , a frequent critic of the show, branded the episodes "North by North Quahog", [59] " The Father, the Son, and the Holy Fonz ", [60] " Brian Sings and Swings ", [61] " Patriot Games ", [62] and " The Courtship of Stewie's Father " as "worst show of the week". [63]
WebPage index: 00020
Central Java
Central Java ( Indonesian : Jawa Tengah , abbreviated as Jateng) is a province of Indonesia . This province is located in the middle of Java . Its administrative capital is Semarang .
The province is 32,800.69 km 2 in area, approximately a quarter of the total land area of Java. Its population was 33,753,023 at the 2015 Census; it the third most populated province in both Java and Indonesia after West Java and East Java .
Central Java is also a cultural concept that includes the Special Region and city of Yogyakarta as well as the Province of Central Java. However, administratively the city and its surrounding regencies have formed a separate special region (equivalent to a province) since Indonesian independence , administrated separately.

Geography
Located in the middle of the island of Java , the Central Java province is bordered by West Java and East Java provinces. A small portion of its south region is the Yogyakarta Special Region province, fully enclosed on the landward side by the Central Java province. To the north and the south, the Central Java province faces the Java Sea and the Indian Ocean . Central Java includes offshore islands such as Karimun Jawa Islands in the north, and Nusakambangan in the southwest. Yogyakarta is historically and culturally part of the Central Java region, although it is now a separate administrative entity.
The average temperature in Central Java is between 18–28 degrees Celsius and the relative humidity varies between 73–94 percent. [1] While a high level of humidity exists in most low-lying parts of the province, it drops significantly in the upper mountains. [1] The highest average annual rainfall of 3,990 mm with 195 rainy days was recorded in Salatiga . [1]
The geography of Central Java is regular [ clarification needed ] with small strips of lowlands near the northern and southern coast with mountain ranges in the centre of the region. [ citation needed ] To the west lies an active stratovolcano Mount Slamet , and further east is the Dieng Volcanic Complex on Dieng Plateau . Southeast of Dieng lies the Kedu Plain , which is bordered to the east side by the twin volcanoes of Mount Merapi and Mount Merbabu . South of Semarang, lies Mount Ungaran , and to the north-east of the city lies Mount Muria on the most northern tip of Java. To the east near the border with East Java lies Mount Lawu , where its eastern slopes are in the East Java province.
Due to its active volcanic history, volcanic ash makes Central Java highly fertile agriculture land. Paddy fields are extensive, except in the southeastern Gunung Kidul region partly due to the high concentration of limestone and its location in a rain shadow from the prevailing weather. [ citation needed ]
The largest rivers are the Serayu in the west, which empties into the Indian Ocean, and the Solo which flows into East Java.

Administrative divisions
On the eve of the World War II in 1942, Central Java was subdivided into 7 residencies ( Dutch residentie or plural residenties , Javanese karésiḍènan or karésidhènan ) which corresponded more or less with the main regions of this area. These residencies were Banjoemas , Kedoe , Pekalongan , Semarang , and Djapara-Rembang plus the so-called Gouvernement Soerakarta and Gouvernement Jogjakarta . However, after the local elections in 1957 the role of these residencies were reduced until they finally disappeared. [2]
Nowadays Central Java (excluding Yogyakarta Special Region) is divided into 29 regencies ( kabupaten ) and 6 cities ( kota , previously kotamadya and kota pradja ), the latter being independent of any regency. These contemporary regencies and cities can further be subdivided into 565 districts ( kecamatan ). These districts are further subdivided into 7,804 rural communes or "villages" ( desa ) and 764 urban communes ( kelurahan ). [1]

History
Java has been inhabited by humans or their ancestors ( hominina ) since prehistoric times. In Central Java and the adjacent territories in East Java remains known as " Java Man " were discovered in the 1890s by the Dutch anatomist and geologist Eugène Dubois . Java Man belongs to the species Homo erectus . [3] They are believed to be about 1.7 million years old. [4]
Then about 40,000 years ago, Australoid peoples related to modern Australian Aboriginals and Melanesians colonised Central Java. They were assimilated or replaced by Mongoloid Austronesians by about 3000 BC, who brought with them technologies of pottery, outrigger canoes, the bow and arrow, and introduced domesticated pigs, fowls, and dogs. They also introduced cultivated rice and millet. [5]
Recorded history began in Central Java in the 7th century AD. The writing, as well as Hinduism and Buddhism, were brought to Central Java by Indians from South Asia. Central Java was a centre of power in Java back then.
In 664 AD, the Chinese monk Hui-neng visited the Javanese port city he called Hēlíng (訶陵) or Ho-ling , where he translated various Buddhist scriptures into Chinese with the assistance of the Javanese Buddhist monk Jñānabhadra. [6] It is not precisely known what is meant by the name Hēlíng . It used to be considered the Chinese transcription of Kalinga but it now most commonly thought of as a rendering of the name Areng . Hēlíng is believed to be located somewhere between Semarang and Jepara . [7]
The first dated inscription in Central Java is the Inscription of Canggal which is from 732 AD (or 654 Saka). This inscription which hailed from Kedu , is written in Sanskrit in Pallava script. [8] In this inscription it is written that a Shaivite king named Sri Sanjaya established a kingdom called Mataram . Under the reign of Sanjaya's dynasty several monuments such as the Prambanan temple complex were built.
In the meantime a competing dynasty arose, which adhered to Buddhism . This was the Sailendra dynasty, also from Kedu, which built the Borobudur temple.
After 820 there is no more mention of Hēlíng in Chinese records. This fact coincides with the overthrow of the Sailendras by the Sanjayas who restored Shaivism as the dominant religion. Then in the middle of the 10th century, for unknown reason, the centre of power moved to Eastern Java. [7]
A few centuries later, after the destruction of the great Hindu Majapahit Empire in the 15th - 16th centuries by the Central Javanese Muslim kingdom of Demak, the Javanese centre of power moved back to Central Java. In the meanwhile European traders began to frequent Central Javanese ports. The Dutch established a presence in the region through their East India Company .
After Demak itself collapsed, a new kingdom on the Kedu Plain emerged. This new kingdom , which was also a sultanate , bore the old name of Mataram. Under the reign of Sultan Agung , Mataram was able to conquer almost all of Java and beyond by the 17th century, but internal disputes and Dutch intrigues forced Mataram to cede more and more land to the Dutch. These cessions finally led to several partitions of Mataram. The first partition was after the 1755 Treaty of Giyanti . This treaty divided the old kingdom in two, the Sultanate of Surakarta and the Sultanate of Yogyakarta . Then few years later Surakarta was divided again with the establishment of the Mangkunegaran after the Treaty of Salatiga (March 17, 1757).
During the Napoleonic Wars in Europe, Central Java, as part of the Netherlands East-Indies, a Dutch colony, was handed over to the British. In 1813, the Sultanate of Yogyakarta was also divided with the establishment of the Pakualamanan .
After the British left, the Dutch came back, as decided by the Congress of Vienna . Between 1825 - 1830 the Java War ravaged Central Java. The result of the war was a consolidation of the Dutch power. The power and the territories of the divided kingdom of Mataram were greatly reduced.
Netherlands enforced Cultivation system which was linked to famines and epidemics in the 1840s, firstly in Cirebon and then Central Java, as cash crops such as indigo and sugar had to be grown instead of rice
However Dutch rule brought modernization to Central Java. In the 1900s the modern province of Central Java, the predecessor of the current one was created. It consisted of five regions or gewesten in Dutch. Surakarta and Yogyakarta were autonomous regions called Vorstenlanden (literally "princely states"). Then after the Indonesian independence the province of Central Java was formalized on August 15, 1950, excluding Yogyakarta but including Surakarta. [1] Since then there have been no (major) changes in the administrative division of Central Java.
After the 30 September Movement 's abortive coup of 1965, an anti-communist purge took place in Central Java, in which Communists and leftists (both actual and alleged) were killed by the army and community vigilante groups. Others were interned in concentration camps , the most infamous of which was on the isle of Buru in the Moluccas (first used as a place of political exile by the Dutch). Some were executed years later but most were released in 1979 [9]
In 1998, preluding the downfall of president Suharto, anti Chinese violence broke out in Surakarta (Solo) and surrounding areas. Much Chinese property and other buildings were burnt down. In 1999, public buildings in Surakarta were burnt again by supporters of Megawati Soekarnoputri after the Indonesia parliament chose Abdurrahman Wahid instead of Soekarnoputri to be President of Indonesia . They carried out 'sweeping actions' against Western foreigners who reside in this city after the September 11, 2001 attacks . [10]
The 2006 Yogyakarta earthquake in the south and Yogyakarta devastated many buildings and caused thousands of deaths and more than 37,000 injuries. Today, some areas are still under reconstruction.

Demographics
As of the 2010 census, Central Java's population stood at some 32,380,687. As of the 1990 census, the population was 28,516,786. [11] So the population has increased approximately 13.5% in 20 years.
Islam 95.7%, Protestant 1.7%, Catholic 3.2%, Hindu 0.08%, Buddhist 0.64%, dan Kejawen 0.33%[3]
The three biggest regencies in terms of population are: Brebes , Banyumas and Cilacap . Together these regencies make up approximately 16% of the Central Javanese population. Major urban population centres are Greater Semarang , Greater Surakarta and the Brebes - Tegal - Slawi area in the north-west of the province.

Religion
Although the overwhelming majority of Javanese are Muslims, many of them also profess indigenous Javanese beliefs . Clifford Geertz , in his book about the religion of Java made a distinction between the so-called santri Javanese and abangan Javanese. [12] He considered santri Javanese as orthodox Muslims while abangan Javanese are nominal Muslims that devote more energy to indigenous traditions.
Dutch Protestants were active in missionary activities and were rather successful. The Dutch Catholic Jesuit missionary man, F.G.C. van Lith also achieved some success, especially in areas around the central-southern parts of Central Java and Yogyakarta in the beginning of the 20th century, [13] and he is buried at the Jesuit necropolis at Muntilan .
After the Overthrow of Sukarno in 1965, religious identification of citizens became compulsory. Therefore, there has been a renaissance of Buddhism and Hinduism since then. As one has to choose a religion out of the five official religions in Indonesia; i.e. Islam, Protestantism, Catholicism, Hinduism, and Buddhism, the latter two became alternatives for people who didn't want to be Muslims or Christians.
Confucianism is also common amongst Chinese Indonesians. Since 2006 it is a recognised official religion.

Ethnicity
The vast majority of the population in Central Java are ethnic Javanese , they constitute approximately 98% of the whole population. [14] In addition to the Javanese, small pockets of Sundanese communities are to be found near the border with West Java, especially in Brebes and Cilacap regencies. Sundanese toponyms are common in these regions such as Dayeuhluhur in Cilacap, Ciputih and Citimbang in Brebes and even Cilongok as far away in Banyumas. [15]
In urban centers, other minorities such as Chinese Indonesians and Arabs are common. The Chinese are even to be found in rural areas. The urban areas that are densely populated by Chinese Indonesian , are called pecinan , which means " China Town ".

Language
As the overwhelming majority of the population of Central Java are Javanese, the most dominant language is Javanese . There are several dialects which are spoken in Central Java, the two main dialects are western Javanese (also called Basa Ngapak which includes the "Banyumasan dialect" and the dialect of Brebes-Tegal-Pekalongan [16] ) and central Javanese.
Sundanese is also spoken in some pockets near the border with West Java, especially in Brebes and Cilacap regencies. However, according to some sources, Sundanese used to be spoken as far away as in Dieng Plateau . [17] This former boundary of Sundanese coincides more or less with the isogloss dividing Central Javanese with Western Javanese. Madurese is also widely spoken on Madura and in the northern coat region of Eastern Java
In urban centers Indonesian is widely spoken.

Culture
Central Java is considered to be the heart of the Javanese culture. Home of the Javanese courts, Central Javanese culture formed what non-Javanese see as the "Javanese Culture" along with it stereotypes. The ideal conducts and morals of the courts (such as politeness, nobility and grace) influence the people tremendously. The people of Central Java are known as soft-spoken, very polite, extremely class-conscious, apathetic, down-to-earth, et cetera. These stereotypes formed what most non-Javanese see as "Javanese Culture", when in fact not all of the Javanese people behave that way. Moreover, most Javanese are far from the court culture. [18]

Mapping the Javanese cultures
The Javanese cultural area can be divided into three distinct main regions: Western Javanese, Central Javanese and Eastern Javanese culture or in their Javanese names as Ngapak , Kejawèn and Arèk .
The boundaries of these cultural regions coincide with the isoglosses of the Javanese dialects. Cultural areas west of Dieng Plateau and Pekalongan Regency are considered Ngapak whereas the boundary of the eastern cultural areas or Arèk lies in East Java . Consequently, culturally, Central Java consists of two cultures, while the Central Javanese Culture proper is not entirely confined to Central Java. [18]

Creative arts

Architecture
The architecture of Central Java is characterised by the juxtaposition of the old and the new and a wide variety of architectural styles, the legacy of many successive influences by the Indians, the Persians and the Arabs, the Chinese, and the Europeans. In particular, northern coastal cities such as Semarang, Tegal and Pekalongan can boast fine colonial European architecture. The European and Chinese influence can be seen in Semarang's temple of Sam Poo Kong dedicated to Zheng He and the Domed Church built in 1753. The latter is the second oldest church in Java and the oldest in Central Java. Inland Surakarta, as a former capital, also has some fine European architecture.
Famous for its religious heritage, Central Java has some notable religious buildings. The Borobudur and the Prambanan temple complexes are among the largest Buddhist and Hindu structures in the world. In general, a characteristic Javanese mosque doesn't have a dome as its roof but a Meru -like roof instead, which is reminiscent of a Hindu or Buddhist temple. The tower of the famous Mosque of Kudus resembles a Hindu-Javanese or Balinese temple more than a traditional Middle-Eastern mosque.

Batik
Central Java is famous and well known for its exquisite batik , a generic wax-resist dyeing technique used on textiles . There are different styles of batik motifs. A centre of batik production is Pekalongan . Other centres are Surakarta and Yogyakarta . Batik in Pekalongan style which represent gaya pesisir (or coastal style) is different from the one in Surakarta and Yogyakarta, which represent batik from the heartland of Java ( gaya kejawèn ). [19]

Dance
You can even see the court influences in the art forms. The dances of the courts of Java are usually slow and graceful, with no excessive gestures. The people followed this approach, and as a result, slow-paced and graceful movements can even be found in folk dances throughout Central Java (with some exceptions). You can enjoy the beauty of Central Javanese dances in "Kamajaya-Kamaratih" or "Karonsih", usually performed in a traditional Javanese wedding.

Theater
There are several kinds of Central Javanese theater and performing arts. The most well known is of course the Javanese wayang theater. There are several kinds of Central Javanese wayang , amongst others: wayang kulit , wayang klitik , wayang bèbèr , wayang golèk , and wayang wong . Wayang kulit are shadow puppets theater with leather puppets. The stories are loosely based on Mahabharata and Ramayana cycles. Wayang klitik are puppets theater with flat wooden puppets. The stories are based on Panji (king) stories. Panji was a native Javanese princes who set of in a 'journeys of desire'. [20] Wayang bèbèr is scroll theater, and it involves "performing" scenes of a story elaborately drawn and painted on rolled sheets. Wayang golèk consists of three-dimensional wooden puppets. The narrative can be based on anything, but usually the stories are drawn from Islamic heroic narratives. Finally wayang wong is wayang theater involving live figures; actors who are performing a play. The narrative however must be based on Mahabharata or Ramayana.
In addition to wayang , there is another form of theater which is called ketoprak . Ketoprak is a staged play by actors accompanied with Javanese gamelan . The narrative is free but cannot be based on Mahabharata or Ramayana. Otherwise it will be some kind of wayang wong .

Music
Central Javanese music is almost synonymous with gamelan . This is a musical ensemble typically featuring a variety of instruments such as metallophones, xylophones, drums, and gongs; bamboo flutes, bowed and plucked strings, and vocalists may also be included. The term refers more to the set of instruments than the players of those instruments. A gamelan as a set of instruments is a distinct entity, built and tuned to stay together — instruments from different gamelan are not interchangeable. However, gamelan is not typically Central Javanese as it is also known somewhere else.
Contemporary Javanese pop music is called campursari . It is a fusion between gamelan and Western instruments, much like kroncong . Usually the lyrics are in Javanese, but not always. One notable singer is Didi Kempot , born in Sragen , north of Surakarta. Didi Kempot mostly sings in Javanese.

Literature
It can be argued that Javanese literature started in Central Java. The oldest known literary work in the Javanese language is the Inscription of Sivagrha from Kedu Plain . This inscription which is from 856 AD, is written as a kakawin or Javanese poetry with Indian metres. [21] Then the oldest of narrative poems, Kakawin Ramayana , which tells the well-known story of Ramayana is believed to have come from Central Java. It can be safely assumed that this kakawin must have been written in Central Java in the 9th century. [22]
After the shift of Javanese power to East Java, it had been quiet from Central Java for several centuries, concerning Javanese literature until the 16th century. At this time the centre of power was shifted back to Central Java. The oldest work written in Modern Javanese language concerning Islam is the so-called "Book of Bonang" or also "The Admonitions of Seh Bari". This work is extant in just one manuscript, now kept in the University Library in Leiden, The Netherlands as codex Orientalis 1928. It is assumed that this manuscript originates from Tuban, in East Java and was taken to the Netherlands after 1598. [23] However this work is attributed to Sunan Bonang , one of the nine Javanese saints who spread Islam in Java ( Wali Songo ) and Sunan Bonang came from Bonang, a place in Demak Regency , Central Java. So it can be argued that this work also mark the beginning of Islamic literature in Central Java.
However the pinnacle of Central Javanese literature was created at the courts of the kings of Mataram in Kartasura and later in Surakarta and Yogyakarta, mostly attributed to the Yasadipura family. The most famous member of this family is Rangga Warsita who lived in the 19th century. He is the best known of all Javanese writers and also one of the most prolific. He is also known as bujangga panutup or "the last court poet".
After the Indonesian independence , the Javanese language as a medium was pushed to the background. Still one of the greatest contemporary Indonesian author, Pramoedya Ananta Toer was born in 1925 in Blora , Central Java. He was an Indonesian author of novels, short stories, essays, polemics, and histories of his homeland and its people. A well-regarded writer in the West, Pramoedya's outspoken and often politically charged writings faced censorship in his native land during the pre-reformation era. For opposing the policies of both founding president Soekarno , as well as those of its successor, the New Order regime of Soeharto , he faced extrajudicial punishment. During the many years in which he suffered imprisonment and house arrest, he became a cause célèbre for advocates of freedom of expression and human rights. In his works he writes much about life and social problems in Java.

Food and drink
Rice is the staple food of Central Java. In addition to rice, dried cassava known locally as gaplèk also serve as staple food. Javanese food tends to taste sweet. Cooked and stewed vegetables, usually in coconut milk ( santen in Javanese) are popular. Raw vegetable which is popular in West Java is less popular in Central Java.
Saltwater fish, both fresh and dried is common, especially among coastal populations. Freshwater fish is not popular in Central Java, unlike in West Java, except perhaps for catfish known locally as lélé . Catfish is usually fried and served with chilli condiment ( sambal ) and raw vegetables.
Chicken, mutton and beef are common meat. Dog meat , known by its euphemism daging jamu (literally "traditional medicine meat") is also occasionally eaten by certain parts of the population.
Tofu and tempe serve as common fish and meat replacement. Famous Central Javanese dishes include gudeg (sweet stew of jackfruit) and Sayur Lodeh (vegetables cooked in coconut milk).
Besides the aforementioned tofu, there is strong Chinese influence in many dishes. Some examples of Sino-Javanese food are noodles , bakso (meatballs), lumpia , soto (some kind of soup made with chicken or beef) et cetera. The widespread use of sweet soybeans sauce ( kecap manis ) in the Javanese cuisine can also be attributed to Chinese influence.

Transportation
Central Java is connected to the interprovincial national way on the northern coast ( Jalur Pantai Utara or Jalur Pantura ) which runs from Anyer in Banten to Banyuwangi , East Java on the opposite of Bali . Losari, the Central Javanese gate at the western border on the northern coast, could be reached from Jakarta in 4 hours drive. On the southern coast, there is also a national way which run from Kroya at the Sundanese-Javanese border, through Yogyakarta to Surakarta and then to Surabaya via Kertosono in East Java. There is furthermore a direct connection from Tegal to Purwokerto and from Semarang to Yogyakarta and Surakarta . In addition to that there is a toll road in Semarang and from Semarang to Ungaran which runs for 14 kilometer. Trans-Java Toll Road also would serves Central Java with highway. Some parts has been opened and the others are under construction.
Central Java was the province that first introduced a railway line in Indonesia. The very first line began in 1873 between Semarang and Yogyakarta by a private company, [24] but this route is now no longer used. Today there are five lines in Central Java: the northern line which runs from Jakarta via Semarang to Surabaya. Then there is the southern line from Kroya through Yogyakarta and Surakarta to Surabaya. There is also a train service between Semarang and Surakarta and a service between Kroya and Cirebon. At last there is a route between Surakarta and Wonogiri. All of these lines are single track lines, except the line between Yogyakarta and Surakarta which is double track.
On the northern coast Central Java is served by 8 harbours. The main port is Tanjung Mas in Semarang, other harbours are located in Brebes, Tegal, Pekalongan, Batang, Jepara, Juwana and Rembang. The southern coast is mainly served by the port Tanjung Intan in Cilacap . [25]
Finally on mainland Central Java there are three commercial airports. There is one additional commercial airport on the Karimunjawa isles. The airports on the mainland are: Adisumarmo International Airport in Surakarta, Achmad Yani Airport in Semarang and Tunggul Wulung Airport in Cilacap. Karimunjawa is served by Dewadaru Airport.

Economy
GDP in the province of Central Java was estimated to be around $US 98 billion in 2010, with a per capita income of around $US 3,300. Economic growth in the province is quite rapid and GDP is forecast to reach $US 180 billion by 2015.

Agriculture
Much of Central Java is a fertile agricultural region. The primary food crop is wet rice. An elaborate irrigation network of canals, dams, aqueducts, and reservoirs has greatly contributed to Central Java's the rice-growing capacity over the centuries. In 2001, productivity of rice was 5,022 kilograms/ha, mostly provided from irrigated paddy field (± 98%). Klaten Regency had the highest productivity with 5525 kilograms/ha. [26]
Other crops, also mostly grown in lowland areas on small peasant landholdings, are corn (maize), cassava, peanuts (groundnuts), soybeans, and sweet potatoes. Terraced hillslopes and irrigated paddy fields are familiar features of the landscape. Kapok, sesame, vegetables, bananas, mangoes, durian fruits, citrus fruits, and vegetable oils are produced for local consumption. Tea, coffee, tobacco, rubber, sugarcane and kapok; and coconuts are exported. Several of these cash crops at a time are usually grown on large family estates. Livestock, especially water buffalo, is raised primarily for use as draft animals. Salted and dried fish are imported. [26] [27]

Education
Central Java is home to such notable state universities, as Diponegoro University , Semarang State University , and Walisongo Islamic University ( Universitas Islam Negeri Walisongo ) in Semarang ; Sebelas Maret University in Surakarta ; and Jenderal Soedirman University in Purwokerto .
The Military Academy ( Akademi Militer ) is located in Magelang Regency while the Police Academy ( Akademi Kepolisian ) is located in Semarang. Furthermore, in Surakarta the Surakarta Institute of Indonesian Arts (ISI Surakarta) is located. In addition to these, Central Java has hundreds of other private higher educations, including religious institutions.
For foreign students requiring language training Salatiga has been a location for generations of students attending courses.

Tourism
There are several interesting places to be found in Central Java. Semarang itself has lots of old picturesque buildings: Puri Maerokoco and Indonesian Record Museum are located in this city.
Borobudur , which is one of the UNESCO World Cultural Heritage sites of Indonesia is also located in this province, in the Magelang regency . Candi Mendut and Candi Pawon can also be found near the Borobudur temple complex.
Candi Prambanan at the border of Klaten regency and Yogyakarta is the biggest complex of Hindu temples. It is also a UNESCO World Cultural Heritage Site. In the region around the Dieng Plateau , one could find several temples. These are built before the era of the ancient Mataram .
Two interesting palaces, the Palace of the Sunan ( Keraton Kasunanan ) and Pura Mangkunegaran , are located in Surakarta , which is considered one of the centers of Javanese culture. The Grojogan Sewu waterfall is located in Karanganyar Regency , which has a beautiful scenery. Several Majapahit temples and Sangiran museum are also located in Central Java.

Coat of arms and symbols
The motto of Central Java is Prasetya Ulah Sakti Bhakti Praja . This is a Javanese phrase meaning "A vow of devotion with all might to the country". The coat of arms of Central Java depicts a legendary flask, Kundi Amerta or Cupu Manik , formed in a pentagon representing Pancasila . In the center of the emblem stands a sharp bamboo spike (representing the fight for independence, and it has 8 sections which represent Indonesia's month of Independence) with a golden five-pointed star (representing faith in God), superimposed on the black profile of a candi (temple) with seven stupas , while the middle stupa is the biggest. This candi is reminiscent of the Borobudur . Under the candi wavy outlines of waters are visible. Behind the candi two golden mountain tops are visible.
These twin mountains represents the unity between the people and their government. The emblem shows a green sky above the candi . Above, the shield is adorned with a red and white ribbon, the colours of the Indonesian flag . Lining the left and right sides of the shield are respectively stalk of rice (17 of them, representing Indonesia's day of Independence) and cotton flowers (5 of them, each one is 4-petaled, representing Indonesia's year of Independence). At the bottom, the shield is adorned with a golden red ribbon. On the ribbon the name "Central Java" ( Jawa Tengah ) is inscribed in black. The floral symbol of the province is the Michelia alba , while the provincial fauna is Oriolus chinensis .

Further reading

See also
WebPage index: 00021
Glyph
In typography , a glyph / ˈ ɡ l ɪ f / is an elemental symbol within an agreed set of symbols, intended to represent a readable character for the purposes of writing . As such, glyphs are considered to be unique marks that collectively add up to the spelling of a word, or otherwise contribute to a specific meaning of what is written, with that meaning dependent on cultural and social usage.
For example, in most languages written in any variety of the Latin alphabet the dot on a lower-case i is not a glyph because it does not convey any distinction, and an i in which the dot has been accidentally omitted is still likely to be recognized correctly. In Turkish, however, it is a glyph, because that language has two distinct versions of the letter i , with and without a dot .
In Japanese syllabaries , a number of the characters are made up of more than one separate mark, but in general these separate marks are not glyphs because they have no meaning by themselves. However, in some cases, additional marks fulfill the role of diacritics , to differentiate distinct characters. Such additional marks constitute glyphs.
In general, a diacritic is a glyph, even if (like a cedilla in French, the ogonek in several languages or the stroke on a Polish " Ł ") it is contiguous with the rest of the character.
Some characters, such as " æ " in Icelandic and the " ß " in German , would probably be regarded as glyphs: they were originally ligatures but over time have become characters in their own right, and these languages treat them as separate letters. However, a ligature such as "ſi", which is treated in some typefaces as a single unit, is arguably not a glyph as this is just a quirk of the typeface, essentially an allographic feature, and includes more than one grapheme . In normal handwriting, even long words are often written "joined up", without the pen leaving the paper, and the form of each written letter will often vary depending on which letters precede and follow it, but that does not make the whole word into a single glyph.
Two or more glyphs which have the same significance, whether used interchangeably or chosen depending on context, are called allographs of each other.

Etymology
The term has been used in English since 1727, borrowed from glyphe (in use by French antiquaries since 1701), from the Greek γλυφή, glyphē , "carving," and the verb γλύφειν, glýphein , "to hollow out, engrave, carve" (cognate with Latin glubere "to peel" and English cleave ). [ citation needed ]
The word hieroglyph (Greek for sacred writing) has a longer history in English, dating from an early use in an English to Italian dictionary published by John Florio in 1598, referencing the complex and mysterious characters of the Egyptian alphabet. [1]
The word glyph first came to widespread European attention with the engravings and lithographs from Frederick Catherwood 's drawings of undeciphered glyphs of the Maya civilization in the early 1840s. [ citation needed ]

Archaeology
In archaeology, a glyph is a carved or inscribed symbol. It may be a pictogram or ideogram , or part of a writing system such as a syllable , or a logogram . In 1897 Dana Evans discovered glyphs written on rocks in the Colorado Desert. These ancient characters have been called the most enlightening discovery in Native American History in the 19th Century. [ by whom? ]

Typography
In typography , a glyph has a slightly different definition: it is "the specific shape, design, or representation of a character". [2] It is a particular graphical representation, in a particular typeface , of an element of written language, which could be a grapheme, or part of a grapheme, or sometimes several graphemes in combination (a composed glyph [note 1] ). If there is more than one allograph of a unit of writing, and the choice between them depends on context or on the preference of the author, they now have to be treated as separate glyphs, because mechanical arrangements have to be available to differentiate between them and to print whichever of them is required. The same is true in computing . In computing as well as typography, the term " character " refers to a grapheme or grapheme-like unit of text, as found in natural language writing systems ( scripts ). In typography and computing, the range of graphemes is broader than in a written language in other ways too: a typographical font often has to cope with a range of different languages each of which contribute their own graphemes, and it may also be required to print other symbols such as dingbats . The range of glyphs required increases correspondingly. In summary, in typography and computing, a glyph is a graphical unit. [3]

Graphonomics
In graphonomics , the term glyph is used for a noncharacter, i.e. either a subcharacter or multicharacter pattern. Most typographic glyphs originate from the characters of a typeface . In a typeface each character typically corresponds to a single glyph, but there are exceptions, such as a font used for a language with a large alphabet or complex writing system, where one character may correspond to several glyphs, or several characters to one glyph.

Other uses

See also

Notes
WebPage index: 00022
Slogan
A slogan is a memorable motto or phrase used in a clan , political , commercial , religious , and other context as a repetitive expression of an idea or purpose, with the goal of persuading members of the public or a more defined target group. The Oxford Dictionary of English defines a slogan as "a short and striking or memorable phrase used in advertising." [1] (Stevenson, 2010) A slogan usually has the attributes of being memorable, very concise and appealing to the audience. [2] (Lim & Loi, 2015). These attributes are necessary in a slogan, as it is only a short phrase. Therefore, it is necessary for slogans to be memorable, as well as concise in what the organisation or brand is trying to say and appealing to who the organisation or brand is trying to reach.

Etymology
The word slogan is derived from slogorn which was an Anglicisation of the Scottish Gaelic and Irish sluagh-ghairm ( sluagh "army", "host" + gairm "cry"). [3] Slogans vary from the written and the visual to the chanted and the vulgar. Their simple rhetorical nature usually leaves little room for detail and a chanted slogan may serve more as social expression of unified purpose than as communication to an intended audience.
George E. Shankel's (1941, as cited in Denton Jr., 1980) research states that, "English-speaking people began using the term by 1704." The term at that time meant "the distinctive note, phrase or cry of any person or body of persons." Slogans were common throughout the European continent during the Middle Ages; they were used primarily as passwords to ensure proper recognition of individuals at night or in the confusion of battle. [4]

Likability
Crimmins' (2000, as cited in Dass, Kumar, Kohli, & Thomas, 2014) research suggests that brands are an extremely valuable corporate asset, and can make up a lot of a business's total value. With this in mind, if we take into consideration Keller's (1993, as cited in Dass, Kumar, Kohli, & Thomas, 2014) research, which suggests that a brand is made up of three different components. These include, name, logo and slogan. Brands names and logos both can be changed by the way the receiver interprets them. Therefore, the slogan has a large job in portraying the brand (Dass, Kumar, Kohli, & Thomas, 2014). [5] Therefore, the slogan should create a sense of likability in order for the brand name to be likable and the slogan message very clear and concise.
Dass, Kumar, Kohli, & Thomas' (2014) research suggests that there are certain factors that make up the likability of a slogan. The clarity of the message the brand is trying to encode within the slogan. The slogan emphasizes the benefit of the product or service it is portraying. The creativity of a slogan is another factor that had a positive effect on the likability of a slogan. Lastly, leaving the brand name out of the slogan will have a positive effect on the likability of the brand itself. [5] Advertisers must keep into consideration these factors when creating a slogan for a brand, as it clearly shows a brand is a very valuable asset to a company, with the slogan being one of the three main components to a brands' image.

Usage
The original usage refers to the usage as a clan motto among Highland clans. Marketing slogans are often called taglines in the United States or straplines in the United Kingdom. Europeans use the terms baselines , signatures , claims or pay-offs . [6] "Sloganeering" is a mostly derogatory term for activity which degrades discourse to the level of slogans. [ better source needed ]
Slogans are used to convey a message about the product, service or cause that it is representing. It can have a musical tone to it or written as a song. Slogans are often used to capture the attention of the audience it is trying to reach. If the slogan is used for commercial purposes, often it is written to be memorable/catchy in order for a consumer to associate the slogan with the product it is representing. [7] [8] A slogan is part of the production aspect that helps create an image for the product, service or cause it's representing. A slogan can be a few simple words used to form a phrase that can be used in a repetitive manner. In commercial advertising, corporations will use a slogan as part of promotional activity. [8] Slogans can become a global way of identifying good or service, for example Nike 's slogan 'Just Do It' helped establish Nike as an identifiable brand worldwide. [9]
Slogans should catch the audience's attention and influence the consumer's thoughts on what to purchase. [10] The slogan is used by companies to affect the way consumers view their product compared to others. Slogans can also provide information about the product, service or cause its advertising. The language used in the slogans is essential to the message it wants to convey. Current words used can trigger different emotions that consumers will associate that product with. [10] The use of good adjectives makes for an effective slogan; when adjectives are paired with describing nouns, they help bring the meaning of the message out through the words. [11] When a slogan is used for advertising purposes its goal is to sell the product or service to as many consumers through the message and information a slogan provides. [12] A slogan's message can include information about the quality of the product. [12] Examples of words that can be used to direct the consumer preference towards a current product and its qualities are: good, beautiful, real, better, great, perfect, best, and pure. [13] Slogans can influence that way consumers behave when choosing what product to buy.
Slogans offer information to consumers in an appealing and creative way. A slogan can be used for a powerful cause where the impact of the message is essential to the cause. [14] [15] The slogan can be used to raise awareness about a current cause; one way is to do so is by showing the truth that the cause is supporting. [15] A slogan should be clear with a supporting message. Slogans, when combined with action, can provide an influential foundation for a cause to be seen by its intended audience. [16] Slogans, whether used for advertising purpose or social causes, deliver a message to the public that shapes the audiences' opinion towards the subject of the slogan.
"It is well known that the text a human hears or reads constitutes merely 7% of the received information. As a result, any slogan merely possesses a supportive task." (Rumšienė & Rumšas, 2014). [17] Looking at a slogan as a supportive role to a brands image and portrayal is helpful to understand why advertisers need to be careful in how they construct their slogan, as it needs to mold with the other components of the brand image, being logo and name. For example, if a slogan was pushing towards "environmentally friendly," yet the logo and name seemed to show very little concern for the environment, it would be harder for the brand to integrate these components into a successful brand image, as they would not integrate together towards a common image.
WebPage index: 00023
GNU Free Documentation License
The GNU Free Documentation License ( GNU FDL or simply GFDL ) is a copyleft license for free documentation, designed by the Free Software Foundation (FSF) for the GNU Project . It is similar to the GNU General Public License , giving readers the rights to copy, redistribute, and modify (only when without "invariant sections" restrictions) a work and requires all copies and derivatives to be available under the same license. Copies may also be sold commercially, but, if produced in larger quantities (greater than 100), the original document or source code must be made available to the work's recipient.
The GFDL was designed for manuals , textbooks, other reference and instructional materials, and documentation which often accompanies GNU software. However, it can be used for any text-based work, regardless of subject matter. For example, the free online encyclopedia Wikipedia uses the GFDL (coupled with the Creative Commons Attribution Share-Alike License ) for all of its text.

History
The GFDL was released in draft form for feedback in September 1999. [1] After revisions, version 1.1 was issued in March 2000, version 1.2 in November 2002, and version 1.3 in November 2008. The current state of the license is version 1.3. [2]
The first discussion draft of the GNU Free Documentation License version 2 was released on September 26, 2006, along with a draft of the new GNU Simpler Free Documentation License .
On December 1, 2007, Wikipedia founder Jimmy Wales announced that a long period of discussion and negotiation between and amongst the Free Software Foundation, Creative Commons, the Wikimedia Foundation and others had produced a proposal supported by both the FSF and Creative Commons to modify the Free Documentation License in such a fashion as to allow the possibility for the Wikimedia Foundation to migrate the projects to the similar Creative Commons Attribution Share-Alike (CC BY-SA) license. [3] [4] These changes were implemented on version 1.3 of the license, which includes a new provision allowing certain materials released under the license to be used under a Creative Commons Attribution Share-Alike license also. [2]

Conditions
Material licensed under the current version of the license can be used for any purpose, as long as the use meets certain conditions.

Secondary sections
The license explicitly separates any kind of "Document" from "Secondary Sections", which may not be integrated with the Document, but exist as front-matter materials or appendices. Secondary sections can contain information regarding the author's or publisher's relationship to the subject matter, but not any subject matter itself. While the Document itself is wholly editable, and is essentially covered by a license equivalent to (but mutually incompatible with) the GNU General Public License , some of the secondary sections have various restrictions designed primarily to deal with proper attribution to previous authors.
Specifically, the authors of prior versions have to be acknowledged and certain "invariant sections" specified by the original author and dealing with his or her relationship to the subject matter may not be changed. If the material is modified, its title has to be changed (unless the prior authors give permission to retain the title).
The license also has provisions for the handling of front-cover and back-cover texts of books, as well as for "History", "Acknowledgements", "Dedications" and "Endorsements" sections. These features were added in part to make the license more financially attractive to commercial publishers of software documentation, some of whom were consulted during the drafting of the GFDL. [5] [6] "Endorsements" sections are intended to be used in official standard documents, where distribution of modified versions should only be permitted if they are not labeled as that standard any more. [6]

Commercial redistribution
The GFDL requires the ability to "copy and distribute the Document in any medium, either commercially or noncommercially" and therefore is incompatible with material that excludes commercial re-use. As mentioned above, the GFDL was designed with commercial publishers in mind, as Stallman explained:
Material that restricts commercial re-use is incompatible with the license and cannot be incorporated into the work. However, incorporating such restricted material may be fair use under United States copyright law (or fair dealing in some other countries) and does not need to be licensed to fall within the GFDL if such fair use is covered by all potential subsequent uses. One example of such liberal and commercial fair use is parody .

Compatibility with Creative Commons licensing terms
Although the two licenses work on similar copyleft principles, the GFDL is not compatible with the Creative Commons Attribution-ShareAlike license.
However, at the request of the Wikimedia Foundation , [2] version 1.3 added a time-limited section allowing specific types of websites using the GFDL to additionally offer their work under the CC BY-SA license. These exemptions allow a GFDL-based collaborative project with multiple authors to transition to the CC BY-SA 3.0 license, without first obtaining the permission of every author, if the work satisfies several conditions: [2]
To prevent the clause from being used as a general compatibility measure, the license itself only allowed the change to occur before August 1, 2009. At the release of version 1.3, the FSF stated that all content added before November 1, 2008 to Wikipedia as an example satisfied the conditions. The Wikimedia Foundation itself after a public referendum, invoked this process to dual-license content released under the GFDL under the CC BY-SA license in June 2009, and adopted a foundation-wide attribution policy for the use of content from Wikimedia Foundation projects. [7] [8] [9]

Enforcement
There have currently been no cases involving the GFDL in a court of law, although its sister license for software, the GNU General Public License , has been successfully enforced in such a setting. [10] Although the content of Wikipedia has been plagiarized and used in violation of the GFDL by other sites, such as Baidu Baike , no contributors have ever tried to bring an organization to court due to violation of the GFDL. In the case of Baidu, Wikipedia representatives asked the site and its contributors to respect the terms of the licenses and to make proper attributions. [11]

Criticism
Some critics consider the GFDL a non-free license. Some reasons for this are that the GFDL allows "invariant" text which cannot be modified or removed, and that its prohibition against digital rights management (DRM) systems applies to valid usages, like for "private copies made and not distributed". [12]
Notably, the Debian project, [13] Thomas Bushnell , [14] Nathanael Nerode, [15] and Bruce Perens [16] have raised objections. Bruce Perens saw the GFDL even outside the "Free Software ethos": [16]
In 2006, Debian developers voted to consider works licensed under the GFDL to comply with their Debian Free Software Guidelines provided the invariant section clauses are not used. [17] The results was GFDL without invariant sections is DFSG compliant. [17] However, their resolution stated that even without invariant sections, GFDL-licensed software documentation "is still not free of trouble", namely because of its incompatibility with the major free software licenses. [17]
Those opposed to the GFDL have recommended the use of alternative licenses such as the BSD Documentation License or the GNU GPL. [17]
The FLOSS Manuals foundation, an organization devoted to creating manuals for free software, decided to eschew the GFDL in favor of the GPL for its texts in 2007, citing the incompatibility between the two, difficulties in implementing the GFDL, and the fact that the GFDL "does not allow for easy duplication and modification", especially for digital documentation. [18]

DRM clause
The GNU FDL contains the statement:
A criticism of this language is that it is too broad, because it applies to private copies made but not distributed. This means that a licensee is not allowed to save document copies "made" in a proprietary file format or using encryption.
In 2003, Richard Stallman said about the above sentence on the debian-legal mailing list: [19]

Invariant sections
A GNU FDL work can quickly be encumbered because a new, different title must be given and a list of previous titles must be kept. This could lead to the situation where there are a whole series of title pages, and dedications, in each and every copy of the book if it has a long lineage. These pages cannot be removed until the work enters the public domain after copyright expires.
Richard Stallman said about invariant sections on the debian-legal mailing list: [20]

GPL incompatible in both directions
The GNU FDL is incompatible in both directions with the GPL—material under the GNU FDL cannot be put into GPL code and GPL code cannot be put into a GNU FDL manual. [21] At the June 22nd and 23rd 2006 international GPLv3 conference in Barcelona, Eben Moglen hinted that a future version of the GPL could be made suitable for documentation: [22]

Burdens when printing
The GNU FDL requires that licensees, when printing a document covered by the license, must also include "this License, the copyright notices, and the license notice saying this License applies to the Document". This means that if a licensee prints out a copy of an article whose text is covered under the GNU FDL, he or she must also include a copyright notice and a physical printout of the GNU FDL, which is a significantly large document in itself. Worse, the same is required for the standalone use of just one (for example, Wikipedia) image. [23] Wikivoyage , a web site dedicated to free content travel guides, chose not to use the GFDL because it considers it unsuitable for short printed texts. [24]

Other free content licenses
Some of these were developed independently of the GNU FDL, while others were developed in response to perceived flaws in the GNU FDL.

List of projects that use the GFDL

See also
WebPage index: 00024
Online encyclopedia
An online encyclopedia is an encyclopedia accessible through the internet , such as the English Wikipedia . The idea to build a free encyclopedia using the Internet can be traced at least to the 1994 Interpedia proposal; it was planned as an encyclopedia on the Internet to which everyone could contribute materials. The project never left the planning stage and was overtaken by a key branch of old printed encyclopedias.

Digitization of old content
In January 1995, Project Gutenberg started to publish the ASCII text of the Encyclopædia Britannica , 11th edition (1911), but disagreement about the method halted the work after the first volume. For trademark reasons this has been published as the Gutenberg Encyclopedia . In 2002, ASCII text of and 48 sounds of music was published on Encyclopædia Britannica Eleventh Edition [1] by source; a copyright claim was added to the materials included. [ original research? ] Project Gutenberg has restarted work on digitising and proofreading this encyclopedia; as of June 2005 it had not yet been published. Meanwhile, in the face of competition from rivals such as Encarta , the latest Britannica was digitized by its publishers, and sold first as a CD-ROM and later as an online service. Other digitization projects have made progress in other titles. One example is Easton's Bible Dictionary (1897) digitized by the Christian Classics Ethereal Library . [2] Probably the most important and successful digitization of an encyclopedia was the Bartleby Project 's online adaptation of the Columbia Encyclopedia , tenth Edition, [3] in early 2000 and is updated periodically.

Creation of new content
Another related branch of activity is the creation of new, free contents on a volunteer basis. In 1991, the participants of the Usenet newsgroup alt.fan.douglas-adams [4] started a project to produce a real version of The Hitchhiker's Guide to the Galaxy , a fictional encyclopedia used in the works of Douglas Adams . It became known as Project Galactic Guide. Although it originally aimed to contain only real, factual articles, policy was changed to allow and encourage semi-real and unreal articles as well. Project Galactic Guide contains over 1700 articles, but no new articles have been added since 2000; this is probably partly due to the founding of h2g2 , a more official project along similar lines.

See also
WebPage index: 00025
Nature (journal)
Nature is an English multidisciplinary scientific journal , first published on 4 November 1869. [1] It was ranked the world's most cited scientific journal by the Science Edition of the 2010 Journal Citation Reports , is ascribed an impact factor of approximately 38.1, and is widely regarded as one of the few remaining academic journals that publishes original research across a wide range of scientific fields. [2] Nature claims an online readership of about 3 million unique readers per month. [3] The journal has a weekly circulation of around 53,000 but studies have concluded that on average a single copy is shared by as many as eight people. [4]
Research scientists are the primary audience for the journal, but summaries and accompanying articles are intended to make many of the most important papers understandable to scientists in other fields and the educated public. Towards the front of each issue are editorials , news and feature articles on issues of general interest to scientists, including current affairs, science funding, business, scientific ethics and research breakthroughs. There are also sections on books and arts. The remainder of the journal consists mostly of research papers (articles or letters), which are often dense and highly technical. Because of strict limits on the length of papers, often the printed text is actually a summary of the work in question with many details relegated to accompanying supplementary material on the journal's website.
There are many fields of research in which important new advances and original research are published as either articles or letters in Nature. The papers that have been published in this journal are internationally acclaimed for maintaining high research standards.
In 2007 Nature (together with Science ) received the Prince of Asturias Award for Communications and Humanity. [5] [6]

History

Background
The enormous progress in science and mathematics during the 19th century was recorded in journals written mostly in German or French, as well as in English. Britain underwent enormous technological and industrial changes and advances particularly in the latter half of the 19th century. [7] In English the most respected scientific journals of this time were the refereed journals of the Royal Society , which had published many of the great works from Isaac Newton , Michael Faraday through to early works from Charles Darwin . In addition, during this period, the number of popular science periodicals doubled from the 1850s to the 1860s. [8] According to the editors of these popular science magazines, the publications were designed to serve as "organs of science", in essence, a means of connecting the public to the scientific world. [8]
Nature , first created in 1869, was not the first magazine of its kind in Britain. One journal to precede Nature was Recreative Science: A Record and Remembrancer of Intellectual Observation , which, created in 1859, began as a natural history magazine and progressed to include more physical observational science and technical subjects and less natural history. [9] The journal's name changed from its original title to Intellectual Observer: A Review of Natural History, Microscopic Research, and Recreative Science and then later to the Student and Intellectual Observer of Science, Literature, and Art . [10] While Recreative Science had attempted to include more physical sciences such as astronomy and archaeology , the Intellectual Observer broadened itself further to include literature and art as well. [10] Similar to Recreative Science was the scientific journal Popular Science Review , created in 1862, [11] which covered different fields of science by creating subsections titled "Scientific Summary" or "Quarterly Retrospect", with book reviews and commentary on the latest scientific works and publications. [11] Two other journals produced in England prior to the development of Nature were the Quarterly Journal of Science and Scientific Opinion , established in 1864 and 1868, respectively. [10] The journal most closely related to Nature in its editorship and format was The Reader , created in 1864; the publication mixed science with literature and art in an attempt to reach an audience outside of the scientific community, similar to Popular Science Review . [10]
These similar journals all ultimately failed. The Popular Science Review survived longest, lasting 20 years and ending its publication in 1881; Recreative Science ceased publication as the Student and Intellectual Observer in 1871. The Quarterly Journal , after undergoing a number of editorial changes, ceased publication in 1885. The Reader terminated in 1867, and finally, Scientific Opinion lasted a mere 2 years, until June 1870. [9]

Creation
Not long after the conclusion of The Reader , a former editor, Norman Lockyer , decided to create a new scientific journal titled Nature , [12] taking its name from a line by William Wordsworth : "To the solid ground of nature trusts the Mind that builds for aye". [13] First owned and published by Alexander Macmillan , Nature was similar to its predecessors in its attempt to "provide cultivated readers with an accessible forum for reading about advances in scientific knowledge." [12] Janet Browne has proposed that "far more than any other science journal of the period, Nature was conceived, born, and raised to serve polemic purpose." [12] Many of the early editions of Nature consisted of articles written by members of a group that called itself the X Club , a group of scientists known for having liberal, progressive, and somewhat controversial scientific beliefs relative to the time period. [12] Initiated by Thomas Henry Huxley , the group consisted of such important scientists as Joseph Dalton Hooker , Herbert Spencer , and John Tyndall , along with another five scientists and mathematicians; these scientists were all avid supporters of Darwin's theory of evolution as common descent , a theory which, during the latter-half of the 19th century, received a great deal of criticism among more conservative groups of scientists. [14] Perhaps it was in part its scientific liberality that made Nature a longer-lasting success than its predecessors. John Maddox , editor of Nature from 1966 to 1973 as well as from 1980 to 1995, suggested at a celebratory dinner for the journal's centennial edition that perhaps it was the journalistic qualities of Nature that drew readers in; "journalism" Maddox states, "is a way of creating a sense of community among people who would otherwise be isolated from each other. This is what Lockyer's journal did from the start." [15] In addition, Maddox mentions that the financial backing of the journal in its first years by the Macmillan family also allowed the journal to flourish and develop more freely than scientific journals before it. [15]

Editors
Norman Lockyer , the founder of Nature , was a professor at Imperial College . He was succeeded as editor in 1919 by Sir Richard Gregory . [16] Gregory helped to establish Nature in the international scientific community. His obituary by the Royal Society stated: "Gregory was always very interested in the international contacts of science, and in the columns of Nature he always gave generous space to accounts of the activities of the International Scientific Unions." [17] During the years 1945 to 1973, editorship of Nature changed three times, first in 1945 to A. J. V. Gale and L. J. F. Brimble (who in 1958 became the sole editor), then to John Maddox in 1965, and finally to David Davies in 1973. [16] In 1980, Maddox returned as editor and retained his position until 1995. Philip Campbell has since become Editor-in-chief of all Nature publications. [16]

Expansion and development
In 1970, Nature first opened its Washington office; other branches opened in New York in 1985, Tokyo and Munich in 1987, Paris in 1989, San Francisco in 2001, Boston in 2004, and Hong Kong in 2005. Starting in the 1980s, the journal underwent a great deal of expansion, launching over ten new journals. These new journals comprise the Nature Publishing Group, which was created in 1999 and includes Nature , Nature Publishing Group Journals, Stockton Press Specialist Journals and Macmillan Reference (renamed NPG Reference).
In 1996, Nature created its own website [18] and in 1999 Nature Publishing Group began its series of Nature Reviews . [16] Some articles and papers are available for free on the Nature website. Others require the purchase of premium access to the site.
Nature claims a readership of about 424,000 total readers. [ improper synthesis? ] The journal has a circulation of around 53,000 but studies have concluded that on average a single copy is shared by as many as 8 people. [4]
On 30 October 2008, Nature endorsed an American presidential candidate for the first time when it supported Barack Obama during his campaign in America's 2008 presidential election . [19] [20]
On October 2012, an Arabic edition of the magazine was launched in partnership with King Abdulaziz City for Science and Technology . As of the time it was released, it had about 10,000 subscribers. [21]
On 2 December 2014, Nature announced that it would allow its subscribers and a group of selected media outlets to share links allowing free, "read-only" access to content from its journals. These articles are presented using the digital rights management system ReadCube (which is funded by the Macmillan subsidiary Digital Science), and does not allow readers to download, copy, print, or otherwise distribute the content. While it does, to an extent, provide free online access to articles, it is not a true open access scheme due to its restrictions on re-use and distribution. [22] [23]
On 15 January 2015, details of a proposed merger with Springer Science+Business Media were announced. [24]
In May 2015 it came under the umbrella of Springer Nature, by the merger of Springer Science+Business Media and Holtzbrinck Publishing Group 's Nature Publishing Group , Palgrave Macmillan , and Macmillan Education . [25]

Publishing of articles
Having a paper (article or letter) published in Nature or any Nature publication such as Nature chemistry or Nature chemical biology is very prestigious, and the papers are often highly cited, which can lead to promotions, grant funding, and attention from the mainstream media. Because of these positive feedback effects, competition among scientists to publish in high-level journals like Nature and its closest competitor, Science , can be very fierce. Nature ' s impact factor , a measure of how many citations a journal generates in other works, was 38.138 in 2015 (as measured by Thomson ISI ), among the highest of any science journal.
As with most other professional scientific journals, papers undergo an initial screening by the editor, followed by peer review (in which other scientists, chosen by the editor for expertise with the subject matter but who have no connection to the research under review, will read and critique articles), before publication. In the case of Nature , they are only sent for review if it is decided that they deal with a topical subject and are sufficiently ground-breaking in that particular field. As a consequence, the majority of submitted papers are rejected without review.
According to Nature ' s original mission statement :
This was revised in 2000 to:

Landmark papers
Many of the most significant scientific breakthroughs in modern history have been first published in Nature . The following is a selection of scientific breakthroughs published in Nature , all of which had far-reaching consequences, and the citation for the article in which they were published.

Controversies
When Paul Lauterbur and Peter Mansfield won a Nobel Prize in Physiology or Medicine for research initially rejected by Nature and published only after Lauterbur appealed the rejection, Nature acknowledged more of its own missteps in rejecting papers in an editorial titled, "Coping with Peer Rejection":
From 2000–2001, a series of five fraudulent papers by Jan Hendrik Schön was published in Nature . The papers, about semiconductors , were revealed to contain falsified data and other scientific fraud. In 2003, Nature retracted the papers. The Schön scandal was not limited to Nature ; other prominent journals, such as Science and Physical Review , also retracted papers by Schön. [29]
In June 1988, after nearly a year of guided scrutiny from its editors, Nature published a controversial and seemingly anomalous paper detailing Dr. Jacques Benveniste and his team's work studying human basophil degranulation in the presence of extremely dilute antibody serum. [30] In short, their paper concluded that less than a single molecule of antibody could trigger an immune response in human basophils, defying the physical law of mass action . The paper excited substantial media attention in Paris, chiefly because their research sought funding from homeopathic medicine companies. Public inquiry prompted Nature to mandate an extensive, stringent and scientifically questionable experimental replication in Benveniste's lab, through which his team's results were categorically disputed. [31]
Before publishing one of its most famous discoveries, Watson and Crick 's 1953 paper on the structure of DNA , Nature did not send the paper out for peer review. John Maddox , Nature ' s editor, stated: "the Watson and Crick paper was not peer-reviewed by Nature ... the paper could not have been refereed: its correctness is self-evident. No referee working in the field ... could have kept his mouth shut once he saw the structure". [32]
An earlier error occurred when Enrico Fermi submitted his breakthrough paper on the weak interaction theory of beta decay . Nature turned down the paper because it was considered too remote from reality. [33] Fermi's paper was published by Zeitschrift für Physik in 1934, [34] and finally published by Nature five years later, [ citation needed ] after Fermi's work had been widely accepted.

Science fiction
In 1999 Nature began publishing science fiction short stories . The brief " vignettes " are printed in a series called "Futures". The stories appeared in 1999 and 2000, again in 2005 and 2006, and have appeared weekly since July 2007. [35] Sister publication Nature Physics also printed stories in 2007 and 2008. [36] In 2005, Nature was awarded the European Science Fiction Society 's Best Publisher award for the "Futures" series. [37] One hundred of the Nature stories between 1999 and 2006 were published as the collection Futures from Nature in 2008. [38]

Publication
Nature is edited and published in the United Kingdom by a division of the international scientific publishing company Springer Nature that publishes academic journals , magazines , online databases, and services in science and medicine. Nature has offices in London, New York City, San Francisco, Washington, D.C., Boston , Tokyo, Hong Kong, Paris, Munich , and Basingstoke . Nature Publishing Group also publishes other specialized journals including Nature Neuroscience , Nature Biotechnology , Nature Methods , the Nature Clinical Practice series of journals, Nature Structural & Molecular Biology , Nature Chemistry , and the Nature Reviews series of journals.
Since 2005, each issue of Nature has been accompanied by a Nature Podcast [39] featuring highlights from the issue and interviews with the articles' authors and the journalists covering the research. It is presented by Kerri Smith, and features interviews with scientists on the latest research, as well as news reports from Nature's editors and journalists. The Nature Podcast was founded – and the first 100 episodes were produced and presented – by clinician and virologist Chris Smith of Cambridge and The Naked Scientists .
In 2007, Nature Publishing Group began publishing Clinical Pharmacology & Therapeutics , the official journal of the American Society of Clinical Pharmacology & Therapeutics and Molecular Therapy , the American Society of Gene Therapy's official journal, as well as the International Society for Microbial Ecology (ISME) Journal . Nature Publishing Group launched Nature Photonics in 2007 and Nature Geoscience in 2008. Nature Chemistry published its first issue in April 2009.
Nature Publishing Group actively supports the self-archiving process and in 2002 was one of the first publishers to allow authors to post their contributions on their personal websites, by requesting an exclusive licence to publish, rather than requiring authors to transfer copyright. In December 2007, Nature Publishing Group introduced the Creative Commons attribution-non commercial-share alike unported licence for those articles in Nature journals that are publishing the primary sequence of an organism's genome for the first time. In 2008, a collection of articles from Nature was edited by John S. Partington under the title H. G. Wells in Nature, 1893–1946: A Reception Reader and published by Peter Lang. [40] [41]

Notes and references

Bibliography

External links
WebPage index: 00026
History of Wikipedia
Wikipedia began with its launch on 15 January 2001, two days after the domain was registered [2] by Jimmy Wales and Larry Sanger . Its technological and conceptual underpinnings predate this; the earliest known proposal for an online encyclopedia was made by Rick Gates in 1993, [3] but the concept of a free-as-in-freedom online encyclopedia (as distinct from mere open source ) [4] was proposed by Richard Stallman in December 2000. [5]
Crucially, Stallman's concept specifically included the idea that no central organization should control editing. This characteristic was in stark contrast to contemporary digital encyclopedias such as Microsoft Encarta , Encyclopædia Britannica , and even Bomis 's Nupedia , which was Wikipedia's direct predecessor. In 2001, the license for Nupedia was changed to GFDL , and Wales and Sanger launched Wikipedia using the concept and technology of a wiki pioneered in 1995 by Ward Cunningham . [6] Initially, Wikipedia was intended to complement Nupedia, an online encyclopedia project edited solely by experts, by providing additional draft articles and ideas for it. In practice, Wikipedia quickly overtook Nupedia, becoming a global project in multiple languages and inspiring a wide range of other online reference projects .
According to Alexa Internet , Wikipedia is the world's fifth-most-popular website in terms of overall visitor traffic. [7] Wikipedia's worldwide monthly readership is approximately 495 million. [8] Worldwide in August 2015, WMF Labs tallied 18 billion page views for the month. [9] According to comScore , Wikipedia receives over 117 million monthly unique visitors from the United States alone. [10]

Historical overview

Background
The concept of compiling the world's knowledge in a single location dates to the ancient Libraries of Alexandria and Pergamum , but the modern concept of a general-purpose, widely distributed, printed encyclopedia originated with Denis Diderot and the 18th-century French encyclopedists . The idea of using automated machinery beyond the printing press to build a more useful encyclopedia can be traced to Paul Otlet 's 1934 book Traité de documentation ; Otlet also founded the Mundaneum , an institution dedicated to indexing the world's knowledge, in 1910. This concept of a machine-assisted encyclopedia was further expanded in H. G. Wells ' book of essays World Brain (1938) and Vannevar Bush 's future vision of the microfilm -based Memex in his essay " As We May Think " (1945). [11] Another milestone was Ted Nelson 's hypertext design Project Xanadu , which was begun in 1960. [11]
Advances in information technology in the late 20th century led to changes in the form of encyclopedias. While previous encyclopedias, notably the Encyclopædia Britannica , were book-based, Microsoft's Encarta , published in 1993, was available on CD-ROM and hyperlinked . The development of the World Wide Web led to many attempts to develop internet encyclopedia projects . An early proposal for a web-based encyclopedia was Interpedia in 1993 by Rick Gates ; [3] this project died before generating any encyclopedic content. Free software proponent Richard Stallman described the usefulness of a "Free Universal Encyclopedia and Learning Resource" in 1999. [5] His published document "aims to lay out what the free encyclopedia needs to do, what sort of freedoms it needs to give the public, and how we can get started on developing it." On Wednesday 17 January 2001, two days after the founding of Wikipedia, the Free Software Foundation 's (FSF) GNUPedia project went online, competing with Nupedia , [12] but today the FSF encourages people "to visit and contribute to [Wikipedia]". [13]

Formulation of the concept
Wikipedia was initially conceived as a feeder project for the Wales-founded Nupedia , an earlier project to produce a free online encyclopedia, volunteered by Bomis , a web-advertising firm owned by Jimmy Wales , Tim Shell and Michael E. Davis . [14] [15] [16] Nupedia was founded upon the use of highly qualified volunteer contributors and an elaborate multi-step peer review process. [17] Despite its mailing list of interested editors, and the presence of a full-time editor-in-chief, Larry Sanger , a graduate philosophy student hired by Wales, [18] the writing of content for Nupedia was extremely slow, with only 12 articles written during the first year. [16]
Wales and Sanger discussed various ways to create content more rapidly. [15] The idea of a wiki -based complement originated from a conversation between Larry M. Sanger and Ben Kovitz. [19] [20] [21] Ben Kovitz was a computer programmer and regular on Ward Cunningham 's revolutionary wiki "the WikiWikiWeb ". He explained to Sanger what wikis were, at that time a difficult concept to understand, over a dinner on Tuesday 2 January 2001. [19] [20] [21] [22] Wales first stated, in October 2001, that "Larry had the idea to use Wiki software", [23] though he later stated in December 2005 that Jeremy Rosenfeld, a Bomis employee, introduced him to the concept. [24] [25] [26] [27] Sanger thought a wiki would be a good platform to use, and proposed on the Nupedia mailing list that a wiki based upon UseModWiki (then v. 0.90) be set up as a "feeder" project for Nupedia. Under the subject "Let's make a wiki", he wrote:
Wales set one up and put it online on Wednesday 10 January 2001. [28]

Founding of Wikipedia
There was considerable resistance on the part of Nupedia's editors and reviewers to the idea of associating Nupedia with a wiki-style website. Sanger suggested giving the new project its own name, Wikipedia , and Wikipedia was soon launched on its own domain, wikipedia.com , on Monday 15 January 2001. The bandwidth and server (located in San Diego) used for these initial projects were donated by Bomis. Many former Bomis employees later contributed content to the encyclopedia: notably Tim Shell , co-founder and later CEO of Bomis, and programmer Jason Richey.
Wales stated in December 2008 that he made Wikipedia's first edit, a test edit with the text " Hello, World !" [29] The oldest article still preserved is the article UuU , created on Tuesday 16 January 2001, at 21:08 UTC. [30] [31] The existence of the project was formally announced and an appeal for volunteers to engage in content creation was made to the Nupedia mailing list on 17 January. [32]
The project received many new participants after being mentioned on the Slashdot website in July 2001, [33] having already earned two minor mentions in March 2001. [34] [35] It then received a prominent pointer to a story on the community-edited technology and culture website Kuro5hin on 25 July. [36] Between these relatively rapid influxes of traffic, there had been a steady stream of traffic from other sources, especially Google , which alone sent hundreds of new visitors to the site every day. Its first major mainstream media coverage was in the New York Times on Thursday 20 September 2001. [37]
The project gained its 1,000th article around Monday 12 February 2001, and reached 10,000 articles around 7 September. In the first year of its existence, over 20,000 encyclopedia entries were created – a rate of over 1,500 articles per month. On Friday 30 August 2002, the article count reached 40,000.
Wikipedia's earliest edits were long believed lost, since the original UseModWiki software deleted old data after about a month. On Tuesday 14 December 2010, developer Tim Starling found backups on SourceForge containing every change made to Wikipedia from its creation in January 2001 to 17 August 2001. [38]

Namespaces, subdomains, and internationalization
Early in Wikipedia's development, it began to expand internationally, with the creation of new namespaces, each with a distinct set of usernames. The first subdomain created for a non-English Wikipedia was deutsche.wikipedia.com (created on Friday 16 March 2001, 01:38 UTC), [39] followed after a few hours by Catalan.wikipedia.com (at 13:07 UTC). [40] The Japanese Wikipedia, started as nihongo.wikipedia.com , was created around that period, [41] [42] and initially used only Romanized Japanese. For about two months Catalan was the one with the most articles in a non-English language, [43] [44] although statistics of that early period are imprecise. [45] The French Wikipedia was created on or around 11 May 2001, [46] in a wave of new language versions that also included Chinese , Dutch , Esperanto , Hebrew , Italian , Portuguese , Russian , Spanish , and Swedish . [47] These languages were soon joined by Arabic [48] and Hungarian . [49] [50] In September 2001, an announcement pledged commitment to the multilingual provision of Wikipedia, [51] notifying users of an upcoming roll-out of Wikipedias for all major languages, the establishment of core standards, and a push for the translation of core pages for the new wikis. At the end of that year, when international statistics first began to be logged, Afrikaans , Norwegian , and Serbian versions were announced. [52]
In January 2002, 90% of all Wikipedia articles were in English. By January 2004, fewer than 50% were English, and this internationalization has continued to increase as the encyclopedia grows. As of 2014 [update] , about 85.5% of all Wikipedia articles are contained within non-English Wikipedia versions. [1]

Development of Wikipedia
In March 2002, following the withdrawal of funding by Bomis during the dot-com bust , Larry Sanger left both Nupedia and Wikipedia. [53] By 2002, Sanger and Wales differed in their views on how best to manage open encyclopedias. Both still supported the open-collaboration concept, but the two disagreed on how to handle disruptive editors, specific roles for experts, and the best way to guide the project to success.
Wales went on to establish self-governance and bottom-up self-direction by editors on Wikipedia. He made it clear that he would not be involved in the community's day-to-day management, but would encourage it to learn to self-manage and find its own best approaches. As of 2007 [update] , Wales mostly restricts his own role to occasional input on serious matters, executive activity, advocacy of knowledge, and encouragement of similar reference projects.
Sanger says he is an "inclusionist" and is open to almost anything. [54] He proposed that experts still have a place in the Web 2.0 world. He returned briefly to academia, then joined the Digital Universe Foundation. In 2006, Sanger founded Citizendium , an open encyclopedia that used real names for contributors in an effort to reduce disruptive editing, and hoped to facilitate "gentle expert guidance" to increase the accuracy of its content. Decisions about article content were to be up to the community, but the site was to include a statement about "family-friendly content". [55] He stated early on that he intended to leave Citizendium in a few years, by which time the project and its management would presumably be established. [56]

Organization
The Wikipedia project has grown rapidly in the course of its life, at several levels. Content has grown organically through the addition of new articles, new wikis have been added in English and non-English languages, and entire new projects replicating these growth methods in other related areas (news, quotations, reference books and so on) have been founded as well. Wikipedia itself has grown, with the creation of the Wikimedia Foundation to act as an umbrella body and the growth of software and policies to address the needs of the editorial community. These are documented below:

Timeline

First decade: 2000–2009

2000
In March 2000, the Nupedia project was started. Its intention was to publish articles written by experts which would be licensed as free content . Nupedia was founded by Jimmy Wales, with Larry Sanger as editor-in-chief, and funded by the web-advertising company Bomis . [57]

2001
In January 2001, Wikipedia began as a side-project of Nupedia, to allow collaboration on articles prior to entering the peer-review process. [58] The name was suggested by Sanger on 11 January 2001. [59] The wikipedia.com and wikipedia.org domain names were registered on 12 [60] and 13 January, [61] respectively, with wikipedia.org being brought online on the same day. [62] The project formally opened on 15 January (" Wikipedia Day "), with the first international Wikipedias – the French, German, Catalan , Swedish, and Italian editions – being created between March and May. The "neutral point of view" (NPOV) policy was officially formulated at this time, and Wikipedia's first slashdotter wave arrived on 26 July. [33] The first media report about Wikipedia appeared in August 2001 in the newspaper Wales on Sunday . [63] The September 11 attacks spurred the appearance of breaking news stories on the homepage, as well as information boxes linking related articles. [64]

2002
2002 saw the end of funding for Wikipedia from Bomis and the departure of Larry Sanger . The forking of the Spanish Wikipedia also took place with the establishment of the Enciclopedia Libre . The first portable MediaWiki software went live on 25 January. Bots were introduced, Jimmy Wales confirmed that Wikipedia would never run commercial advertising, and the first sister project ( Wiktionary ) and first formal Manual of Style were launched. A separate board of directors to supervise the project was proposed and initially discussed at Meta-Wikipedia . [ citation needed ]

2003
The English Wikipedia passed 100,000 articles in 2003, while the next largest edition, the German Wikipedia, passed 10,000. The Wikimedia Foundation was established, and Wikipedia adopted its jigsaw world logo . Mathematical formulae using TeX were reintroduced to the website. The first Wikipedian social meeting took place in Munich , Germany, in October. The basic principles of Wikipedia's Arbitration system and committee (known colloquially as "ArbCom") were developed, mostly by Florence Devouard , Fred Bauder and other early Wikipedians. [ citation needed ]
Wikisource was created as a separate project on November 24, 2003, to host free textual sources.

2004
The worldwide Wikipedia article pool continued to grow rapidly in 2004, doubling in size in 12 months, from under 500,000 articles in late 2003 to over 1 million in over 100 languages by the end of 2004. The English Wikipedia accounted for just under half of these articles. The website's server farms were moved from California to Florida , Categories and CSS style configuration sheets were introduced, and the first attempt to block Wikipedia occurred, with the website being blocked in China for two weeks in June. The formal election of a board and Arbitration Committee began. The first formal projects were proposed to deliberately balance content and seek out systemic bias arising from Wikipedia's community structure.
Bourgeois v. Peters , [65] (11th Cir. 2004), a court case decided by the United States Court of Appeals for the Eleventh Circuit was one of the earliest court opinions to cite and quote Wikipedia . [66] It stated: "We also reject the notion that the Department of Homeland Security's threat advisory level somehow justifies these searches. Although the threat level was 'elevated' at the time of the protest, 'to date, the threat level has stood at yellow (elevated) for the majority of its time in existence. It has been raised to orange (high) six times. ' " [65]
Wikimedia Commons was created in September 7, 2004 to host media files for Wikipedia in all languages.

2005
In 2005, Wikipedia became the most popular reference website on the Internet, according to Hitwise , with the English Wikipedia alone exceeding 750,000 articles. Wikipedia's first multilingual and subject portals were established in 2005. A formal fundraiser held in the first quarter of the year raised almost US$100,000 for system upgrades to handle growing demand. China again blocked Wikipedia in October 2005.
The first major Wikipedia scandal, the Seigenthaler incident , occurred in 2005, when a well-known figure was found to have a vandalized biography which had gone unnoticed for months. In the wake of this and other concerns, [67] the first policy and system changes specifically designed to counter this form of abuse were established. These included a new Checkuser privilege policy update to assist in sock puppetry investigations, a new feature called semi-protection , a more strict policy on biographies of living people and the tagging of such articles for stricter review. A restriction of new article creation to registered users only was put in place in December 2005. [68]
Wikimania 2005, the first Wikimania conference, was held from 4 to 8 August 2005 at the Haus der Jugend in Frankfurt , Germany, attracting about 380 attendees.

2006
The English Wikipedia gained its one-millionth article, Jordanhill railway station , on 1 March 2006. The first approved Wikipedia article selection was made freely available to download, and "Wikipedia" became registered as a trademark of the Wikimedia Foundation. The congressional aides biography scandals – multiple incidents in which congressional staffers and a campaign manager were caught trying to covertly alter Wikipedia biographies – came to public attention, leading to the resignation of the campaign manager. Nonetheless, Wikipedia was rated as one of the top five global brands of 2006. [69]
Jimmy Wales indicated at Wikimania 2006 that Wikipedia had achieved sufficient volume and called for an emphasis on quality, perhaps best expressed in the call for 100,000 feature-quality articles . A new privilege, "oversight", was created, allowing specific versions of archived pages with unacceptable content to be marked as non-viewable. Semi-protection against anonymous vandalism, introduced in 2005, proved more popular than expected, with over 1,000 pages being semi-protected at any given time in 2006.

2007
Wikipedia continued to grow rapidly in 2007, possessing over 5 million registered editor accounts by 13 August. [70] The 250 language editions of Wikipedia contained a combined total of 7.5 million articles, totalling 1.74 billion words in approximately 250 languages, by 13 August. [71] The English Wikipedia gained articles at a steady rate of 1,700 a day, [72] with the wikipedia.org domain name ranked the 10th-busiest in the world. Wikipedia continued to garner visibility in the press – the Essjay controversy broke when a prominent member of Wikipedia was found to have lied about his credentials. Citizendium , a competing online encyclopedia, launched publicly. A new trend developed in Wikipedia, with the encyclopedia addressing people whose notability stemmed from being a participant in a news story by adding a redirect from their name to the larger story, rather than creating a distinct biographical article. [73] On 9 September 2007, the English Wikipedia gained its two-millionth article, El Hormiguero . [74] There was some controversy in late 2007 when the Volapük Wikipedia jumped from 797 to over 112,000 articles, briefly becoming the 15th-largest Wikipedia edition, due to automated stub generation by an enthusiast for the Volapük constructed language. [75] [76]
According to the MIT Technology Review , the number of regularly active editors on the English-language Wikipedia peaked in 2007 at more than 51,000, and has since been declining. [77]

2008
Various WikiProjects in many areas continued to expand and refine article contents within their scope. In April 2008, the 10-millionth Wikipedia article was created, and by the end of the year the English Wikipedia exceeded 2.5 million articles.

2009
In June 25, 2009 at 3:15 pm PDT (22:15 UTC), following pop icon Michael Jackson's death, the website temporarily crashed.
The Wikimedia Foundation reported nearly a million visitors to Jackson's biography within one hour, probably the most visitors in a one-hour period to any article in Wikipedia's history. By late August 2009, the number of articles in all Wikipedia editions had exceeded 14 million. [78] The three-millionth article on the English Wikipedia, Beate Eriksen , was created on 17 August 2009 at 04:05 UTC. [79] On 27 December 2009, the German Wikipedia exceeded one million articles, becoming the second edition after the English Wikipedia to do so. A TIME article listed Wikipedia among 2009's best websites. [80]
Wikipedia content became licensed under Creative Commons in 2009.

Second decade: 2010–present

2010
On 24 March, the European Wikipedia servers went offline due to an overheating problem. Failover to servers in Florida turned out to be broken, causing DNS resolution for Wikipedia to fail across the world. The problem was resolved quickly, but due to DNS caching effects, some areas were slower to regain access to Wikipedia than others. [81] [82]
On 13 May, the site released a new interface. New features included an updated logo, new navigation tools, and a link wizard. [83] However, the classic interface remained available for those who wished to use it. On 12 December, the English Wikipedia passed the 3.5-million-article mark, while the French Wikipedia 's millionth article was created on 21 September. The 1-billionth Wikimedia project edit was performed on 16 April. [84]

2011
Wikipedia and its users held hundreds of celebrations worldwide to commemorate the site's 10th anniversary on 15 January. [86] The site began efforts to expand its growth in India, holding its first Indian conference in Mumbai in November 2011. [87] [88] The English Wikipedia passed the 3.6-million-article mark on 2 April, and reached 3.8 million articles on 18 November. On 7 November 2011, the German Wikipedia exceeded 100 million page edits, becoming the second language edition to do so after the English edition, which attained 500 million page edits on 24 November 2011. The Dutch Wikipedia exceeded 1 million articles on 17 December 2011, becoming the fourth Wikipedia edition to do so.
Between 4 and 6 October 2011, the Italian Wikipedia became intentionally inaccessible in protest against the Italian Parliament 's proposed DDL intercettazioni law, which, if approved, would allow any person to force websites to remove information that is perceived as untrue or offensive, without the need to provide evidence. [89]
Also in October 2011, Wikimedia announced the launch of Wikipedia Zero , an initiative to enable free mobile access to Wikipedia in developing countries through partnerships with mobile operators. [90] [91]

2012
On 16 January, Wikipedia co-founder Jimmy Wales announced that the English Wikipedia would shut down for 24 hours on 18 January as part of a protest meant to call public attention to the proposed Stop Online Piracy Act and PROTECT IP Act , two anti- piracy laws under debate in the United States Congress . Calling the blackout a "community decision", Wales and other opponents of the laws believed that they would endanger free speech and online innovation. [92] A similar blackout was staged on 10 July by the Russian Wikipedia , in protest against a proposed Russian internet regulation law. [93]
In late March 2012, the Wikimedia Deutschland announced Wikidata , a universal platform for sharing data between all Wikipedia language editions. [94] The US$1.7-million Wikidata project was partly funded by Google , the Gordon and Betty Moore Foundation , and the Allen Institute for Artificial Intelligence. [95] Wikimedia Deutschland assumed responsibility for the first phase of Wikidata, and initially planned to make the platform available to editors by December 2012. Wikidata's first phase became fully operational in March 2013. [96] [97]
In April 2012, Justin Knapp became the first single contributor to make over one million edits to Wikipedia. [98] [99] Jimmy Wales congratulated Knapp for his work and presented him with the site's Special Barnstar medal and the Golden Wiki award for his achievement. [100] Wales also declared that 20 April would be "Justin Knapp Day". [101]
On 13 July 2012, the English Wikipedia gained its 4-millionth article, Izbat al-Burj . [102] In October 2012, historian and Wikipedia editor Richard J. Jensen opined that the English Wikipedia was "nearing completion", noting that the number of regularly active editors had fallen significantly since 2007, despite Wikipedia's rapid growth in article count and readership. [103]
According to Alexa Internet , Wikipedia was the world's sixth-most-popular website as of November 2012. [104] Dow Jones ranked Wikipedia fifth worldwide as of December 2012. [105]

2013
On 22 January 2013, the Italian Wikipedia became the fifth language edition of Wikipedia to exceed 1 million articles, while the Russian and Spanish Wikipedias gained their millionth articles in May 11 and 16 respectively. On 15 July the Swedish and on 24 September the Polish Wikipedias gained their millionth articles, becoming the eighth and ninth Wikipedia editions to do so.
On 27 January, the main belt asteroid 274301 was officially renamed "Wikipedia" by the Committee for Small Body Nomenclature . [106]
The first phase of the Wikidata database, automatically providing interlanguage links and other data, became available for all language editions in March 2013. [97]
In April 2013, the French secret service was accused of attempting to censor Wikipedia by threatening a Wikipedia volunteer with arrest unless "classified information" about a military radio station was deleted. [107]
In July, the VisualEditor editing system was launched, forming the first stage of an effort to allow articles to be edited with a word processor -like interface instead of using wikimarkup . [108] An editor specifically designed for smartphones and other mobile devices was also launched. [109]

2014
A print edition of the English Wikipedia, comprising 1,000 volumes and over 1,100,000 pages, was exhibited by German Wikipedia contributors in 2014. [8] The project sought funding through Indiegogo , and was intended to honor the contributions of Wikipedia's editors. [110] On 22 October 2014, the first monument to Wikipedia was unveiled in the Polish town of Slubice. [111]

2015
In mid-2015, Wikipedia was the world's seventh-most-popular website according to Alexa Internet , [7] down one place from the position it held in November 2012. At the start of 2015, Wikipedia remained the largest general-knowledge encyclopedia online, with a combined total of over 36 million mainspace articles across all 291 language editions. [1] On average, Wikipedia receives a total of 10 billion global pageviews from around 495 million unique visitors every month, [8] [112] including 85 million visitors from the United States alone, [10] where it is the sixth-most-popular site. [7]
Print Wikipedia was an art project by Michael Mandiberg that printed out the 7473 volumes of Wikipedia as it existed on April 7, 2015. Each volume has 700 pages. [113]
On November 1, 2015, the English Wikipedia reached 5,000,000 articles with the creation of an article on Persoonia terminalis , a type of shrub.

2016
In mid-2016, Wikipedia was once again the world's sixth-most-popular website according to Alexa Internet , [114] up one place from the position it held in the previous year.
In October 2016, the mobile version of Wikipedia got a new look.

History by subject area

Hardware and software

Look and feel

Internal structures

The Wikimedia Foundation and legal structures

Projects and milestones

Fundraising
Every year, Wikipedia runs a fundraising campaign to support its operations.

External impact

Effect of biographical articles
Because Wikipedia biographies are often updated as soon as new information comes to light, they are often used as a reference source on the lives of notable people . This has led to attempts to manipulate and falsify Wikipedia articles for promotional or defamatory purposes (see Controversies ). It has also led to novel uses of the biographical material provided. Some notable people's lives are being affected by their Wikipedia biography.

Early roles of Wales and Sanger
Sanger played an important role in the early stages of creating Wikipedia. [175] [176] Wales says that Sanger was his subordinate employee. [176] Sanger initially brought the wiki concept to Wales and suggested it be applied to Nupedia and then, after some initial skepticism, Wales agreed to try it. [20] It was Jimmy Wales, along with other people, who came up with the broader idea of an open-source, collaborative encyclopedia that would accept contributions from ordinary people and it was Wales who invested in it. [16] Wales stated in October 2001 that "Larry had the idea to use Wiki software." [23] Sanger coined the portmanteau "Wikipedia" as the project name. [16] In review, Larry Sanger conceived of a wiki-based encyclopedia as a strategic solution to Nupedia's inefficiency problems. [176] In terms of project roles, Sanger spearheaded and pursued the project as its leader in its first year, and did most of the early work in formulating policies (including "Ignore all rules") [177] and "Neutral point of view" [53] and building up the community. [176] Upon departure in March 2002, Sanger emphasized the main issue was purely the cessation of Bomis' funding for his role, which was not viable part-time, and his changing personal priorities; [18] however, by 2004, the two had drifted apart and Sanger became more critical. Two weeks after the launch of Citizendium, Sanger criticized Wikipedia, describing the latter as "broken beyond repair." [178] By 2005 Wales began to dispute Sanger's role in the project, three years after Sanger left. [179] [180] [181]
In 2005, Wales described himself simply as the founder of Wikipedia; [179] however, according to Brian Bergstein of the Associated Press , "Sanger has long been cited as a co-founder." [176] There is evidence that Sanger was called co-founder, along with Wales, as early as 2001, and he is referred to as such in early Wikipedia press releases and Wikipedia articles and in a September 2001 New York Times article for which both were interviewed. [182] In 2006, Wales said, "He used to work for me [...] I don't agree with calling him a co-founder, but he likes the title"; [183] nonetheless, before January 2004, Wales did not dispute Sanger's status as co-founder [184] and, indeed, identified himself as "co-founder" as late as August 2002. [185] In Sanger's introductory message to the Nupedia mailing list, he said that "Jimmy Wales contacted me and asked me to apply as editor-in-chief of Nupedia. Apparently, Bomis, Inc. (which owns Nupedia)... who could manage this sort of long-term project, he thought I would be perfect for the job. This is indeed my dream job". [186] Sanger said "He [Wales] had had the idea for Nupedia since at least last fall". [186]
As of March 2007: Wales emphasized this employer–employee relationship and his ultimate authority, terming himself Wikipedia's sole founder; and Sanger emphasized their statuses as co-founders, referencing earlier versions of Wikipedia pages (2004, 2006), press releases (2002–2004), and media coverage from the time of his involvement routinely terming them in this manner. [176] [182] [187] [188]

Controversies
The goals which led to GNUpedia were published at least as early as 18 December 2000, [190] [191] and these exact goals were finalized on the 12th [189] and 13th [192] of January 2001, albeit with a copyright of 1999, from when Stallman had first started considering the problem. The only sentence added between 18 December and the unveiling of GNUpedia the week of 12–16 January was this: "The GNU Free Documentation License would be a good license to use for courses."
GNUpedia was 'formally' announced on the slashdot website, [193] on January 16, the same day that their mailing list first went online with a test-message. Wales posted to the list on January 17, the first full day of messages, explaining the discussions with Stallman concerning the change in Nupedia content-licensing, and suggesting cooperation. [194] [195] Stallman himself first posted on January 19, and, in his second post on January 22, mentioned that discussions about merging Wikipedia and GNUpedia were ongoing. [196] Within a couple of months, Wales had changed his email signature from the open source encyclopedia to the free encyclopedia; [197] both Nupedia and Wikipedia had adopted the GFDL ; and the merger [198] of GNUpedia into Wikipedia was effectively accomplished.
In a separate but similar incident, the campaign manager for Cathy Cox , Morton Brilliant, resigned after being found to have added negative information to the Wikipedia entries of political opponents. [208] Following media publicity, the incidents tapered off around August 2006.

Notable forks and derivatives
There are a number of Wikipedia mirrors and forks . No list of sites using the software is maintained, A significant number of sites use the MediaWiki software and concept, popularized by Wikipedia .
Specialized foreign language forks using the Wikipedia concept include Enciclopedia Libre (Spanish), Wikiweise (German), WikiZnanie (Russian), Susning.nu (Swedish), and Baidu Baike (Chinese). Some of these (such as Enciclopedia Libre ) use GFDL or compatible licenses as used by Wikipedia, leading to exchange of material with their respective language Wikipedias.
In 2006, Larry Sanger founded Citizendium , based upon a modified version of MediaWiki . [231] The site cited its aims were 'to improve on the Wikipedia model with "gentle expert oversight", among other things'. [56] [232] (See also Nupedia ).

Publication on other media
The German Wikipedia was the first to be partly published also using other media (rather than online on the internet), including releases on CD in November 2004 [233] and more extended versions on CDs or DVD in April 2005 and December 2006. In December 2005, the publisher Zenodot Verlagsgesellschaft mbH, a sister company of Directmedia, published a 139-page book explaining Wikipedia, its history and policies, which was accompanied by a 7.5 GB DVD containing 300,000 articles and 100,000 images from the German Wikipedia. [234] Originally, Directmedia also announced plans to print the German Wikipedia in its entirety, in 100 volumes of 800 pages each. Publication was due to begin in October 2006, and finish in 2010. In March 2006, however, this project was called off. [235]
In September 2008, Bertelsmann published a 1000 pages volume with a selection of popular German Wikipedia articles. Bertelsmann paid voluntarily 1 Euro per sold copy to Wikimedia Deutschland . [236]
The first CD version containing a selection of articles from the English Wikipedia was published in April 2006 by SOS Children as the 2006 Wikipedia CD Selection . [237] In April 2007, "Wikipedia Version 0.5", a CD containing around 2000 articles selected from the online encyclopedia was published by the Wikimedia Foundation and Linterweb. The selection of articles included was based on both the quality of the online version and the importance of the topic to be included. This CD version was created as a test-case in preparation for a DVD version including far more articles. [238] [239] The CD version can be purchased online, downloaded as a DVD image file or Torrent file , or accessed online at the project's website .
A free software project has also been launched to make a static version of Wikipedia available for use on iPods . The "Encyclopodia" project was started around March 2006 and can currently be used on 1st to 4th generation iPods. [240]

Lawsuits
In limited ways, the Wikimedia Foundation is protected by Section 230 of the Communications Decency Act . In the defamation action Bauer et al. v. Glatzer et al. , it was held that Wikimedia had no case to answer because of this section. [241] A similar law in France caused a lawsuit to be dismissed in October 2007. [242] In 2013, a German appeals court (the Higher Regional Court of Stuttgart ) ruled that Wikipedia is a "service provider" not a "content provider", and as such is immune from liability as long as it takes down content that is accused of being illegal. [243]

See also
WebPage index: 00027
Open content
Open content is a neologism coined by David Wiley in 1998 [1] which describes a creative work that others can copy or modify. The term evokes the related concept of open source software . [2]

History
Originally, the Open content concept and term was evangelized via the Open Content Project by David A. Wiley in 1998, and described works licensed under the Open Content License (a non-free share-alike license, see 'Free content' below) and other works licensed under similar terms. [2]
It has since come to describe a broader class of content without conventional copyright restrictions. The openness of content can be assessed under the '5Rs Framework' based on the extent to which it can be reused, revised, remixed and redistributed by members of the public without violating copyright law. [3] Unlike open source and free content , there is no clear threshold that a work must reach to qualify as 'open content'.
Although open content has been described as a counterbalance to copyright , [4] open content licenses rely on a copyright holder's power to license their work, similarly as copyleft which also utilizes copyright for such a purpose.
In 2003 Wiley announced that the Open Content Project has been succeeded by Creative Commons and their licenses , where he joined as "Director of Educational Licenses". [5] [6]
In 2006 the Creative Commons ' successor project was the Definition of Free Cultural Works [7] for free content , put forth by Erik Möller , [8] Richard Stallman , Lawrence Lessig , Benjamin Mako Hill , [8] Angela Beesley, [8] and others. The Definition of Free Cultural Works is used by the Wikimedia Foundation . [9] In 2008, the Attribution and Attribution-ShareAlike Creative Commons licenses were marked as "Approved for Free Cultural Works" among other licenses. [10]
Another successor project is the Open Knowledge Foundation ( OKF ), [11] founded by Rufus Pollock in Cambridge , UK in 2004 [12] as a global non-profit network to promote and share open content and data. [13] In 2007 the Open Knowledge Foundation gave a Open Knowledge Definition for "Content such as music, films, books; Data be it scientific, historical, geographic or otherwise; Government and other administrative information" . [14] In October 2014 with version 2.0 Open Works and Open Licenses were defined and "open" is described as synonymous to the definitions of open/free in the Open Source Definition , the Free Software Definition and the Definition of Free Cultural Works . [15] A distinct difference is the focus given to the public domain and that it focuses also on the accessibility (" open access ") and the readability (" open formats "). Among several conformant licenses, six are recommended, three own (Open Data Commons Public Domain Dedication and Licence (PDDL), Open Data Commons Attribution License (ODC-BY), Open Data Commons Open Database License (ODbL)) and the CC BY , CC BY-SA , and CC0 creative commons licenses. [16] [17] [18]

"Open Content" definition
The OpenContent website once defined OpenContent as 'freely available for modification, use and redistribution under a license similar to those used by the Open Source / Free Software community'. [2] However, such a definition would exclude the Open Content License (OPL) because that license forbade charging 'a fee for the [OpenContent] itself', a right required by free and open source software licenses. [ citation needed ]
The term since shifted in meaning. OpenContent "is licensed in a manner that provides users with free and perpetual permission to engage in the 5R activities." [3]
The 5Rs are put forward on the OpenContent website as a framework for assessing the extent to which content is open:
This broader definition distinguishes open content from open source software, since the latter must be available for commercial use by the public. However, it is similar to several definitions for open educational resources , which include resources under noncommercial and verbatim licenses. [19] [20]
The later Open Definition by the Open Knowledge Foundation (now kown as Open Knowledge International ) define open knowledge with open content and open data as sub-elements and draws heavily on the Open Source Definition ; it preserves the limited sense of open content as free content, [21] unifying both.

Open access
" Open access " refers to toll-free or gratis access to content, consisting mainly of published peer-reviewed scholarly journal articles. Some open access works are also licensed for reuse and redistribution, which would qualify them as open content.

Open content and education
Over the past decade, open content has been used to develop alternative routes towards higher education. Traditional universities are expensive, and their tuition rates are increasing. [22] Open content allows a free way of obtaining higher education that is "focused on collective knowledge and the sharing and reuse of learning and scholarly content." [23] There are multiple projects and organizations that promote learning through open content, including OpenCourseWare Initiative , The Saylor Foundation and Khan Academy . Some universities, like MIT , Yale , and Tufts are making their courses freely available on the internet. [24]

Textbooks
The textbook industry is one of the educational industries in which open content can make the biggest impact. [25] Traditional textbooks, aside from being expensive, can also be inconvenient and out of date, because of publishers' tendency to constantly print new editions. [26] Open textbooks help to eliminate this problem, because they are online and thus easily updatable. Being openly licensed and online can be helpful to teachers, because it allows the textbook to be modified according to the teacher's unique curriculum. [25] There are multiple organizations promoting the creation of openly licensed textbooks. Some of these organizations and projects include The University of Minnesota's Open Textbook Library , Connexions , OpenStax College , The Saylor Foundation Open Textbook Challenge and Wikibooks

Licenses
According to the current definition of open content on the OpenContent website, any general, royalty-free copyright license would qualify as an open license because it 'provides users with the right to make more kinds of uses than those normally permitted under the law. These permissions are granted to users free of charge.' [3]
However, the narrower definition used in the Open Definition effectively limits open content to libre content , any free content license, defined by the Definition of Free Cultural Works , would qualify as an open content license. According to this narrower criteria, the following still-maintained licenses qualify:
(For more licenses see Open Knowledge , Free content and Free Cultural Works licenses)

See also
WebPage index: 00028
Canadian Broadcasting Corporation
The Canadian Broadcasting Corporation ( French : Société Radio-Canada ), branded as CBC/Radio-Canada , is a Canadian federal crown corporation that serves as the national public radio and television broadcaster. [1] The English- and French-language service units of the corporation are commonly known as CBC and Radio-Canada respectively, and both short-form names are also commonly used in the applicable language to refer to the corporation as a whole.
Although some local stations in Canada predate CBC's founding, CBC is the oldest existing broadcasting network in Canada, first established in its present form on November 2, 1936. [2] Radio services include CBC Radio One , CBC Radio 2 , Ici Radio-Canada Première , Ici Musique and the international radio service Radio Canada International . Television operations include CBC Television , Ici Radio-Canada Télé , CBC News Network , Ici RDI , Ici Explora , Documentary Channel (part ownership), and Ici ARTV . The CBC operates services for the Canadian Arctic under the names CBC North and Radio-Canada Nord. The CBC also operates digital services including CBC.ca / Ici.Radio-Canada.ca , CBC Radio 3 , CBC Music / ICI.mu and Ici.TOU.TV , and owns 20.2% of satellite radio broadcaster Sirius XM Canada , which carries several CBC-produced audio channels.
CBC/Radio-Canada offers programming in English, French and eight aboriginal languages on its domestic radio service, and in five languages on its web-based international radio service, Radio Canada International (RCI). [3] However, budget cuts in the early 2010s have contributed to the corporation reducing its service via the airwaves, discontinuing RCI's shortwave broadcasts as well as terrestrial television broadcasts in all communities served by network-owned rebroadcast transmitters , including communities not subject to Canada's over-the-air digital television transition .
The financial structure and the nature of the CBC differs from other national broadcasters, such as the British broadcaster BBC , as the CBC employs commercial advertising to supplement its federal funding on its television broadcasts. The radio service employed commercials from its inception to 1974. Since then, its primary radio networks, like the BBC, have been commercial-free. However, in the fall of 2013, CBC's secondary radio networks Radio 2 and Ici Musique introduced limited advertising of up to four minutes an hour.

History
In 1929, the Aird Commission on public broadcasting recommended the creation of a national radio broadcast network. A major concern was the growing influence of American radio broadcasting as U.S.-based networks began to expand into Canada. Meanwhile, Canadian National Railways was making a radio network to keep its passengers entertained and give it an advantage over its rival, CP. This, the CNR Radio, is the forerunner of the CBC. Graham Spry and Alan Plaunt lobbied intensely for the project on behalf of the Canadian Radio League . In 1932 the government of R.B. Bennett established the CBC's predecessor, the Canadian Radio Broadcasting Commission (CRBC).
The CRBC took over a network of radio stations formerly set up by a federal Crown corporation, the Canadian National Railway . The network was used to broadcast programming to riders aboard its passenger trains, with coverage primarily in central and eastern Canada. On November 2, 1936, the CRBC was reorganised under its present name. While the CRBC was a state-owned company, the CBC was a Crown corporation on the model of the BBC . Leonard Brockington was the CBC's first chairman.
For the next few decades, the CBC was responsible for all broadcasting innovation in Canada. This was in part because, until 1958, it was not only a broadcaster, but the chief regulator of Canadian broadcasting. It used this dual role to snap up most of the clear-channel licences in Canada. It began a separate French-language radio network in 1937. It introduced FM radio to Canada in 1946, though a distinct FM service wasn't launched until 1960.
Television broadcasts from the CBC began on September 6, 1952, with the opening of a station in Montreal , Quebec ( CBFT ), and a station in Toronto , Ontario ( CBLT ) opening two days later. The CBC's first privately owned affiliate television station, CKSO in Sudbury , Ontario, launched in October 1953. (At the time, all private stations were expected to affiliate with the CBC, a condition that relaxed in 1960–61 with the launch of CTV .)
From 1944 to 1962, the CBC split its English-language radio network into two services known as the Trans-Canada Network and the Dominion Network . The latter, carrying lighter programs including American radio shows, was dissolved in 1962, while the former became known as CBC Radio. (In the late 1990s, CBC Radio was rebranded as CBC Radio One and CBC Stereo as CBC Radio Two. The latter was re-branded slightly in 2007 as CBC Radio 2 .)
On July 1, 1958, CBC's television signal was extended from coast to coast. The first Canadian television show shot in colour was the CBC's own The Forest Rangers in 1963. Colour television broadcasts began on July 1, 1966, and full-colour service began in 1974. In 1978, CBC became the first broadcaster in the world to use an orbiting satellite for television service, linking Canada "from east to west to north".

Frontier Coverage Package
Starting in 1967 and continuing until the mid-1970s, the CBC provided limited television service to remote and northern communities. Transmitters were built in a few locations and carried a four-hour selection of black-and-white videotaped programs each day. The tapes were flown into communities to be shown, then transported to other communities, often by the "bicycle" method used in television syndication . Transportation delays ranged from one week for larger centres to almost a month for small communities.
The first FCP station was started in Yellowknife in 1967, the second in Whitehorse in 1968. Additional stations were added from 1969 to 1972. Most stations were fitted for the Anik satellite signal during 1973, carrying 12 hours of colour programming. Broadcasts were geared to either the Atlantic time zone (UTC−4 or −3) or the Pacific time zone (UTC−8 or −7) even though the audience resided in communities in time zones varying from UTC−5 to UTC−8.
Some of these stations used non-CBC callsigns such as CFWH-TV in Whitehorse, while some others used the standard CB_T callsign.
Television programs originating in the north without the help of the south began with one half-hour per week in the 1980s with Focus North and graduating to a daily half-hour newscast, Northbeat , in the late 1990s.

CBC Television slogans

Logos
The original logo of the CBC, designed by École des Beaux Arts student Hortense Binette [4] and used between 1940 and 1958, featured a map of Canada (and from 1940 to 1949, the Newfoundland ) and a thunderbolt design used to symbolise broadcasting.
In 1958, the CBC adopted a new logo for use at the end of network programs. Designed by scale model artist Jean-Paul Boileau, it consisted of the legends "CBC" and "Radio-Canada" overlaid on a map of Canada. For French programming, the "Radio-Canada" was placed on top.
The "Butterfly" logo was designed for the CBC by Hubert Tison in 1966 to mark the network's progressing transition from black-and-white to colour television, much in the manner of the NBC peacock logo . It was used at the beginning of programs broadcast in colour, and was used until all CBC television programs had switched to colour. A sketch on the CBC Television program Wayne & Shuster once referred to this as the logo of the "Cosmic Butterfly Corporation." [5]
The fourth logo, known internally as "the gem", was designed for the CBC by graphic artist Burton Kramer in December 1974, and it is the most widely recognised symbol of the corporation. The main on-air identification featured the logo kaleidoscopically morphing into its form while radiating outward from the centre of the screen on a blue background. This animated version, which went to air in December 1974, is also known colloquially as "The Exploding Pizza". The appearance of this logo marked the arrival of full-colour network television service. The large shape in the middle is the letter C, which stands for Canada, and the radiating parts of the C symbolise broadcasting. The original theme music for the 1974 CBC ident was a three-note woodwind orchestral fanfare accompanied by the voiceover "This is CBC" or "Ici Radio-Canada". [6] This was later replaced by the more familiar 11-note synthesised jingle, which was used until December 31, 1985. [7] [8] The logo is also referred to as the "CBC Pizza".
The updated one-colour version of the gem/pizza logo, created by Hubert Tison and Robert Innes, [4] was introduced on January 1, 1986, and with it was introduced a new series of computer graphic-generated television idents for CBC and Radio-Canada. These idents consisted of different background colours corresponding to the time of day behind a translucent CBC gem logo, accompanied by different arrangements of the CBC's new, orchestrated five-note jingle. The logo was changed to one colour, generally dark blue on white, or white on dark blue, in 1986. Print ads and most television promos, however, have always used a single-colour version of this logo since 1974.
In 1992, CBC updated its logo design to make it simpler and more red (or white on a red background). The new logo design, created by Swiss-Canadian design firm Gottschalk + Ash, [4] reduces the number of geometric sections in the logo to 13 instead of the previous logo's 25, and the "C" in the centre of the logo became a simple red circle. According to graphic designer Todd Falkowsky, the logo's red colour also represents Canada in a symbolic way. With the launch of the current design, new television idents were introduced in November that year, also using CGI. Since the early 2000s, it has also appeared in white (sometimes red) on a textured or coloured background. It is now CBC/Radio-Canada's longest-used logo, surpassing the original incarnation of the Gem logo and the CBC's 1940 logo.

Nicknames
As the oldest operating Canadian broadcaster, and the largest in terms of national availability of its various networks, the nickname "Mother Corp" and variants thereof are sometimes used in reference to the CBC. [9]
A popular satirical nickname for the CBC, commonly used in the pages of Frank , is "the Corpse."
There is an urban legend that a CBC announcer once referred to the network on the air as the "Canadian Broadcorping Castration", which also sometimes remains in use as a satirical nickname. Quotations of the supposed spoonerism are wildly variable in detail on what was said, when it was said or even who the announcer was, but there is no evidence to confirm the truth of the story. The only known recording of this phrase being spoken was created by American radio producer Kermit Schaefer for one of his best-selling Pardon My Blooper record albums in the 1950s, and is not in fact a real recording of a CBC broadcast.
The Conservative Party referred to it as the "Communist Broadcasting Corporation" for the supposed left-wing bias in its news coverage. Some have referred to the CBC as the "Corporate Broadcasting Corporation" for an alleged free market bias, though the CBC is largely publicly funded. [10]
The CBC has also been mistakenly referred to as the Canadian Broadcasting Company; [11] the CBC has been a crown corporation since its foundation.

Corporation

Mandate
The 1991 Broadcasting Act [12] states that...

Management
As a crown corporation , the CBC operates at arm's length (autonomously) from the government in its day-to-day business. The corporation is governed by the Broadcasting Act [12] of 1991, under a board of directors and is directly responsible to Parliament through the Department of Canadian Heritage . General management of the organization is in the hands of a president, who is appointed by the Governor General of Canada in Council , on the advice of the prime minister.
According to The Hill Times , a clause in Bill C-60 , an omnibus budget implementation bill introduced by the government of Stephen Harper in 2013, "appears to contradict a longstanding arm’s-length relationship between the independent CBC and any government in power." [13] [14] The clause allows the "prime minister’s cabinet to approve salaries, working conditions and collective bargaining positions for the CBC." [13]

Board of directors
In accordance with the Broadcasting Act , a board of directors is responsible for the management of the Canadian Broadcasting Corporation. The board is made up of 12 members, including the Chair and the President and CEO. A current list of directors is available from the Canadian Governor in Council here. [15]

Presidents

Ombudsmen
English
French

Financing
For the fiscal year 2006, the CBC received a total of $1.53 billion from all revenue sources, including government funding via taxpayers, subscription fees, advertising revenue, and other revenue (e.g., real estate). Expenditures for the year included $616 million for English television, $402 million for French television, $126 million for specialty channels, a total of $348 million for radio services in both languages, $88 million for management and technical costs, and $124 million for " amortization of property and equipment." Some of this spending was derived from amortization of funding from previous years. [18]
Among its revenue sources for the year ending March 31, 2006 , the CBC received $946 million in its annual funding from the federal government, as well as $60 million in "one-time" supplementary funding for programming. However, this supplementary funding has been repeated annually for a number of years. This combined total is just over a billion dollars annually and is a source of heated debate. To supplement this funding, the CBC's television networks and websites sell advertising, while cable/satellite-only services such as CBC News Network additionally collect subscriber fees, in line with their privately owned counterparts. CBC's radio services do not sell advertising except when required by law (for example, to political parties during federal elections).
CBC's funding differs from that of the public broadcasters of many European nations, which collect a licence fee , or those in the United States, such as PBS and NPR , which receive some public funding but rely to a large extent on voluntary contributions from individual viewers and listeners. A Nanos Research poll from August 2014 conducted for Asper Media (National Post, Financial Post) showed 41% of Canadians wanted funding increased, 46% wanted it maintained at current levels, and only 10% wanted to see it cut. [19]
The network's defenders note that the CBC's mandate differs from private media's, particularly in its focus on Canadian content; that much of the public funding actually goes to the radio networks; and that the CBC is responsible for the full cost of most of its prime-time programming, while private networks can fill up most of their prime-time schedules with American series acquired for a fraction of their production cost. CBC supporters also point out that additional, long-term funding is required to provide better Canadian dramas and improved local programming to attract and sustain a strong viewership.
According to the Canadian Media Guild , the $115-million deficit reduction action plan cuts to CBC which started with the 2012 budget and were fully realized in 2014, amounted to "one of the biggest layoffs of content creators and journalists in Canadian history."The 2014 cuts combined with earlier ones totaled "3,600 jobs lost at CBC since 2008. The CMG asked the federal government to reverse the cuts [20] and to repeal Clause 17 of omnibus budget bill C-60 "to remove government’s interference in CBC’s day-to-day operations." [20]
In September 2015, the Canadian Media Guild announced that the CBC planned to sell all of its properties across Canada to gain a temporary increase in available funds. Media relations manager Alexandra Fortier denied this and stated that the corporation planned to only sell half of its assets. [21]
In September 2015 Hubert Lacroix, president of CBC/Radio-Canada, spoke at the international public broadcasters’ conference in Munich, Germany. He claimed for the first time that public broadcasters were "at risk of extinction." [22] The Canadian Media Guild responded that Lacroix had "made a career of shredding" the CBC by cutting one quarter of its staff—approximately 2,000 jobs since 2010 under Lacroix's tenure. More than 600 jobs were cut in 2014 in order "to plug a $130-million budget shortfall." [22] Isabelle Montpetit, president of Syndicat des communications de Radio-Canada (SCRC), observed that Lacroix was hand-picked by Stephen Harper for the job as president of the CBC. [22] For the fiscal year 2015, the CBC received $1.036 billion from government funding and took 5% funding cuts from the previous year. [23]
In 2015, the Liberal Party was returned to power. As part of its election platform, it promised to restore the $115 million of funding to the CBC that was cut by the Harper Government, over three years, and add $35 million, for a total extra funding of $150 million. [24]
On November 28, 2016, the CBC issued a request for $400 million in additional funding, which it planned to use towards removing advertising from its television services, production and acquisition of Canadian content, and "additional funding of new investments to face consumer and technology disruption". The broadcaster argued that it had operated "[under] a business model and cultural policy framework that is profoundly broken", while other countries "[reaped] the benefits of strong, stable, well-funded public broadcasters." [25]

Services

News
CBC News is the largest broadcast newsgathering operation in Canada, providing services to CBC radio as well as CBC News Network , local supper-hour newscasts, CBC News Online, and Air Canada 's in-flight entertainment. New CBC News services are also proving popular such as news alerts to mobile phones and PDAs. Desktop news alerts, e-mail alerts, and digital television alerts are also available.

Radio
CBC Radio has five separate services, three in English, known as CBC Radio One , CBC Radio 2 and CBC Radio 3 , and two in French, known as Ici Radio-Canada Première and Ici Musique . CBC Radio One and Première focus on news and information programming, but they air some music programs, variety shows and comedy; in the past, they also aired some sports programming. CBC Radio One and Première used to broadcast primarily on the AM band , but many stations have moved over to FM . Over the years, a number of CBC radio transmitters with a majority of them on the AM band have either moved to FM or had shut down completely.
The CBC plans to phase out more CBC AM transmitters across Canada. [26] This goal however remains to be seen in light of the CBC budget cutbacks.

Long-range radio plan
The CBC's long-range radio plan (LRRP) was developed by the Canadian Radio-television and Telecommunications Commission (CRTC) in collaboration with the CBC to identify those FM frequencies that would likely be required to deliver the CBC's radio services to the maximum number of Canadians. The CBC is not subject to any conditions or expectations concerning its LRRP. The CBC noted that Première Chaîne (now Ici Radio-Canada Première) and CBC Radio One were available to about 99 percent of the Canadian population. The CBC stated that it plans to maintain its radio service but has no plans to grow the coverage area. It described the LRRP as a planning vehicle and indicated that it would no longer use it. Given reductions in public funding to the CBC and given that Première Chaîne and Radio One are available to the vast majority of Canadians, the commission considers that the CBC's plan to maintain current coverage and discontinue the LRRP is reasonable. Accordingly, the Commission accepts the CBC's proposal to discontinue the LRRP. [27]

Other CBC Radio services
CBC Radio 2 and Ici musique , found exclusively on FM, air arts and cultural programming, with a focus on music. CBC Radio 3, found only online and on satellite radio, airs exclusively independent Canadian music.
CBC Radio also operated two shortwave services. One, Radio Nord Québec , broadcast domestically to Northern Quebec on a static frequency of 9625 kHz , and the other, Radio Canada International , provided broadcasts to the United States and around the world in eight languages. Both shortwave services were shut down in 2012 due to budget cuts ; the Sackville transmitter site was dismantled in 2014. [28]
Additionally, the Radio One stations in St. John's and Vancouver operated shortwave relay transmitters, broadcasting at 6160 kHz. Some have suggested [29] that CBC/Radio-Canada create a new high-power shortwave digital radio service for more effective coverage of isolated areas.
In November 2004, the CBC, in partnership with Standard Broadcasting and Sirius Satellite Radio , applied to the Canadian Radio-television and Telecommunications Commission (CRTC) for a licence to introduce satellite radio service to Canada. The CRTC approved the subscription radio application, as well as two others for satellite radio service, on June 16, 2005. Sirius Canada launched on December 1, 2005, with a number of CBC Radio channels, including the new services CBC Radio 3 and Bande à part .
In some areas, especially national or provincial parks, the CBC also operates an AM or FM transmitter rebroadcasting weather alerts from the Meteorological Service of Canada 's Weatheradio Canada service.

Television
The CBC operates two national broadcast television networks; CBC Television in English, and Ici Radio-Canada Télé in French. Like private broadcasters, both those networks sell advertising, but offer more Canadian-produced programming. Most CBC television stations, including those in the major cities, are owned and operated by the CBC itself and carry a common schedule, aside from local programming.
Some stations that broadcast from smaller cities are private affiliates of the CBC, that is, stations which are owned by commercial broadcasters and air a predominantly CBC schedule. However, most affiliates of the English network opt out of some network programs to air local programming or more popular foreign programs acquired from other broadcasters. Private affiliates of the French network, all of which are located in Quebec , rarely have the means to provide alternate programming, and thus diverge from the main network schedule only for local newscasts. Such private affiliates are becoming increasingly rare, and there have been indications that the CBC plans to discontinue all affiliation agreements with non-CBC owned television stations in the 2010s.
CBC television stations in Nunavut , the Northwest Territories and Yukon tailor their programming mostly to the local native population, and broadcast in many native languages, such as Inuktitut , Gwich'in , and Dene .
One of the most popular shows is the weekly Saturday night broadcast of NHL hockey games. In English, the program is known as Hockey Night in Canada , and in French, it was called La Soirée du hockey . Both shows began in 1952. The French edition was discontinued in 2004, though Radio-Canada stations outside of Quebec simulcast some Saturday night games produced by RDS until 2006. The network suffered considerable public embarrassment when it lost the rights to the show's theme music following a protracted lawsuit launched by the song's composer and publishers. [30] In 2013, CBC lost the rights to telecast NHL games to Rogers Media -owned Sportsnet . Although CBC continues to broadcast the NHL as a licensed broadcaster until 2026 [ citation needed ] all editorial content is produced by Rogers under a time-brokerage agreement. [31]
Ratings for CBC Television have declined in recent years. In Quebec , where the majority speaks French, la Télévision de Radio-Canada is popular and garners some of the highest ratings in the province.
Both terrestrial networks have also begun to roll out high-definition television feeds, with selected National Hockey League and Canadian Football League games produced in HD for the English network. After the digital switchover, CBC chose to use the 720p format on CBC and Radio-Canada. [32]
The CBC also wholly owns and operates three specialty television channels – CBC News Network , an English-language news channel; Réseau de l'information (RDI), a French-language news channel; and Explora , a Category B digital service. It owns a managing interest in the Francophone arts service ARTV , and (82%) of the digital channel, documentary

Children's programming
Children's programming air under the commercial-free preschool programming block called Kids' CBC.

Online
The CBC has two main websites. One is in English, at CBC.ca , which was established in 1996; [33] the other is in French. [34] The website allows the CBC to produce sections which complement the various programs on television and radio. In 2012, the corporation launched CBC Music , a digital music service which produces and distributes 40 music-related webstreams, including the existing audio streams of CBC Radio 2 and CBC Radio 3. [35]
In 2012, the CBC announced its plans for a new local news service in Hamilton, Ontario . [36] With the Hamilton area already within the broadcast range of CBC Radio and CBC Television's services in Toronto , it was not financially or technically feasible for the public broadcaster to launch new conventional radio or television stations in Hamilton; accordingly, the corporation has developed a new model, with Hamilton as its test project, to launch a local digital service that would be accessible on the Internet and telecommunications devices such as tablets and smartphones . [36] The project launched in May 2012. [37]

Merchandising
Established in 2002, the CBC/Radio Canada merchandising business operates retail locations and cbcshop.ca, [38] its educational sales department CBC Learning [39] sells CBC content and media to educational institutions, CBC Merchandising also licenses brands such as Hockey Night in Canada (whose branding is still owned by the CBC) [40] and Coronation Street (as a Canadian licensee under arrangement from ITV Studios ).

Interactive television
CBC provides viewers with interactive on demand television programs every year through digital-cable services like Rogers Cable .

Commercial services
CBC Records is a Canadian record label which distributes CBC programming, including live concert performances and album transcripts of news and information programming such as the Massey Lectures , in album format. Music albums on the label, predominantly in the classical and jazz genres, are distributed across Canada in commercial record stores, while albums containing spoken word programming are predominantly distributed by the CBC's own retail merchandising operations.

Miscellaneous
CBC provides news, business, weather and sports information on Air Canada 's inflight entertainment as Enroute Journal.

Unions
Unions representing employees at CBC/Radio-Canada include: [41]

Labour issues
During the summer of 1981 there was a major disruption of CBC programming as the technicians union, the National Association of Broadcast Employees and Technicians, went on strike. Local newscasts were cut back to the bare minimum. This had the effect of delaying the debut of The Journal , which had to wait until January 1982.
On August 15, 2005, 5,500 employees of the CBC (about 90%) were locked out by CBC CEO Robert Rabinovitch in a dispute over future hiring practices. At issue were the rules governing the hiring of contract workers in preference to full-time hires. The locked-out employees were members of the Canadian Media Guild , representing all production, journalistic and on-air personnel outside Quebec and Moncton , including several foreign correspondents. While CBC services continued during the lockout, they were primarily made up of repeats, with news programming from the BBC and newswires. Major CBC programs such as The National and Royal Canadian Air Farce were not produced during the lockout; some non-CBC-owned programs seen on the network, such as The Red Green Show , shifted to other studios. Meanwhile, the locked-out employees produced podcasts and websites such as CBCunplugged.com.
After a hiatus, talks re-opened. On September 23, t Joe Fontana , the federal minister of labour, called Robert Rabinovitch and Arnold Amber (the president of the CBC branch of the Canadian Media Guild ) to his office for talks aimed at ending the dispute.
Late in the evening of October 2, 2005, it was announced that the CBC management and staff had reached a tentative deal which resulted in the CBC returning to normal operations on October 11. Some speculated that the looming October 8 start date for the network's most important television property, Hockey Night in Canada , had acted as an additional incentive to resolve the dispute.
The CBC has been affected by a number of other labour disputes since the late 1990s:
While all labour disputes resulted in cut-back programming and numerous repeat airings, the 2005 lockout may have been the most damaging to CBC. All local programming in the affected regions was cancelled and replaced by abbreviated national newscasts and national radio morning shows. BBC World (television) and World Service (radio) and Broadcast News feeds were used to provide the remainder of original news content, and the CBC website consisted mainly of rewritten wire copy. Some BBC staff protested against their material being used during the CBC lockout. "The NUJ and BECTU will not tolerate their members' work being used against colleagues in Canada", said a joint statement by BBC unions. The CMG questioned [59] whether, with its limited Canadian news content, the CBC was meeting its legal requirements under the Broadcasting Act and its CRTC licences.
Galaxie (which CBC owned at the time) supplied some music content for the radio networks. Tapes of aired or produced documentaries, interviews and entertainment programs were also aired widely. Selected television sports coverage, including that of the Canadian Football League , continued, but without commentary.
As before, French-language staff outside of Quebec were also affected by the 2005 lockout, although with Quebec producing the bulk of the French networks' programming, those networks were not as visibly affected by the dispute apart from local programs.

Cultural significance
In the 1950s the CBC provided hands-on training and employment for actors, writers, and directors in the developing field of its television dramatic services. Later many of these people moved to the United States to work in New York and Hollywood.
The CBC was the only television network broadcasting in Canada until the creation of ITO, a short-lived predecessor of today's CTV , in 1960; even then, large parts of Canada did not receive CTV service until the late 1960s or early 1970s. The CBC also had the only national radio network. Its cultural impact was therefore significant since many Canadians had little or no choice for their information and entertainment other than from these two powerful media outlets.
Even after the introduction of commercial television and radio, the CBC has remained one of the main elements in Canadian popular culture through its obligation to produce Canadian television and radio programming. The CBC has made programs for mass audiences and for smaller audiences interested in drama, performance arts, documentaries, current affairs, entertainment and sport.
The CBC's cultural influence, like that of many public broadcasters, has decreased in recent decades. This is partly due to severe budget cuts by the Canadian federal government, which began in the late 1980s and levelled off in the late 1990s. It is also due to industry-wide fragmentation of television audiences (the decline of network television generally, due to the rise in specialty channel viewership, as well as the increase of non-television entertainment options such as video games, the Internet, etc.) Private networks in Canada face the same competition, but their viewership is declining more slowly than CBC Television's.
In English-speaking Canada, the decline in CBC viewership can be partly attributed to popularity of private television networks' rebroadcast of American programming with substituted Canadian advertising. American programs appear to attract higher audiences than do much of the made-in-Canada programming that is a CBC specialty.
Viewership on the CBC's French television network has also declined, mostly because of stiff competition from private French-language networks. Audience fragmentation is another issue. However, in contrast to the anglophone audience, French Canadians prefer home-grown television programming, a vibrant Quebec star system is in place, and little American or foreign content airs on French-language networks, public or private. And the CBC's French-language radio channel is sometimes the top- rated network.
In the case of breaking news, including federal elections , CBC Television may obtain the largest number of viewers. For instance, after election night 2006 , CBC Television took out full-page newspaper ads claiming that 2.2 million Canadians watched their coverage, more than any other broadcaster. However, in similar ads, CTV also claimed to be number one, stating there was a CBC audience of only 1.2 million. In both cases, the methodologies were not clear from the ads, such as time periods and whether simulcasts on one or both of the networks' news channels (Newsworld for CBC, Newsnet for CTV) were counted.
Competition from private broadcasters like CTV, Global , and other broadcast television stations and specialty channels has lessened the CBC's reach, but nevertheless it remains a major influence on Canadian popular culture. According to the corporation's research, in 2011 92% of Canadians considered the CBC to be an essential service. [60]

International broadcasts

Newsworld International and Trio
From 1994 to 2000, the CBC, in a venture with Power Broadcasting (former owner of CKWS in Kingston ), jointly owned two networks:
In 2000, CBC and Power Broadcasting sold these channels to Barry Diller 's USA Networks . Diller's company was later acquired by Vivendi Universal , which in turn was partially acquired by NBC to form NBC Universal . NBC Universal still owns the Trio brand, which no longer has any association with the CBC (and became an Internet-only broadband channel which was later folded into Bravo .) The channel was shut down and was replaced with the NBC Universal channel "Sleuth", which later became " Cloo ".
However, the CBC continued to program NWI, with much of its programming simulcast on the domestic Newsworld service. In late 2004, as a result of a further change in NWI's ownership to the INdTV consortium (including Joel Hyatt and former Vice-President of the United States Al Gore ), NWI ceased airing CBC programming on August 1, 2005, when it became Current TV . Current later folded and became Al Jazeera America on August 20, 2013.

U.S. border audiences
In U.S. border communities such as Bellingham and Seattle , Washington; Buffalo , New York; Detroit , Michigan and Burlington , Vermont, CBC radio and television stations can be received over-the-air and have a significant audience. [61] Farther from the border, some American fans of the network have acquired Canadian IP addresses to stream its sports broadcasts. [62] Some CBC programming is also rebroadcast on local public radio, such as New Hampshire Public Radio , Vermont Public Radio and the Maine Public Broadcasting Network . CBC television channels are available on cable systems located near the Canada–US border. For example, CBET Windsor is available on cable systems in the Detroit, Michigan , and Toledo, Ohio , areas; much of the rest of the state of Michigan receives CBMT Montreal on cable. CBUT Vancouver is broadcast on Comcast in the Seattle , Washington , area. At night, the AM radio transmissions of both CBC and Radio-Canada services can be received over much of the northern portion of the United States, from stations such as CBW in Winnipeg , CBK in Saskatchewan and CJBC in Toronto .

Carriage of CBC News
On September 11, 2001, several American broadcasters without their own news operations, including C-SPAN , carried the CBC's coverage of the September 11 attacks in New York City and Washington, D.C. In the days after September 11, C-SPAN carried CBC's nightly newscast, The National , anchored by Peter Mansbridge . The quality of this coverage was recognised specifically by the Canadian Journalism Foundation ; editor-in-chief Tony Burman later accepted the Excellence in Journalism Award (2004), for "rigorous professional practice, accuracy, originality and public accountability", on behalf of the service.
C-SPAN has also carried CBC's coverage of major events affecting Canadians, including: Canadian federal elections , key proceedings in Canadian Parliament , Six days in September 2000 that marked the death and state funeral of Pierre Elliott Trudeau , the power outage crisis in summer 2003, U.S. presidential elections (e.g. in 2004 , C-SPAN picked up The National the day after the election for the view from Canadians), state visits and official visits of American presidents to Canada, and Barack Obama inauguration in 2009.
Several PBS stations also air some CBC programming. However, these programs are syndicated by independent distributors, and are not governed by the PBS "common carriage" policy.
Other American broadcast networks sometimes air CBC reports, especially for Canadian events of international significance. For example, in the early hours after the Swissair Flight 111 disaster, CNN aired CBC's live coverage of the event. Also in the late 1990s, CNN Headline News aired a few CBC reports of events that were not significant outside Canada.

CBC Radio
Some CBC Radio One programs, such as Definitely Not the Opera , WireTap , Q , and As It Happens , also air on some stations associated with American Public Media or Public Radio International . Some of the CBC's radio networks are available to SiriusXM subscribers in the United States, including CBC Radio One (a special feed that exclusively contains CBC-produced content and no regional programs) and Première (a simulcast of its Montreal flagship CBF-FM ), CBC Radio 3, and music-oriented services exclusive to SiriusXM.

Caribbean and Bermuda
Several Caribbean nations carry feeds of CBC TV:

Availability of CBC channels and programming
CBC Television, Ici Radio-Canada Télé, CBC News Network and all other CBC channels can be received through cable and satellite TV channel providers across Canada, like through Bell TV , Rogers Cable , Videotron , Cogeco , and other smaller TV providers. The CBC and Radio-Canada channel signals can also be obtained free of charge, over-the-air, through antenna receivers in Canada's largest markets or in some border states along the Canada-U.S. border; however, CBC is not obtainable as a " free-to-air " (FTA) channel on FTA satellites (signals are encrypted on the Anik space satellites and require a dedicated satellite receiver).

Controversies

Closed captioning
CBC Television was an early leader in broadcasting programming with closed captioning for the hearing impaired, airing its first captioned programming in 1981. [63] Captioned programming in Canada began with the airing of Clown White in English-language and French-language versions on CBC Television and Radio-Canada, respectively. Most sources list that event as occurring in 1981, [64] while others list the year as 1982. [65]
In 1997, Henry Vlug, a deaf lawyer in Vancouver, filed a complaint with the Canadian Human Rights Commission alleging that an absence of captioning on some programming on CBC Television and Newsworld infringed on his rights as a person with a disability. A ruling in 2000 by the Canadian Human Rights Tribunal, which later heard the case, sided with Vlug and found that an absence of captioning constituted discrimination on the basis of disability. [66] The Tribunal ordered CBC Television and Newsworld to caption the entirety of their broadcast days, "including television shows, commercials, promos and unscheduled news flashes, from sign-on until sign-off."
The ruling recognized that "there will inevitably be glitches with respect to the delivery of captioning" but that "the rule should be full captioning." In a negotiated settlement to avoid appealing the ruling to the Federal Court of Canada , CBC agreed to commence 100% captioning on CBC Television and Newsworld beginning November 1, 2002. [67] CBC Television and Newsworld are apparently the only broadcasters in the world required to caption the entire broadcast day. However, published evidence asserts that CBC is not providing the 100% captioning ordered by the Tribunal. [68]
In 2004, retired Canadian Senator Jean-Robert Gauthier , a hard-of-hearing person, filed a complaint with the Canadian Human Rights Commission against Radio-Canada concerning captioning, particularly the absence of real-time captioning on newscasts and other live programming. As part of the settlement process, Radio-Canada agreed to submit a report on the state of captioning, especially real-time captioning, on Radio-Canada and RDI . [69] The report, which was the subject of some criticism, proposed an arrangement with Cité Collégiale , a college in Ottawa, to train more French-language real-time captioners. [70] [71]
English-language specialty networks owned or co-owned by CBC, including documentary , have the lower captioning requirements typical of larger Canadian broadcasters (90% of the broadcast day by the end of both networks' licence terms [72] [73] ). ARTV , the French-language specialty network co-owned by CBC, has a maximum captioning requirement of 53%. [74]

Beyond the Red Wall
In November 2007, the CBC replaced its documentary Beyond the Red Wall: Persecution of Falun Gong , about persecution of Falun Gong members in China, at the last minute with a rerun episode regarding President Pervez Musharaf in Pakistan. The broadcaster had said to the press that "the crisis in Pakistan was considered more urgent and much more newsworthy", but sources from within the network itself had stated that the Chinese government had called the Canadian Embassy and demanded repeatedly that the program be taken off the air. The documentary in question was to air on Tuesday, November 6, 2007 on CBC Newsworld , but was replaced. [75] The documentary aired two weeks later on November 20, 2007, [76] after editing. [77]

Radio-Canada rebranding
On June 5, 2013, the CBC announced that it would be phasing out the Radio-Canada brand from its French-language broadcast properties, and unifying them under names prefixed with "Ici" ("here" or "this is"); for instance, the CBC planned to re-brand Télévision de Radio-Canada as "Ici Télé", Première Chaîne as "Ici Première", and move its French-language website from radio-canada.ca to ici.ca. Radio-Canada vice president Louis Lalande stated that the new name complemented its multi-platform operations, while also serving as an homage to the broadcaster's historic station identification slogan "ici Radio-Canada" ("this is Radio-Canada"). [78]
The announcement was criticized by politicians (such as Minister of Canadian Heritage James Moore ), who felt that the new "Ici" brand was too confusing, and that the CBC was diminishing the value of the Radio-Canada name through its plans to downplay it. The re-branding was also criticized for being unnecessary spending, reportedly costing $400,000, in the midst of budget cuts at the CBC. [79] On June 10, in response to the criticism, Hubert Lacroix apologized for the decision and announced that the new brands for its main radio and television networks would be revised to restore the Radio-Canada name alongside Ici, such as "Ici Radio-Canada Première". [80] [81]
The CBC also filed a trademark lawsuit against Sam Norouzi, founder of CFHD-DT , a new multicultural station in Montreal, seeking to have his own registration on the name "ICI" (as an abbreviation of "International Channel/Canal International") cancelled because it was too similar to its own Ici-related trademarks. Despite Norouzi's "ICI" trademark having been registered prior to the registration of CBC's own "Ici" trademarks, the corporation argued that Norouzi's application contained incorrect information surrounding his first use of the name in commerce, and also asserted the long-time use of "Ici Radio-Canada" as part of its imaging. Norouzi stated that he planned to fight the CBC in court. [82]

Employee harassment policy
In 2015, after allegations that CBC Radio host Jian Ghomeshi had harassed colleagues, Ghomeshi was placed on leave; his employment was terminated in October when the CBC indicated that they had “graphic evidence” that he had injured a female employee. [83] The corporation commissioned an independent investigation. The resulting report by Janice Rubin, a partner at law firm Rubin Thomlinson LLP, discussed employee complaints about Ghomeshi that were not seriously considered by the CBC. Rubin concluded that CBC management had "failed to take adequate steps" when it became aware of Ghomeshi's “problematic behaviour.” [84]
Ghomeshi was charged by police on multiple counts of sexual assault but was found not guilty of all but one of these in March 2016. He was to be tried in June on the last remaining charge, relating to a complainant who had also worked at CBC; her name was later revealed to be Kathryn Borel . On May 11, 2016 however, the Crown withdrew the charge after Ghomeshi signed a peace bond (which does not include an admission of guilt) and apologized to Borel. [85] Borel was critical of the CBC for its handling of her initial complaint about Ghomeshi's behavior. "When I went to the CBC for help, what I received in return was a directive that, yes, he could do this and, yes, it was my job to let him," she told the assembled media representatives. [86]
The CBC apologized to Borel publicly on May 11 in a statement by the head of public affairs Chuck Thompson. "What Ms. Borel experienced in our workplace should never have happened and we sincerely apologize...," he stated. [87] The Corporation has also maintained that it had accepted Rubin's report and had "since made significant progress" on a revised policy of improved training and methods for handling bullying and harassment complaints. [88]
In the May 11, 2016 Toronto Star article by Jacques Gallant cited above, public relations expert Martin Waxman spoke of a “damning indictment” of the CBC which included the following comment. “Yes, they did their inquiry, but if I were the CBC, I would think strongly about what is wrong with the culture and what they can do to repair it,” he said. The Star also quoted employment lawyer Howard Levitt stating that "harassment has not been fully addressed at the CBC" in his estimation. Levitt called the Rubin report a "whitewash" and reiterated his suggestion that a federal commission should conduct a more detailed enquiry into workplace issues at the public broadcaster.

Allegations of bias
Several outlets and politicians have accused CBC News of bias. [89] [90] [91] The CBC has denied these allegations. [92]

Over-the-air digital television transition
The CRTC ordered that in 28 "mandatory markets", full power over-the-air analogue television transmitters had to cease transmitting by August 31, 2011. Broadcasters could either continue serving those markets by transitioning analogue transmitters to digital or cease broadcasting over-the-air. Cable, IPTV, and satellite services are not involved or affected by this digital transition deadline.
While its fellow Canadian broadcasters converted most of their transmitters to digital by the Canadian digital television transition deadline of August 31, 2011, CBC converted only about half of the analogue transmitters in mandatory to digital (15 of 28 markets with CBC TV, and 14 of 28 markets with SRC). Due to financial difficulties reported by the corporation, the corporation published a plan whereby communities that receive analogue signals by re-broadcast transmitters in mandatory markets would lose their over-the-air (OTA) signals as of the deadline. Rebroadcast transmitters account for 23 of the 48 CBC and SRC transmitters in mandatory markets. Mandatory markets losing both CBC and SRC over-the-air signals include London, Ontario (metropolitan area population 457,000) and Saskatoon , Saskatchewan (metro area 257,000). In both of those markets, the corporation's television transmitters are the only ones that were not converted to digital.
On July 31, 2012, CBC shut down all of its approximately 620 analogue television transmitters, following an announcement of these plans on April 4, 2012. This reduced the total number of the corporation's television transmitters across the country to 27. According to the CBC, this would reduce the corporation's yearly costs by $10 million. No plans have been announced to use subchannels to maintain over-the-air signals for both CBC and SRC in markets where the corporation has one digital transmitter. In fact, in its CRTC application to shut down all of its analogue television transmitters, the CBC communicated its opposition to use of subchannels, citing, amongst other reasons, costs. [93] CBC/R-C claims that only 1.7 percent of Canadian viewers actually lost access to CBC and Radio-Canada programming due to the very high penetration of cable and satellite. In some areas (particularly remote and rural regions), cable or satellite have long been essential for acceptable television. [94]

Personalities
Notable CBC alumni have included television and radio personalities, former Governors General of Canada Jeanne Sauvé , Adrienne Clarkson , and Michaëlle Jean , as well as former Quebec premier René Lévesque .

See also

Notes and references

Further reading

Primary sources

In French

External links
WebPage index: 00029
Enciclopedia Libre Universal en Español
Enciclopedia Libre Universal en Español is a Spanish language wiki encyclopedia , released under the Creative Commons Attribution-ShareAlike License 3.0 . It uses the MediaWiki software. It started as a fork of the Spanish Wikipedia .

History
The Enciclopedia Libre was founded by contributors to the Spanish-language Wikipedia who decided to start an independent project. Led by Edgar Enyedy, they left Wikipedia on 26 February 2002, and created the new website, provided by the University of Seville for free, with the freely licensed articles of the Spanish-language Wikipedia. [2]
The reasons for the split are explained on Enciclopedia Libre. [3] Key issues included concerns about censorship and the possibility of advertising on Wikipedia. [4] From the interview with Edgar Enyedy, [5] the main reasons for splitting at that time were:

Post-split history
In 2011, Enyedy said that the sole reason for the failure of Enciclopedia Libre Universal en Español as a long term project was because it "was not intended to last. It was merely a form of pressure. Some of the goals were achieved, not all of them, but it was worth the cost." [6]
In 2011, Enyedy said "Nowadays, the romantic point of view is that EL survived and is still going strong." [6] He argued that while the viewpoint is positive, it is not factual. [6]

Statistics
While Enciclopedia Libre initially grew much more rapidly than the Spanish Wikipedia, the Spanish Wikipedia overtook it in 2004. Since then, the Spanish Wikipedia has grown at a much greater rate. As of January 2015, the Spanish Wikipedia hosts approximately twenty-three times as many articles as Enciclopedia Libre.

See also
WebPage index: 00030
MIT Technology Review
MIT Technology Review is a magazine published by the Massachusetts Institute of Technology . [3] It was founded in 1899 as The Technology Review , [4] and was re-launched without the "The" in its name on April 23, 1998 under then publisher R. Bruce Journey. In September 2005, it underwent another transition under the current editor-in-chief and publisher, Jason Pontin , to a form resembling the historical magazine.
Before the 1998 re-launch, the editor stated that "nothing will be left of the old magazine except the name." It is therefore necessary to distinguish between the modern and the historical Technology Review . [4] The historical magazine had been published by the MIT Alumni Association, was more closely aligned with the interests of MIT alumni, and had a more intellectual tone and much smaller public circulation. The magazine, billed from 1998 to 2005 as "MIT's Magazine of Innovation," and from 2005 onwards as simply "published by MIT", focused on new technology and how it is commercialized; was mass-marketed to the public; and was targeted at senior executives, researchers, financiers, and policymakers, as well as MIT alumni. [4]
In 2011, Technology Review received an Utne Reader Independent Press Award for Best Science/Technology Coverage. [5]

History

Original magazine: 1899–1998
Technology Review was founded in 1899 under the name "The Technology Review" and relaunched in 1998 without the "The" in its original name. It currently claims to be "the oldest technology magazine in the world." [6]
In 1899 The New York Times commented: [7]
The career path of James Rhyne Killian illustrates the close ties between Technology Review and the Institute. In 1926, Killian graduated from college and got his first job as assistant managing editor of Technology Review; he rose to editor-in-chief; became executive assistant to then-president Karl Taylor Compton in 1939; vice-president of MIT in 1945; and succeeded Compton as president in 1949.
The May 4, 1929 issue contained an article by Dr. Norbert Wiener , then Assistant Professor of Mathematics, describing some deficiencies in a paper Albert Einstein had published earlier that year. Wiener also commented on a cardinal's critique of the Einstein theory saying:
The historical Technology Review often published articles that were controversial, or critical of certain technologies. A 1980 issue contained an article by Jerome Wiesner attacking the Reagan administration's nuclear defense strategy. The cover of a 1983 issue stated "Even if the fusion program produces a reactor, no one will want it," and contained an article by Lawrence M. Lidsky , [8] associate director of MIT's Plasma Fusion Center , challenging the feasibility of fusion power (which at the time was often fancied to be just around the corner). The May 1984 issue contained an expose about microchip manufacturing hazards.
As late as 1967, the New York Times described Technology Review as a "scientific journal." Of its writing style, writer George V. Higgins complained:
In 1984, Technology Review printed an article about a Russian scientist using ova from frozen mammoths to create a mammoth-elephant hybrid called a "mammontelephas.". [10] Apart from being dated "April 1, 1984," there were no obvious giveaways in the story. The Chicago Tribune News Service picked it up as a real news item, and it was printed as fact in hundreds of newspapers.
The prank was presumably forgotten by 1994, when a survey of "opinion leaders" ranked Technology Review [4] No. 1 in the nation in the "most credible" category. [11]
Contributors to the magazine also included Thomas A. Edison , Winston Churchill , and Tim Berners-Lee . [12]

Relaunch: 1998–2005
A radical transition of the magazine occurred in 1996. At that time, according to the Boston Business Journal , [13] in 1996 Technology Review had lost $1.6 million over the previous seven years and was "facing the possibility of folding" due to "years of declining advertising revenue."
R. Bruce Journey was named publisher, the first full-time publisher in the magazine's history. According to previous publisher William J. Hecht, although Technology Review had "long been highly regarded for its editorial excellence," the purpose of appointing Journey was to enhance its "commercial potential" and "secure a prominent place for Technology Review in the competitive world of commercial publishing." [14] John Benditt replaced Steven J. Marcus as editor-in-chief, the entire editorial staff was fired, and the modern Technology Review was born.
Boston Globe columnist David Warsh [15] described the transition by saying that the magazine had been serving up "old 1960s views of things: humanist , populist , ruminative, suspicious of the unseen dimensions of new technologies" and had now been replaced with one that "takes innovation seriously and enthusiastically." Former editor Marcus characterized the magazine's new stance as "cheerleading for innovation."
Under Bruce Journey, Technology Review billed itself as "MIT's Magazine of Innovation." Since 2001, it has been published by Technology Review Inc., a nonprofit independent media company owned by MIT. [16]
Intending to appeal to business leaders, editor John Benditt said in 1999, "We're really about new technologies and how they get commercialized." Technology Review covers breakthroughs and current issues on fields such as biotechnology , nanotechnology , and computing . Articles are also devoted to more mature disciplines such as energy , telecommunications , transportation , and the military .
Since Journey, Technology Review has been distributed as a regular mass-market magazine and appears on newsstands. By 2003, circulation had more than tripled from 92,000 to 315,000, about half that of Scientific American , and included 220,000 paid subscribers and 95,000 sent free to MIT alumni. Additionally, in August 2003, a German edition of Technology Review was started in cooperation with the publishing house Heinz Heise (circulation of about 50,000 as of 2005). According to The New York Times , [17] as of 2004 the magazine was still "partly financed by M.I.T. (though it is expected to turn a profit eventually)."
Technology Review also functions as the MIT alumni magazine; the edition sent to alumni contains a separate section, "MIT News," containing items such as alumni class notes. This section is not included in the edition distributed to the general public.
The magazine is published by Technology Review, Inc, an independent media company owned by MIT. MIT's website lists it as a MIT publication, [18] and the MIT News Office states that "the magazine often uses MIT expertise for some of its content." In 1999 The Boston Globe noted that (apart from the alumni section) "few Technology Review articles actually concern events or research at MIT." [19] However, in the words of editor Jason Pontin:
From 1997 to 2005, R. Bruce Journey held the title of "publisher"; Journey was also the president and CEO of Technology Review, Inc. Editors-in-chief have included John Benditt (1997), Robert Buderi (2002), and Jason Pontin (2004).
The magazine has won numerous Folio! awards, presented at the annual magazine publishing trade show conducted by Folio! magazine. In 2001, these included a "Silver Folio: Editorial Excellence Award" in the consumer science and technology magazine category and many awards for typography and design . [21] In 2006, Technology Review was named a finalist in the "general excellence" category of the annual National Magazine Awards, sponsored by the American Society of Magazine Editors. [22]
On June 6, 2001, Fortune and CNET Networks launched a publication entitled FORTUNE/CNET Technology Review . [23] MIT sued [24] FORTUNE's parent corporation, Time, Inc. for infringement of the Technology Review trademark. [25] The case was quickly settled. In August the MIT student newspaper reported that lawyers for MIT and Time were reluctant to discuss the case, citing a confidentiality agreement that both sides described as very restrictive. Jason Kravitz, a Boston attorney who represented MIT in the case, suggested that the magazine’s change of name to Fortune/CNET Tech Review, a change that occurred in the middle of the case, may have been part of the settlement. [26]
Many publications covering specific technologies have used "technology review" as part of their names, such as Lawrence Livermore Labs 's Energy & Technology Review , [27] AACE 's Educational Technology Review , [28] and the International Atomic Energy Agency 's Nuclear Technology Review. [29]
In 2005, Technology Review, along with Wired News and other technology publications, was embarrassed by the publication of a number of stories by freelancer Michelle Delio containing information which could not be corroborated. Editor-in-chief Pontin said, "Of the ten stories which were published, only three were entirely accurate. In two of the stories, I'm fairly confident that Michelle Delio either did not speak to the person she said she spoke to, or misrepresented her interview with him." [30] The stories were retracted.

Modern magazine: 2005-present
On August 30, 2005, Technology Review announced that R. Bruce Journey, publisher from 1996 to 2005, would be replaced by the current Editor in Chief, Jason Pontin, and would reduce the print publication frequency from eleven to six issues per year while enhancing the publication's website. [30] The Boston Globe characterized the change as a "strategic overhaul." Editor and publisher Jason Pontin stated that he would "focus the print magazine on what print does best: present[ing] longer-format, investigative stories and colorful imagery." Technology Review's Web site, Pontin said, would henceforth publish original, daily news and analysis (whereas before it had merely republished the print magazine's stories). Finally, Pontin said that Technology Review's stories in print and online would identify and analyze emerging technologies. [31] This focus resembles that of the historical Technology Review.
Every year the magazine publishes a list of the 10 technologies it considers the most influential. [32] [33] [34] [35] [36] [37] [38] [39] [40] [41]

Annual Lists
Each year, MIT Technology Review publishes three annual lists:

TR35
MIT Technology Review has become well known for its annual TR35 list of the top 35 innovators in the world under the age of 35. In 1999, and then in 2002–2004, TR produced the TR100, a list of "100 remarkable innovators under the age of 35." In 2005, this list was renamed the TR35 and shortened to 35 individuals under the age of 35. Notable recipients of the award include Google co-founders Larry Page and Sergey Brin , PayPal co-founder Max Levchin , Geekcorps creator Ethan Zuckerman , Linux developer Linus Torvalds , BitTorrent developer Bram Cohen , MacArthur "genius" bioengineer Jim Collins , investor Micah Siegel and Netscape co-founder Marc Andreessen . [42] [43] The list was renamed Innovators Under 35 in 2013.

Recognition
In 2006, Technology Review was a finalist in the National Magazine Awards in the category of General Excellence. [44]
In 2010, Technology Review won the gold and silver prizes for best full issue of a technology magazine (for its November and June 2009 issues) and the gold, silver, and bronze prizes for best single article in a technology magazine (for “Natural Gas Changes the Energy Map” by David Rotman; [45] “Prescription: Networking” by David Talbot; [46] and “Chasing the Sun“ by David Rotman) [47] in the Folio Magazine Eddie Awards. [48]
In 2007, Technology Review won the bronze prizes in the Folio Magazine Eddie Awards in the categories of best issue of a technology magazine and best single technology article. [49] That same year, technologyreview.com won third place in the MPA Digital Awards for best business or news Website and second place for best online video or video series. [50]
In 2008, Technology Review won the gold prize for the best issue of a technology magazine (for its May 2008 issue); the gold, silver, and bronze prizes for best single articles in a technology magazine (for The Price of Biofuels by David Rotman; [51] Brain Trauma in Iraq by Emily Singer; [52] and Una Laptop por Niño by David Talbot); [53] the gold prize for best online community; and the bronze prize for best online tool in the Folio Magazine Eddie Awards. [54] That same year, Technology Review won third place in the Magazine Publishers of America (MPA) Digital Awards for best online videos. [55]
In 2009, Technology Review won the gold prize for Best Online News Coverage; the gold and silver prizes for best single articles in a technology magazine (for "How Obama Really Did It" by David Talbot) [56] and "Can Technology Save the Economy?" by David Rotman [57] and the silver prize for best online community in the Folio Magazine Eddie Awards. [58]
In 2011, Technology Review won the silver prize for best full issue of a technology magazine (for its January 2011 issue) and the gold and silver prizes for best single article in a technology magazine (for “Moore's Outlaws” by David Talbot [59] and "Radical Opacity" by Julian Dibbell) [60] in the Folio Magazine Eddie Awards. [61] That same year, Technology Review was recognized for the best science and technology coverage in the Utne Reader Independent Press Awards. [62]
In 2012, MIT Technology Review won the gold and silver prizes for best full issue of a technology magazine (for its June and October 2012 issues), and the gold and bronze prizes for best single article in a technology magazine (for "People Power 2.0" by John Pollock [63] and "The Library of Utopia" by Nicholas Carr) [64] in the Folio Magazine Eddie Awards. [65] That same year, MIT Technology Review won the gold prize for best feature design (for "The Library of Utopia" by Nicholas Carr) [64] in the Folio Magazine Ozzie Awards. [66]

See also
WebPage index: 00031
List of most popular websites
This is a list of the most popular websites worldwide according to the top 100 lists published by Alexa Internet , as of April 3, 2017, [update] and SimilarWeb , as of February 2017. [update]

Ranking measures
Alexa ranks websites based on a combined measure of page views and unique site users, and creates a list of most popular websites based on this ranking time-averaged over three-month periods. [1] Only the site's highest-level domain is recorded, aggregating any subdomains. [1]
SimilarWeb ranks websites based on a panel of millions of Internet users, International/U.S. internet service providers , direct measurement of web traffic from data from thousands of websites and web crawlers scanning public websites. [2]

List of websites
WebPage index: 00032
Unique user
According to IFABC Global Web Standards, a unique user ( UU ) is "An IP address plus a further identifier. The term "unique visitor" may be used instead of "unique user" but both terms have essentially the same meaning (see below). Sites may use User Agent , Cookie and/or Registration ID." Note that where users are allocated IP addresses dynamically (for example by dial-up Internet service providers ), this definition may overstate or understate the real number of individual users concerned. [1]
Unique users is a common way of measuring the popularity of a website and is often quoted to potential advertisers or investors. A website's unique users are usually measured over a standard period of time, typically a month. Use of performance indicators such as unique visitors/users is controversial, with Greg Harmon of Belden Research inferring that many companies reporting their online performance "may overstate" the number of unique visitors. Remember, it's just an identifier of a computer, not a person. And usually, the computer is identified by a "cookie" which is most often specific to an individual browser on that computer. Since an increasing percentage of people in the United States (at least) now have access to a computer at home and at work or school, one may have to divide the reported total of unique users in half. Then, another increasing fraction of people regularly delete cookies from their machines—presumably both at home and at work—and yet another large fraction use more than one browser on each of their machines. This means that for a typical news site, for example, which people might typically visit more than once a day to keep up with breaking news, the reported unique users might overstate the number of different people by a factor of four. On the plus side, for those wishing to impress advertisers or investors, the reported number of sessions or visits and pageviews are probably more accurate, so that smaller group of people visits much more often and looks at more pages than the raw numbers would suggest.

Understanding unique users numbers
Similar to the TURF ( Total Unduplicated Reach and Frequency ) metric often used in television, radio and newspaper analyses, Unique Users is a measure of the distribution of content to a number of distinct consumers.
A common mistake in using Unique User numbers is adding up Unique User numbers across dimensions. A Unique User metric is only valid for its given set of dimensions e.g. time, browsers. For example, a website may have 100 unique users on each day (day being the dimension) of a particular week. With only this data, one cannot extrapolate the number of weekly Unique Users (only that the Unique User count for the week is between 100 and 700). However, website administrators who can track unique user traffic over a longer period of time can build up a reliable view on their performance against direct competitors within the sector. Online businesses tend to have a static conversation rate ratio between unique users and new business clients.
When calculating movement of unique users through conversion funnel the same time period must be used at every step.

Limitations of unique user numbers
Unique user counts for websites are typically counted by using cookies. When a browser visits a website, the website checks for the existence of a particular cookie. If the cookie is present, the cookie value is captured. If the cookie is not present, the website will create a cookie.

Unique visitor
Unique visitors refers to the number of distinct individuals requesting pages from the website during a given period, regardless of how often they visit. Visits refers to the number of times a site is visited, no matter how many visitors make up those visits. When an individual goes to a website on Tuesday, then again on Wednesday, this is recorded as two visits from one visitor . [2]
The purpose of tracking unique visitors is to help marketers understand website user behavior.
Because a visitor can make multiple visits in a specified period, the number of visits may be greater than the number of visitors. A visitor is sometimes referred to as a unique visitor or a unique user to clearly convey the idea that each visitor is only counted once. [2]
The measurement of users or visitors requires a standard time period and can be distorted by automatic activity (such as bots ) that classify web content. Estimation of visitors, visits, and other traffic statistics are usually filtered to remove this type of activity by eliminating known IP addresses for bots, by requiring registration or cookies , or by using panel data. [2]

See also

Notes
WebPage index: 00033
Procrastination
Procrastination (from latin's "procrastinare", that translates in to : the prefix pro-, 'forward', and suffix -crastinus, 'till next day' from cras, 'tomorrow' ) is the avoidance of doing a task that needs to be accomplished. [1] It is the practice of doing more pleasurable things in place of less pleasurable ones, or carrying out less urgent tasks instead of more urgent ones, thus putting off impending tasks to a later time. Sometimes, procrastination takes place until the "last minute" before a deadline . Procrastination can take hold on any aspect of life—putting off cleaning the stove, repairing a leaky roof, seeing a doctor or dentist, submitting a job report or academic assignment or broaching a stressful issue with a partner. Procrastination can lead to feelings of guilt, inadequacy, depression and self-doubt.

Prevalence
In a study of academic procrastination from the University of Vermont, published in 1984, 46% of the subjects reported that they "always" or "nearly always" procrastinate writing papers, while approximately 30% reported procrastinating studying for exams and reading weekly assignments (27.6 l.percent by and 30.1 percent respectively). Nearly a quarter of the subjects reported that procrastination was a problem for them, regarding the same tasks. However, as many as 65% indicated that they would like to reduce their procrastination when writing papers and approximately 62 percent indicated the same for studying for exams and 55% for reading weekly assignments. [2]
A 1992 study showed that "52 [percent] of surveyed students indicated having a moderate to high need for help concerning procrastination." [3] It is estimated that 80–95 percent of college students engage in procrastination, and approximately 75 percent consider themselves procrastinators.
In a study performed on university students, procrastination was shown to be greater on tasks that were perceived as unpleasant or as impositions than on tasks for which the student believed he or she lacked the required skills for accomplishing the task. [4]

Behavioral criteria
Gregory Schraw, Theresa Wadkins, and Lori Olafson in 2007 proposed three criteria for a behavior to be classified as academic procrastination: it must be counterproductive, needless, and delaying. [5] Steel reviewed all previous attempts to define procrastination, and concluded in a 2007 study that procrastination is "to voluntarily delay an intended course of action despite expecting to be worse off for the delay." [6] Sabini & Silver argued that postponement and irrationality are the two key features of procrastination. Putting a task off is not procrastination, they argue, if there are rational reasons for doing so.
An approach that integrates several core theories of motivation as well as meta-analytic research on procrastination is the temporal motivation theory . It summarizes key predictors of procrastination (expectancy, value, and impulsiveness) into a mathematical equation. [6] Recently, a Universal Law of Procrastination was proposed. [7]

Psychological perspective
The pleasure principle may be responsible for procrastination; one may prefer to avoid negative emotions, and to delay stressful tasks. The belief that one works best under pressure provides an additional incentive to the postponement of tasks. [8] Some psychologists cite such behavior as a mechanism for coping with the anxiety associated with starting or completing any task or decision. [9] Piers Steel indicated in 2010 that anxiety is just as likely to get people to start working early as late, and that the focus of studies on procrastination should be impulsiveness . That is, anxiety will cause people to delay only if they are impulsive. [10]

Perfectionism
Traditionally, procrastination has been associated with perfectionism: a tendency to negatively evaluate outcomes and one's own performance, intense fear and avoidance of evaluation of one's abilities by others, heightened social self-consciousness and anxiety, recurrent low mood, and " workaholism ". However, adaptive perfectionists— egosyntonic perfectionism—were less likely to procrastinate than non-perfectionists, while maladaptive perfectionists, who saw their perfectionism as a problem— egodystonic perfectionism—had high levels of procrastination and anxiety. [11] In a regression analysis study of Steel, from 2007, it is found that mild to moderate level of perfectionists typically procrastinate slightly less than others, with "the exception being perfectionists who were also seeking clinical counseling". [6]

Coping responses
The Stoic philosopher Epictetus held that " men (people) are not moved by events but by their interpretations". [12] Negative coping responses of procrastinating individuals tend to be avoidant or emotional rather than task-oriented or focused on problem-solving. Emotional and avoidant coping is employed to reduce stress (and cognitive dissonance ) associated with putting off intended and important personal goals. This option provides immediate pleasure and is consequently very attractive to impulsive procrastinators at their first knowledge of achievable goals. [13] [14] There are several emotion-oriented strategies, similar to Freudian defense mechanisms , coping styles and self-handicapping .
Coping responses of procrastinators include the following. [ citation needed ]
Task- or problem-solving measures are taxing from a procrastinator's outlook. If such measures are pursued, it is less likely the procrastinator would remain a procrastinator. However, pursuing such measures requires actively changing one's behavior or situation to prevent and minimize the re-occurrence of procrastination.
In 2006, it was suggested that neuroticism has no direct links to procrastination and that any relationship is fully mediated by conscientiousness . [15] In 1982, it had been suggested that irrationality was an inherent feature of procrastination. "Putting things off even until the last moment isn't procrastination if there is a reason to believe that they will take only that moment". [16] Steel et al. explained in 2001, "actions must be postponed and this postponement must represent poor, inadequate, or inefficient planning". [17]

Health perspective
To a certain degree it is normal to procrastinate and it can be regarded as a useful way to identify what is important, due to a lower tendency of procrastination on truly valued tasks (for most people). [18] On the other hand, excessive procrastination can become a problem and impede normal functioning. When this happens, procrastination has been found to result in health problems, stress , [19] anxiety , sense of guilt and crisis as well as loss of personal productivity and social disapproval for not meeting responsibilities or commitments. Together these feelings may promote further procrastination and for some individuals procrastination gets almost chronic . Such procrastinators may have difficulties seeking support due to procrastination itself, but also social stigma and the belief that task-aversion is caused by laziness, lack of willpower or low ambition. In some cases problematic procrastination might be a sign of some underlying psychological disorder , but not necessarily. [6]
Research on the physiological roots of procrastination have been concerned with the role of the prefrontal cortex , [20] the area of the brain that is responsible for executive brain functions such as impulse control , attention and planning. Which is consistent with the notion that procrastination is strongly related to exactly these functions, or lack of them. The prefrontal cortex also acts as a filter, decreasing distracting stimuli, from other brain regions. Damage or low activation in this area of the brain can reduce an individual's ability to filter out distracting stimuli and result in poorer organization, a loss of attention, and increased procrastination. This is similar to the prefrontal lobe's role in attention-deficit hyperactivity disorder , where it is commonly underactivated. [21]
In a 2014 U.S. study surveying procrastination and impulsiveness in fraternal- and identical twin pairs, both traits were found to be "moderately heritable". The two traits were not separable at the genetic level (r genetic = 1.0), meaning no unique genetic influences of either trait alone was found. [22] The authors confirmed three constructs developed from the evolutionary hypothesis that procrastination arose as a by-product of impulsivity: "(a) Procrastination is heritable, (b) the two traits share considerable genetic variation, and (c) goal-management ability is an important component of this shared variation." [22]

Management
Psychologist William J. Knaus estimated that 90% of college students procrastinate. [23] Of these students, 25% are chronic procrastinators and they are usually the ones who end up dropping out of college.
Perfectionism is a prime cause for procrastination [24] because demanding perfection usually results in failure. Unrealistic expectations destroy self-esteem and lead to self-repudiation, self-contempt, and widespread unhappiness. To overcome procrastination, it is essential to recognize and accept the power of failure without condemning, [25] [ better source needed ] to stop focusing on faults and flaws and to set goals that are easier to achieve.
To overcome procrastination: [ citation needed ]
Making a plan to complete tasks in a rigid schedule format might not work for everyone. There is no hard-and-fast rule to follow such a process if it turns out to be counter-productive. Instead of scheduling, it may be better to execute tasks in a flexible, unstructured schedule which has time slots for only necessary activities. [26]
Piers Steel suggests [27] that better time management is a key to overcoming procrastination, including being aware of and using one's "power hours" (being a "morning person" or "night owl"). A good approach is to creatively tap one's internal circadian rhythms that are best suited for the most challenging and productive work. Steel says that it is essential to have realistic goals, to tackle one problem at a time and to cherish the "small successes". Ann McGee-Cooper says that "if we learn to balance excellence in work with excellence in play, fun, and relaxation, our lives become happier, healthier, and a great deal more creative." [28] [ better source needed ]
After contemplating his own procrastination habits, philosopher John Perry authored an essay entitled "Structured Procrastination", [29] wherein he proposes a "cheat" method as a safer approach for tackling procrastination: using a pyramid scheme to reinforce the unpleasant tasks needed to be completed in a quasi-prioritized order. In other words, the procrastinator should postpone tasks with a mental note that one feels to do while engaged in a work that requires their current attentional focus.

Negative impact
For some people, procrastination can be persistent and tremendously disruptive to everyday life. For these individuals, procrastination may be symptomatic of a psychological disorder. Procrastination has been linked to a number of negative associations, such as depression , irrational behaviour, low self-esteem , anxiety and neurological disorders such as ADHD . Others have found relationships with guilt [30] and stress. [19] Therefore, it is important for people whose procrastination has become chronic and is perceived to be debilitating to seek out a trained therapist or psychiatrist to see if an underlying mental health issue may be present. [ citation needed ] [31]
With a distant deadline, procrastinators report significantly less stress and physical illness than do non-procrastinators. However, as the deadline approaches, this relationship is reversed. Procrastinators report more stress, more symptoms of physical illness, and more medical visits, [19] to the extent that, overall, procrastinators suffer more stress and health problems.

Correlates
[ self-published source ]
As noted above, procrastination is consistently found to be strongly correlated with conscientiousness, and moderately so with impulsiveness.
Though the reasons for the relationship are not clear, there also exists a relationship between procrastination and eveningness ; that is, those who procrastinate more are more likely to go to sleep later and wake later. It is known that conscientiousness increases across the lifespan, as does morningness. [32] Procrastination too decreases with age. [6] [33] However, even controlling for age, there still exists a relationship between procrastination and eveningness, which is yet to be explained.
Testing the hypothesis that procrastinators have less of a focus on the future due to a greater focus on more immediate concerns, college undergraduates completed several self-report questionnaires, which did indeed find that procrastinators focus less on the future. Researchers had also expected to find that procrastination would be associated with a hedonistic and "devil-may-care" perspective on the present; against their expectations, they found that procrastination was better predicted by a fatalistic and hopeless attitude towards life. [34] This finding fits well with previous research relating procrastination and depression. [2]

Academic
According to an Educational Science Professor, Hatice Odaci, academic procrastination is a significant problem during college years in part because many college students lack efficient time management skills in using the Internet. Also, Odaci notes that most colleges provide free and fast twenty-four-hour Internet service which some students are not usually accustomed to, and as a result of irresponsible use or lack of firewalls these students become engulfed in a world of procrastination. [35]
" Student syndrome " refers to the phenomenon where a student will begin to fully apply himself or herself to a task only immediately before a deadline. This negates the usefulness of any buffers built into individual task duration estimates . Results from a 2002 study indicate that many students are aware of procrastination and accordingly set binding deadlines long before the date for which a task is due. These self-imposed binding deadlines are correlated with a better performance than without binding deadlines though performance is best for evenly spaced external binding deadlines. Finally, students have difficulties optimally setting self-imposed deadlines, with results suggesting a lack of spacing before the date at which results are due. [36] In one experiment, participation in online exercises was found to be five times higher in the final week before a deadline than in the summed total of the first three weeks for which the exercises were available. Procrastinators end up being the ones doing most of the work in the final week before a deadline. [17]
Other reasons cited on why students procrastinate include fear of failure and success, perfectionist expectations, as well as legitimate activities that may take precedence over school work, such as a job. [37]
Procrastinators have been found to receive worse grades than non-procrastinators. Tice et al. (1997) report that more than one-third of the variation in final exam scores could be attributed to procrastination. The negative association between procrastination and academic performance is recurring and consistent. Howell et al. (2006) found that, though scores on two widely used procrastination scales [2] [38] were not significantly associated with the grade received for an assignment, self-report measures of procrastination on the assessment itself were negatively associated with grade. [39]
In 2005 a study conducted by Angela Chu and Jin Nam Choi was published in the Journal of Social Psychology , in which they intended to understand task performance among procrastinators with the definition of procrastination as the absence of self-regulated performance, from the 1977 work of Ellis & Knaus. In their study they identified two types of procrastination: the traditional procrastination which they denote as passive, and active procrastination where the person finds enjoyment of a goal-oriented activity only under pressure. The study calls this active procrastination positive procrastination, as it is a functioning state in a self-handicapping environment. In addition, it was observed that active procrastinators have more realistic perceptions of time and perceive more control over their time than passive procrastinators, which is considered a major differentiator between the two types. But surprisingly, active and passive procrastinators showed similar levels of academic performance. The population of the study was college students and the majority of the sample size were women and Asian in origin. Comparisons with chronic - pathological procrastination traits were avoided. [40]
Different findings emerge when observed and self-report procrastination are compared. Steel et al. constructed their own scales based on Silver and Sabini’s "irrational" and "postponement" criteria. They also sought to measure this behavior objectively. [17] During a course, students could complete exam practice computer exercises at their own pace, and during the supervised class time could also complete chapter quizzes. A weighted average of the times at which each quiz was finished formed the measure of observed procrastination, whilst observed irrationality was quantified with the number of practice exercises that were left uncompleted. Researchers found that there was only a moderate correlation between observed and self-reported procrastination (r = 0.35). There was a very strong inverse relationship between the number of exercises completed and the measure of postponement (r = −0.78). Observed procrastination was very strongly negatively correlated with course grade (r = −0.87), as was self-reported procrastination (though less so, r = −0.36). As such, self-reported measures of procrastination, on which the majority of the literature is based, may not be the most appropriate measure to use in all cases. It was also found that procrastination itself may not have contributed significantly to poorer grades. Steel et al. noted that those students who completed all of the practice exercises "tended to perform well on the final exam no matter how much they delayed."
Procrastination is considerably more widespread in students than in the general population, with over 70 percent of students reporting procrastination for assignments at some point. [41] A 2014 panel study from Germany among several thousand university students found that increasing academic procrastination increases the frequency of seven different forms of academic misconduct, i.e., using fraudulent excuses, plagiarism, copying from someone else in exams, using forbidden means in exams, carrying forbidden means into exams, copying parts of homework from others, fabrication or falsification of data and the variety of academic misconduct. [42] This study argues that academic misconduct can be seen as a means to cope with the negative consequences of academic procrastination such as performance impairment.

See also
WebPage index: 00034
Vandalism on Wikipedia
On Wikipedia , vandalism is the act of editing the project in a malicious manner that is intentionally disruptive . Vandalism includes the addition, removal, or other modification of the text or other material that is either humorous, nonsensical, a hoax , or that is of an offensive, humiliating, or otherwise degrading nature.
Throughout its history, Wikipedia has struggled to maintain a balance between allowing the freedom of open editing and protecting the truth and accuracy of its information when false information can be potentially damaging to its subjects. [1] Vandalism is easy to commit on Wikipedia because anyone can edit the site, [2] [3] with the exception of articles that are currently semi-protected , which means that new and unregistered users cannot edit them.
Vandalism can be committed by either guest editors or those with registered accounts; however, a semi-protected or protected page can only be edited by autoconfirmed or confirmed Wikipedia editors, or administrators, respectively. [3] Frequent targets of vandalism include articles on hot and controversial topics, famous celebrities and current events. [4] [5] In some cases, people have been falsely reported as having died. This has notably occurred to United States Senators Ted Kennedy and Robert Byrd (both of whom are now deceased), and American rapper Kanye West (who is alive). [6]
The challenge from vandalism in Wikipedia was once characterized by the former Encyclopædia Britannica editor-in-chief Robert McHenry : [7] [8] "The user who visits Wikipedia ... is rather in the position of a visitor to a public restroom. It may be obviously dirty, so that he knows to exercise great care, or it may seem fairly clean, so that he may be lulled into a false sense of security. What he certainly does not know is who has used the facilities before him."

Fighting vandalism
There are various measures taken by Wikipedia to prevent or reduce the amount of vandalism. These include:
Editors are generally warned prior to being blocked. Wikipedia employs a 4-stage warning process up to a block. This includes: [13]
In 2005, Wikipedia started to require those who create new articles to have a registered account in an effort to fight vandalism. This occurred after inaccurate information was added to Wikipedia in which a journalist was accused of taking part in Kennedy's assassination. [2]
Wikipedia has experimented with systems in which edits to some articles, especially those of living people, are delayed until it can be reviewed and determined that they are not vandalism, and in some cases, that a source to verify accuracy is provided. This is in an effort to prevent inaccurate and potentially damaging information about living people from appearing on the site. [15] [16]

Notable acts of vandalism

Seigenthaler incident
In May 2005, a user edited the biographical article about John Seigenthaler, Sr. so that it contained several false and defamatory statements. [17] The inaccurate claims went unnoticed between May and September 2005, when they were discovered by Victor S. Johnson, Jr. , a friend of Seigenthaler. Wikipedia content is often mirrored at sites such as Answers.com , which means that incorrect information can be replicated alongside correct information through a number of websites. Such information can develop a misleading air of authority because of its presence at such sites: [18]

Stephen Colbert
Comedian Stephen Colbert made repeated references to Wikipedia on his TV show The Colbert Report , frequently suggesting on air that his viewers vandalize selected pages. These instances include the following:
When Wikipedia co-founder Jimmy Wales appeared as a guest on 24 May 2007 episode of The Colbert Report , they discussed Colbert-related vandalism. Wales later said on the show that he may have to lock down the entire Spanish-language Wikipedia for a few days after Colbert commented that perhaps it should learn English. [21]

Hillsborough disaster vandalism
In April 2014, the Liverpool Echo reported that computers on an intranet used by the United Kingdom government had been used to post offensive remarks about the Hillsborough disaster on Wikipedia pages relating to the subject. The government announced that it would launch an inquiry into the reports. [22] Following the allegations, The Daily Telegraph subsequently reported that government computers appeared to have been used to make rogue edits to a number of other articles, often adding insulting remarks to biographical articles, and in one case reporting the false death of an individual. [23]

Other notable acts of vandalism

See also
WebPage index: 00035
Freedom Forum
The Freedom Forum was founded in 1991 when the Gannett Foundation, started by publisher Frank E. Gannett as a charitable foundation to aid communities where his company had newspapers, sold its name and assets back to Gannett Company for $670 million. Retired Gannett chairman and USA Today newspaper founder Al Neuharth took the money and the shell of the foundation and formed the Freedom Forum. Its mission was to foster "free press, free speech and free spirit." [1] [2]
It runs the First Amendment Center and the Newseum Institute at Vanderbilt University in Nashville , Tennessee . It is also the creator and parent organization to the Newseum .
Neuharth's daughter, Jan A. Neuharth is acting chief executive officer and chair of the Freedom Forum. [3]
The financial losses of the Freedom Forum and Newseum have been controversial from the beginning, leading to criticism of high salaries [4] and some unusual proposals. [5]
WebPage index: 00036
Copyright
Copyright is a legal right created by the law of a country that grants the creator of an original work exclusive rights for its use and distribution. This is usually only for a limited time. The exclusive rights are not absolute but limited by limitations and exceptions to copyright law, including fair use. A major limitation on copyright is that copyright protects only the original expression of ideas, and not the underlying ideas themselves. [1] [2]
Copyright is a form of intellectual property , applicable to certain forms of creative work. Some, but not all jurisdictions require "fixing" copyrighted works in a tangible form. It is often shared among multiple authors, each of whom holds a set of rights to use or license the work, and who are commonly referred to as rights holders. [3] [4] [5] [6] These rights frequently include reproduction, control over derivative works , distribution, public performance , and " moral rights " such as attribution. [7]
Copyrights are considered territorial rights , which means that they do not extend beyond the territory of a specific jurisdiction. While many aspects of national copyright laws have been standardized through international copyright agreements , copyright laws vary by country. [8]
Typically, the duration of a copyright spans the author's life plus 50 to 100 years (that is, copyright typically expires 50 to 100 years after the author dies, depending on the jurisdiction). Some countries require certain copyright formalities to establishing copyright, but most recognize copyright in any completed work, without formal registration. Generally, copyright is enforced as a civil matter, though some jurisdictions do apply criminal sanctions.
Most jurisdictions recognize copyright limitations, allowing "fair" exceptions to the creator's exclusivity of copyright and giving users certain rights. The development of digital media and computer network technologies have prompted reinterpretation of these exceptions, introduced new difficulties in enforcing copyright, and inspired additional challenges to copyright law's philosophic basis. Simultaneously, businesses with great economic dependence upon copyright, such as those in the music business, have advocated the extension and expansion of copyright and sought additional legal and technological enforcement.

History

Background
Copyright came about with the invention of the printing press and with wider literacy. As a legal concept, its origins in Britain were from a reaction to printers' monopolies at the beginning of the 18th century. The English Parliament was concerned about the unregulated copying of books and passed the Licensing of the Press Act 1662 , [9] which established a register of licensed books and required a copy to be deposited with the Stationers' Company , essentially continuing the licensing of material that had long been in effect.
Copyright laws allow products of creative human activities, such as literary and artistic production, to be preferentially exploited and thus incentivized. Different cultural attitudes, social organizations, economic models and legal frameworks are seen to account for why copyright emerged in Europe and not, for example, in Asia. In the Middle Ages in Europe, there was generally a lack of any concept of literary property due to the general relations of production, the specific organization of literary production and the role of culture in society. The latter refers to the tendency of oral societies, such as that of Europe in the medieval period, to view knowledge as the product and expression of the collective, rather than to see it as individual property. However, with copyright laws, intellectual production comes to be seen as a product of an individual, with attendant rights. The most significant point is that patent and copyright laws support the expansion of the range of creative human activities that can be commodified. This parallels the ways in which capitalism led to the commodification of many aspects of social life that earlier had no monetary or economic value per se. [10]
Copyright has grown from a legal concept regulating copying rights in the publishing of books and maps to one with a significant effect on nearly every modern industry, covering such items as sound recordings , films, photographs, software, and architectural works.

National copyrights
Often seen as the first real copyright law, the 1709 British Statute of Anne gave the publishers rights for a fixed period, after which the copyright expired. [11] The act also alluded to individual rights of the artist. It began, "Whereas Printers, Booksellers, and other Persons, have of late frequently taken the Liberty of Printing ... Books, and other Writings, without the Consent of the Authors ... to their very great Detriment, and too often to the Ruin of them and their Families:". [12] A right to benefit financially from the work is articulated, and court rulings and legislation have recognized a right to control the work, such as ensuring that the integrity of it is preserved. An irrevocable right to be recognized as the work's creator appears in some countries' copyright laws.
The Copyright Clause of the United States Constitution (1787) authorized copyright legislation: "To promote the Progress of Science and useful Arts, by securing for limited Times to Authors and Inventors the exclusive Right to their respective Writings and Discoveries." That is, by guaranteeing them a period of time in which they alone could profit from their works, they would be enabled and encouraged to invest the time required to create them, and this would be good for society as a whole. A right to profit from the work has been the philosophical underpinning for much legislation extending the duration of copyright, to the life of the creator and beyond, to their heirs.
The original length of copyright in the United States was 14 years, and it had to be explicitly applied for. If the author wished, they could apply for a second 14‑year monopoly grant, but after that the work entered the public domain , so it could be used and built upon by others.
Copyright law was enacted rather late in German states , and the historian Eckhard Höffner argues that the absence of copyright laws in the early 19th century encouraged publishing, was profitable for authors, led to a proliferation of books, enhanced knowledge, and was ultimately an important factor in the ascendency of Germany as a power during that century. [13]

International copyright treaties
The 1886 Berne Convention first established recognition of copyrights among sovereign nations , rather than merely bilaterally. Under the Berne Convention, copyrights for creative works do not have to be asserted or declared, as they are automatically in force at creation: an author need not "register" or "apply for" a copyright in countries adhering to the Berne Convention. [14] As soon as a work is "fixed", that is, written or recorded on some physical medium, its author is automatically entitled to all copyrights in the work, and to any derivative works unless and until the author explicitly disclaims them, or until the copyright expires. The Berne Convention also resulted in foreign authors being treated equivalently to domestic authors, in any country signed onto the Convention. The UK signed the Berne Convention in 1887 but did not implement large parts of it until 100 years later with the passage of the Copyright, Designs and Patents Act of 1988 . Specially, for educational and scientific research purposes, the Berne Convention provides the developing countries issue compulsory licenses for the translation or reproduction of copyrighted works within the limits prescribed by the Convention. This was a special provision that had been added at the time of 1971 revision of the Convention, because of the strong demands of the developing countries.The United States did not sign the Berne Convention until 1989. [15]
The United States and most Latin American countries instead entered into the Buenos Aires Convention in 1910, which required a copyright notice on the work (such as all rights reserved ), and permitted signatory nations to limit the duration of copyrights to shorter and renewable terms. [16] [17] [18] The Universal Copyright Convention was drafted in 1952 as another less demanding alternative to the Berne Convention, and ratified by nations such as the Soviet Union and developing nations.
The regulations of the Berne Convention are incorporated into the World Trade Organization 's TRIPS agreement (1995), thus giving the Berne Convention effectively near-global application. [19]
In 1961, the United International Bureaux for the Protection of Intellectual Property signed the Rome Convention for the Protection of Performers, Producers of Phonograms and Broadcasting Organizations . In 1996, this organization was succeeded by the founding of the World Intellectual Property Organization , which launched the 1996 WIPO Performances and Phonograms Treaty and the 2002 WIPO Copyright Treaty , which enacted greater restrictions on the use of technology to copy works in the nations that ratified it. The Trans-Pacific Partnership includes intellectual Property Provisions relating to copyright.
Copyright laws are standardized somewhat through these international conventions such as the Berne Convention and Universal Copyright Convention . These multilateral treaties have been ratified by nearly all countries, and international organizations such as the European Union or World Trade Organization require their member states to comply with them.

Obtaining protection

Ownership
The original holder of the copyright may be the employer of the author rather than the author himself if the work is a " work for hire ". [20] For example, in English law the Copyright, Designs and Patents Act 1988 provides that if a copyrighted work is made by an employee in the course of that employment, the copyright is automatically owned by the employer which would be a "Work for Hire". Typically, the first owner of a copyright is the person who created the work i.e. the author . [21] [22] But when more than one person creates the work, then a case of joint authorship can be made provided some criteria are met.

Eligible works
Copyright may apply to a wide range of creative, intellectual, or artistic forms, or "works". Specifics vary by jurisdiction , but these can include poems , theses , fictional characters plays and other literary works , motion pictures , choreography , musical compositions , sound recordings , paintings , drawings , sculptures , photographs , computer software , radio and television broadcasts , and industrial designs . Graphic designs and industrial designs may have separate or overlapping laws applied to them in some jurisdictions. [23] [24]
Copyright does not cover ideas and information themselves, only the form or manner in which they are expressed. [25] For example, the copyright to a Mickey Mouse cartoon restricts others from making copies of the cartoon or creating derivative works based on Disney's particular anthropomorphic mouse, but does not prohibit the creation of other works about anthropomorphic mice in general, so long as they are different enough to not be judged copies of Disney's. [25] Note additionally that Mickey Mouse is not copyrighted because characters cannot be copyrighted; rather, Steamboat Willie is copyrighted and Mickey Mouse, as a character in that copyrighted work, is afforded protection.

Originality
Typically, a work must meet minimal standards of originality in order to qualify for copyright, and the copyright expires after a set period of time (some jurisdictions may allow this to be extended). Different countries impose different tests, although generally the requirements are low; in the United Kingdom there has to be some "skill, labour, and judgment" that has gone into it. [26] In Australia and the United Kingdom it has been held that a single word is insufficient to comprise a copyright work. However, single words or a short string of words can sometimes be registered as a trademark instead.
Copyright law recognizes the right of an author based on whether the work actually is an original creation, rather than based on whether it is unique; two authors may own copyright on two substantially identical works, if it is determined that the duplication was coincidental, and neither was copied from the other.

Registration
In all countries where the Berne Convention standards apply, copyright is automatic, and need not be obtained through official registration with any government office. Once an idea has been reduced to tangible form, for example by securing it in a fixed medium (such as a drawing, sheet music, photograph, a videotape, or a computer file), the copyright holder is entitled to enforce his or her exclusive rights. [14] However, while registration isn't needed to exercise copyright, in jurisdictions where the laws provide for registration, it serves as prima facie evidence of a valid copyright and enables the copyright holder to seek statutory damages and attorney's fees. [27] (In the USA, registering after an infringement only enables one to receive actual damages and lost profits.)
A widely circulated strategy to avoid the cost of copyright registration is referred to as the poor man's copyright . It proposes that the creator send the work to himself in a sealed envelope by registered mail, using the postmark to establish the date. This technique has not been recognized in any published opinions of the United States courts. The United States Copyright Office says the technique is not a substitute for actual registration. [28] The United Kingdom Intellectual Property Office discusses the technique and notes that the technique (as well as commercial registries) does not constitute dispositive proof that the work is original nor who the creator of the work is. [29] [30]

Fixing
The Berne Convention allows member countries to decide whether creative works must be "fixed" to enjoy copyright. Article 2, Section 2 of the Berne Convention states: "It shall be a matter for legislation in the countries of the Union to prescribe that works in general or any specified categories of works shall not be protected unless they have been fixed in some material form." Some countries do not require that a work be produced in a particular form to obtain copyright protection. For instance, Spain, France, and Australia do not require fixation for copyright protection. The United States and Canada, on the other hand, require that most works must be "fixed in a tangible medium of expression" to obtain copyright protection. [31] U.S. law requires that the fixation be stable and permanent enough to be "perceived, reproduced or communicated for a period of more than transitory duration." Similarly, Canadian courts consider fixation to require that the work be "expressed to some extent at least in some material form, capable of identification and having a more or less permanent endurance." [31]

Copyright notice
Before 1989, United States law required the use of a copyright notice, consisting of the copyright symbol (©, the letter C inside a circle), the abbreviation "Copr.", or the word "Copyright", followed by the year of the first publication of the work and the name of the copyright holder. [32] [33] Several years may be noted if the work has gone through substantial revisions. The proper copyright notice for sound recordings of musical or other audio works is a sound recording copyright symbol (℗, the letter P inside a circle), which indicates a sound recording copyright, with the letter P indicating a "phonorecord". In addition, the phrase All rights reserved was once required to assert copyright, but that phrase is now legally obsolete. Almost everything on the Internet has some sort of copyright attached to it. Whether these things are watermarked, signed, or have any other sort of indication of the copyright is a different story however. [34]
In 1989 the United States enacted the Berne Convention Implementation Act, amending the 1976 Copyright Act to conform to most of the provisions of the Berne Convention. As a result, the use of copyright notices has become optional to claim copyright, because the Berne Convention makes copyright automatic. [35] However, the lack of notice of copyright using these marks may have consequences in terms of reduced damages in an infringement lawsuit – using notices of this form may reduce the likelihood of a defense of "innocent infringement" being successful. [36]

Enforcement
Copyrights are generally enforced by the holder in a civil law court, but there are also criminal infringement statutes in some jurisdictions. While central registries are kept in some countries which aid in proving claims of ownership, registering does not necessarily prove ownership, nor does the fact of copying (even without permission) necessarily prove that copyright was infringed. Criminal sanctions are generally aimed at serious counterfeiting activity, but are now becoming more commonplace as copyright collectives such as the RIAA are increasingly targeting the file sharing home Internet user. Thus far, however, most such cases against file sharers have been settled out of court. (See: Legal aspects of file sharing )
In most jurisdictions the copyright holder must bear the cost of enforcing copyright. This will usually involve engaging legal representation, administrative or court costs. In light of this, many copyright disputes are settled by a direct approach to the infringing party in order to settle the dispute out of court.
"...by 1978, the scope was expanded to apply to any 'expression' that has been 'fixed' in any medium, this protection granted automatically whether the maker wants it or not, no registration required." [37]

Copyright infringement
For a work to be considered to infringe upon copyright, its use must have occurred in a nation that has domestic copyright laws or adheres to a bilateral treaty or established international convention such as the Berne Convention or WIPO Copyright Treaty . Improper use of materials outside of legislation is deemed "unauthorized edition", not copyright infringement. [38]
Copyright infringement most often occurs to software, film and music. However, infringement upon books and other text works remains common, especially for educational reasons. Statistics regarding the effects of copyright infringement are difficult to determine. Studies have attempted to determine whether there is a monetary loss for industries affected by copyright infringement by predicting what portion of pirated works would have been formally purchased if they had not been freely available. [39] Other reports indicate that copyright infringement does not have an adverse effect on the entertainment industry, and can have a positive effect. [40] In particular, a 2014 university study concluded that free music content, accessed on YouTube, does not necessarily hurt sales, instead has the potential to increase sales. [41]

Rights granted

Exclusive rights
Several exclusive rights typically attach to the holder of a copyright:
The phrase "exclusive right" means that only the copyright holder is free to exercise those rights, and others are prohibited from using the work without the holder's permission. Copyright is sometimes called a "negative right", as it serves to prohibit certain people (e.g., readers, viewers, or listeners, and primarily publishers and would be publishers) from doing something they would otherwise be able to do, rather than permitting people (e.g., authors) to do something they would otherwise be unable to do. In this way it is similar to the unregistered design right in English law and European law . The rights of the copyright holder also permit him/her to not use or exploit their copyright, for some or all of the term. There is, however, a critique which rejects this assertion as being based on a philosophical interpretation of copyright law that is not universally shared. There is also debate on whether copyright should be considered a property right or a moral right . [43]
If a pictorial, graphic or sculptural work is a useful article, it is copyrighted only if its aesthetic features are separable from its utilitarian features. A useful article is an article having an intrinsic utilitarian function that is not merely to portray the appearance of the article or to convey information. They must be separable from the functional aspect to be copyrighted. [44]

Duration
Copyright subsists for a variety of lengths in different jurisdictions. The length of the term can depend on several factors, including the type of work (e.g. musical composition, novel), whether the work has been published , and whether the work was created by an individual or a corporation. In most of the world, the default length of copyright is the life of the author plus either 50 or 70 years. In the United States, the term for most existing works is a fixed number of years after the date of creation or publication. Under most countries' laws (for example, the United States [45] and the United Kingdom [46] ), copyrights expire at the end of the calendar year in question.
The length and requirements for copyright duration are subject to change by legislation, and since the early 20th century there have been a number of adjustments made in various countries, which can make determining the duration of a given copyright somewhat difficult. For example, the United States used to require copyrights to be renewed after 28 years to stay in force, and formerly required a copyright notice upon first publication to gain coverage. In Italy and France, there were post-wartime extensions that could increase the term by approximately 6 years in Italy and up to about 14 in France. Many countries have extended the length of their copyright terms (sometimes retroactively). International treaties establish minimum terms for copyrights, but individual countries may enforce longer terms than those. [47]
In the United States, all books and other works published before 1923 have expired copyrights and are in the public domain. [48] In addition, works published before 1964 that did not have their copyrights renewed 28 years after first publication year also are in the public domain. Hirtle points out that the great majority of these works (including 93% of the books) were not renewed after 28 years and are in the public domain. [49] Books originally published outside the US by non-Americans are exempt from this renewal requirement, if they are still under copyright in their home country.
But if the intended exploitation of the work includes publication (or distribution of derivative work, such as a film based on a book protected by copyright) outside the U.S., the terms of copyright around the world must be considered. If the author has been dead more than 70 years, the work is in the public domain in most, but not all, countries.
In 1998, the length of a copyright in the United States was increased by 20 years under the Copyright Term Extension Act . This legislation was strongly promoted by corporations which had valuable copyrights which otherwise would have expired, and has been the subject of substantial criticism on this point. [50]

Limitations and exceptions
In many jurisdictions, copyright law makes exceptions to these restrictions when the work is copied for the purpose of commentary or other related uses. It should be noted that US copyright does NOT cover names, title, short phrases or Listings (such as ingredients, recipes, labels, or formulas). [51] However, there are protections available for those areas copyright does not cover – such as trademarks and patents .
There are some exceptions to what copyright will protect. Copyright will not protect:

Idea–expression dichotomy and the merger doctrine
The idea–expression divide differentiates between ideas and expression, and states that copyright protects only the original expression of ideas, and not the ideas themselves. This principle, first clarified in the 1879 case of Baker v. Selden , has since been codified by the Copyright Act of 1976 at 17 U.S.C. § 102(b).

The first-sale doctrine and exhaustion of rights
Copyright law does not restrict the owner of a copy from reselling legitimately obtained copies of copyrighted works, provided that those copies were originally produced by or with the permission of the copyright holder. It is therefore legal, for example, to resell a copyrighted book or CD . In the United States this is known as the first-sale doctrine , and was established by the courts to clarify the legality of reselling books in second-hand bookstores .
Some countries may have parallel importation restrictions that allow the copyright holder to control the aftermarket . This may mean for example that a copy of a book that does not infringe copyright in the country where it was printed does infringe copyright in a country into which it is imported for retailing. The first-sale doctrine is known as exhaustion of rights in other countries and is a principle which also applies, though somewhat differently, to patent and trademark rights. It is important to note that the first-sale doctrine permits the transfer of the particular legitimate copy involved. It does not permit making or distributing additional copies.
In Kirtsaeng v. John Wiley & Sons, Inc. , [52] in 2013, the United States Supreme Court held in a 6-3 decision that the first-sale doctrine applies to goods manufactured abroad with the copyright owner's permission and then imported into the US without such permission. The case involved a plaintiff who imported Asian editions of textbooks that had been manufactured abroad with the publisher-plaintiff's permission. The defendant, without permission from the publisher, imported the textbooks and resold on eBay. The Supreme Court's holding severely limits the ability of copyright holders to prevent such importation.
In addition, copyright, in most cases, does not prohibit one from acts such as modifying, defacing, or destroying his or her own legitimately obtained copy of a copyrighted work, so long as duplication is not involved. However, in countries that implement moral rights , a copyright holder can in some cases successfully prevent the mutilation or destruction of a work that is publicly visible.

Fair use and fair dealing
Copyright does not prohibit all copying or replication. In the United States, the fair use doctrine , codified by the Copyright Act of 1976 as 17 U.S.C. Section 107, permits some copying and distribution without permission of the copyright holder or payment to same. The statute does not clearly define fair use, but instead gives four non-exclusive factors to consider in a fair use analysis. Those factors are:
In the United Kingdom and many other Commonwealth countries, a similar notion of fair dealing was established by the courts or through legislation . The concept is sometimes not well defined; however in Canada , private copying for personal use has been expressly permitted by statute since 1999. In Alberta (Education) v. Canadian Copyright Licensing Agency (Access Copyright) , 2012 SCC 37, the Supreme Court of Canada concluded that limited copying for educational purposes could also be justified under the fair dealing exemption. In Australia, the fair dealing exceptions under the Copyright Act 1968 (Cth) are a limited set of circumstances under which copyrighted material can be legally copied or adapted without the copyright holder's consent. Fair dealing uses are research and study; review and critique; news reportage and the giving of professional advice (i.e. legal advice ). Under current Australian law , although it is still a breach of copyright to copy, reproduce or adapt copyright material for personal or private use without permission from the copyright owner, owners of a legitimate copy are permitted to “format shift” that work from one medium to another for personal, private use, or to “time shift” a broadcast work for later, once and only once, viewing or listening. Other technical exemptions from infringement may also apply, such as the temporary reproduction of a work in machine readable form for a computer.
In the United States the AHRA ( Audio Home Recording Act Codified in Section 10, 1992) prohibits action against consumers making noncommercial recordings of music, in return for royalties on both media and devices plus mandatory copy-control mechanisms on recorders.
Later acts amended US Copyright law so that for certain purposes making 10 copies or more is construed to be commercial, but there is no general rule permitting such copying. Indeed, making one complete copy of a work, or in many cases using a portion of it, for commercial purposes will not be considered fair use. The Digital Millennium Copyright Act prohibits the manufacture, importation, or distribution of devices whose intended use, or only significant commercial use, is to bypass an access or copy control put in place by a copyright owner. [23] An appellate court has held that fair use is not a defense to engaging in such distribution.
The copyright directive allows EU member states to implement a set of exceptions to copyright. Examples of those exceptions are:

Accessible copies
It is legal in several countries including the United Kingdom and the United States to produce alternative versions (for example, in large print or braille) of a copyrighted work to provide improved access to a work for blind and visually impaired persons without permission from the copyright holder. [54] [55]

 Transfer, assignment and licensing
A copyright, or aspects of it (e.g. reproduction alone, all but moral rights), may be assigned or transferred from one party to another. [56] For example, a musician who records an album will often sign an agreement with a record company in which the musician agrees to transfer all copyright in the recordings in exchange for royalties and other considerations. The creator (and original copyright holder) benefits, or expects to, from production and marketing capabilities far beyond those of the author. In the digital age of music, music may be copied and distributed at minimal cost through the Internet ; however, the record industry attempts to provide promotion and marketing for the artist and his or her work so it can reach a much larger audience. A copyright holder need not transfer all rights completely, though many publishers will insist. Some of the rights may be transferred, or else the copyright holder may grant another party a non-exclusive license to copy or distribute the work in a particular region or for a specified period of time.
A transfer or licence may have to meet particular formal requirements in order to be effective, [57] for example under the Australian Copyright Act 1968 the copyright itself must be expressly transferred in writing. Under the U.S. Copyright Act, a transfer of ownership in copyright must be memorialized in a writing signed by the transferor. For that purpose, ownership in copyright includes exclusive licenses of rights. Thus exclusive licenses, to be effective, must be granted in a written instrument signed by the grantor. No special form of transfer or grant is required. A simple document that identifies the work involved and the rights being granted is sufficient. Non-exclusive grants (often called non-exclusive licenses) need not be in writing under U.S. law . They can be oral or even implied by the behavior of the parties. Transfers of copyright ownership, including exclusive licenses, may and should be recorded in the U.S. Copyright Office. (Information on recording transfers is available on the Office's web site.) While recording is not required to make the grant effective, it offers important benefits, much like those obtained by recording a deed in a real estate transaction.
Copyright may also be licensed . [56] Some jurisdictions may provide that certain classes of copyrighted works be made available under a prescribed statutory license (e.g. musical works in the United States used for radio broadcast or performance). This is also called a compulsory license , because under this scheme, anyone who wishes to copy a covered work does not need the permission of the copyright holder, but instead merely files the proper notice and pays a set fee established by statute (or by an agency decision under statutory guidance) for every copy made. [58] Failure to follow the proper procedures would place the copier at risk of an infringement suit. Because of the difficulty of following every individual work, copyright collectives or collecting societies and performing rights organizations (such as ASCAP , BMI , and SESAC ) have been formed to collect royalties for hundreds (thousands and more) works at once. Though this market solution bypasses the statutory license, the availability of the statutory fee still helps dictate the price per work collective rights organizations charge, driving it down to what avoidance of procedural hassle would justify.

Free licences
Copyright licenses known as open or free licenses seek to grant several rights to licensees, either for a fee or not, to an effect inspired by the public domain . Free in this context is not as much of a reference to price as it is to freedom. What constitutes free licensing has been characterised in a number of similar definitions, including by order of longevity the Free Software Definition , the Debian Free Software Guidelines , the Open Source Definition and the Definition of Free Cultural Works . Further refinements to these licenses have resulted in categories such as copyleft and permissive . Common examples of free licences are the GNU General Public License , BSD licenses and some Creative Commons licenses .
Founded in 2001 by James Boyle , Lawrence Lessig , and Hal Abelson , the Creative Commons (CC) is a non-profit organization [59] which aims to facilitate the legal sharing of creative works. To this end, the organization provides a number of generic copyright license options to the public, gratis . These licenses allow copyright holders to define conditions under which others may use a work and to specify what types of use are acceptable. [59]
Terms of use have traditionally been negotiated on an individual basis between copyright holder and potential licensee. Therefore, a general CC license outlining which rights the copyright holder is willing to waive enables the general public to use such works more freely. Six general types of CC licenses are available (although some of them are not properly free per the above definitions and per Creative Commons' own advice). These are based upon copyright-holder stipulations such as whether he or she is willing to allow modifications to the work, whether he or she permits the creation of derivative works and whether he or she is willing to permit commercial use of the work. [60] As of 2009 [update] approximately 130 million individuals had received such licenses. [60]

Criticism
Some sources are critical of particular aspects of the copyright system. This is known as a debate over copynorms . Particularly on the internet, there is discussion about the copyright aspects of downloading and streaming , the copyright aspects of hyperlinking and framing . Such concerns are often couched in the language of digital rights and database rights . Discussions include Free Culture , a 2004 book by Lawrence Lessig . Lessig coined the term permission culture to describe a worst-case system. Good Copy Bad Copy (documentary) and RiP!: A Remix Manifesto , discuss copyright. Some suggest an alternative compensation system .
Some groups reject copyright altogether, taking an anti-copyright stance. The perceived inability to enforce copyright online leads some to advocate ignoring legal statutes when on the web .

Public domain
Copyright, like other intellectual property rights , is subject to a statutorily determined term. Once the term of a copyright has expired, the formerly copyrighted work enters the public domain and may be freely used or exploited by anyone. Courts in common law countries, such as the United States and the United Kingdom, have rejected the doctrine of a common law copyright . Public domain works should not be confused with works that are publicly available. Works posted in the internet , for example, are publicly available, but are not generally in the public domain. Copying such works may therefore violate the author's copyright.

See also
WebPage index: 00037
Democracy
Democracy ( Greek : δημοκρατία , Dēmoskrátos literally "rule of the people"), in modern usage, is a system of government in which the citizens exercise power directly or elect representatives from among themselves to form a governing body, such as a parliament . [1] Democracy is sometimes referred to as "rule of the majority". [2] Western democracy, as distinct from that which existed in pre-civilized societies, is generally considered to have originated in city states such as Classical Athens and the Roman Republic , where various schemes and degrees of enfranchisement of the free male population were observed before the form disappeared in the West at the beginning of late antiquity . The English word dates to the 16th century, from the older Middle French and Middle Latin equivalents.
According to political scientist Larry Diamond , democracy consists of four key elements: (a) A political system for choosing and replacing the government through free and fair elections ; (b) The active participation of the people, as citizens, in politics and civic life; (c) Protection of the human rights of all citizens, and (d) A rule of law , in which the laws and procedures apply equally to all citizens. [3]
In the 5th century BC, to denote the political systems then existing in Greek city-states , notably Athens , the term is an antonym to aristocracy ( ἀριστοκρατία , aristokratía ), meaning "rule of an elite". While theoretically these definitions are in opposition, in practice the distinction has been blurred historically. [4] The political system of Classical Athens, for example, granted democratic citizenship to free men and excluded slaves and women from political participation. In 1906, Finland became the first government to herald a more inclusive democracy at the national level. In virtually all democratic governments throughout ancient and modern history, democratic citizenship consisted of an elite class until full enfranchisement was won for all adult citizens in most modern democracies through the suffrage movements of the 19th and 20th centuries.
Democracy contrasts with forms of government where power is either held by an individual, as in an absolute monarchy , or where power is held by a small number of individuals, as in an oligarchy . Nevertheless, these oppositions, inherited from Greek philosophy, [5] are now ambiguous because contemporary governments have mixed democratic, oligarchic, and monarchic elements. Karl Popper defined democracy in contrast to dictatorship or tyranny , thus focusing on opportunities for the people to control their leaders and to oust them without the need for a revolution. [6]

Characteristics
No consensus exists on how to define democracy, but legal equality , political freedom and rule of law have been identified as important characteristics. [7] [8] These principles are reflected in all eligible citizens being equal before the law and having equal access to legislative processes. For example, in a representative democracy , every vote has equal weight, no unreasonable restrictions can apply to anyone seeking to become a representative, [ according to whom? ] and the freedom of its eligible citizens is secured by legitimised rights and liberties which are typically protected by a constitution . [9] [10] Other uses of "democracy" include that of direct democracy .
One theory holds that democracy requires three fundamental principles: (1) upward control, i.e. sovereignty residing at the lowest levels of authority, (2) political equality, and (3) social norms by which individuals and institutions only consider acceptable acts that reflect the first two principles of upward control and political equality. [11]
The term "democracy" is sometimes used as shorthand for liberal democracy , which is a variant of representative democracy that may include elements such as political pluralism ; equality before the law; the right to petition elected officials for redress of grievances; due process ; civil liberties ; human rights ; and elements of civil society outside the government. [ citation needed ] Roger Scruton argues that democracy alone cannot provide personal and political freedom unless the institutions of civil society are also present. [12]
In some countries, notably in the United Kingdom which originated the Westminster system , the dominant principle is that of parliamentary sovereignty , while maintaining judicial independence . [13] [14] In the United States , separation of powers is often cited as a central attribute. In India , parliamentary sovereignty is subject to the Constitution of India which includes judicial review . [15] Though the term "democracy" is typically used in the context of a political state , the principles also are applicable to private organisations .
Majority rule is often listed as a characteristic of democracy. Hence, democracy allows for political minorities to be oppressed by the " tyranny of the majority " in the absence of legal protections of individual or group rights. An essential part of an "ideal" representative democracy is competitive elections that are substantively and procedurally " fair ," i.e., just and equitable . In some countries, freedom of political expression , freedom of speech , freedom of the press , and internet democracy are considered important to ensure that voters are well informed, enabling them to vote according to their own interests. [16] [17]
It has also been suggested that a basic feature of democracy is the capacity of all voters to participate freely and fully in the life of their society. [18] With its emphasis on notions of social contract and the collective will of all the voters, democracy can also be characterised as a form of political collectivism because it is defined as a form of government in which all eligible citizens have an equal say in lawmaking. [19]
While representative democracy is sometimes equated with the republican form of government, the term " republic " classically has encompassed both democracies and aristocracies . [20] [21] Many democracies are constitutional monarchies , such as the United Kingdom .

History

Ancient origins
The term "democracy" first appeared in ancient Greek political and philosophical thought in the city-state of Athens during classical antiquity . [22] [23] The word comes from demos , "common people" and kratos , strength. [24] Led by Cleisthenes , Athenians established what is generally held as the first democracy in 508–507 BC. Cleisthenes is referred to as "the father of Athenian democracy ." [25]
Athenian democracy took the form of a direct democracy, and it had two distinguishing features: the random selection of ordinary citizens to fill the few existing government administrative and judicial offices, [26] and a legislative assembly consisting of all Athenian citizens. [27] All eligible citizens were allowed to speak and vote in the assembly, which set the laws of the city state. However, Athenian citizenship excluded women, slaves, foreigners (μέτοικοι / métoikoi ), non-landowners, and males under 20 years old. [ citation needed ] [ contradictory ] The exclusion of large parts of the population from the citizen body is closely related to the ancient understanding of citizenship. In most of antiquity the benefit of citizenship was tied to the obligation to fight war campaigns. [28]
Athenian democracy was not only direct in the sense that decisions were made by the assembled people, but also the most direct in the sense that the people through the assembly, boule and courts of law controlled the entire political process and a large proportion of citizens were involved constantly in the public business. [29] Even though the rights of the individual were not secured by the Athenian constitution in the modern sense (the ancient Greeks had no word for "rights" [30] ), the Athenians enjoyed their liberties not in opposition to the government but by living in a city that was not subject to another power and by not being subjects themselves to the rule of another person. [31]
Range voting appeared in Sparta as early as 700 BC. The Apella was an assembly of the people, held once a month, in which every male citizen of age 30 and above could participate. In the Apella, Spartans elected leaders and cast votes by range voting and shouting. Aristotle called this "childish", as compared with the stone voting ballots used by the Athenians. Sparta adopted it because of its simplicity, and to prevent any bias voting, buying, or cheating that was predominant in the early democratic elections. [32] [33]
Even though the Roman Republic contributed significantly to many aspects of democracy, only a minority of Romans were citizens with votes in elections for representatives. The votes of the powerful were given more weight through a system of gerrymandering , so most high officials, including members of the Senate , came from a few wealthy and noble families. [34] In addition, the Roman Republic was the first government in the western world to have a Republic as a nation-state, although it didn't have much of a democracy. The Romans invented the concept of classics and many works from Ancient Greece were preserved. [35] Additionally, the Roman model of governance inspired many political thinkers over the centuries, [36] and today's modern representative democracies imitate more the Roman than the Greek models because it was a state in which supreme power was held by the people and their elected representatives, and which had an elected or nominated leader. [37] Other cultures, such as the Iroquois Nation in the Americas between around 1450 and 1600 AD also developed a form of democratic society before they came in contact with the Europeans. This indicates that forms of democracy may have been invented in other societies around the world.

Middle Ages
During the Middle Ages , there were various systems involving elections or assemblies, although often only involving a small part of the population. These included:
Most regions in medieval Europe were ruled by clergy or feudal lords.
The Kouroukan Fouga divided the Mali Empire into ruling clans (lineages) that were represented at a great assembly called the Gbara . However, the charter made Mali more similar to a constitutional monarchy than a democratic republic . A little closer to modern democracy were the Cossack republics of Ukraine in the 16th and 17th centuries: Cossack Hetmanate and Zaporizhian Sich . The highest post – the Hetman – was elected by the representatives from the country's districts.
The Parliament of England had its roots in the restrictions on the power of kings written into Magna Carta (1215), which explicitly protected certain rights of the King's subjects and implicitly supported what became the English writ of habeas corpus , safeguarding individual freedom against unlawful imprisonment with right to appeal. [39] [40] The first representative national assembly in England was Simon de Montfort's Parliament in 1265. [41] The emergence of petitioning is some of the earliest evidence of parliament being used as a forum to address the general grievances of ordinary people. However, the power to call parliament remained at the pleasure of the monarch. [42]

Modern era

Early modern period
During the early modern period , the power of the Parliament of England continually increased. Passage of the Petition of Right in 1628 and Habeas Corpus Act in 1679 established certain liberties and remain in effect. The idea of a political party took form with groups freely debating rights to political representation during the Putney Debates of 1647. After the English Civil Wars (1642–1651) and the Glorious Revolution of 1688, the Bill of Rights was enacted in 1689, which codified certain rights and liberties, and is still in effect. The Bill set out the requirement for regular elections, rules for freedom of speech in Parliament and limited the power of the monarch, ensuring that, unlike much of Europe at the time, royal absolutism would not prevail. [43] [44]
In North America, representative government began in Jamestown, Virginia , with the election of the House of Burgesses (forerunner of the Virginia General Assembly ) in 1619. English Puritans who migrated from 1620 established colonies in New England whose local governance was democratic and which contributed to the democratic development of the United States ; [45] although these local assemblies had some small amounts of devolved power, the ultimate authority was held by the Crown and the English Parliament. The Puritans ( Pilgrim Fathers ), Baptists , and Quakers who founded these colonies applied the democratic organisation of their congregations also to the administration of their communities in worldly matters. [46] [47] [48]

18th and 19th centuries
The first Parliament of Great Britain was established in 1707, after the merger of the Kingdom of England and the Kingdom of Scotland under the Acts of Union . Although the monarch increasingly became a figurehead , [49] only a small minority actually had a voice; Parliament was elected by only a few percent of the population (less than 3% as late as 1780). [50] During the Age of Liberty in Sweden (1718–1772), civil rights were expanded and power shifted from the monarch to parliament. The taxed peasantry was represented in parliament, although with little influence, but commoners without taxed property had no suffrage.
The creation of the short-lived Corsican Republic in 1755 marked the first nation in modern history to adopt a democratic constitution (all men and women above age of 25 could vote [51] ). This Corsican Constitution was the first based on Enlightenment principles and included female suffrage , something that was not granted in most other democracies until the 20th century.
In the American colonial period before 1776 , and for some time after, often only adult white male property owners could vote; enslaved Africans, most free black people and most women were not extended the franchise. [52] On the American frontier , democracy became a way of life, with more widespread social, economic and political equality. [53] Although not described as a democracy by the founding fathers , [54] they shared a determination to root the American experiment in the principles of natural freedom and equality. [55]
The American Revolution led to the adoption of the United States Constitution in 1787, the oldest surviving, still active, governmental codified constitution . The Constitution provided for an elected government and protected civil rights and liberties for some, but did not end slavery nor extend voting rights in the United States beyond white male property owners (about 6% of the population). [56] The Bill of Rights in 1791 set limits on government power to protect personal freedoms but had little impact on judgements by the courts for the first 130 years after ratification. [57]
In 1789, Revolutionary France adopted the Declaration of the Rights of Man and of the Citizen and, although short-lived, the National Convention was elected by all males in 1792. [58] However, in the early 19th century, little of democracy – as theory, practice, or even as word – remained in the North Atlantic world. [59]
During this period, slavery remained a social and economic institution in places around the world. This was particularly the case in the United States , and especially in the last fifteen slave states that kept slavery legal in the American South until the Civil War . A variety of organisations were established advocating the movement of black people from the United States to locations where they would enjoy greater freedom and equality.
The United Kingdom's Slave Trade Act 1807 banned the trade across the British Empire , which was enforced internationally by the Royal Navy under treaties Britain negotiated with other nations. [60] As the voting franchise in the U.K. was increased, it also was made more uniform in a series of reforms beginning with the Reform Act 1832 . In 1833, the United Kingdom passed the Slavery Abolition Act which took effect across the British Empire.
Universal male suffrage was established in France in March 1848 in the wake of the French Revolution of 1848 . [61] In 1848, several revolutions broke out in Europe as rulers were confronted with popular demands for liberal constitutions and more democratic government. [62]
In the 1860 United States Census , the slave population in the United States had grown to four million, [63] and in Reconstruction after the Civil War (late 1860s), the newly freed slaves became citizens with a nominal right to vote for men. Full enfranchisement of citizens was not secured until after the Civil Rights Movement gained passage by the United States Congress of the Voting Rights Act of 1965 . [64] [65]

20th and 21st centuries
20th-century transitions to liberal democracy have come in successive "waves of democracy", variously resulting from wars, revolutions, decolonisation , and religious and economic circumstances. [66] Global waves of "democratic regression" reversing democratization, have also occurred in the 1920s and 30s, in the 1960s and 1970s, and in the 2010s. [67] [68]
World War I and the dissolution of the Ottoman and Austro-Hungarian empires resulted in the creation of new nation-states from Europe, most of them at least nominally democratic.
In the 1920s democracy flourished and women's suffrage advanced, but the Great Depression brought disenchantment and most of the countries of Europe, Latin America, and Asia turned to strong-man rule or dictatorships. Fascism and dictatorships flourished in Nazi Germany , Italy , Spain and Portugal , as well as non-democratic governments in the Baltics , the Balkans , Brazil , Cuba , China , and Japan , among others. [69]
World War II brought a definitive reversal of this trend in western Europe. The democratisation of the American, British, and French sectors of occupied Germany (disputed [70] ), Austria, Italy, and the occupied Japan served as a model for the later theory of government change . However, most of Eastern Europe , including the Soviet sector of Germany fell into the non-democratic Soviet bloc .
The war was followed by decolonisation , and again most of the new independent states had nominally democratic constitutions. India emerged as the world's largest democracy and continues to be so. [71] Countries that were once part of the British Empire often adopted the British Westminster system . [72] [73]
By 1960, the vast majority of country-states were nominally democracies, although most of the world's populations lived in nations that experienced sham elections, and other forms of subterfuge (particularly in "Communist" nations and the former colonies.)
A subsequent wave of democratisation brought substantial gains toward true liberal democracy for many nations. Spain , Portugal (1974), and several of the military dictatorships in South America returned to civilian rule in the late 1970s and early 1980s ( Argentina in 1983 , Bolivia , Uruguay in 1984 , Brazil in 1985 , and Chile in the early 1990s ). This was followed by nations in East and South Asia by the mid-to-late 1980s.
Economic malaise in the 1980s, along with resentment of Soviet oppression, contributed to the collapse of the Soviet Union , the associated end of the Cold War , and the democratisation and liberalisation of the former Eastern bloc countries. The most successful of the new democracies were those geographically and culturally closest to western Europe, and they are now members or candidate members of the European Union .
The liberal trend spread to some nations in Africa in the 1990s, most prominently in South Africa . Some recent examples of attempts of liberalisation include the Indonesian Revolution of 1998 , the Bulldozer Revolution in Yugoslavia , the Rose Revolution in Georgia , the Orange Revolution in Ukraine , the Cedar Revolution in Lebanon , the Tulip Revolution in Kyrgyzstan , and the Jasmine Revolution in Tunisia .
According to Freedom House , in 2007 there were 123 electoral democracies (up from 40 in 1972). [74] According to World Forum on Democracy , electoral democracies now represent 120 of the 192 existing countries and constitute 58.2 percent of the world's population. At the same time liberal democracies i.e. countries Freedom House regards as free and respectful of basic human rights and the rule of law are 85 in number and represent 38 percent of the global population. [75]
In 2007 the United Nations declared September 15 the International Day of Democracy . [76]
According to Freedom House, starting in 2005, there have been eleven consecutive years in which declines in political rights and civil liberties throughout the world have outnumbered improvements, [77] as populist and nationalist political forces have gained ground everywhere from Poland (under the Law and Justice Party ) to the Philippines (under Rodrigo Duterte ). [77] [67]

Measurement of democracy
Several freedom indices are published by several organisations according to their own various definitions of the term:

Types of governmental democracies
Democracy has taken a number of forms, both in theory and practice. Some varieties of democracy provide better representation and more freedom for their citizens than others. [87] [88] However, if any democracy is not structured so as to prohibit the government from excluding the people from the legislative process, or any branch of government from altering the separation of powers in its own favour, then a branch of the system can accumulate too much power and destroy the democracy. [89] [90] [91]
The following kinds of democracy are not exclusive of one another: many specify details of aspects that are independent of one another and can co-exist in a single system.

Basic forms
Several variants of democracy exist, but there are two basic forms, both of which concern how the whole body of all eligible citizens executes its will. One form of democracy is direct democracy , in which all eligible citizens have active participation in the political decision making, for example voting on policy initiatives directly. [92] In most modern democracies, the whole body of eligible citizens remain the sovereign power but political power is exercised indirectly through elected representatives; this is called a representative democracy .

Direct
Direct democracy is a political system where the citizens participate in the decision-making personally, contrary to relying on intermediaries or representatives. The use of a lot system, a characteristic of Athenian democracy , is unique to direct democracies. In this system, important governmental and administrative tasks are performed by citizens picked from a lottery. [93] A direct democracy gives the voting population the power to:
Within modern-day representative governments, certain electoral tools like referendums, citizens' initiatives and recall elections are referred to as forms of direct democracy. [94] Direct democracy as a government system currently exists in the Swiss cantons of Appenzell Innerrhoden and Glarus , [95] and kurdish cantons of Rojava . [96]

Representative
Representative democracy involves the election of government officials by the people being represented. If the head of state is also democratically elected then it is called a democratic republic . [97] The most common mechanisms involve election of the candidate with a majority or a plurality of the votes. Most western countries have representative systems. [95]
Representatives may be elected or become diplomatic representatives by a particular district (or constituency ), or represent the entire electorate through proportional systems, with some using a combination of the two. Some representative democracies also incorporate elements of direct democracy, such as referendums . A characteristic of representative democracy is that while the representatives are elected by the people to act in the people's interest, they retain the freedom to exercise their own judgement as how best to do so. Such reasons have driven criticism upon representative democracy, [98] [99] pointing out the contradictions of representation mechanisms' with democracy [100] [101]

Parliamentary
Parliamentary democracy is a representative democracy where government is appointed by, or can be dismissed by, representatives as opposed to a "presidential rule" wherein the president is both head of state and the head of government and is elected by the voters. Under a parliamentary democracy, government is exercised by delegation to an executive ministry and subject to ongoing review, checks and balances by the legislative parliament elected by the people. [102] [103] [104] [105]
Parliamentary systems have the right to dismiss a Prime Minister at any point in time that they feel he or she is not doing their job to the expectations of the legislature. This is done through a Vote of No Confidence where the legislature decides whether or not to remove the Prime Minister from office by a majority support for his or her dismissal. [106] In some countries, the Prime Minister can also call an election whenever he or she so chooses, and typically the Prime Minister will hold an election when he or she knows that they are in good favour with the public as to get re-elected. In other parliamentary democracies extra elections are virtually never held, a minority government being preferred until the next ordinary elections. An important feature of the parliamentary democracy is the concept of the "loyal opposition". The essence of the concept is that the second largest political party (or coalition) opposes the governing party (or coalition), while still remaining loyal to the state and its democratic principles.

Presidential
Presidential Democracy is a system where the public elects the president through free and fair elections. The president serves as both the head of state and head of government controlling most of the executive powers. The president serves for a specific term and cannot exceed that amount of time. Elections typically have a fixed date and aren't easily changed. The president has direct control over the cabinet, specifically appointing the cabinet members. [106]
The president cannot be easily removed from office by the legislature, but he or she cannot remove members of the legislative branch any more easily. This provides some measure of separation of powers . In consequence however, the president and the legislature may end up in the control of separate parties, allowing one to block the other and thereby interfere with the orderly operation of the state. This may be the reason why presidential democracy is not very common outside the Americas, Africa, and Central and Southeast Asia. [106]
A semi-presidential system is a system of democracy in which the government includes both a prime minister and a president. The particular powers held by the prime minister and president vary by country. [106]

Hybrid or semi-direct
Some modern democracies that are predominantly representative in nature also heavily rely upon forms of political action that are directly democratic. These democracies, which combine elements of representative democracy and direct democracy, are termed hybrid democracies , [107] semi-direct democracies or participatory democracies . Examples include Switzerland and some U.S. states , where frequent use is made of referendums and initiatives .
The Swiss confederation is a semi-direct democracy. [95] At the federal level, citizens can propose changes to the constitution ( federal popular initiative ) or ask for a referendum to be held on any law voted by the parliament . [95] Between January 1995 and June 2005, Swiss citizens voted 31 times, to answer 103 questions (during the same period, French citizens participated in only two referendums). [95] Although in the past 120 years less than 250 initiatives have been put to referendum. The populace has been conservative, approving only about 10% of the initiatives put before them; in addition, they have often opted for a version of the initiative rewritten by government. [ citation needed ]
In the United States , no mechanisms of direct democracy exists at the federal level, but over half of the states and many localities provide for citizen-sponsored ballot initiatives (also called "ballot measures", "ballot questions" or "propositions"), and the vast majority of states allow for referendums. Examples include the extensive use of referendums in the US state of California , which is a state that has more than 20 million voters. [108]
In New England , Town meetings are often used, especially in rural areas, to manage local government. This creates a hybrid form of government, with a local direct democracy and a state government which is representative. For example, most Vermont towns hold annual town meetings in March in which town officers are elected, budgets for the town and schools are voted on, and citizens have the opportunity to speak and be heard on political matters. [109]

Variants

Constitutional monarchy
Many countries such as the United Kingdom, Spain, the Netherlands, Belgium, Scandinavian countries , Thailand , Japan and Bhutan turned powerful monarchs into constitutional monarchs with limited or, often gradually, merely symbolic roles. For example, in the predecessor states to the United Kingdom, constitutional monarchy began to emerge and has continued uninterrupted since the Glorious Revolution of 1688 and passage of the Bill of Rights 1689 . [13] [43]
In other countries, the monarchy was abolished along with the aristocratic system (as in France, China, Russia, Germany, Austria, Hungary, Italy, Greece and Egypt). An elected president, with or without significant powers, became the head of state in these countries.
Elite upper houses of legislatures, which often had lifetime or hereditary tenure, were common in many nations. Over time, these either had their powers limited (as with the British House of Lords ) or else became elective and remained powerful (as with the Australian Senate ).

Republic
The term republic has many different meanings, but today often refers to a representative democracy with an elected head of state , such as a president , serving for a limited term, in contrast to states with a hereditary monarch as a head of state, even if these states also are representative democracies with an elected or appointed head of government such as a prime minister . [110]
The Founding Fathers of the United States rarely praised and often criticised democracy, which in their time tended to specifically mean direct democracy, often without the protection of a constitution enshrining basic rights; James Madison argued, especially in The Federalist No. 10 , that what distinguished a democracy from a republic was that the former became weaker as it got larger and suffered more violently from the effects of faction, whereas a republic could get stronger as it got larger and combats faction by its very structure.
What was critical to American values, John Adams insisted, [111] was that the government be "bound by fixed laws, which the people have a voice in making, and a right to defend." As Benjamin Franklin was exiting after writing the U.S. constitution, a woman asked him "Well, Doctor, what have we got—a republic or a monarchy?". He replied "A republic—if you can keep it." [112]

Liberal democracy
A liberal democracy is a representative democracy in which the ability of the elected representatives to exercise decision-making power is subject to the rule of law , and moderated by a constitution or laws that emphasise the protection of the rights and freedoms of individuals, and which places constraints on the leaders and on the extent to which the will of the majority can be exercised against the rights of minorities (see civil liberties ).
In a liberal democracy, it is possible for some large-scale decisions to emerge from the many individual decisions that citizens are free to make. In other words, citizens can "vote with their feet" or "vote with their dollars", resulting in significant informal government-by-the-masses that exercises many "powers" associated with formal government elsewhere.

Socialist
Socialist thought has several different views on democracy. Social democracy , democratic socialism , and the dictatorship of the proletariat (usually exercised through Soviet democracy ) are some examples. Many democratic socialists and social democrats believe in a form of participatory , industrial , economic and/or workplace democracy combined with a representative democracy .
Within Marxist orthodoxy there is a hostility to what is commonly called "liberal democracy", which they simply refer to as parliamentary democracy because of its often centralised nature. Because of their desire to eliminate the political elitism they see in capitalism, Marxists , Leninists and Trotskyists believe in direct democracy implemented through a system of communes (which are sometimes called soviets ). This system ultimately manifests itself as council democracy and begins with workplace democracy. (See Democracy in Marxism .)

Anarchist
Anarchists are split in this domain, depending on whether they believe that a majority-rule is tyrannic or not . The only form of democracy considered acceptable to many anarchists is direct democracy. Pierre-Joseph Proudhon argued that the only acceptable form of direct democracy is one in which it is recognised that majority decisions are not binding on the minority, even when unanimous. [114] However, anarcho-communist Murray Bookchin criticised individualist anarchists for opposing democracy, [115] and says "majority rule" is consistent with anarchism. [116]
Some anarcho-communists oppose the majoritarian nature of direct democracy, feeling that it can impede individual liberty and opt in favour of a non-majoritarian form of consensus democracy , similar to Proudhon's position on direct democracy. [117] Henry David Thoreau , who did not self-identify as an anarchist but argued for "a better government" [118] and is cited as an inspiration by some anarchists, argued that people should not be in the position of ruling others or being ruled when there is no consent.
Anarcho-capitalists , voluntaryists and other right-anarchists oppose institutional democracy as they consider it in conflict with widely held moral values and ethical principles and their conception of individual rights . The a priori Rothbardian argument is that the state is a coercive institution which necessarily violates the non-aggression principle (NAP). Some right-anarchists also criticise democracy on a posteriori consequentialist grounds, in terms of inefficiency or disability in bringing about maximisation of individual liberty . They maintain the people who participate in democratic institutions are foremost driven by economic self-interest. [119] [120]

Sortition
Sometimes called "democracy without elections", sortition chooses decision makers via a random process. The intention is that those chosen will be representative of the opinions and interests of the people at large, and be more fair and impartial than an elected official. The technique was in widespread use in Athenian Democracy and Renaissance Florence [121] and is still used in modern jury selection .

Consociational
A consociational democracy allows for simultaneous majority votes in two or more ethno-religious constituencies, and policies are enacted only if they gain majority support from both or all of them.

Consensus democracy
A consensus democracy, in contrast, would not be dichotomous. Instead, decisions would be based on a multi-option approach, and policies would be enacted if they gained sufficient support, either in a purely verbal agreement, or via a consensus vote—a multi-option preference vote. If the threshold of support were at a sufficiently high level, minorities would be as it were protected automatically. Furthermore, any voting would be ethno-colour blind.

Supranational
Qualified majority voting is designed by the Treaty of Rome to be the principal method of reaching decisions in the European Council of Ministers . This system allocates votes to member states in part according to their population, but heavily weighted in favour of the smaller states. This might be seen as a form of representative democracy, but representatives to the Council might be appointed rather than directly elected.

Inclusive
Inclusive democracy is a political theory and political project that aims for direct democracy in all fields of social life: political democracy in the form of face-to-face assemblies which are confederated, economic democracy in a stateless , moneyless and marketless economy, democracy in the social realm, i.e. self-management in places of work and education, and ecological democracy which aims to reintegrate society and nature. The theoretical project of inclusive democracy emerged from the work of political philosopher Takis Fotopoulos in "Towards An Inclusive Democracy" and was further developed in the journal Democracy & Nature and its successor The International Journal of Inclusive Democracy .
The basic unit of decision making in an inclusive democracy is the demotic assembly, i.e. the assembly of demos, the citizen body in a given geographical area which may encompass a town and the surrounding villages, or even neighbourhoods of large cities. An inclusive democracy today can only take the form of a confederal democracy that is based on a network of administrative councils whose members or delegates are elected from popular face-to-face democratic assemblies in the various demoi. Thus, their role is purely administrative and practical, not one of policy-making like that of representatives in representative democracy.
The citizen body is advised by experts but it is the citizen body which functions as the ultimate decision-taker . Authority can be delegated to a segment of the citizen body to carry out specific duties, for example to serve as members of popular courts, or of regional and confederal councils. Such delegation is made, in principle, by lot, on a rotation basis, and is always recallable by the citizen body. Delegates to regional and confederal bodies should have specific mandates.

Participatory politics
A Parpolity or Participatory Polity is a theoretical form of democracy that is ruled by a Nested Council structure. The guiding philosophy is that people should have decision making power in proportion to how much they are affected by the decision. Local councils of 25–50 people are completely autonomous on issues that affect only them, and these councils send delegates to higher level councils who are again autonomous regarding issues that affect only the population affected by that council.
A council court of randomly chosen citizens serves as a check on the tyranny of the majority , and rules on which body gets to vote on which issue. Delegates may vote differently from how their sending council might wish, but are mandated to communicate the wishes of their sending council. Delegates are recallable at any time. Referendums are possible at any time via votes of most lower-level councils, however, not everything is a referendum as this is most likely a waste of time. A parpolity is meant to work in tandem with a participatory economy .

Cosmopolitan
Cosmopolitan democracy, also known as Global democracy or World Federalism , is a political system in which democracy is implemented on a global scale, either directly or through representatives. An important justification for this kind of system is that the decisions made in national or regional democracies often affect people outside the constituency who, by definition, cannot vote. By contrast, in a cosmopolitan democracy, the people who are affected by decisions also have a say in them. [122]
According to its supporters, any attempt to solve global problems is undemocratic without some form of cosmopolitan democracy. The general principle of cosmopolitan democracy is to expand some or all of the values and norms of democracy, including the rule of law; the non-violent resolution of conflicts; and equality among citizens, beyond the limits of the state. To be fully implemented, this would require reforming existing international organisations , e.g. the United Nations , as well as the creation of new institutions such as a World Parliament , which ideally would enhance public control over, and accountability in, international politics.
Cosmopolitan Democracy has been promoted, among others, by physicist Albert Einstein, [123] writer Kurt Vonnegut, columnist George Monbiot , and professors David Held and Daniele Archibugi . [124] The creation of the International Criminal Court in 2003 was seen as a major step forward by many supporters of this type of cosmopolitan democracy.

Creative democracy
Creative Democracy is advocated by American philosopher John Dewey . The main idea about Creative Democracy is that democracy encourages individual capacity building and the interaction among the society. Dewey argues that democracy is a way of life in his work of "Creative Democracy: The Task Before Us" [125] and an experience built on faith in human nature, faith in human beings, and faith in working with others. Democracy, in Dewey's view, is a moral ideal requiring actual effort and work by people; it is not an institutional concept that exists outside of ourselves. "The task of democracy", Dewey concludes, "is forever that of creation of a freer and more humane experience in which all share and to which all contribute".

Guided democracy
Guided democracy is a form of democracy which incorporates regular popular elections, but which often carefully "guides" the choices offered to the electorate in a manner which may reduce the ability of the electorate to truly determine the type of government exercised over them. Such democracies typically have only one central authority which is often not subject to meaningful public review by any other governmental authority. Russian-style democracy has often been referred to as a "Guided democracy." [126] Russian politicians have referred to their government as having only one center of power/ authority, as opposed to most other forms of democracy which usually attempt to incorporate two or more naturally competing sources of authority within the same government. [127]

Non-governmental democracy
Aside from the public sphere, similar democratic principles and mechanisms of voting and representation have been used to govern other kinds of groups. Many non-governmental organisations decide policy and leadership by voting. Most trade unions and cooperatives are governed by democratic elections. Corporations are controlled by shareholders on the principle of one share, one vote . An analogous system, that fuses elements of democracy with sharia law , has been termed islamocracy . [128]

Theory

Aristotle
Aristotle contrasted rule by the many (democracy/ polity ), with rule by the few ( oligarchy / aristocracy ), and with rule by a single person ( tyranny or today autocracy / absolute monarchy ). He also thought that there was a good and a bad variant of each system (he considered democracy to be the degenerate counterpart to polity). [129] [130]
For Aristotle the underlying principle of democracy is freedom, since only in a democracy the citizens can have a share in freedom. In essence, he argues that this is what every democracy should make its aim. There are two main aspects of freedom: being ruled and ruling in turn, since everyone is equal according to number, not merit, and to be able to live as one pleases.

Early Republican theory
A common view among early and renaissance Republican theorists was that democracy could only survive in small political communities. [131] Heeding the lessons of the Roman Republic's shift to monarchism as it grew larger, these Republican theorists held that the expansion of territory and population inevitably led to tyranny. [131] Democracy was therefore highly fragile and rare historically, as it could only survive in small political units, which due to their size were vulnerable to conquest by larger political units. [131] Montesquieu famously said, "if a republic is small, it is destroyed by an outside force; if it is large, it is destroyed by an internal vice." [131] Rousseau asserted, "It is, therefore the natural property of small states to be governed as a republic, of middling ones to be subject to a monarch, and of large empires to be swayed by a despotic prince." [131]

Rationale
Among modern political theorists, there are three contending conceptions of the fundamental rationale for democracy: aggregative democracy, deliberative democracy , and radical democracy . [132]

Aggregative
The theory of aggregative democracy claims that the aim of the democratic processes is to solicit citizens' preferences and aggregate them together to determine what social policies society should adopt. Therefore, proponents of this view hold that democratic participation should primarily focus on voting , where the policy with the most votes gets implemented.
Different variants of aggregative democracy exist. Under minimalism , democracy is a system of government in which citizens have given teams of political leaders the right to rule in periodic elections. According to this minimalist conception, citizens cannot and should not "rule" because, for example, on most issues, most of the time, they have no clear views or their views are not well-founded. Joseph Schumpeter articulated this view most famously in his book Capitalism, Socialism, and Democracy . [133] Contemporary proponents of minimalism include William H. Riker , Adam Przeworski , Richard Posner .
According to the theory of direct democracy , on the other hand, citizens should vote directly, not through their representatives, on legislative proposals. Proponents of direct democracy offer varied reasons to support this view. Political activity can be valuable in itself, it socialises and educates citizens, and popular participation can check powerful elites. Most importantly, citizens do not really rule themselves unless they directly decide laws and policies.
Governments will tend to produce laws and policies that are close to the views of the median voter—with half to their left and the other half to their right. This is not actually a desirable outcome as it represents the action of self-interested and somewhat unaccountable political elites competing for votes. Anthony Downs suggests that ideological political parties are necessary to act as a mediating broker between individual and governments. Downs laid out this view in his 1957 book An Economic Theory of Democracy . [134]
Robert A. Dahl argues that the fundamental democratic principle is that, when it comes to binding collective decisions, each person in a political community is entitled to have his/her interests be given equal consideration (not necessarily that all people are equally satisfied by the collective decision). He uses the term polyarchy to refer to societies in which there exists a certain set of institutions and procedures which are perceived as leading to such democracy. First and foremost among these institutions is the regular occurrence of free and open elections which are used to select representatives who then manage all or most of the public policy of the society. However, these polyarchic procedures may not create a full democracy if, for example, poverty prevents political participation. [135] Similarly, Ronald Dworkin argues that "democracy is a substantive, not a merely procedural, ideal." [136]

Deliberative
Deliberative democracy is based on the notion that democracy is government by deliberation . Unlike aggregative democracy, deliberative democracy holds that, for a democratic decision to be legitimate, it must be preceded by authentic deliberation, not merely the aggregation of preferences that occurs in voting. Authentic deliberation is deliberation among decision-makers that is free from distortions of unequal political power, such as power a decision-maker obtained through economic wealth or the support of interest groups. [137] [138] [139] If the decision-makers cannot reach consensus after authentically deliberating on a proposal, then they vote on the proposal using a form of majority rule. Many theorists is discussing the conception of Debliberative Democracy, considering specially the thought of Jürgen Habermas.

Radical
Radical democracy is based on the idea that there are hierarchical and oppressive power relations that exist in society. Democracy's role is to make visible and challenge those relations by allowing for difference, dissent and antagonisms in decision making processes.

Criticism

Inefficiencies
Some economists have criticized the efficiency of democracy, citing the premise of the irrational voter, or a voter who makes decisions without all of the facts or necessary information in order to make a truly informed decision. Another argument is that democracy slows down processes because of the amount of input and participation needed in order to go forward with a decision. A common example often quoted to substantiate this point is the high economic development achieved by China (a non-democratic country) as compared to India (a democratic country). According to economists, the lack of democratic participation in countries like China allows for unfettered economic growth. [140]

Popular rule as a façade
The 20th-century Italian thinkers Vilfredo Pareto and Gaetano Mosca (independently) argued that democracy was illusory, and served only to mask the reality of elite rule. Indeed, they argued that elite oligarchy is the unbendable law of human nature, due largely to the apathy and division of the masses (as opposed to the drive, initiative and unity of the elites), and that democratic institutions would do no more than shift the exercise of power from oppression to manipulation. [141] As Louis Brandeis once professed, "We may have democracy, or we may have wealth concentrated in the hands of a few, but we can't have both." [142]
Between 1946 and 2000 Soviet Union/Russia and USA have intervened in at least 117 elections. [143]

Mob rule
Plato 's The Republic presents a critical view of democracy through the narration of Socrates : "Democracy, which is a charming form of government, full of variety and disorder, and dispensing a sort of equality to equals and unequaled alike." [144] In his work, Plato lists 5 forms of government from best to worst. Assuming that the Republic was intended to be a serious critique of the political thought in Athens, Plato argues that only Kallipolis, an aristocracy led by the unwilling philosopher-kings (the wisest men), is a just form of government. [145]
James Madison critiqued direct democracy (which he referred to simply as "democracy") in Federalist No. 10 , arguing that representative democracy—which he described using the term "republic"—is a preferable form of government, saying: "... democracies have ever been spectacles of turbulence and contention; have ever been found incompatible with personal security or the rights of property; and have in general been as short in their lives as they have been violent in their deaths." Madison offered that republics were superior to democracies because republics safeguarded against tyranny of the majority , stating in Federalist No. 10 : "the same advantage which a republic has over a democracy, in controlling the effects of faction, is enjoyed by a large over a small republic".

Political instability
More recently, democracy is criticised for not offering enough political stability. As governments are frequently elected on and off there tends to be frequent changes in the policies of democratic countries both domestically and internationally. Even if a political party maintains power, vociferous, headline grabbing protests and harsh criticism from the popular media are often enough to force sudden, unexpected political change. Frequent policy changes with regard to business and immigration are likely to deter investment and so hinder economic growth. For this reason, many people have put forward the idea that democracy is undesirable for a developing country in which economic growth and the reduction of poverty are top priorities. [146]
This opportunist alliance not only has the handicap of having to cater to too many ideologically opposing factions, but it is usually short lived since any perceived or actual imbalance in the treatment of coalition partners, or changes to leadership in the coalition partners themselves, can very easily result in the coalition partner withdrawing its support from the government.
Biased media has been accused of causing political instability, resulting in the obstruction of democracy, rather than its promotion. [147]

Fraudulent elections
In representative democracies, it may not benefit incumbents to conduct fair elections. A study showed that incumbents who rig elections stay in office 2.5 times as long as those who permit fair elections. [148] Democracies in countries with high per capita income have been found to be less prone to violence, but in countries with low incomes the tendency is the reverse. [148] Election misconduct is more likely in countries with low per capita incomes, small populations, rich in natural resources, and a lack of institutional checks and balances. Sub-Saharan countries, as well as Afghanistan, all tend to fall into that category. [148]
Governments that have frequent elections tend to have significantly more stable economic policies than those governments who have infrequent elections. However, this trend does not apply to governments where fraudulent elections are common. [148]

Opposition
Democracy in modern times has almost always faced opposition from the previously existing government, and many times it has faced opposition from social elites. The implementation of a democratic government within a non-democratic state is typically brought about by democratic revolution .
Post-Enlightenment ideologies such as fascism , nazism , and neo-fundamentalism oppose democracy on different grounds, generally citing that the concept of democracy as a constant process is flawed and detrimental to a preferable course of development.

Development
Several philosophers and researchers have outlined historical and social factors seen as supporting the evolution of democracy. Cultural factors like Protestantism influenced the development of democracy, rule of law, human rights and political liberty (the faithful elected priests, religious freedom and tolerance has been practiced).
Other commentators have mentioned the influence of wealth (e.g. S. M. Lipset, 1959). In a related theory, Ronald Inglehart suggests that improved living-standards can convince people that they can take their basic survival for granted, leading to increased emphasis on self-expression values , which is highly correlated to democracy. [149]
Carroll Quigley concludes that the characteristics of weapons are the main predictor of democracy: [150] [151] Democracy tends to emerge only when the best weapons available are easy for individuals to buy and use. [152] By the 1800s, guns were the best personal weapons available, and in America, almost everyone could afford to buy a gun, and could learn how to use it fairly easily. Governments couldn't do any better: it became the age of mass armies of citizen soldiers with guns [152] Similarly, Periclean Greece was an age of the citizen soldier and democracy. [153]
Recent theories stress the relevance of education and of human capital – and within them of cognitive ability to increasing tolerance, rationality, political literacy and participation. Two effects of education and cognitive ability are distinguished: a cognitive effect (competence to make rational choices, better information-processing) and an ethical effect (support of democratic values, freedom, human rights etc.), which itself depends on intelligence. [154] [155] [156]
Evidence that is consistent with conventional theories of why democracy emerges and is sustained has been hard to come by. Recent statistical analyses have challenged modernisation theory by demonstrating that there is no reliable evidence for the claim that democracy is more likely to emerge when countries become wealthier, more educated, or less unequal. [157] Neither is there convincing evidence that increased reliance on oil revenues prevents democratisation, despite a vast theoretical literature on " the Resource Curse " that asserts that oil revenues sever the link between citizen taxation and government accountability, seen as the key to representative democracy. [158] The lack of evidence for these conventional theories of democratisation have led researchers to search for the "deep" determinants of contemporary political institutions, be they geographical or demographic. [159] [160] More inclusive institutions lead to democracy because as people gain more power, they are able to demand more from the elites, who in turn have to concede more things to keep their position. This virtuous circle, may end up in democracy.
An example of this is the disease environment. Places with different mortality rates had different populations and productivity levels around the world. For example, in Africa, the Tsetse fly which is harmful to humans and livestock reduced the ability of the Africans to plow the land. This made Africa less settled. As a consequence, political power was less concentrated [161] . This also affected the colonial institutions that where set in place by the European countries in Africa [162] . If the colonial settlers could live or not in a place made them develop different institutions which led to different economic and social paths. This also affected the distribution of power and the collective actions people could take. As a result, some African countries ended up having democracies and others autocracies . Another example of geographical determinants for democracy is having access to coastal areas and rivers. This natural endowment has a positive relation with economic development thanks to the benefits of trade [163] . Trade brought economic development, which in turn, broaden the power. If the ruler wanted to increase his revenues, he had to protect property rights to create incentives for people to invest. As more people had more power, more concessions had to be made by the ruler and in many places this process lead to democracy. These determinants defined the structur of the society moving the balance of political power [164] .
In the 21st century, democracy has become such a popular method of reaching decisions that its application beyond politics to other areas such as entertainment, food and fashion, consumerism, urban planning, education, art, literature, science and theology has been criticised as "the reigning dogma of our time". [165] The argument suggests that applying a populist or market-driven approach to art and literature (for example), means that innovative creative work goes unpublished or unproduced. In education, the argument is that essential but more difficult studies are not undertaken. Science, as a truth-based discipline, is particularly corrupted by the idea that the correct conclusion can be arrived at by popular vote. However, more recently, theorists have also advanced the concept epistemic democracy to assert that democracy actually does a good job tracking the truth.
Robert Michels asserts that although democracy can never be fully realised, democracy may be developed automatically in the act of striving for democracy: "The peasant in the fable, when on his death-bed, tells his sons that a treasure is buried in the field. After the old man's death the sons dig everywhere in order to discover the treasure. They do not find it. But their indefatigable labor improves the soil and secures for them a comparative well-being. The treasure in the fable may well symbolise democracy." [166]
Dr. Harald Wydra, in his book Communism and The Emergence of Democracy (2007), maintains that the development of democracy should not be viewed as a purely procedural or as a static concept but rather as an ongoing "process of meaning formation". [167] Drawing on Claude Lefort's idea of the empty place of power, that "power emanates from the people [...] but is the power of nobody", he remarks that democracy is reverence to a symbolic mythical authority as in reality, there is no such thing as the people or demos . Democratic political figures are not supreme rulers but rather temporary guardians of an empty place. Any claim to substance such as the collective good, the public interest or the will of the nation is subject to the competitive struggle and times of for [ clarification needed ] gaining the authority of office and government. The essence of the democratic system is an empty place, void of real people which can only be temporarily filled and never be appropriated. The seat of power is there, but remains open to constant change. As such, what "democracy" is or what is "democratic" progresses throughout history as a continual and potentially never ending process of social construction. [ citation needed ]
In 2010 a study by a German military think-tank analyzed how peak oil might change the global economy. The study raises fears for the survival of democracy itself. It suggests that parts of the population could perceive the upheaval triggered by peak oil as a general systemic crisis. This would create "room for ideological and extremist alternatives to existing forms of government". [168]

See also
WebPage index: 00038
Wikipedia community
The Wikipedia community is the community of contributors to the online encyclopedia Wikipedia . Individual contributors are known as " Wikipedians ". The Oxford English Dictionary added the word "Wikipedian" in August 2012. [1]
Almost all Wikipedians are volunteers . With the increased maturity and visibility of Wikipedia, other categories of Wikipedians have emerged, such as Wikipedians in residence and students with assignments related to editing Wikipedia.

Size
Studies of the size of the community of Wikipedia showed an exponential growth in the number of Wikipedia contributors during the early years. In April 2008, writer and lecturer Clay Shirky and computer scientist Martin Wattenberg estimated the total time spent creating Wikipedia at roughly 100 million hours. [2] In November 2011, there were approximately 31.7 million registered user accounts across all language editions, of which around 270,000 were "active" (made at least one edit every month). [3]
The English Wikipedia , the largest language edition, currently has 132,963 editors who have performed an edit in the last 30 days ("active users"), and an unknown number of contributors without an account. About half of the active editors spend at least one hour a day editing, and a fifth spend more than three hours a day. [4]

Motivation
Various studies have been done with regard to the motivations of Wikipedia contributors. In a 2003 study of Wikipedia as a community, economics Ph.D. student Andrea Ciffolilli argued that the low transaction costs of participating in wiki software create a catalyst for collaborative development, and that a "creative construction" approach encourages participation. [5] A paper written by Andrea Forte and Amy Bruckman in 2005, called "Why Do People Write for Wikipedia? Incentives to Contribute to Open-Content Publishing", discussed the possible motivations of Wikipedia contributors. It applied Latour and Woolgar's concept of the cycle of credit to Wikipedia contributors, suggesting that the reason that people write for Wikipedia is to gain recognition within the community. [6]
Oded Nov, in his 2007 paper "What Motivates Wikipedians", related the motivations of volunteers in general to the motivations of people who contribute to Wikipedia. [7] Nov carried out a survey using the six motivations of volunteers, identified in an earlier paper. [8] The six motivations he used were:
To these six motivations he also added:
The survey found that the most commonly indicated motives were "fun", "ideology", and "values", whereas the least frequently indicated motives were "career", "social", and "protective". [7]
The Wikimedia Foundation has carried out several surveys of Wikipedia contributors and users. In 2008, the Wikimedia Foundation, alongside the Collaborative Creativity Group at UNU-Merit , launched a survey of readers and editors of Wikipedia. It was the most comprehensive survey of Wikipedia ever conducted. [9] The results of the survey were published two years later on March 24, 2010. [10] The Wikimedia Foundation began a process in 2011 of semi-annual surveys in order to understand Wikipedia editors more and better cater to their needs. [11] [12]
"Motivations of Wikipedia Content Contributors", a paper by Heng-Li Yang and Cheng-Yu Lai, hypothesised that, because contributing to Wikipedia is voluntary, an individual's enjoyment of participating would be the highest motivator. [13] However, their study showed that although people might initially start editing Wikipedia out of enjoyment, the most likely motivation for continuing to participate is self-concept based motivations such as "I like to share knowledge which gives me a sense of personal achievement." [13]
A further study in 2014 by Cheng-Yu Lai and Heng-Li Yang explored the reasons why people continue editing Wikipedia content. The study used authors of the English-language version of the site and received 288 valid online survey responses. Their results indicated and confirmed that subjective task value, commitment, and procedural justice were significant to satisfaction of Wikipedians; and satisfaction significantly influenced an author’s continued intention to edit Wikipedia content. [14]
Editors of Wikipedia have occasionally given personal testimonials of why they contribute to Wikipedia. A common theme of these testimonials is the enjoyment that editors seem to get from contributing to Wikipedia and being part of the Wikipedia community. Also mentioned is the potential addictive quality of editing Wikipedia. Gina Trapani of Lifehacker said "it turns out editing an article isn't scary at all. It's easy, surprisingly satisfying and can become obsessively addictive." [15] Jimmy Wales has also commented on the addictive quality of Wikipedia, saying "The main thing about Wikipedia ... is that it’s fun and addictive". [16] Wikipedians sometimes award one another " barnstars " for good work. These personalized tokens of appreciation reveal a wide range of valued work extending far beyond simple editing to include social support, administrative actions, and types of articulation work. The barnstar phenomenon has been analyzed by researchers seeking to determine what implications it might have for other communities engaged in large-scale collaborations. [17]

Media
Wikipedia has spawned several community news publications. An online newsletter, The Signpost , has been published weekly since 10 January 2005. [18] Professional cartoonist Greg Williams created a webcomic called " WikiWorld " which ran in The Signpost from 2006 to 2008. [19] A podcast called Wikipedia Weekly was active from 2006 to 2009 and sporadically thereafter, [20] [21] while a series of conference calls titled "Not the Wikipedia Weekly" ran from 2008 to 2009. [21] Some topic-specific communities within Wikipedia called "WikiProjects" have also distributed newsletters and other correspondence.

Socializing
Offline activities are organized by the Wikimedia Foundation or the community of Wikipedia.

Wikimania
Wikimania is an annual international conference for users of the wiki projects operated by the Wikimedia Foundation (such as Wikipedia and other sister projects ). Topics of presentations and discussions include Wikimedia Foundation projects, other wikis, open-source software , free knowledge and free content, and the different social and technical aspects which relate to these topics.

Wiknics

United States
The annual Great American Wiknic is a social gathering that takes place, in major cities of the United States, each year during the summer, usually just prior to the 4th of July. The Wiknic concept allows Wikipedians to bring together picnic food and to interact in a personal way. [22]

Criticism
Wikipedia has been subject to several kinds of criticism. [23] [24] For example, the Seigenthaler and Essjay incidents caused criticism of Wikipedia's reliability and usefulness as a reference. [25] [26] [27] The complaints related to the community include the effects of users' anonymity, the attitudes towards newcomers, the abuse of privileges by administrators , biases in the social structure of the community, in particular, gender bias and lack of female contributors, [28] and the role of the project's co-founder Jimmy Wales , in the community. [29] Sue Gardner , former executive director of the Wikimedia Foundation , described Wikipedians as being like a "crusty old desk guy who knows the style guide backwards." [30] A significant controversy was stirred with paid contributors to Wikipedia, which prompted the Wikimedia Foundation to send a cease and desist letter to the Wiki-PR agency. [31] Wikipedia relies on the efforts of its community members to remove vandalism from articles.
Wikipedia's co-founder Larry Sanger , who later founded Citizendium - a rival project, characterizes the Wikipedia community as ineffective and abusive, stating that "The community does not enforce its own rules effectively or consistently. Consequently, administrators and ordinary participants alike are able essentially to act abusively with impunity, which begets a never-ending cycle of abuse." [32] Oliver Kamm , of The Times , expressed skepticism toward Wikipedia's reliance on consensus in forming its content: "Wikipedia seeks not truth but consensus, and like an interminable political meeting the end result will be dominated by the loudest and most persistent voices." [33]

Recognition
A Wikipedia Monument was erected in Słubice, Poland in 2014 to honor the Wikipedia community. [34]
The 2015 Erasmus Prize was awarded to the Wikipedia community for "[promoting] the dissemination of knowledge through a comprehensive and universally accessible encyclopaedia. To achieve that, the initiators of Wikipedia have designed a new and effective democratic platform. The prize specifically recognizes Wikipedia as a community — a shared project that involves tens of thousands of volunteers around the world." [35]

See also
WebPage index: 00039
Hoxne Hoard
Coordinates : 52°20′N 1°11′E ﻿ / ﻿ 52.333°N 1.183°E ﻿ / 52.333; 1.183
The Hoxne Hoard ( / ˈ h ɒ k s ən / HOK -sən ) [2] is the largest hoard of late Roman silver and gold discovered in Britain, [3] and the largest collection of gold and silver coins of the fourth and fifth century found anywhere within the Roman Empire . Found by Eric Lawes, a metal detectorist in the village of Hoxne in Suffolk , England, on 16 November 1992, the hoard consists of 14,865 Roman gold, silver and bronze coins from the late fourth and early fifth centuries, and approximately 200 items of silver tableware and gold jewellery. [4] The objects are now in the British Museum in London, where the most important pieces and a selection of the rest are on permanent display. In 1993, the Treasure Valuation Committee valued the hoard at £ 1.75 million (today £3.21 million). [5]
The hoard was buried as an oak box or small chest filled with items in precious metal, sorted mostly by type with some in smaller wooden boxes and others in bags or wrapped in fabric. Remnants of the chest, and of fittings such as hinges and locks, were recovered in the excavation. The coins of the hoard date it after AD 407, which coincides with the end of Britain as a Roman province . [6] The owners and reasons for burial of the hoard are unknown, but it was carefully packed and the contents appear consistent with what a single very wealthy family might have owned. Given the lack of large silver serving vessels and of some of the most common types of jewellery, it is likely that the hoard represents only a part of the wealth of its owner.
The Hoxne Hoard contains several rare and important objects, including a gold body-chain and silver-gilt pepper-pots ( piperatoria ), including the Empress pepper pot . The Hoxne Hoard is also of particular archaeological significance because it was excavated by professional archaeologists with the items largely undisturbed and intact. The find helped to improve the relationship between metal detectorists and archaeologists, and influenced a change in English law regarding finds of treasure. [7]

Archaeological history

Discovery and initial excavation
The hoard was discovered in a field of a farm, about 2.4 kilometres (1.5 mi) southwest of the village of Hoxne in Suffolk , on 16 November 1992. Peter Whatling, the tenant farmer , had lost a hammer and asked his friend Eric Lawes, a retired gardener and amateur metal detectorist, to help look for it. [8] While searching the field with his metal detector, Lawes discovered silver spoons, gold jewellery and numerous gold and silver coins. After retrieving a few items, he and Whatling notified the landowners ( Suffolk County Council ), and the police, without attempting to dig out any more objects. [9]
The following day, a team of archaeologists from the Suffolk Archaeological Unit carried out an emergency excavation of the site. The entire hoard was excavated in a single day, with the removal of several large blocks of unbroken material for laboratory excavation. [10] The area within a radius of 30 metres (98 ft) from the find spot was searched using metal detectors. [11] Peter Whatling's missing hammer was also recovered and donated to the British Museum. [12] [13]
The hoard was concentrated in a single location, within the completely decayed remains of a wooden chest. [8] The objects had been grouped within the chest; for example, pieces such as ladles and bowls were stacked inside each other, and other items were grouped in a way consistent with being held within an inner box. [14] Some items had been disturbed by burrowing animals and ploughing, but the overall amount of disturbance was low. [15] It was possible to determine the original layout of the artefacts within the container, and the existence of the container itself, due to Lawes' prompt notification of the find, which allowed it to be excavated in situ by professional archaeologists. [9]
The excavated hoard was taken to the British Museum. The discovery was leaked to the press, and on 19 November, the Sun newspaper ran a front-page story, alongside a picture of Lawes with his metal detector. Although the full contents of the hoard and its value were still unknown, the newspaper article claimed that the hoard was worth £10 million. [8] In response to the unexpected publicity, the British Museum held a press conference at the museum on 20 November to announce the discovery. Newspapers lost interest in the hoard quickly, allowing British Museum curators to sort, clean and stabilize the hoard without further disruption from the press. [8] The initial cleaning and basic conservation of the hoard was completed within a month of its discovery. [10]

Inquest and valuation
On 3 September 1993, a Coroner's inquest was held at Lowestoft , and the hoard was declared a treasure trove , meaning that it was deemed to have been hidden with the intention of being recovered at a later date. Under English common law , anything declared as such belongs to the Crown if no one claims title to it. [17] However, at the time, the customary practice was to reward anyone who found and reported a treasure trove promptly with money equivalent to the market value of the treasure, with the money being provided by the national institution that wished to acquire the treasure. In November 1993, the Treasure Trove Reviewing Committee valued the hoard at £1.75 million (today £3.21 million), which was paid to Lawes, as finder of the treasure. He shared his reward with the farmer, Peter Whatling. [18] Three years later, the 1996 Treasure Act was enacted, which allowed for the finder, tenant and landowner to share in any reward. [19]

Subsequent archaeological investigations
In September 1993, after the field of the hoard find was ploughed, the Suffolk County Council Archaeological Service surveyed the field, finding four gold coins and 81 silver coins, all considered part of the same hoard. [20] Both earlier Iron Age and later mediaeval materials were also discovered, but there was no evidence of a Roman settlement in the vicinity. [11]
In 1994, in response to illegal metal detecting near the hoard find, a follow-up excavation of the field was carried out by the Suffolk County Council Archaeological Service. The hoard burial hole was re-excavated, and a single post hole at the southwest corner was identified; this may have been the location of a marker post to enable the depositors of the cache to locate and recover it in the future. [11] Soil was removed in 10 cm (3.9 in) spits for analysis in the area 1,000 square metres (11,000 sq ft) around the find spot, and metal detectors were used to locate metal artefacts. This excavation recovered 335 items datable to the Roman period, mostly coins but also some box fittings. A series of late Bronze Age or early Iron Age post holes, which may have formed a structure, were found. However, no structural features of the Roman period were detected. [11] [21]
The coins discovered during the 1994 investigation were spread out in an ellipse centred on the hoard find spot, running east–west up to a distance of 20 metres (66 ft) on either side. [22] This distribution can be explained by the fact that, in 1990, the farmer carried out deep ploughing in an east–west direction on the part of the field where the hoard was found. Previously (since 1967 or 1968, when the land was cleared for agricultural use), the farmer had ploughed in a north–south direction, but the absence of coins north and south of the find spot suggests that the ploughing before 1990 had not disturbed the hoard. [22]

Items discovered
The hoard is mainly made up of gold and silver coins and jewellery, amounting to a total of 3.5 kilograms (7.7 lb) of gold and 23.75 kilograms (52.4 lb) of silver. [23] It had been placed in a wooden chest, made mostly or entirely of oak , that measured approximately 60×45×30 cm (23.6×17.7×11.8 in). Within the chest, some objects had evidently been placed in smaller boxes made of yew and cherry wood, while others had been packed in with woollen cloth or hay. The chest and the inner boxes had decayed almost completely after being buried, but fragments of the chest and its fittings were recovered during the excavation. [24] The main objects found are:

Coins
The Hoxne Hoard contains 569 gold solidi , struck between the reigns of Valentinian I (364–75) and Honorius (393–423); 14,272 silver coins, including 60 miliarenses and 14,212 siliquae , struck between the reigns of Constantine II (337–40) and Honorius; and 24 bronze nummi . [4]
The most significant coin find from the end of Roman Britain, the hoard contains all major denominations of coinage of the time, and many examples of clipped silver coinage typical of late Roman Britain. The only find from Roman Britain with a larger number of gold coins was the Eye Hoard found in 1780 or 1781, for which there are poor records. [29] The largest single Romano-British hoard was the Cunetio Hoard , of 54,951 third-century coins, but these were debased radiates with little precious-metal content. The Frome Hoard , unearthed in Somerset in April 2010, contains 52,503 coins minted between 253 and 305, also mostly debased silver or bronze. [30] Larger hoards of Roman coins have been found at Misrata , Libya [31] and reputedly also at Evreux , France (100,000 coins) and Komin , Croatia (300,000 coins). [32]
The gold solidi are all close to their theoretical weight of 4.48 g ( 1 ⁄ 72 of a Roman pound). The fineness of a solidus in this period was 99% gold. The total weight of the solidi in the hoard is almost exactly 8 Roman pounds, suggesting that the coins had been measured out by weight rather than number. [33] Analysis of the siliquae suggests a range of fineness of between 95% and 99% silver, with the highest percentage of silver found just after a reform of the coinage in 368. [34] Of the siliquae , 428 are locally produced imitations, generally of high quality and with as much silver as the official siliquae of the period. However, a handful are cliché forgeries where a core of base metal has been wrapped in silver foil . [35]

Historical spread and minting
Coins are the only items in the Hoxne Hoard for which a definite date and place of manufacture can be established. All of the gold coins, and many of the silver coins, bear the names and portraits of the emperor in whose reign they were minted. Most also retain the original mint marks that identify where they were minted, illustrating the Roman system of regional mints producing coins to a uniform design. The coins' manufacture has been traced back to a total of 14 sources: Trier, Arles and Lyon (in Gaul ), Ravenna, Milan, Aquileia , Rome (in modern Italy); Siscia (modern Croatia), Sirmium (modern Serbia), Thessaloniki (Greece), Constantinople , Cyzicus , Nicomedia , and Antioch (modern Turkey). [37]
The coins were minted under three dynasties of Roman emperors. The earliest are the successors of the Constantinian dynasty , followed by the Valentinianic emperors , and finally the Theodosian emperors . The collegiate system of rule (or Consortium imperii ) meant that imperial partners would mint coins in each other's names at the mints under their jurisdiction. The overlapping reigns of Eastern and Western emperors often allow changes of type to be dated to within part of a reign. So the latest coins in the hoard, of Western ruler Honorius (393–423) and his challenger Constantine III (407–11), can be demonstrated to belong to the earlier parts of their reigns as they correspond to the lifetime of the Eastern Emperor Arcadius , who died in 408. [38] Thus, the coins provide a terminus post quem or earliest possible date for the deposition of the hoard of 408. [39]
The siliquae in the Hoard were struck mainly at Western mints in Gaul and Italy. It is unknown whether this is because coins from further East rarely reached Britain through trade, or because the Eastern mints rarely struck siliquae . [40] The production of coins seems to follow the location of the Imperial court at the time; for instance, the concentration of Trier coins is much greater after 367, perhaps associated with Gratian moving his court to Trier. [40]

Clipping of the silver coins
Almost every silver siliqua in the hoard has had its edge clipped to some degree. This is typical of Roman silver coin finds of this period in Britain, although clipped coins are very unusual through the rest of the Roman Empire. [42] The clipping process invariably leaves the imperial portrait on the front of the coin intact, but often damages the mint mark, inscription, and the image on the obverse. [42]
The possible reasons for the clipping of coins are controversial. Possible explanations include fraud, a deliberate attempt to maintain a stable ratio between gold and silver coins, or an official attempt to provide a new source of silver bullion while maintaining the same number of coins in circulation. [42]
The huge number of clipped coins in the Hoxne Hoard has made it possible for archaeologists to observe the process of coin-clipping in detail. The coins were evidently cut face-up to avoid damaging the portrait. The average level of clipping is roughly the same for coins dating from 350 onwards. [43]

Gold jewellery
All the jewellery in the hoard is gold, and all gold items in the hoard, other than coins, are jewellery. None of the jewellery is unequivocally masculine, although several pieces, like the rings, might have been worn by either gender. [46] There is one body chain, six necklaces, three rings, and nineteen bracelets. The total weight of the gold jewellery is about 1 kilogram (2.2 lb), [47] and the average metal content of the jewellery pieces is 91.5% gold (about 22 carat ), with small proportions in the metal of silver and copper . [48]
The most important gold item in the hoard is the body chain, which consists of four finely looped gold chains, made using the "loop-in-loop" method called "fox tail" in modern jewellery, and attached at front and back to plaques. [49] At the front, the chains have terminals in the shape of lions' heads and the plaque has jewels mounted in gold cells, with a large amethyst surrounded by four smaller garnets alternating with four empty cells, which probably held pearls that have decayed. At the back, the chains meet at a mount centred on a gold solidus of Gratian ( r. 375–383), which has been converted from an earlier use, probably as a pendant, and which may have been a family heirloom . [49] Body chains of this type appear in Roman art, sometimes on the goddess Venus or nymphs ; some examples have erotic contexts, but they are also worn by respectable high-ranking ladies. They may have been regarded as a suitable gift for a bride. [50] The Hoxne body chain, worn tightly, would fit a woman with a bust-size of 76–81 cm (30–32 in). [51] Few body chains have survived; one of the most complete, from the early Byzantine era and found in Egypt, is also in the British Museum. [52]
One of the necklaces features lion-headed terminals, and another includes stylized dolphins. The other four are relatively plain loop-in-loop chains, although one has a Chi-Rho symbol ( ☧ ) on the clasp, the only Christian element in the jewellery. [54] Necklaces of similar lengths would normally be worn in the Roman period with a pendant , but no pendants were found in the hoard. [55] The three rings were originally set with gems, which might have been natural gemstones, or pieces of coloured glass; however, these were taken from the rings before they were buried, perhaps for reuse. The rings are of similar design, one with an oval bezel , one with a circular bezel, and one with a large oblong bezel. [56] The 19 bracelets buried in the hoard include three sets of four matching gold bracelets. Though many similar bracelets have survived, sets of four are most unusual; they may have been worn two on each arm, or possibly were shared by two related women. [57] One set has been decorated by corrugating the gold with lateral and transverse grooves; the other two sets bear pierced-work geometric designs. Another five bracelets bear hunting scenes, common in Late Roman decorative art. Three have the designs executed in pierced-work, whereas two others are in repoussé . One bracelet is the sole gold item in the hoard to carry an inscription: it reads " VTERE FELIX DOMINA IVLIANE " in Latin , meaning "Use [this] happily, Lady Juliane". [57] The expression utere felix (or sometimes uti felix ) is the second most common inscriptional formula on items from Roman Britain, and is used to wish good luck, well-being and joy. [58] The formula is not specifically Christian, but it sometimes occurs in an explicitly Christian context, for example, together with a Chi-Rho symbol. [58]
The jewellery may have represented the "reserve" items rarely or never used from the collection of a wealthy woman or family. Some of the most common types of jewellery are absent; brooches, pendants, and ear-rings for example. Items set with gems are notably missing, although they were very much in the taste of the day. Catherine Johns, former Senior Curator for Roman Britain at the British Museum, speculates that the current or favourite jewellery of the owner was not included in the hoard. [59]

Silver items
The hoard contains about 100 silver and silver-gilt items; the number is imprecise because there are unmatched broken parts. They include a statuette of a leaping tigress, made as a handle for an object such as a jug or lamp; four pepper-pots ( piperatoria ); a beaker; a vase or juglet (a small jug); four bowls; a small dish; and 98 silver spoons and ladles. The beaker and juglet are decorated with similar leaf and stem patterns, and the juglet has three gilded bands. In contrast, the small bowls and dish are plain, and it is presumed that the owners of the Hoard had many more such items, probably including the large decorated dishes found in other hoards. [16] Many pieces are gilded in parts to accentuate the decoration. The technique of fire-gilding with mercury was used, [60] as was typical at the time. [61]

Piperatoria
The pepper-pots include one vessel, finely modelled after a wealthy or imperial lady, which soon became known as the "Empress" pepper-pot . [note 1] The woman's hair, jewellery, and clothing are carefully represented, and gilding is used to emphasize many details. She is holding a scroll in her left hand, giving the impression of education as well as wealth. Other pepper-pots in the hoard are modelled into a statue of Hercules and Antaeus , an ibex , and a hare and hound together. Not all such spice dispensers held pepper — they were used to dispense other spices as well — but are grouped in discussions as pepper-pots. Each of those found in this hoard has a mechanism in the base to rotate an internal disc, which controls the aperture of two holes in the base. When fully open, the containers could have been filled using a funnel; when part-open they could have been shaken over food or drink to add the spices.
Piperatorium is generally translated as pepper-pot , and black pepper is considered the most likely condiment these were used for. Pepper is only one of a number of expensive, high-status spices which these vessels might have dispensed, however. The piperatoria are rare examples of this type of Roman silverware, and according to Johns the Hoxne finds have "significantly expanded the date range, the typology and the iconographic scope of the type". [64] The trade and use of pepper in this period has been supported with evidence of mineralized black pepper at three Northern Province sites recovered in the 1990s, [note 2] [66] and from the Vindolanda tablets which record the purchase of an unspecified quantity of pepper for two denarii . [67] Archaeological sites with contemporary finds have revealed spices, including coriander , poppy , celery , dill , summer savory , mustard , and fennel . [66] [note 3]

Other silver pieces
The tigress is a solid-cast statuette weighing 480 grams (17 oz) and measuring 15.9 cm (6.3 in) from head to tail. She was designed to be soldered onto some other object as its handle; traces of tin were found beneath her rear paws, which have a "smoothly concave curve". [73] She looks most aesthetically pleasing when the serpentine curves of her head, back, rump, and tail form a line at an angle of about 45°, when the rear paws are flat, allowing for their curve. [74] Her gender is obvious as there are six engorged teats under her belly. She is carefully decorated on her back, but her underside is "quite perfunctorily finished". [75] Her stripes are represented by two engraved lines, with a black niello inlay between them, in most places not meeting the engraved lines. Neither her elongated body, nor the distribution of the stripes are accurate for the species; she has a long dorsal stripe running from the skull along the spine to the start of the tail, which is typical of tabby cats rather than tigers. The figure has no stripes around her tail, which thickens at the end, suggesting a thick fur tip as in a lion's tail, which tigers do not have, although Roman art usually gives them one. [75]
The large collection of spoons includes 51 cochlearia , which are small spoons with shallow bowls and long, tapering handles with a pointed end which was used to pierce eggs and spear small pieces of food—as the Romans did not use forks at the table. [76] There are 23 cigni , which are much rarer, having large rather shallow spoons with shorter, bird-headed handles; and about 20 deep round spoons or small ladles and strainer-spoons. Many are decorated with abstract motifs and some with dolphins or fanciful marine creatures. Many of the spoons are decorated with a Christian monogram cross or Chi-Rho symbol, and sometimes, also with the Greek letters alpha and omega (an appellation for Jesus , who is described as the alpha and omega in the Book of Revelation ). Three sets of ten spoons, and several other spoons, are decorated with such Christian symbols. As is often the case with Roman silver spoons, many also have a Latin inscription on them, either simply naming their owner or wishing their owner long life. In total, eight different people are named; seven on the spoons, and one on the single beaker in the hoard: Aurelius Ursicinus, Datianus, Euherius, Faustinus, Peregrinus, Quintus, Sanctus, and Silvicola. The most common name is "Aurelius Ursicinus", which occurs on a set of five cochlearia and five ladles. [77] It is unknown whether any of the people named in these inscriptions would have been involved in hiding the hoard or were even alive at the time it was buried.
Although only one of these inscriptions is explicitly Christian ( vivas in deo ), [78] inscriptions on silver spoons comprising a name followed by vivas or vivat usually can be identified as Christian in other late Roman hoards; for example the Mildenhall Treasure has five spoons, three with Chi-Rho monograms, and two with vivas inscriptions (PASCENTIA VIVAS and PAPITTEDO VIVAS). [79] The formula vir bone vivas also occurs on a spoon from the Thetford Hoard , but whereas the Thetford Hoard spoons have mostly pagan inscriptions (e.g. Dei Fau[ni] Medugeni "of the god Faunus Medugenus [the Mead begotten]"), [80] the Hoxne Hoard does not have any inscriptions of a specifically pagan nature, and the hoard may be considered to have come from a Christian household (or households). It often is assumed that Roman spoons with Chi-Rho monograms or the vivas in deo formula are either christening spoons (perhaps presented at adult baptism) or were used in the Eucharist ceremony, but that is not certain. [81]
There are also a number of small items of uncertain function, described as toiletry pieces. Some are picks, others perhaps scrapers, and three have empty sockets at one end, which probably contained organic material such as bristle , to make a brush. The size of these would be appropriate for cleaning the teeth or applying cosmetics, among other possibilities. [82]
The average purity of the silver items is 96%. The remainder of the metal is made up of copper and a small amount of zinc , with trace amounts of lead , gold, and bismuth present. The zinc is likely to have been present in a copper brass used to alloy the silver when the objects were made, and the lead, gold, and bismuth probably were present in the unrefined silver ore . [83]

Iron and organic materials
The iron objects found in the hoard are likely to all be from the remains of the outer wooden chest. These comprise large iron rings, double-spiked loops and hinges, strap hinges, probable components of locks, angle brackets, wide and narrow iron strips, and nails. [84]
Organic finds are rarely well documented with hoards, because most coin and treasure finds are removed hastily by the finder or have previously been disrupted by farm work rather than excavated. The Hoxne organic finds included bone, wood and other plant material, and leather. Small fragments from a decorated ivory pyxis (a cylindrical lidded box) were found, along with more than 150 tiny shaped pieces of bone inlay or veneer, probably from a wooden box or boxes that have decayed. Minuscule fragments of wood adhering to metal objects were identified as belonging to nine species of timber, all native to Britain: wood traces associated with the iron fittings of the outer chest established that it was made of oak. Silver locks and hinges were from two small wooden boxes or caskets, one made of decorative cherry wood and one made of yew. [85] Some wheat straw survived from padding between the plain silver bowls, which also bore faint traces of linen cloth. [86] Leather fragments were too degraded for identification.

Scientific analysis of finds
The initial metallurgical analysis of the hoard was carried out in late 1992 and early 1993 by Cowell and Hook for the procedural purposes of the coroner's inquest. This analysis used X-ray fluorescence , a technique that was applied again later to cleaned surfaces on specimens.
All 29 items of gold jewellery were analysed, with silver and copper found to be present. Results were typical for Roman silver in hoards of the period, in terms of the presence of copper alloyed with the silver to harden it, and trace elements. One repaired bowl showed a mercury-based solder . [60]
The large armlet of pierced gold ( opus interrasile ) showed traces of hematite on the reverse side, which probably would have been used as a type of jeweller's rouge . [87] This is the earliest known and documented use of this technique on Roman jewellery. [88] Gilt items showed the presence of mercury, indicating the mercury gilding technique. [60] The black inlay on the cast silver tigress shows the niello technique, but with silver sulphide rather than lead sulphide . [88] The settings of stones where garnet and amethyst remain, in the body chain, have vacant places presumed to be where pearls were set, and show elemental sulphur as adhesive or filler. [88]

Burial and historical background
The Hoxne Hoard was buried during a period of great upheaval in Britain, marked by the collapse of Roman authority in the province, the departure of the majority of the Roman army , and the first of a wave of attacks by the Anglo-Saxons . [89] Attacks on Italy by the Visigoths around the turn of the fifth century caused the general Stilicho to recall Roman army units from Rhaetia , Gaul , and Britannia . [90] While Stilicho held off the Visigoth attack, the Western provinces were left defenceless against Suebi , Alans , and Vandals who crossed the frozen Rhine in 406 and overran Gaul. The remaining Roman troops in Britain, fearing that the invaders would cross the Channel, elected a series of emperors of their own to lead the defence.
The first two such emperors were put to death by the dissatisfied soldiery in a matter of months, but the third, who would declare himself Constantine III , led a British force across the English Channel to Gaul in his bid to become Roman Emperor. After scoring victories against the "barbarians" in Gaul, Constantine was defeated by an army loyal to Honorius and beheaded in 411. [91] Meanwhile, Constantine's departure had left Britain vulnerable to attacks from Saxon and Irish raiders. [92]
After 410, Roman histories give little information about events in Britain. [93] Writing in the next decade, Saint Jerome described Britain after 410 as a "province fertile of tyrants", [94] suggesting the collapse of central authority and the rise of local leaders in response to repeated raids by Saxons and others. By 452, a Gaulish chronicler was able to state that some ten years previously "the Britons, which to this time had suffered from various disasters and misfortunes, are reduced by the power of the Saxons". [95]

Burial
Exactly who owned the Hoxne Hoard, and their reasons for burying it, are not known, and probably never will be. However, the hoard itself and its context provide some important clues. The hoard evidently was buried carefully, some distance from any buildings. [96] The hoard very likely represents only a portion of the precious-metal wealth of the person, or people, who owned it; many common types of jewellery are missing, as are large tableware items such as those found in the Mildenhall Treasure . It is unlikely that anyone would have possessed the rich gold and silver items found in the Hoxne Hoard without owning items in those other categories. Whoever owned the hoard also would have had wealth in the form of land, livestock, buildings, furniture, and clothing. At most, the Hoxne Hoard represents a moderate portion of the wealth of someone rich; conversely, it may represent a minuscule fraction of the wealth of a family that was incredibly wealthy. [97]
The appearance of the names "Aurelius Ursicinus" and "Juliane" on items in the Hoxne Hoard need not imply that people by those names owned the rest of the hoard, either at the time of its burial or previously. [98] [99] There are no historical references to an "Aurelius Ursicinus" in Britain in this period. While a "Marcus Aurelius Ursicinus" is recorded in the Praetorian Guard in Rome in the period 222–235, [100] a soldier or official of the late fourth or early fifth century would be more likely to take the imperial nomen Flavius, rather than Aurelius. This leads Tomlin to speculate "The name "Aurelius Ursicinus" might sound old-fashioned; it would certainly have been more appropriate to a provincial landowner than an army officer or government official". [100]
There are a number of theories about why the hoard was buried. One is that the hoard represented a deliberate attempt to keep wealth safe, perhaps in response to one of the many upheavals facing Roman Britain in the early fifth century. This is not the only hypothesis, however. [101] Archaeologist Peter Guest argues that the hoard was buried because the items in it were used as part of a system of gift-exchange, and as Britain separated from the Roman Empire, they were no longer required. [102] A third hypothesis is that the Hoxne Hoard represents the proceeds of a robbery, buried to avoid detection. [98]

Late Roman hoards
The Hoxne Hoard comes from the later part of a century ( c . 350–450) from which an unusually large number of hoards have been discovered, mostly from the fringes of the Empire. [104] Such hoards vary in character, but many include the large pieces of silver tableware lacking in the Hoxne Hoard: dishes, jugs and ewers, bowls and cups, some plain, but many highly decorated. [104] Two other major hoards discovered in modern East Anglia in the last century are from the fourth century; both are now in the British Museum. The Mildenhall Treasure from Suffolk consists of thirty items of silver tableware deposited in the late fourth century, many large and elaborately decorated, such as the "Great Dish". [105] The Water Newton Treasure from Cambridgeshire is smaller, but is the earliest hoard to have a clearly Christian character, apparently belonging to a church or chapel; [106] the assorted collection probably includes items made in Britain. [107] The Kaiseraugst Treasure from the site at Augusta Raurica in modern Switzerland (now in Basel ) contained 257 items, including a banqueting service with sophisticated decoration. [108] The Esquiline Treasure , found in Rome, evidently came from a wealthy Roman family of the late fourth century, and includes several large items, including the "Casket of Projecta". [109] Most of the Esquiline Treasure is in the British Museum, as are bowls and dishes from the Carthage Treasure which belonged to a known family in Roman Africa around 400. [110]
The Mildenhall, Kaiseraugst, and Esquiline treasures comprise large items of tableware. Other hoards, however, such as those found at Thetford and Beaurains consist mostly of coins, jewellery, and small tableware items; these two hoards probably are pagan votive offerings . [111] A hoard from Traprain Law in Scotland contains decorated Roman silver pieces cut up and folded, showing regard for the value of their metal alone, and may represent loot from a raid. [112]

Local context
Hoxne, where the hoard was discovered, is located in Suffolk in modern-day East Anglia . Although no large, aristocratic villa has been located in the Hoxne area, there was a Roman settlement nearby from the first through fourth centuries at Scole , about 3.2 km (2.0 mi) north–west of Hoxne, at the intersection of two Roman roads . One of these, Pye Road , (today's A140 ), linked Venta Icenorum ( Caistor St Edmund ) to Camulodunum ( Colchester ) and Londinium ( London ). [11] [113] [114]
The field in which the hoard was discovered was shown by the 1994 excavation to probably have been cleared by the early Bronze Age, when it began to be used for agriculture and settlement. Some settlement activity occurred near the hoard findspot by the first half of the first millennium BC, [22] but there is no evidence of Roman buildings in the immediate vicinity. The field where the hoard was deposited may have been in cultivation during the early phase of the Roman period but the apparent absence of fourth-century coins suggests that it may have been converted to pasture or else had reverted to woodland by that time. [22]
The Hoxne Hoard is not the only cache of Roman treasure to have been discovered in the area. In 1781 some labourers unearthed a lead box by the river at Clint Farm in Eye , 4.8 km (3.0 mi) south of Scole and 3.2 km (2.0 mi) south–west of Hoxne. The box contained about 600 Roman gold coins dating to the reigns of Valens and Valentinian I (reigned 364–375), Gratian (375–383), Theodosius I (378–395), Arcadius (395–408), and Honorius (393–423). [115] This was the largest hoard of Roman gold coins ever discovered in Britain, but the coins were dispersed during the 18th and 19th centuries, and cannot now be easily be identified in coin collections. [116] As a result, the relationship (if any) between the Eye hoard and that in Hoxne cannot be determined, even if the proximity suggests they may have been related. [117]
Soon after the Hoxne Hoard was discovered, there was speculation, based on the name "Faustinus" engraved on one of the spoons, that it may have come from the "Villa Faustini" that is recorded in Itinerary V of the Antonine Itinerary . [118] The exact location of Villa Faustini is unknown, but as it was the first station after Colchester, it is believed to have been somewhere on the Pye Road (modern A140) and one of the possible locations for it is the modern village of Scole, only a couple of miles from Hoxne. This early theory has since been rejected, however, because "Faustinus" was historically a common name, and it only occurs on a single spoon in the hoard. [118] Furthermore, the logic of using inscriptions on individual items in the hoard to determine ownership of the hoard as a whole is considered flawed. [99] Based on the dating of the coins in the hoard, the majority of which belong to the period 394–405, [119] it also has been speculated that the contents of the hoard originally belonged to a military family that accompanied Count Theodosius to Britain in 368–369, and which may have left with Constantine III in 407. [99]

Acquisition, display, and impact
The hoard was acquired by the British Museum in April 1994. [1] As the Museum's entire purchase fund amounted to only £1.4 million at the time, [23] the hoard had to be purchased with the assistance of donors that included the National Heritage Memorial Fund , the National Art Collections Fund (now the Art Fund), and the J. Paul Getty Trust . [1] The grants from these and other benefactors enabled the museum to raise the £1.75 million needed for the acquisition. [5] [18]
Items from the hoard have been on display almost continuously since the treasure was received at the British Museum. Some items were displayed at the Museum as early as September 1993 in response to public interest. Much of the hoard was exhibited at Ipswich Museum in 1994–1995. From 1997, the most important items went on permanent display at the British Museum in a new and enlarged Roman Britain gallery (Room 49), alongside the roughly contemporary Thetford Hoard, [1] and adjacent to the Mildenhall Treasure, which contains large silver vessels of types that are absent from the Hoxne Hoard. Some items from the Hoxne Hoard were included in Treasure: Finding Our Past , a touring exhibition that was shown in five cities in England and Wales in 2003. A perspex reconstruction of the chest and inner boxes in which it was deposited was created for this tour, showing the arrangement of the different types of items with sample items inside. It is now part of the permanent display in London, along with other items laid out more traditionally. [1]
The first comprehensive research on the Hoard was published in the full catalogue of the coins by Peter Guest in 2005, [120] and the catalogue of the other objects by Catherine Johns in 2010. [121] The hoard was third in the list of British archaeological finds selected by experts at the British Museum for the 2003 BBC Television documentary Our Top Ten Treasures , which included archive footage of its finder, Eric Lawes, [122] and the "Empress" pepper-pot was selected as item 40 in the 2010 BBC Radio 4 series A History of the World in 100 Objects . [3]
The discovery and excavation of the Hoxne Hoard improved the relationship between the archaeological profession and the community of metal detectorists. Archaeologists were pleased that Lawes reported the find promptly and largely undisturbed, allowing a professional excavation. Metal detectorists noted that Lawes' efforts were appreciated by the archaeological profession. [9] The 1996 Treasure Act is thought to have contributed to more hoards being made available to archaeologists. The act changed the law so that the owner of the land and the person who finds the hoard have a strong stake in the value of the discovery. [19] The manner of the finding of the Hoxne Hoard by metal detector, and its widespread publicity, contributed to changing the previous system of common law for dealing with treasure trove into a statutory legal framework that takes into account technology such as metal detectors, provides incentives for treasure hunters to report finds, and considers the needs of museums and scholars. [19] [123]

See also
WebPage index: 00040
IP address
An IP address (abbreviation of Internet Protocol address ) is an identifier assigned to each computer and other device (e.g., printer, router , mobile device , etc.) connected to a TCP/IP network [1] that is used to locate and identify the node in communications with other nodes on the network. IP addresses are usually written and displayed in human-readable notations, such as 172.16.254.1 in IPv4, and 2001:db8:0:1234:0:567:8:1 in IPv6.
Version 4 of the Internet Protocol (IPv4) defines an IP address as a 32-bit number. [1] However, because of the growth of the Internet and the depletion of available IPv4 addresses , a new version of IP ( IPv6 ), using 128 bits for the IP address, was developed in 1995, [2] and standardized as RFC 2460 in 1998. [3] Its deployment commenced in the mid-2000s and is ongoing.
The IP address space is managed globally by the Internet Assigned Numbers Authority (IANA), and by five regional Internet registries (RIR) responsible in their designated territories for assignment to end users and local Internet registries , such as Internet service providers . Addresses have been distributed by IANA to the RIRs in blocks of approximately 16.8 million addresses each. Each ISP or private network administrator assigns an IP address to each device connected to its network. Such assignments may be on a static (fixed or permanent) or dynamic basis, depending on its software and practices.

Role in Internet scheme
An IP address serves two principal functions: host or network interface identification and location addressing . Its role has been characterized as follows: "A name indicates what we seek. An address indicates where it is. A route indicates how to get there." [4]
The header of each IP packet sent over the Internet must contain the IP address of both the destination server or website and of the sender (the client ). The Domain Name System (DNS) translates domain names to the corresponding destination IP address, identifying the computer or device where the services or resources requested by a client are located. Both the source address and the destination address may be changed in transit by a network address translation device.
The sender's IP address is available to the server (which may log it or block it) and becomes the destination address when the server responds to a client request. Geolocation software can use a device's IP address to deduce its geolocation to determine the country [5] and even the city and post/ ZIP code , [6] organization, or user the IP address has been assigned to, and then to determine a device's actual location. A sender wanting to remain anonymous to the server may use a proxy server , which substitutes that server's IP address, as far as the destination server is aware, in place of the true source address. When the destination server responds to the proxy server, it would forward it on to the true client—ie., change the IP address to that of the originator of the request. A reverse DNS lookup involves the querying of DNS to determine the domain name associated with an IP address.

IP blocking and firewalls
The sender's IP address is available to the server which can use it in a variety of ways, such as IP address blocking using a firewall to control access to a website or network, or to selectively tailor the response to the client's request depending on criteria such as their location, besides other strategies. Whether using a blacklist or a whitelist , the IP address that is blocked is the perceived IP address of the client, meaning that if the client is using a proxy server or network address translation , blocking one IP address may block other, innocent clients.

IP address translation
Multiple client devices can appear to share an IP address, either because they are part of a shared hosting web server environment or because an IPv4 network address translator (NAT) or proxy server acts as an intermediary agent on behalf of the client, in which case the real originating IP address might be masked from the server receiving a request. A common practice is to have a NAT mask a large number of devices in a private network . Only the "outside" interface(s) of the NAT needs to have an Internet-routable address. [7]
Most commonly, the NAT device maps TCP or UDP port numbers on the side of the larger, public network to individual private addresses on the masqueraded network.
In small home networks, NAT functions are usually implemented in a residential gateway device, typically one marketed as a "router". In this scenario, the computers connected to the router would have private IP addresses and the router would have a public address to communicate on the Internet. This type of router allows several computers to share one public IP address.

IP versions
There are two versions of the Internet Protocol (IP): IP version 4 and IP version 6. Each version defines an IP address differently. Because of its prevalence, the generic term IP address typically still refers to the addresses defined by IPv4 . The gap in version sequence between IPv4 and IPv6 resulted from the assignment of number 5 to the experimental Internet Stream Protocol in 1979, which was never referred to as IPv5.

IPv4 addresses
An IP address in IPv4 is 32-bits in size, which limits the address space to 4 294 967 296 (2 32 ) IP addresses. Of this number, IPv4 reserves some addresses for special purposes such as private networks (~18 million addresses) or multicast addresses (~270 million addresses).
IPv4 addresses are usually represented in dot-decimal notation , consisting of four decimal numbers, each ranging from 0 to 255, separated by dots, e.g., 172.16.254.1. Each part represents a group of 8 bits ( octet ) of the address. In some cases of technical writing, IPv4 addresses may be presented in various hexadecimal , octal , or binary representations.

Subnetting
In the early stages of development of the Internet Protocol, [1] network administrators interpreted an IP address in two parts: network number portion and host number portion. The highest order octet (most significant eight bits) in an address was designated as the network number and the remaining bits were called the rest field or host identifier and were used for host numbering within a network.
This early method soon proved inadequate as additional networks developed that were independent of the existing networks already designated by a network number. In 1981, the Internet addressing specification was revised with the introduction of classful network architecture. [4]
Classful network design allowed for a larger number of individual network assignments and fine-grained subnetwork design. The first three bits of the most significant octet of an IP address were defined as the class of the address. Three classes ( A , B , and C ) were defined for universal unicast addressing. Depending on the class derived, the network identification was based on octet boundary segments of the entire address. Each class used successively additional octets in the network identifier, thus reducing the possible number of hosts in the higher order classes ( B and C ). The following table gives an overview of this now obsolete system.
Classful network design served its purpose in the startup stage of the Internet, but it lacked scalability in the face of the rapid expansion of the network in the 1990s. The class system of the address space was replaced with Classless Inter-Domain Routing (CIDR) in 1993. CIDR is based on variable-length subnet masking (VLSM) to allow allocation and routing based on arbitrary-length prefixes.
Today, remnants of classful network concepts function only in a limited scope as the default configuration parameters of some network software and hardware components (e.g. netmask), and in the technical jargon used in network administrators' discussions.

Private addresses
Early network design, when global end-to-end connectivity was envisioned for communications with all Internet hosts, intended that IP addresses be uniquely assigned to a particular computer or device. However, it was found that this was not always necessary as private networks developed and public address space needed to be conserved.
Computers not connected to the Internet, such as factory machines that communicate only with each other via TCP/IP, need not have globally unique IP addresses. Three non-overlapping ranges of IPv4 addresses for private networks were reserved in RFC 1918 . These addresses are not routed on the Internet and thus their use need not be coordinated with an IP address registry.
Today, when needed, such private networks typically connect to the Internet through network address translation (NAT).
Any user may use any of the reserved blocks. Typically, a network administrator will divide a block into subnets ; for example, many home routers automatically use a default address range of 192.168.0.0 through 192.168.0.255 (192.168.0.0/24).

IPv4 address exhaustion
There has been a higher than originally anticipated demand for IP addresses available for assignment to Internet service providers and end user organizations since the 1980s, leading to attempts to mitigate the effects of the shortage. IANA's primary address pool was exhausted on 3 February 2011, when the last five blocks were allocated to the five RIRs . [8] [9] APNIC was the first RIR to exhaust its regional pool on 15 April 2011, except for a small amount of address space reserved for the transition to IPv6, intended to be allocated in a restricted process. [10] Individual ISPs still had unassigned pools of IP addresses, and could recycle addresses no longer needed by their subscribers.

IPv6 addresses
The rapid exhaustion of IPv4 address space prompted the Internet Engineering Task Force (IETF) to explore new technologies to expand the addressing capability in the Internet. The permanent solution was deemed to be a redesign of the Internet Protocol itself. This new generation of the Internet Protocol was eventually named Internet Protocol Version 6 (IPv6) in 1995. [2] [3] The address size was increased from 32 to 128 bits (16 octets ), thus providing up to 2 128 (approximately 7038340299999999999♠ 3.403 × 10 38 ) addresses. This is deemed sufficient for the foreseeable future.
The intent of the new design was not to provide just a sufficient quantity of addresses, but also redesign routing in the Internet by more efficient aggregation of subnetwork routing prefixes. This resulted in slower growth of routing tables in routers. The smallest possible individual allocation is a subnet for 2 64 hosts, which is the square of the size of the entire IPv4 Internet. At these levels, actual address utilization rates will be small on any IPv6 network segment. The new design also provides the opportunity to separate the addressing infrastructure of a network segment, i.e. the local administration of the segment's available space, from the addressing prefix used to route traffic to and from external networks. IPv6 has facilities that automatically change the routing prefix of entire networks, should the global connectivity or the routing policy change, without requiring internal redesign or manual renumbering.
The large number of IPv6 addresses allows large blocks to be assigned for specific purposes and, where appropriate, to be aggregated for efficient routing. With a large address space, there is no need to have complex address conservation methods as used in CIDR.
All modern desktop and enterprise server operating systems include native support for the IPv6 protocol, but it is not yet widely deployed in other devices, such as residential networking routers, voice over IP (VoIP) and multimedia equipment, and network peripherals.

Private addresses
Just as IPv4 reserves addresses for private networks, blocks of addresses are set aside in IPv6. In IPv6, these are referred to as unique local addresses (ULA). RFC 4193 reserves the routing prefix fc00::/7 for this block which is divided into two /8 blocks with different implied policies. The addresses include a 40-bit pseudorandom number that minimizes the risk of address collisions if sites merge or packets are misrouted. [11]
Early practices used a different block for this purpose (fec0::), dubbed site-local addresses. [12] However, the definition of what constituted sites remained unclear and the poorly defined addressing policy created ambiguities for routing. This address type was abandoned and must not be used in new systems. [13]
Addresses starting with fe80:, called link-local addresses, are assigned to interfaces for communication on the attached link. The addresses are automatically generated by the operating system for each network interface. This provides instant and automatic communication between all IPv6 host on a link. This feature is required in the lower layers of IPv6 network administration, such as for the Neighbor Discovery Protocol .
Private address prefixes may not be routed on the public Internet.

IP subnetworks
IP networks may be divided into subnetworks in both IPv4 and IPv6 . For this purpose, an IP address is logically recognized as consisting of two parts: the network prefix and the host identifier , or interface identifier (IPv6). The subnet mask or the CIDR prefix determines how the IP address is divided into network and host parts.
The term subnet mask is only used within IPv4. Both IP versions however use the CIDR concept and notation. In this, the IP address is followed by a slash and the number (in decimal) of bits used for the network part, also called the routing prefix . For example, an IPv4 address and its subnet mask may be 192.0.2.1 and 255.255.255.0, respectively. The CIDR notation for the same IP address and subnet is 192.0.2.1/24, because the first 24 bits of the IP address indicate the network and subnet.

IP address assignment 
IP addresses are assigned to a host by the controlling Internet service provider or network administrator. IP addresses may be assigned either permanently by a fixed configuration of the hardware or software or it may take place anew at the time of booting. Persistent configuration is also known as a static IP address . In contrast, when a computer's IP address is assigned newly each time a reboot takes place, it is known as a dynamic IP address .

Methods
Static IP addresses are manually assigned to a computer or other device by an administrator. The exact procedure varies according to platform. This contrasts with dynamic IP addresses, which are assigned either by the computer interface or host software itself, as in Zeroconf , or assigned by a server using Dynamic Host Configuration Protocol (DHCP). Even though IP addresses assigned using DHCP may stay the same for long periods of time, they can generally change. In some cases, a network administrator may implement dynamically assigned static IP addresses. In this case, a DHCP server is used, but it is specifically configured to always assign the same IP address to a particular computer. This allows static IP addresses to be configured centrally, without having to specifically configure each computer on the network in a manual procedure.
In the absence or failure of static or stateful (DHCP) address configurations, an operating system may assign an IP address to a network interface using state-less auto-configuration methods, such as Zeroconf.

Uses of dynamic address assignment
IP addresses are most frequently assigned dynamically on LANs and broadband networks by DHCP. They are used because it avoids the administrative burden of assigning specific static addresses to each device on a network. It also allows devices to share the limited address space on a network if only some of them will be online at a particular time. In most current desktop operating systems, dynamic IP configuration is enabled by default so that a user does not need to manually enter any settings to connect to a network with a DHCP server. DHCP is not the only technology used to assign IP addresses dynamically. Dialup and some broadband networks use dynamic address features of the Point-to-Point Protocol .

Sticky dynamic IP address 
A sticky dynamic IP address is an informal term used by cable and DSL Internet access subscribers to describe a dynamically assigned IP address which seldom changes. The addresses are usually assigned with DHCP. Since the modems are usually powered on for extended periods of time, the address leases are usually set to long periods and simply renewed. If a modem is turned off and powered up again before the next expiration of the address lease, it will most likely receive the same IP address.

Address autoconfiguration
RFC 3330 defines an address block, 169.254.0.0/16, for the special use in link-local addressing for IPv4 networks. In IPv6 , every interface, whether using static or dynamic address assignments, also receives a local-link address automatically in the block fe80::/10.
These addresses are only valid on the link, such as a local network segment or point-to-point connection, that a host is connected to. These addresses are not routable and like private addresses cannot be the source or destination of packets traversing the Internet.
When the link-local IPv4 address block was reserved, no standards existed for mechanisms of address autoconfiguration. Filling the void, Microsoft created an implementation that is called Automatic Private IP Addressing ( APIPA ). APIPA has been deployed on millions of machines and has, thus, become a de facto standard in the industry. In RFC 3927 , the IETF defined a formal standard for this functionality, entitled Dynamic Configuration of IPv4 Link-Local Addresses .

Uses of static addressing
Some infrastructure situations have to use static addressing, such as when finding the Domain Name System (DNS) host that will translate domain names to IP addresses. Static addresses are also convenient, but not absolutely necessary, to locate servers inside an enterprise. An address obtained from a DNS server comes with a time to live , or caching time , after which it should be looked up to confirm that it has not changed. Even static IP addresses may change as a result of network administration ( RFC 2072 ).

Conflict
An IP address conflict occurs when two devices on the same local physical or wireless network claim to have the same IP address – that is, they conflict with each other. Since only one of the devices is supposed to be on the network at a time, the second one to arrive will generally stop the IP functionality of one or both of the devices. In many cases with modern Operating Systems , the Operating System will notify the user of one of the devices that there is an IP address conflict (displaying the symptom error message) [14] [15] and then either stop functioning on the network or function very poorly on the network. If one of the devices is the gateway, the network will be crippled. Since IP addresses are assigned by multiple people and systems in multiple ways, any of them can be at fault. [16] [17] [18] [19] [20]

Routing
IP addresses are classified into several classes of operational characteristics: unicast, multicast, anycast and broadcast addressing.

Unicast addressing
The most common concept of an IP address is in unicast addressing, available in both IPv4 and IPv6 . It normally refers to a single sender or a single receiver, and can be used for both sending and receiving. Usually, a unicast address is associated with a single device or host, but a device or host may have more than one unicast address. Some individual PCs have several distinct unicast addresses, each for its own distinct purpose. Sending the same data to multiple unicast addresses requires the sender to send all the data many times over, once for each recipient.

Broadcast addressing
In IPv4 it is possible to send data to all possible destinations ("all-hosts broadcast"), which permits the sender to send the data only once, and all receivers receive a copy of it. In the IPv4 protocol, the address 255.255.255.255 is used for local broadcast. In addition, a directed (limited) broadcast can be made by combining the network prefix with a host suffix composed entirely of binary 1s. For example, the destination address used for a directed broadcast to devices on the 192.0.2.0/24 network is 192.0.2.255. IPv6 does not implement broadcast addressing and replaces it with multicast to the specially-defined all-nodes multicast address.

Multicast addressing
A multicast address is associated with a group of interested receivers. In IPv4, addresses 224.0.0.0 through 239.255.255.255 (the former Class D addresses) are designated as multicast addresses. [21] IPv6 uses the address block with the prefix ff00::/8 for multicast applications. In either case, the sender sends a single datagram from its unicast address to the multicast group address and the intermediary routers take care of making copies and sending them to all receivers that have joined the corresponding multicast group.

Anycast addressing
Like broadcast and multicast, anycast is a one-to-many routing topology. However, the data stream is not transmitted to all receivers, just the one which the router decides is logically closest in the network. Anycast address is an inherent feature of only IPv6. In IPv4, anycast addressing implementations typically operate using the shortest-path metric of BGP routing and do not take into account congestion or other attributes of the path. Anycast methods are useful for global load balancing and are commonly used in distributed DNS systems.

Public address
A public IP address, in common parlance, is a globally routable unicast IP address, meaning that the address is not an address reserved for use in private networks , such as those reserved by RFC 1918 , or the various IPv6 address formats of local scope or site-local scope, for example for link-local addressing . Public IP addresses may be used for communication between hosts on the global Internet.

Diagnostic tools
Computer operating systems provide various diagnostic tools to examine their network interface and address configuration. Windows provides the command-line interface tools ipconfig and netsh and users of Unix-like systems can use ifconfig , netstat , route , lanstat , fstat , or iproute2 utilities to accomplish the task.

See also
WebPage index: 00041
University of Minnesota
The University of Minnesota, Twin Cities (often referred to as The University of Minnesota , Minnesota , the U of M , UMN , or simply the U ) is a public research university in Minneapolis and Saint Paul, Minnesota . The Minneapolis and St. Paul campuses are approximately 3 miles (4.8 km) apart, and the Saint Paul campus is actually in neighboring Falcon Heights . [7] It is the oldest and largest campus within the University of Minnesota system and has the sixth-largest main campus student body in the United States, with 51,147 students in 2013–14. The university is the flagship institution of the University of Minnesota system, and is organized into 19 colleges and schools, with sister campuses in Crookston , Duluth , Morris , and Rochester .
Minnesota is one of America's Public Ivy universities, which refers to top public universities in the United States capable of providing a collegiate experience comparable with the Ivy League . Founded in 1851, The University of Minnesota is categorized as an R1 Doctoral University with the highest research activity in the Carnegie Classification of Institutions of Higher Education . [8] Minnesota is a member of the Association of American Universities and is ranked 14th in research activity with $881 million in research and development expenditures in the fiscal year ending June 30, 2015. [9]
Minnesota faculty, alumni, and researchers have won 25 Nobel Prizes [10] and three Pulitzer Prizes . [11] Notable University of Minnesota alumni include two Vice Presidents of the United States, Hubert Humphrey and Walter Mondale , and Bob Dylan , who received the 2016 Nobel Prize in Literature.

Academics
The University of Minnesota Twin Cities is also a member of the Association of American Universities [12] which is an association of the 62 leading research universities in the United States and Canada.

Organization and administration
The University has 19 colleges, schools, and other major academic units: [13]

Institutes and centers
The University has six University-wide interdisciplinary centers and institutes whose work crosses collegiate lines: [14]

Rankings

Global
In 2016, Minnesota was ranked 33rd in the world by the Academic Ranking of World Universities (ARWU) . In its 2017 edition, U.S. News & World Report ranked Minnesota 38th in their "Best Global University Rankings". [23] The Times Higher Education World University Rankings for 2015 ranks Minnesota 46th in the world. [24] The Center for World University Rankings (CWUR) ranked the university 45th in the world based on quality of education, alumni employment, quality of faculty, publications, influence, citations, broad impact, and patents in 2016. [25] In 2016, the Nature Index ranked Minnesota 34th in the world based on research publication output in top tier academic journals in the life sciences, chemistry, earth and environmental sciences and physical sciences based on publication data from 2015. [26] In 2015, Academic Ranking of World Universities ranked the university 11th in the world for mathematics. [27]

National
The University of Minnesota is ranked 14 over-all among the nation's top research universities by the Center for Measuring University Performance . [28] The U.S. News & World Report 's 2016 rankings placed the undergraduate program of the University as the 69th-best National University in the United States. It also ranked the Chemical Engineering program third-best, the Doctor of Pharmacy (PharmD) program third best, the Economics PhD program tenth, Psychology eighth, Statistics sixteenth, Audiology ninth, and the University of Minnesota Medical School 6th for primary care and 34th for research. [29] The Law School , consistently recognized as a 'Top Law School' by U.S. News & World Report , is ranked 20th in the nation, and is a national leader in commercial law , international law , and clinical education. [30] Additionally, nineteen of the University's graduate-school departments have been ranked in the nation's top-twenty by the U.S. National Research Council . [31] In both 2008 and 2012 U.S. News & World Report ranked the College of Pharmacy 2nd in the nation. 2016 U.S. News & Report now rank the College of Pharmacy 2nd in the nation. [32] In 2011, U.S. News & World Report ranked the School of Public Health 8th in the nation, [33] which is home to the 2nd ranked program for the Master of Healthcare Administration degree. [34] The University of Minnesota ranked 19th in NIH funding in 2008. [35] Minnesota is listed as a " Public Ivy " in 2001 Greenes' Guides The Public Ivies: America's Flagship Public Universities . [36]

Discoveries and innovation
The university also became a member of the Laser Interferometer Gravitational-wave Observatory (LIGO) in 2007, and has led data analysis projects searching for gravitational waves – the existence of which were confirmed by scientists in February 2016. [37]
The university developed Gopher , [38] a precursor to the World Wide Web which used hyperlinks to connect documents across computers on the internet. However, the version produced by CERN was favored by the public since it was freely distributed and could more easily handle multimedia webpages. [39] The University also houses the Charles Babbage Institute , a research and archive center specializing in computer history. The department has strong roots in early days of supercomputing with Seymour Cray of Cray supercomputers. [40] Notable faculty of the department are Yousef Saad , Vipin Kumar, Jaideep Srivastava, John Riedl , and Joseph Konstan. Some notable alumni of the department are Ed Chi , Imrich Chlamtac , Leah Culver , Jeff Dean , Mark P. McCahill , Arvind Mithal , and Calvin Mooers .

Campuses

Demographics
As the largest of five campuses across the University of Minnesota system, its more than 50,000 students make it the sixth largest campus student body in the US overall. It has more than 300 research, education, and outreach centers and institutes, on everything from the life sciences to public policy and technology. [37]
The University offers 143 undergraduate degree programs [41] and 200 graduate degree programs. [42] The University has all three branches of the Reserve Officer Training Corps (ROTC). [43] The University of Minnesota Twin Cities as well as its sister campuses at Crookston , Duluth , and Morris are accredited by the Higher Learning Commission (HLC) of the North Central Association of Colleges and Schools . [44]
The racial/ethnic breakdown of the student population is: 65.3% White, 12.7% International Students (that are undesignated race/ethnicity), 9.2% Asian, 4.3% Black, 3.1% Hispanic/Latino, 1.2% American/Native American Indian, and 4.2% Unknown. 63% of matriculants to the university are considered Minnesota residents, and 37% of matriculants are considered out-of-state residents. [45] According to the University Office of Institutional Research, as of Fall 2015, there are 30,511 undergraduates at the University of Minnesota Twin Cities campus. Of that number, 5,771 are first-time degree seeking freshmen. There are 12,659 graduate students.

Minneapolis campus
The original Minneapolis campus overlooked the Saint Anthony Falls on the Mississippi River , but it was later moved about a mile (1.6 km) downstream to its current location. The original site is now marked by a small park known as Chute Square at the intersection of University and Central Avenues. The school shut down following a financial crisis during the American Civil War, but reopened in 1867 with considerable financial help from John S. Pillsbury . It was upgraded from a preparatory school to a college in 1869. Today, the University's Minneapolis campus is divided by the Mississippi River into an East and West Bank.
The campus now has buildings on both river banks. The " East Bank ", the main portion of the campus, covers 307 acres (124 ha). The West Bank is home to the University of Minnesota Law School , the Humphrey School of Public Affairs , the Carlson School of Management , various social science buildings, and the performing arts center. The St. Paul campus is home to the College of Biological Sciences (CBS), the College of Design (CDes), the College of Food, Agriculture and Natural Resource Sciences (CFANS), and the veterinary program.

East Bank
To help ease navigation of the large campus, the University has divided the East Bank into several areas: the Knoll area , the Mall area , the Health area , the Athletic area , and the Gateway area .
The Knoll area , the oldest part of the University's current location, is located in the northwestern part of the campus. [46] Most disciplines in this area relate to the humanities . Burton Hall is home to the College of Education and Human Development . Many buildings in this area are well over 100 years old; a 13-building group comprises the Old Campus Historic District that is on the U.S. National Register of Historic Places . [47] A residence hall, Sanford Hall, and a student-apartment complex, Roy Wilkins Hall, are located in this area. The Institute for Advanced Study [48] is located in the Nolte Center. This area is located just south of the Dinkytown neighborhood and business area.
Northrop Mall , or the Mall area , is arguably the center of the Minneapolis campus. It was based on a design by Cass Gilbert , although his plans were too extravagant to be fully implemented. [49] Several of the campus's primary buildings surround the Mall area. The Cyrus Northrop Memorial Auditorium provides a northern anchor, with Coffman Memorial Union (CMU) to the south. Four of the larger buildings to the sides of it are the primary mathematics , physics , and chemistry buildings, (Vincent Hall, Tate Laboratory and Smith Hall, respectively) and Walter Library . The Mall area is home to both the College of Liberal Arts , which is Minnesota's largest public or private college, and the College of Science and Engineering . Behind CMU is another residence hall, Comstock Hall, and another student-apartment complex, Yudof Hall.
The Health area is to the southeast of the Mall area and focuses on undergraduate buildings for biological-science students, as well as homes to the College of Pharmacy , the School of Nursing , the School of Dentistry , the Medical School , the School of Public Health , and Fairview Hospitals and Clinics . This complex of buildings forms what is known as the University of Minnesota Medical Center . Part of the College of Biological Sciences is housed in this area.
Across the street from Fairview Hospital is an area known as the " Superblock ". The Superblock is a four-city-block space housing four residence halls (Pioneer, Frontier, Centennial and Territorial Halls). The Superblock is one of the most popular locations for on-campus housing because it has the largest concentration of students living on campus and has a multitude of social activities between the residence halls.
The Athletic area is directly north of the Superblock and includes four recreation/athletic facilities: the University Recreation Center, Cooke Hall, the University Fieldhouse, and the University Aquatic Center . These facilities are all connected by tunnels and skyways allowing students to use one locker-room facility. North of this complex is the TCF Bank Stadium , Williams Arena , Mariucci Arena , Ridder Arena , and the Baseline Tennis Center .
The Gateway area , an easternmost section, is primarily office buildings instead of classrooms and lecture halls. The most prominent building is McNamara Alumni Center . The University is also heavily invested in a biomedical-research initiative and has built five biomedical-research buildings that form a biomedical complex directly north of TCF Bank Stadium.

Notable architecture
The Armory , northeast of the Northrop Mall, is built like a Norman castle , with a sally-port entrance facing Church Street, and a tower originally intended to be the Professor of Military Science 's residence, until it was found to be too cold. It originally held the athletics department as well as the military-science classes that it now holds.
One of the oldest buildings on campus is Pillsbury Hall , designed in the Richardsonian Romanesque style and built using varieties of sandstone available in Minnesota. It has a unique color that is hard to capture in a photograph. Many of the buildings on the East Bank campus were designed by the prolific Minnesota architect Clarence Johnston , including the Jacobean Folwell Hall and the Roman Renaissance Walter Library , which he considered the heart of the university.
In more recent times, Frank Gehry designed the Frederick R. Weisman Art Museum . It is a typical example of his work with curving metallic structures.
Another new building is the addition to the Architecture building designed by Steven Holl and completed in 2002. It won an American Institute of Architects award for its innovative design. The Architecture building was then renamed Rapson Hall after the local modernist architect and School of Architecture Dean Ralph Rapson .
The University also has historic fraternities and sororities buildings (a "Greek row") north of Northrop Mall on University Avenue SE.

West Bank
The West Bank covers 53 acres (21 ha). The West Bank Arts Quarter includes:
The Quarter is home to several annual interdisciplinary arts festivals.
The Social Sciences are also on the West Bank and include the Carlson School of Management , the Law School , and the Hubert H. Humphrey School of Public Affairs .
Wilson Library, the largest library in the University system, is also located on the West Bank as is Middlebrook Hall, the largest residence hall on campus. Approximately 900 students reside in the building named in honor of William T. Middlebrook.

Getting around
The Washington Avenue Bridge crossing the Mississippi River provides access between the East and West Banks, either on foot, designated bike lanes, or via free shuttle service. The bridge has two separate decks : the lower deck for vehicles and the newly constructed light rail, and the upper deck for pedestrian and bicycle traffic. An unheated enclosed walkway runs the length of the bridge and shelters students from the weather. Walking and riding bicycles are the most common modes of transportation among students. University Police occasionally cite individuals for jaywalking as well as riding bicycles on restricted sidewalk areas in areas surrounding the University resulting in fines as high as $250. This is often done at the beginning of a school year or after pedestrians interfere with traffic. [50]
There are some pedestrian tunnels to get from building to building during harsh weather; they are marked with signs reading " The Gopher Way ".
The Minneapolis campus is located near Interstates 94 and 35W and is bordered by the Minneapolis neighborhoods of Dinkytown (on the north), Cedar-Riverside (on the west), Stadium Village (on the southeast), and Prospect Park (on the east).
Three light-rail stations serve the University along the Green Line . The stations include Stadium Village , the East Bank , and the West Bank . The university partnered with Metro to offer students, staff, and faculty members a Campus Zone Pass that enables free travel between the three stations that pass through campus. [51]

St. Paul campus
The St. Paul campus is in the city of Falcon Heights , about 3 miles (4.8 km) away from the Minneapolis campus. The default place name for the ZIP code serving the campus is "St. Paul", but "Falcon Heights" is also recognized for use in the street addresses of all campus buildings. The College of Food, Agricultural and Natural Resource Sciences , including the University of Minnesota Food Industry Center and many other disciplines from social sciences to vocational education are located on this campus. This also includes the College of Continuing Education, [52] College of Veterinary Medicine, [53] and the College of Biological Sciences. [54] The extensive lawns, flowers, trees, woods, and the surrounding University research farm plots creates a greener and quieter campus. It has a grassy mall of its own and can be a bit of a retreat from the more-urban Minneapolis campus. Prominent on this campus is Bailey Hall, the St. Paul campus's only residence hall. There are campus connectors running every 5 minutes on the weekdays when school is in session, and every 20 minutes on weekends, allowing students easy access to both campuses.
The Continuing Education and Conference Center, [55] which serves over 20,000 conference attendees per year, is also located on the St. Paul campus.
The St. Paul campus is home to the College of Design's department of Design, Housing, and Apparel (DHA). Located in McNeal Hall, DHA includes the departmental disciplines of Apparel Design, Graphic Design, Housing Studies, Interior Design, and Retail Merchandising.
The St. Paul campus is known to University students and staff for the Dairy Salesroom, [56] which sells food (including ice cream) produced in the University's state-certified dairy plant by students, faculty and staff, and the similar Meat Sales Room. [57]
The St. Paul campus borders the Minnesota State Fairgrounds , which hosts the largest state fair in the United States by daily attendance. [58] The fair lasts twelve days, from late August through Labor Day in early September. The grounds also serve a variety of functions during the rest of the year.
Although the Falcon Heights area code is 651, the University telephone system trunk lines use Minneapolis exchanges and its 612 area code.

Commuting between Minneapolis and St. Paul campuses
During the school year on regular weekdays, the Campus Connectors operate with schedule-less service as often as every five minutes during the busiest parts of the school day between 7:00am and 5:30pm, slowing to once every 15 or 20 minutes during earlier or later hours. [59] In 2008, the system carried 3.55 million riders. Despite the fact that the shuttle service is free, it is comparatively inexpensive to operate: with an operating cost of $4.55 million in 2008, the operating subsidy was only $1.28 per passenger. Even Metro Transit 's busy METRO Blue Line light rail required a subsidy of $1.44 that year, and that was with many riders paying $1.75 or more for a ride. [60]

Campus safety
The Step Up campaign is a program that helps students do the right thing and prevent crimes, sexual assault, and excessive drinking by teaching students how to intervene and prevent in a positive way. [61] They do this by explaining the Bystander effect . The U of M also has a TXT-U emergency notification text messaging system that sends out a notification to all faculty, staff, and students in case of emergency. [62] Similarly, there are different resources which students are able to get help while getting home. 624-WALK, an escort to walk to adjacent campuses and neighborhoods, and Gopher Chauffeur, a van service that offers rides near and on campus. Both of these are free and open to all students, staff, and faculty. [63]
In addition, there are almost 200 AED's on campus and 200 yellow phones for emergency only calls. The University Police Station has 20 Code Blue Phones around campus that immediately connect people to their office. There are also over 2,000 security cameras being monitored 24 hours a day. [63]

Sexual assaults
More than 1,000 sexual assaults on campus were reported between the years 2010 and 2015. [64] The total number of prosecutions for rape was zero [64] until the conviction of Daniel Drill-Mellum in 2016, for the rapes of two fellow students. [65] Of the sexual assaults on campus, few are reported to university police. [66] Six resulted in arrest from 2010 to 2015; one was determined to be unfounded. [66] In a study by campus police, in the years between 2005 and 2015, sexual assaults at the university either remained the same or increased [67] despite six sexual assault resources and many anti-crime programs on campus. [68] [69] [70] [71]

Environmental record
Another building that has won an award is the new Science Teaching and Student Services Building (STSS), renamed as the Robert H. Bruininks Hall on May 1, 2015. This building has been awarded the prestigious LEED Gold certification. LEED, or Leadership in Energy and Environmental Design, is an internationally recognized green building certification system administered by the U.S. Green Building Council. LEED measures multiple dimensions of a building's design and construction including sustainable sites, energy and atmosphere, materials and resources, water efficiency, and indoor environmental quality.
"It's appropriate that a building that supports science education and overlooks a great river would be built with principles of sustainability at the forefront," said, U of M President Robert Bruininks at the opening of STSS in August 2010.
Highlights of sustainability in STSS include:

Big Ten Academic Alliance
The University of Minnesota is a participant in the Big Ten Academic Alliance . The Big Ten Academic Alliance (BTAA) is the academic consortium of the universities in the Big Ten Conference . Engaging in $10 billion in research in 2014-2015, BTAA universities provide powerful insight into important issues in medicine, technology, agriculture, and communities. Students at participating schools are also allowed "in-house" borrowing privileges at other schools' libraries. [72] The BTAA uses collective purchasing and licensing, and has saved member institutions $19 million to date. [73] Course sharing, [74] professional development programs, [75] study abroad and international collaborations, [76] and other initiatives are also part of the BTAA.

Student life and traditions

Greek life, professional and honor societies
The number of fraternities and sororities at the University of Minnesota is extensive. Including defunct branches, the Greek System numbers more than 200 organizations. More than half of these remain active today, whose pioneers have had a presence on the Twin Cities campus for over 140 years. [77] The University's Greek societies include the highly visible residential Academic and Social chapters. Although, membership in the Greek System has also extended to a wide variety of different types. This has included traditionally unrelated or unaligned Professional Fraternities , Honor Societies and Service Fraternities . Many of these built and occupy historically significant "Fraternity Row" homes along University Ave. SE, 10th Ave. SE, 4th Street SE, and 5th Street SE, all in Minneapolis, or along Cleveland Ave. near the St. Paul campus. [78]
According to self-produced studies from the University of Minnesota Greek Society and the Greek Society office, Greek System participants are more likely to graduate than the average student, are most likely to graduate with a higher GPA, and Greek alumni contribute more money than their percentage of population. [79] [80] [81] Although significant differences exist in the average GPAs, participation rates and contributions between professional, academic, honorific societies and those of the traditional social fraternities. [80] As of June, 2014, approximately 2,800 system members made up about 8% of the campus population. Minnesota hosts 38 academic fraternities, 20 academic sororities, 56 honors societies, 31 professional societies, and two service-focused chapters. [82] [83]

Media

Print
The Minnesota Daily is published only twice a week during the normal school season as of the Fall 2016 semester. [84] It is printed once each week during the summer. The Daily is operated by an autonomous organization run entirely by students. It was first published on May 1, 1900. Outside of every day news coverage the paper has also published special issues such as the Grapevine Awards, Ski-U-Mah, the Bar & Beer Guide, Sex-U-Mah, and others.
A long-defunct but fondly remembered humor magazine, Ski-U-Mah , was published from about 1930 to 1950. It launched the career of novelist and scriptwriter Max Shulman .
A relative newcomer to the University's print-media community is The Wake Student Magazine , a weekly magazine that covers University-related stories and provides a forum for student expression. It was founded in November 2001 in an effort to diversify campus media and achieved student group status in February 2002. [85] Students from many disciplines do all of the reporting, writing, editing, illustration, photography, layout and business management for the publication. The magazine was founded by James DeLong and Chris Ruen. [86] The Wake was named the nation's best campus publication (2006) by the Independent Press Association. [85]
Additionally, the Wake publishes Liminal , a literary journal that began in 2005. Liminal was created in the absence of an undergraduate literary journal and continues to bring poetry and prose to the University community.
The Wake has faced a number of challenges during its existence, due in part to the reliance on student fees funding. In April 2004, the needed $60,000 in funding was restored, which allowed for the magazine's continued existence after the Student Services Fees Committee had initially declined to fund it. [86] They faced further challenges in 2005 when their request for additional funding to publish weekly was denied [87] and then partially restored. [88]
In 2005 conservatives on campus began formulating a new monthly magazine named The Minnesota Republic . The first issue was released in February 2006, and funding by student service fees started in September 2006.

Radio
The campus radio station, KUOM "Radio K", broadcasts an eclectic variety of independent music during the day on 770 kHz AM . Its 5,000-watt signal has a range of 80 miles (130 km), but shuts down at dusk because of Federal Communications Commission regulations. In 2003, the station added a low-power (8-watt) signal on 106.5 MHz FM overnight and on weekends. In 2005, a 10-watt translator began broadcasting from Falcon Heights on 100.7 FM at all times. Radio K also streams its content at www.radiok.org. With roots in experimental transmissions that began before World War I, the station received the first AM broadcast license in the state on January 13, 1922, and began broadcasting as WLB , changing to the KUOM call sign about two decades later. The station had an educational format until 1993 when it merged with a smaller campus-only music station to become what is now known as Radio K. A small group of full-time employees are joined by over 20 part-time student employees who oversee the station. Most of the on-air talent consists of student volunteers.

Television
Some television programs made on campus have been broadcast on local PBS station KTCI channel 17. Several episodes of Great Conversations have been made since 2002, featuring one-on-one discussions between University faculty and experts brought in from around the world. Tech Talk is a show meant to help people who feel intimidated by modern technology, including cellular phones and computers.

Mobile
There are multiple ways in which students can connect with the university via their devices. Free and secured wireless internet is abundantly available throughout the campus to any individual with valid University credentials. [89] In addition, there are applications that can be downloaded that relate to the university and surrounding areas. Such applications include Groupon , Foursquare , and ByMe, an application exclusive to the University of Minnesota area.

Minnesota Student Association
The Minnesota Student Association (MSA) is the undergraduate student government at the University of Minnesota. It advocates for student interests on local, state, and federal levels, and focuses on efforts that directly benefit student population.
"Gopher Chauffeur," originally titled the MSA Express, is a student-operated late night ride service . [90] Piloted by MSA, the 2007–2008 administration of Emma Olson and Ross Skattum began the process of transitioning the service to the University's Boynton Health Services. [91] This was done to ensure its longevity. Student response was overwhelmingly positive, [92] and the program was expanded in recent years due to campus safety concerns. [93]
MSA was instrumental in passing legislation in the 2013 Minnesota legislature for medical amnesty, and has focused more heavily on legislative advocacy in recent years. [94]

Graduate and Professional Student Assembly
The Graduate and Professional Student Assembly (GAPSA) is responsible for graduate and professional student governance at the University of Minnesota. It is the largest and most comprehensive graduate/professional student governance organization in the United States. GAPSA serves students in the Carlson School of Management , the Dental School, the Graduate School, the Law School , the Medical School , the School of Nursing, the College of Pharmacy, the School of Public Health, the College of Veterinary Medicine, and the College of Education and Human Development . GAPSA is also a member of the National Association of Graduate-Professional Students.
The University of Minnesota has the second largest number of graduate and professional students in the United States at over 16,000. All registered graduate and professional students at the University of Minnesota are members of GAPSA. It was established in 1990 as a non-profit (IRS 501 (c)(3)) confederation of independent college councils representing all graduate and professional students at the University of Minnesota to the Board of Regents, the President of the University, the University Senate, the University at large and wider community. GAPSA serves as a resource for member councils, as the primary contact point for administrative units, as a graduate and professional student policy-making and policy-influencing body, and as a center of intercollegiate and intra-collegiate interaction among students.

Athletics
Minnesota's athletic teams at the Twin Cities campus are known as the Minnesota Golden Gophers and compete in the NCAA 's Division I as members of the Big Ten Conference . They have won 20 national championships as of 2015. [95]
The University's intercollegiate sports teams are called the Golden Gophers and are members of the Big Ten Conference and the Western Collegiate Hockey Association (WCHA) in the National Collegiate Athletic Association (NCAA). Since the 2013–14 school year, the only Minnesota team that does not compete in the Big Ten is the women's ice hockey team , which competes in the WCHA. The Gophers men's ice hockey team was a longtime WCHA member, but left when the Big Ten began operating a men's ice hockey league with six inaugural members. The current athletic director is Mark Coyle who took the position from interim athletic director Beth Goetz , after Norwood Teague resigned in August 2015 amid sexual assault allegations. Teague replaced Joel Maturi .
The Golden Gophers' most notable rivalry is the annual college football game against the Wisconsin Badgers ( University of Wisconsin–Madison , Madison , Wisconsin) for Paul Bunyan's Axe , the longest continuous rivalry in NCAA Division I football. The two universities also compete in the Border Battle, a year-long athletic competition in which each sport season is worth 40 points divided by the number of times the teams play each other (i.e. football is worth 40 points because they play each other only once, while women's ice hockey is worth 10 points per game because they play four times a year). Conference and post-season playoffs do not count in the point standings.
Goldy Gopher is the mascot for the Twin Cities campus and the associated sports teams. The gopher mascot is a tradition as old as the state which was tabbed the "Gopher State" in 1857 after a political cartoon ridiculing the US$5-million railroad loan which helped open up the West. The cartoon portrayed shifty railroad barons as striped gophers pulling a railroad car carrying the Territorial Legislature. Later, the University picked up the nickname with the first University yearbook bearing the name "Gopher Annual" appearing in 1887.
The " Minnesota Rouser " is the University of Minnesota's fight song . It is commonly played and sung at various events such as commencement , convocation , and athletic games by the University of Minnesota Marching Band . It is among a number of songs associated with the University, including the Minnesota March , which was composed for the University by John Philip Sousa .

Football
The Minnesota Golden Gophers are one of the oldest programs in college-football history. They have won 7 National Championships and 18 Big Ten Conference Championships . The Golden Gophers played their first game on September 29, 1882, a 4–0 victory over Hamline University , St. Paul. In 1887, the Golden Gophers played host to the Wisconsin Badgers in a 63–0 victory. With the exception of 1906, the Golden Gophers and the Badgers have played each other every year since. The 124 games played against each other is the most-played rivalry in NCAA Division I FBS college football.
In 1981, the Golden Gophers played their last game in Memorial Stadium . Between 1982 and 2008, the school hosted their home games in the Hubert H. Humphrey Metrodome in downtown Minneapolis until they moved back to campus on September 12, 2009, when their new home, TCF Bank Stadium , opened with a game against the Air Force Falcons of the U.S. Air Force Academy .

Basketball
The Golden Gophers men's basketball team has won two National Championships , two National Invitation Tournament (NIT) Championships and eight Big Ten Regular Season Championships. They also have six NCAA Tournament , including a Final Four appearance in 1997 and three Sweet 16 appearances. However, because of NCAA sanctions for academic fraud , all postseason appearances from 1994 to 1998—in the NCAA Tournament in 1994, 1995, and 1997 and NIT in 1996 and 1998—were vacated. Most recently in April 2014, the Golden Gophers defeated SMU to win the NIT championship at Madison Square Garden in New York City.
The Golden Gophers women's basketball team has enjoyed success in recent years under Pam Borton , including a Final Four appearance in 2004. Overall, they have six NCAA Tournament appearances and three Sweet 16 appearances.

Men's ice hockey
Ice hockey is one of the most strongly supported athletic programs at the University of Minnesota, referred to by the University as "Minnesota's Pride on Ice." [96] The high amount of support is due to the State of Minnesota's high affinity for the sport of ice hockey at all levels. [97]
The Golden Gophers men's ice-hockey program has won five Division I National Championships and 13 Western Collegiate Hockey Association (WCHA) Regular Season Championships , most recently in 2012. They have won 14 WCHA Tournament Championships and have 20 NCAA Frozen Four appearances. A Golden Gophers hockey tradition is to stock the roster almost exclusively (sometimes completely) with Minnesota natives. Home games are played at Mariucci Arena. The Golden Gophers' big rivals are the University of Wisconsin–Madison and the University of North Dakota .

Women's hockey
The Golden Gophers women's hockey team has also won six National Championships, most recently in 2016, and six WCHA Regular Season Championships. They have also won four WCHA Tournament Championships and have eleven NCAA Frozen Four appearances. They play their home games in Ridder Arena . They were the first collegiate women's hockey team to play in an arena dedicated solely to women's ice hockey. In the 2012–2013 season they finished undefeated at 41–0, and are the first and only NCAA Women's Hockey team to do so. After winning the NCAA tournament their winning streak stood at 49 games, dating back to February 17, 2012 when they lost to North Dakota.

Notable people

See also
WebPage index: 00042
List of Wikipedias
This is a list of the different language editions of Wikipedia ; as of 26 May 2017 there are 296 Wikipedias of which 285 are active.

Wikipedia edition codes
Each Wikipedia has a code, which is used as a subdomain below wikipedia.org. Interlanguage links are sorted by that code. The codes represent the language codes defined by ISO 639-1 and ISO 639-3 , and the decision of which language code to use is usually determined by the IETF language tag policy. Wikipedias also vary by how thinly they slice dialects and variants; for example, the English Wikipedia includes most modern varieties of English (American English, Indian English, South African English, etc.), but does not include other related languages such as Scots , or Anglo-Saxon , all of which have separate Wikipedias. The Spanish Wikipedia includes both Peninsular Castilian and Latin American Spanish ; Malay Wikipedia includes a large number of Malay languages; and so on.
Differences between the ISO mappings and Wikipedia codes include:
Additionally, Wikipedias vary in wikt:orthography at times. Chinese Wikipedia automatically translates from modern Mandarin Chinese into four standard forms: Mainland China and Singapore in simplified Chinese characters, and Taiwan and Hong Kong / Macau in traditional Chinese characters. Belarussian , however, has a separate Wikipedia for the 'normative' orthography (be) and Taraškievica (be-tarask).

List
An approximation to the number of active users is given in powers of ten (see common logarithm ): so "5" means at least 10,000, "4" means at least 1000, "3" means at least 100, and so on.

Detailed list

Notes

Grand total

See also
WebPage index: 00043
Vietnamese Wikipedia
The Vietnamese Wikipedia ( Vietnamese : Wikipedia tiếng Việt ) is the Vietnamese-language edition of Wikipedia , a free, publicly editable, online encyclopedia supported by the Wikimedia Foundation . As with other language editions of Wikipedia, the project's content is both created and accessed using the MediaWiki wiki software. The Vietnamese Wikipedia's primary competitor is the Encyclopedic Dictionary of Vietnam ( Từ điển Bách khoa toàn thư Việt Nam ), a state-funded encyclopedic dictionary also available online.

Content
As of May 2017, it has about 1,157,000 articles. [1] It is the largest Wikipedia in a non-European language, as well as the largest for a language which is official in only one country. However, it has only 375,000 articles manually created, 67% of its articles having been made by bots, and thus ranks 3rd among non-European language Wikipedias after the Japanese and Chinese ones. [2]
This also makes the article depth (30.9) of the edition lower than the Japanese (77.5) and the Chinese (168.2) editions, which both have fewer articles. It also has fewer active users, with 1,485 (12,828 and 7,363 for the Japanese and Chinese versions, respectively).

History
The Vietnamese Wikipedia initially went online in November 2002, with a front page and an article about the Internet Society . The project received little attention and did not begin to receive significant contributions until it was "restarted" in October 2003 [3] and the newer, Unicode -capable MediaWiki software was installed soon after.
By August 2008, the Vietnamese Wikipedia had grown to more than 50,000 articles – a milestone it achieved on August 26 – approximately 432 of which were created by bots . [4] By the time the project reached the 100,000-article milestone on September 12, 2009, bot-generated articles made up around 5% of its corpus. [5] Short articles are designated "stubs"; such articles number in the tens of thousands and include most of the bot-generated articles. [6]
An experimental Wikipedia edition in the obsolete chữ Nôm script began in October 2006 at the Wikimedia Incubator . [7] It was deleted in April 2010. [8] An unrelated wiki encyclopedia project, VinaWiki, transliterates Vietnamese Wikipedia articles into chữ Nôm as part of a project to revive the script. [9]
The Vietnamese Wikipedia's article count reached 500,000 on 28 September 2012 and 1,000,000 on 15 June 2014.

Software
The Vietnamese Wikipedia uses AVIM, a JavaScript -based input method that allows the user to type accented Vietnamese text in popular input methods, such as Telex , VNI and VIQR . (See Vietnamese language and computers .) The preferred input system can be selected using a box under the sidebar. [10]

See also
WebPage index: 00044
Catalan Wikipedia
The Catalan Wikipedia ( Catalan : Viquipèdia en català ) is the Catalan-language edition of the free online encyclopedia Wikipedia . It was created on 16 March 2001, just a few minutes after the first non- English Wikipedia , the German edition . With about 543,000 articles, it is currently the 18th-largest Wikipedia as measured by the number of articles , and the fifth-largest Wikipedia in a Romance language . [2] [3] In April 2016, the project had 582 active editors who made at least five edits in that month. The edition is most notable for its large number of quality articles, which illustrates the Catalan language's important online presence despite being a minority language . [4] [5]

Creation
On March 16, 2001, Jimmy Wales announced that he wanted to create Wikipedias in other languages and mentioned that there was interest in creating a Catalan version. [6] The first tests were made on the deutsche.wikipedia.com , [7] and a few minutes later, the Catalan Wikipedia was created in the catalan.wikipedia.com domain. [8]
The first edit on a non-English Wikipedia was at 21:07 UTC, March 16, 2001, made to the Catalan Main Page. The first contribution in a non-English article dates from March 17 at 01:41 UTC in the article Àbac . [9] Despite being created after the German Wikipedia, for about two months it was the only non-English Wikipedia that contained articles. [10]
After some time, the domain changed to ca.wikipedia.com and later to ca.wikipedia.org . About 2003 its community started to use the name "Viquipèdia" when talking about this edition of Wikipedia. Nowadays this word is used in Catalan language to refer the whole Wikipedia . About 2005 the domain www.viquipedia.net was registered and it redirects to ca.wikipedia.org . In 2007 www.viquipedia .cat was also registered and redirected.
The first registered user was probably AstroNomer , presumably used only to make some registration tests, but the first registered user to make lasting contributions was Cdani, the same user cited in Jimbo Wales' message.
In 2005 the Catalan Wikipedia community debated on which name(s) of the Catalan language to present in the main page and other policy pages, either català , "Catalan", valencià , " Valencian ", or a combination of both català-valencià and català o valencià . Although there was not a consensus on any of the proposals, users agreed that in articles relevant to the Valencian Community , the name "Valencian" was to be used; on all other articles "Catalan" is preferred. [11] The main page avoids making reference to a particular nomenclature, by simply stating aquesta versió , "this version".

Milestones and historical main pages

See also

Notes
WebPage index: 00045
Norwegian Wikipedia
There are two Norwegian language editions of Wikipedia : one for articles written in Bokmål or Riksmål , and one for articles written in Nynorsk . There are currently 467,206 articles on the Norwegian Wikipedia edition in Bokmål/Riksmål, and 133,300 articles on the Nynorsk edition.

Timeline

History
The first site, the original Norwegian Wikipedia ( Norsk Wikipedia ), launched on November 26, 2001, and originally did not specify which written standard could/should be used, although de facto almost all the articles were written in Bokmål/Riksmål. The Norwegian Wikipedia originally had the address no.wikipedia.com , but in August 2002 all Wikipedia editions moved from the wikipedia.com domain to the wikipedia.org domain, shortly after Jimbo Wales had announced that Wikipedia would never have commercial advertisements.
A Nynorsk-specific Wikipedia was launched on July 31, 2004 and grew quickly. Following a vote in 2005, the main Norwegian site became Bokmål and Riksmål only.
By February 2007, the Bokmål/Riksmål edition had over 100,000 articles and the Nynorsk site had over 20,000 articles. In February 2006, the Bokmål/Riksmål edition became the thirteenth Wikipedia to have more than 50,000 articles, and one year later it was the fourteenth to reach 100,000. After the Finnish Wikipedia surpassed it in April 2006 it was once again the 14th largest Wikipedia by article count. [1] However, in September 2007, the Bokmål/Riksmål Wikipedia surpassed the Finnish, and has since then been the 13th largest Wikipedia. [2] As of June 1, 2010, the Bokmål/Riksmål contains more than 260,000 articles, while the Nynorsk version contains over 57,000. Norwegian, Danish, and Swedish are mutually intelligible languages and can be understood by most speakers of each. The sites collaborate with the other Scandinavian Wikipedias through the Skanwiki section of Wikimedia 's Meta-Wiki site. One effect of this combined effort is the sharing of their weekly featured front page articles among these four different Wikipedias.
While the ISO 639 two-letter code for Bokmål is nb , the Bokmål and Riksmål Norwegian Wikipedia continues to be hosted at no.wikipedia.org , while the narrower nb code redirects to that site. The Nynorsk code is nn , and the Nynorsk Wikipedia is hosted at nn.wikipedia.org .
On April 9, 2013 at 22:55 UTC Nynorsk Wikipedia passed 100,000 articles.
Wikimedia Norge is a Norwegian private membership association with the purpose to support Wikimedia's projects, in particular those in Norwegian and Sami languages . The association was formed at a meeting at the National Library in Oslo on June 23, 2007. However, it has no formal role in relation to the Norwegian Wikipedia projects.

See also
WebPage index: 00046
Interwiki links
Interwiki linking ( W-link ) is a facility for creating links to the many wikis on the World Wide Web . Users avoid pasting in entire URLs (as they would for regular web pages) and instead use a shorthand similar to links within the same wiki (intrawiki links).
Unlike domain names on the Internet, there is no globally defined list of interwiki prefixes, so owners of a wiki must define an interwiki map (InterMap) appropriate to their needs. Users generally have to create separate accounts for each wiki they intend to use (unless they intend to edit anonymously). Variations in text formatting and layout can also hinder a seamless transition from one wiki to the next.
By making wiki links simpler to type for the members of a particular community, these features help bring the different wikis closer together. Furthering that goal, interwiki "bus tours" (similar to webrings ) have been created to explain the purposes and highlights of different wikis. Such examples on Wikipedia include Wikipedia:TourBusStop and Wikipedia:WikiNode .

Syntax
Interwiki link notation varies, depending largely on the syntax a wiki uses for markup. The two most common link patterns in wikis are CamelCase and free links (arbitrary phrases surrounded by some set delimiter , such as [[double square brackets]]). CURIE syntax -an emerging W3C standard- uses a single set of square brackets.
Interwiki links on a CamelCase-based wiki frequently take the form of "Code:PageName", where Code is the defined InterMap prefix for another wiki. Thus, a link "WikiPedia:InterWiki" could be rendered in HTML as a link to an article on Wikipedia: for example, Wikipedia:Interlanguage links . Linking from a CamelCase-wiki to a page that contains spaces in its title typically requires replacing the spaces with underscores (e.g. WikiPedia:Main_Page).
Interwiki links on wikis based on free links, such as Wikipedia, typically follow the same principle, but using the delimiters that would be used for internal links. These links can then be parsed and escaped as they would be if they were internal, allowing easier typing of spaces but potentially causing problems with other special characters. For example, on Wikipedia, [[MeatBall:AssumeGoodFaith]] appears as MeatBall:AssumeGoodFaith , and [[:de:InterWiki]] (former syntax: [[DeWikipedia:InterWiki]] ) appears as de:InterWiki .
The MediaWiki software has an additional feature which uses similar notation to create automatic interlanguage links — for instance, the link [[de:InterWiki]] (with no leading colon) automatically creates a reference labeled "Other languages: Deutsch | ..." at the top and bottom of, or in a sidebar next to, the article display. Various other wiki software systems have features for "semi-internal" links of this kind, such as support for namespaces or multiple sub-communities.
Most InterMap implementations simply substitute the interwiki prefix with a full URL prefix, so many non-wiki websites can also be referred to using the system. A reference to a definition on the Free On-line Dictionary of Computing , for instance, could take the form [[Foldoc:foo]] which would tell the system to append and display the link as Foldoc:foo . This makes it very easy to link to commonly referenced resources from within a wiki page, without the need to even know the form of the URL in question.
The interwiki concept can equally be applied to links from non-wiki websites. Advogato , for instance, offers a syntax for creating shorthand links based on a MeatBall-derived InterMap.

Implementation
Internally, a wiki that uses interwiki links needs to have a mapping from wiki-code links to full URLs. For example, [[MeatBall:InterWiki]] might appear as MeatBall:InterWiki , but link to http://usemod.com/cgi-bin/mb.pl?InterWiki .
Since most wiki systems use URLs for individual pages where the page's title appears at the end of an otherwise unchanging address, the simplest way of defining such mappings is by substituting the interwiki prefix for the unchanging part of the URL. So in the example above, the MeatBall: has simply been replaced by http://usemod.com/cgi-bin/mb.pl? in creating the target of the HTML rendered link.
Rather than creating a new list from scratch for every wiki, it is often useful to obtain a copy of that from another site. Sites such as MeatballWiki [1] and the UseModWiki site contain comprehensive lists which are often used for this purpose - the former being publicly editable in the same way as any other wiki page, and the latter being verified as usable but potentially out of date. MediaWiki's default list of interwiki links is derived from an old version of MeatballWiki's list. [1] [2]

See also
WebPage index: 00047
Criticism of Wikipedia
Criticism of Wikipedia —of its content, procedures, and operations, and of the Wikipedia community —covers many subjects, topics, and themes about the nature of Wikipedia as an open source encyclopedia of subject entries that almost anyone can edit. Wikipedia has been criticized for the uneven handling, acceptance, and retention of articles about controversial subjects. The principal concerns of its critics are the factual reliability of the content; the readability of the prose; and a clear article layout; the existence of systemic bias ; of gender bias ; and of racial bias among the editorial community that is Wikipedia. Further concerns are that the organization allows the participation of anonymous editors (leading to editorial vandalism); the existence of social stratification (allowing cliques); and over-complicated rules (allowing editorial quarrels), the conditions of which permit the misuse of Wikipedia.
Wikipedia is described as unreliable at times. In "Wikipedia: The Dumbing Down of World Knowledge" (2010), Edwin Black characterized the editorial content of articles as a mixture of "truth, half-truth, and some falsehoods". [1] Similarly, in "Wisdom?: More like Dumbness of the Crowds" (2011), Oliver Kamm said that the encyclopedic articles usually are dominated by the editors with the loudest and most persistent editorial voices (talk pages and edit summaries), usually by an interest group with an ideological "axe to grind" on the subject, topic, or theme of the article in question. [2] Politics and ideology entries are also criticized on Wikipedia. In two works published in 2012, "The 'Undue Weight' of Truth on Wikipedia" by Timothy Messer–Kruse, and "You Just Type in What You are Looking for: Undergraduates' Use of Library Resources vs. Wikipedia" by Mónica Colón–Aguirre and Rachel A. Fleming–May, the authors analyzed and criticized the undue-weight policy (relative importance of a given source), and concluded that Wikipedia is not about providing correct and definitive information about a subject, [3] but instead presenting, as editorially dominant, the perspective taken by most authors of the sources for the article. This allegedly uneven application of the undue-weight policy creates omissions (of fact and of interpretation) that might give the reader false impressions about the subject matter, based upon the factually incomplete content of the Wikipedia article. [3] [4] [5]
Wikipedia is also sometimes characterized as having a hostile editing environment. In Common Knowledge?: An Ethnography of Wikipedia (2014), Dariusz Jemielniak , a steward for Wikimedia Foundation projects, stated that the complexity of the rules and laws governing editorial content and editors' behavior is a licence for the "office politics" of disruptive editors and drives away new, potentially constructive editors. [6] [7] In a follow-up article, "The Unbearable Bureaucracy of Wikipedia" (2014), Jemielniak said that abridging and rewriting the editorial rules and laws of Wikipedia for clarity of purpose and simplicity of application would resolve the bureaucratic bottleneck of too many rules. [6] [7] In "The Rise and Decline of an Open Collaboration System: How Wikipedia's Reaction to Popularity is Causing its Decline" (2013), Aaron Halfaker stated that the over-complicated rules and laws of Wikipedia unintentionally provoked the decline in editorial participation that began in 2009—frightening away new editors who otherwise would contribute to Wikipedia. [8]

Criticism of content
Wikipedia is described as unreliable at times. Edwin Black has characterized the editorial content of articles as a mixture of "truth, half-truth, and some falsehoods". [1] and Oliver Kamm has said that articles are usually dominated by the editors with the loudest and most persistent editorial voices (talk pages and edit summaries), usually by an interest group with an ideological "axe to grind" on the subject, topic, or theme of the article in question. [2]
Wikipedia articles on politics and ideology have also been criticized. Two works published in 2012 are critical of the undue-weight policy (relative importance of a given source), and concluded that, because the purpose of Wikipedia is not to provide correct and definitive information about a subject, [3] but to present, as the consensus opinion, the majority opinion advanced by the authors of the entry's sources. The uneven application of the undue-weight policy creates omissions (of fact and of interpretation) that might give the reader false impressions about the subject matter, based upon the incompleteness of the Wikipedia article. [3] [9] [5]
Wikipedia is sometimes characterized as having a hostile editing environment. In Common Knowledge?: An Ethnography of Wikipedia (2014), Dariusz Jemielniak , a steward for Wikimedia Foundation projects, stated that the complexity of the rules and laws governing editorial content and the behavior of the editors is a burden for new editors and a licence for the "office politics" of disruptive editors. [6] [10] In a follow-up article, Jemielniak said that abridging and rewriting the editorial rules and laws of Wikipedia for clarity of purpose and simplicity of application would resolve the bureaucratic bottleneck of too many rules. [10] In The Rise and Decline of an Open Collaboration System: How Wikipedia's Reaction to Popularity is Causing its Decline (2013), Aaron Halfaker stated that the over-complicated rules and laws of Wikipedia unintentionally provoked the decline in editorial participation that began in 2009—frightening away new editors who otherwise would contribute to Wikipedia. [8]
There have also been works that describe the possible misuse of Wikipedia. In "Wikipedia or Wickedpedia?" (2008), the Hoover Institution said that Wikipedia is an unreliable resource for correct knowledge, information, and facts about a subject, because, as an open source website, the editorial content of the articles is readily subjected to manipulation and propaganda . [11] The 2014 edition of the Massachusetts Institute of Technology 's official student handbook, Academic Integrity at MIT , informs students that Wikipedia is not a reliable academic source, stating, "the bibliography published at the end of the Wikipedia entry may point you to potential sources. However, do not assume that these sources are reliable—use the same criteria to judge them as you would any other source. Do not consider the Wikipedia bibliography as a replacement for your own research ." [12]

Accuracy of information

Not authoritative
Wikipedia acknowledges that the encyclopedia should not be used as a primary source for research, either academic or informational. The British librarian Philip Bradley said that "the main problem is the lack of authority . With printed publications, the publishers have to ensure that their data are reliable, as their livelihood depends on it. But with something like this, all that goes out the window." [13] Likewise, Robert McHenry , editor-in-chief of Encyclopædia Britannica from 1992 to 1997, said that readers of Wikipedia articles cannot know who wrote the article they are reading—it might have been written by an expert in the subject matter or by an amateur. [14] In November 2015, Wikipedia co-founder Larry Sanger told Zach Schwartz in Vice : "I think Wikipedia never solved the problem of how to organize itself in a way that didn't lead to mob rule" and that since he left the project, "People that I would say are trolls sort of took over. The inmates started running the asylum." [15]

Comparative study of science articles
In "Internet Encyclopaedias Go Head-to-head", a 2005 article published in the Nature scientific journal, the results of a blind experiment (single-blind study), which compared the factual and informational accuracy of entries from Wikipedia and the Encyclopædia Britannica , were reported. The 42-entry sample included science articles and biographies of scientists, which were compared for accuracy by anonymous academic reviewers; they found that the average Wikipedia entry contained four errors and omissions, while the average Encyclopædia Britannica entry contained three errors and omissions. The study concluded that Wikipedia and Britannica were comparable in terms of the accuracy of its science entries". [17] Nevertheless, the reviewers had two principal criticisms of the Wikipedia science entries: (i) thematically confused content, without an intelligible structure (order, presentation, interpretation); and (ii) that undue weight is given to controversial, fringe theories about the subject matter. [18]
The dissatisfaction of the Encyclopædia Britannica editors led to Nature publishing additional survey documentation that substantiated the results of the comparative study. [19] Based upon the additional documents, Encyclopædia Britannica denied the validity of the study, stating it was flawed, because the Britannica extracts were compilations that sometimes included articles written for the youth version of the encyclopedia. [20] In turn, Nature acknowledged that some Britannica articles were compilations, but denied that such editorial details invalidated the conclusions of the comparative study of the science articles. [21]
The editors of Britannica also said that while the Nature study showed that the rate of error between the two encyclopedias was similar, the errors in a Wikipedia article usually were errors of fact, while the errors in a Britannica article were errors of omission. According to the editors of Britannica , Britannica was more accurate than Wikipedia in that respect. [20] Subsequently, Nature magazine rejected the Britannica response with a rebuttal of the editors' specific objections about the research method of the study. [22] [23]

Lack of methodical fact-checking
Inaccurate information that is not obviously false may persist in Wikipedia for a long time before it is challenged. The most prominent cases reported by mainstream media involved biographies of living people.
The Wikipedia Seigenthaler biography incident demonstrated that the subject of a biographical article must sometimes fix blatant lies about his own life. In May 2005, an anonymous user edited the biographical article on American journalist and writer John Seigenthaler so that it contained several false and defamatory statements. [24] [25] The inaccurate claims went unnoticed from May until September 2005 when they were discovered by Victor S. Johnson Jr. , a friend of Seigenthaler. Wikipedia content is often mirrored at sites such as Answers.com , which means that incorrect information can be replicated alongside correct information through a number of web sources. Such information can develop a misleading authority because of its presence at such sites. [26]
In another example, on March 2, 2007, MSNBC.com reported that then- New York Senator and former Secretary of State Hillary Clinton had been incorrectly listed for 20 months in her Wikipedia biography as valedictorian of her class of 1969 at Wellesley College , when in fact she was not (though she did speak at commencement ). [27] The article included a link to the Wikipedia edit, [28] where the incorrect information was added on July 9, 2005. The inaccurate information was removed within 24 hours after the MSNBC.com report appeared. [29]
Attempts to perpetrate hoaxes may not be confined to editing existing Wikipedia articles, but can also include creating new articles. In October 2005, Alan Mcilwraith , a former call center worker from Scotland , created a Wikipedia article in which he wrote that he was a highly decorated war hero. The article was quickly identified as a hoax by other users and deleted. [30] [ better source needed ]
There have also been instances of users deliberately inserting false information into Wikipedia in order to test the system and demonstrate its alleged unreliability. Gene Weingarten , a journalist, ran such a test in 2007, in which he inserted false information into his own Wikipedia article; it was removed 27 hours later by a Wikipedia editor. [31] Wikipedia considers the deliberate insertion of false and misleading information to be vandalism . [32]

Neutral point of view and conflicts of interest
Wikipedia regards the concept of a neutral point of view as one of its non-negotiable principles; however, it acknowledges that such a concept has its limitations—its NPOV policy states that articles should be "as far as possible" written "without editorial bias". Mark Glaser, a journalist, also wrote that this may be an impossible ideal due to the inevitable biases of editors. [33]
In August 2007, a tool called WikiScanner—developed by Virgil Griffith, a visiting researcher from the Santa Fe Institute in New Mexico —was released to match edits to the encyclopedia by non-registered users with an extensive database of IP addresses . [34] News stories appeared about IP addresses from various organizations such as the Central Intelligence Agency , the National Republican Congressional Committee , the Democratic Congressional Campaign Committee , Diebold, Inc. and the Australian government being used to make edits to Wikipedia articles, sometimes of an opinionated or questionable nature. Another story stated that an IP address from the BBC itself had been used to vandalize the article on George W. Bush . [35] The BBC quoted a Wikipedia spokesperson as praising the tool: "We really value transparency and the scanner really takes this to another level. Wikipedia Scanner may prevent an organisation or individuals from editing articles that they're really not supposed to." [36] Not everyone hailed WikiScanner as a success for Wikipedia. Oliver Kamm , in a column for The Times , argued instead that: [2]
WikiScanner only reveals conflicts of interest when the editor does not have a Wikipedia account and their IP address is used instead. Conflict of interest editing done by editors with accounts is not detected, since those edits are anonymous to everyone except some Wikipedia administrators . [37]

Scientific disputes
The 2005 Nature study also gave two brief examples of challenges that Wikipedian science writers purportedly faced on Wikipedia. The first concerned the addition of a section on violence to the schizophrenia article, which exhibited the view of one of the article's regular editors, neuropsychologist Vaughan Bell , that it was little more than a "rant" about the need to lock people up, and that editing it stimulated him to look up the literature on the topic. [17]
Another dispute involved the climate researcher William Connolley , a Wikipedia editor who was opposed by others. The topic in this second dispute was the greenhouse effect , and The New Yorker reported that this dispute, which was far more protracted, had led to arbitration , which took three months to produce a decision. The outcome of arbitration, as reported by Nature , was a six-month parole for Connolley, during which he was restricted to undoing edits on articles once per day. [38]

Exposure to political operatives and advocates
While Wikipedia policy requires articles to have a neutral point of view, it is not immune from attempts by outsiders (or insiders) with an agenda to place a spin on articles. In January 2006 it was revealed that several staffers of members of the U.S. House of Representatives had embarked on a campaign to cleanse their respective bosses' biographies on Wikipedia, as well as inserting negative remarks on political opponents. References to a campaign promise by Martin Meehan to surrender his seat in 2000 were deleted, and negative comments were inserted into the articles on United States Senator Bill Frist and Eric Cantor , a congressman from Virginia . Numerous other changes were made from an IP address assigned to the House of Representatives. [39] In an interview, Wikipedia de facto leader Jimmy Wales remarked that the changes were "not cool". [40]
Larry Delay and Pablo Bachelet wrote that from their perspective, some articles dealing with Latin American history and groups (such as the Sandinistas and Cuba ) lack political neutrality and are written from a sympathetic Marxist perspective which treats socialist dictatorships favorably at the expense of alternate positions. [41] [42] [43]
In 2008, the pro-Israel group Committee for Accuracy in Middle East Reporting in America (CAMERA) organized an e-mail campaign to encourage readers to correct perceived Israel-related biases and inconsistencies in Wikipedia. [44] CAMERA argued the excerpts were unrepresentative and that it had explicitly campaigned merely "toward encouraging people to learn about and edit the online encyclopedia for accuracy". [45] Defenders of CAMERA and the competing group, Electronic Intifada , went into mediation. [44] Israeli diplomat David Saranga said that Wikipedia is generally fair in regard to Israel. When it was pointed out that the entry on Israel mentioned the word "occupation" nine times, whereas the entry on the Palestinian People mentioned "terror" only once, he responded, "It means only one thing: Israelis should be more active on Wikipedia. Instead of blaming it, they should go on the site much more, and try and change it." [46]
Political commentator Haviv Rettig Gur, reviewing widespread perceptions in Israel of systemic bias in Wikipedia articles, has argued that there are deeper structural problems creating this bias: anonymous editing favors biased results, especially if the editors organize concerted campaigns of defamation as has been done in articles dealing with Arab-Israeli issues, and current Wikipedia policies, while well-meant, have proven ineffective in handling this. [47]
On August 31, 2008, The New York Times ran an article detailing the edits made to the biography of Alaska governor Sarah Palin in the wake of her nomination as the running mate of Arizona Senator John McCain . During the 24 hours before the McCain campaign announcement, 30 edits , many of them flattering details, were made to the article by Wikipedia single-purpose user identity Young_Trigg. [48] This person has later acknowledged working on the McCain campaign, and having several Wikipedia user accounts. [49]
In November 2007, libelous accusations were made against two politicians from southwestern France, Jean-Pierre Grand and Hélène Mandroux-Colas , on their Wikipedia biographies. Jean-Pierre Grand asked the president of the French National Assembly and the Prime Minister of France to reinforce the legislation on the penal responsibility of Internet sites and of authors who peddle false information in order to cause harm. [50] Senator Jean Louis Masson then requested the Minister of Justice to tell him whether it would be possible to increase the criminal responsibilities of hosting providers, site operators, and authors of libelous content; the minister declined to do so, recalling the existing rules in the LCEN law. [51]
On August 25, 2010, the Toronto Star reported that the Canadian "government is now conducting two investigations into federal employees who have taken to Wikipedia to express their opinion on federal policies and bitter political debates." [52]
In 2010, Al Jazeera 's Teymoor Nabili suggested that the article Cyrus Cylinder had been edited for political purposes by "an apparent tussle of opinions in the shadowy world of hard drives and 'independent' editors that comprise the Wikipedia industry." He suggested that after the Iranian presidential election, 2009 and the ensuing "anti-Iranian activities" a "strenuous attempt to portray the cylinder as nothing more than the propaganda tool of an aggressive invader" was visible. The edits following his analysis of the edits during 2009 and 2010, represented "a complete dismissal of the suggestion that the cylinder, or Cyrus' actions, represent concern for human rights or any kind of enlightened intent," in stark contrast to Cyrus ' own reputation as documented in the Old Testament and the people of Babylon. [53]

Commandeering or sanitizing articles
Articles of particular interest to an editor or group of editors are sometimes modified based on these editors' respective points of views. [54] Some companies and organizations—such as Sony , Diebold , Nintendo , Dell , the United States' Central Intelligence Agency , and the Church of Scientology —as well as individuals, such as United States Congressional staffers , were all shown to have modified the Wikipedia pages about themselves in order to present a point of view that describes them positively; these organizations may have editors who revert negative changes as soon as these changes are submitted. [55] [56]

Editing for financial rewards
In January 2007 Rick Jelliffe stated in a story carried by CBS [57] and IDG News Service [58] [59] that Microsoft had offered him compensation in exchange for his future editorial services on Wikipedia's articles related to OOXML (Office Open Extensible Markup Language). A Microsoft spokesperson, quoted by CBS, commented that "Microsoft and the writer, Rick Jelliffe, had not determined a price and no money had changed hands—but they had agreed that the company would not be allowed to review his writing before submission". Also quoted by CBS, Jimmy Wales expressed his disapproval of Microsoft's involvement: "We were very disappointed to hear that Microsoft was taking that approach".

Quality of the presentation

Quality of articles on U.S. history
In the essay, “Can History be Open Source?: Wikipedia and the Future of the Past” (2006), the academic historian Roy Rosenzweig criticized the encyclopedic content and writing style used in Wikipedia, for not distinguishing subjects that are important from subjects that are merely sensational. That Wikipedia is “surprisingly accurate in reporting names, dates, and events in U.S. history”, and that most of the factual errors he found “were small and inconsequential”, some of which “simply repeat widely held, but inaccurate, beliefs”, which are also repeated in the Microsoft Encarta encyclopedia and in the Encyclopædia Britannica ; yet Rosenzweig’s major criticism is that:
Rosenzweig also criticized the “waffling—encouraged by the [neutral point of view] policy—[which] means that it is hard to discern any overall interpretive stance in Wikipedia history [articles]”, and quoted the historical conclusion of the biography of William Clarke Quantrill , a Confederate guerrilla in the United States Civil War, as an example of weasel-word waffling:
In American National Biography Online , the historian James M. McPherson contrasted the content and writing style of Wikipedia's biography of United States President Abraham Lincoln to that of the U.S. Civil War article, and found that each entry was essentially accurate in covering the major episodes of President Lincoln’s life. The "richer contextualization" of McPherson's work, as well as "his artful use of quotations to capture Lincoln’s voice" and "his ability to convey a profound message in a handful of words," were contrasted with Wikipedia's history-article prose. The prose of the Wikipedia articles was "both verbose and dull" and thus difficult to read, because "the skill and confident judgment of a seasoned historian” are absent from the antiquarian writing style of Wikipedia, as opposed to the writing style used by professional historians in the American Heritage magazine. It was also mentioned that while Wikipedia usually provides many references, these are not the most accurate references. [60]

Quality of medical articles
In the article "Wikipedia Cancer Information Accurate," a study of medical articles, Yaacov Lawrence of the Kimmel Cancer Center of Thomas Jefferson University found that the cancer entries were mostly accurate. However, Wikipedia's articles were written in college-level prose, as opposed to in the easier-to-understand ninth-grade-level prose found in the Physician Data Query (PDQ) of the National Cancer Institute . According to Lawrence, "Wikipedia’s lack of readability may reflect its varied origins and haphazard editing.” [61]
In its 2007 article "Fact or Fiction? Wikipedia’s Variety of Contributors is Not Only a Strength," the magazine The Economist stated that the quality of the writing in Wikipedia articles usually indicates the quality of the editorial content: "Inelegant or ranting prose usually reflects muddled thoughts and incomplete information.” [62]

The Wall Street Journal
In the September 12, 2006, edition of The Wall Street Journal , Jimmy Wales debated with Dale Hoiberg , editor-in-chief of Encyclopædia Britannica . [63] Hoiberg focused on a need for expertise and control in an encyclopedia and cited Lewis Mumford that overwhelming information could "bring about a state of intellectual enervation and depletion hardly to be distinguished from massive ignorance." Wales emphasized Wikipedia's differences, and asserted that openness and transparency lead to quality. Hoiberg said that he "had neither the time nor space to respond to [criticisms]" and "could corral any number of links to articles alleging errors in Wikipedia", to which Wales responded: "No problem! Wikipedia to the rescue with a fine article", and included a link to the Wikipedia article about criticism of Wikipedia. [63]

Systemic bias in coverage
Wikipedia has been accused of systemic bias, which is to say its general nature leads, without necessarily any conscious intention, to the propagation of various prejudices. Although many articles in newspapers have concentrated on minor factual errors in Wikipedia articles, there are also concerns about large-scale, presumably unintentional effects from the increasing influence and use of Wikipedia as a research tool at all levels. In an article in the Times Higher Education magazine (London) philosopher Martin Cohen describes Wikipedia as having "become a monopoly" with "all the prejudices and ignorance of its creators," which he calls a "youthful cab-driver's" perspective. [64] Cohen concludes that "[t]o control the reference sources that people use is to control the way people comprehend the world. Wikipedia may have a benign, even trivial face, but underneath may lie a more sinister and subtle threat to freedom of thought ." [64] That freedom is undermined by what he sees as what matters on Wikipedia, "not your sources but the 'support of the community'." [64]
Researchers from Washington University developed a statistical model to measure systematic bias in the behavior of Wikipedia's users regarding controversial topics. The authors focused on behavioral changes of the encyclopedia's administrators after assuming the post, writing that systematic bias occurred after the fact. [65]
Critics also point to the tendency to cover topics in a detail disproportionate to their importance. For example, Stephen Colbert once mockingly praised Wikipedia for having a longer entry on ' lightsabers ' than it does on the ' printing press '. [66] In an interview with The Guardian , Dale Hoiberg, the editor-in-chief of Encyclopædia Britannica , noted: [13]
This critical approach has been satirised as "Wikigroaning", a term coined by Jon Hendren [67] of the website Something Awful . [68] In the game, two articles (preferably with similar names) are compared: one about an acknowledged, serious, or classical subject and the other about a popular or current one. [69] Defenders of a broad inclusion criteria have held that the encyclopedia's coverage of pop culture does not impose space constraints on the coverage of more serious subjects (see " Wiki is not paper "). As Ivor Tossell noted:
In 2014, supporters of holistic healing and energy psychology began a change.org petition asking for "true scientific discourse" on Wikipedia, complaining that "much of the information [on Wikipedia] related to holistic approaches to healing is biased, misleading, out-of-date, or just plain wrong". In response, Jimmy Wales said that Wikipedia only covers works that are published in respectable scientific journals. [71] [72]

Notability of article topics
Wikipedia's notability guidelines , which are used by editors to determine if a subject merits its own article, and the application thereof, are the subject of much criticism. [73] Nicholson Baker considers the notability standards arbitrary and essentially unsolvable: [74]
Criticizing the " deletionists ", Baker then writes: [73]
Another criticism about the deletionists is: "The increasing difficulty of making a successful edit; the exclusion of casual users; slower growth—all are hallmarks of the deletionists approach." [75]
Complaining that his own biography was on the verge of deletion for lack of notability, Timothy Noah argued that:
In the same article, Noah mentions that the Pulitzer Prize-winning writer Stacy Schiff was not considered notable enough for a Wikipedia entry before she wrote an extensive New Yorker article on Wikipedia itself.
A 2014 study found no correlation between characteristics of a given Wikipedia page about an academic and the academic's notability as determined by citation counts. The metrics of each Wikipedia page examined included length, number of links to the page from other articles, and number of edits made to the page. This study also found that Wikipedia did not cover notable ISI highly cited researchers properly. [77]

Partisanship
There have been suggestions that a politically liberal viewpoint is predominant . According to Jimmy Wales: "The Wikipedia community is very diverse, from liberal to conservative to libertarian and beyond. If averages mattered, and due to the nature of the wiki software (no voting) they almost certainly don't, I would say that the Wikipedia community is slightly more liberal than the U.S. population on average, because we are global and the international community of English speakers is slightly more liberal than the U.S. population. There are no data or surveys to back that." [78] Andrew Schlafly created Conservapedia because of his perception that Wikipedia contained a liberal bias. [79] Conservapedia's editors have compiled a list of alleged examples of liberal bias in Wikipedia. [80] In 2007, an article in The Christian Post criticised Wikipedia's coverage of intelligent design , saying that it was biased and hypocritical. [81] Lawrence Solomon of the National Review considered the Wikipedia articles on subjects like global warming , intelligent design , and Roe v. Wade all to be slanted in favor of liberal views. [82]
In a September 2010 issue of the conservative weekly Human Events , Rowan Scarborough presented a critique of Wikipedia's coverage of American politicians prominent in the approaching midterm elections as evidence of systemic liberal bias. [83] Scarborough compares the biographical articles of liberal and conservative opponents in Senate races in the Alaska Republican primary and the Delaware and Nevada general election, emphasizing the quantity of negative coverage of Tea Party -endorsed candidates. He also cites some criticism by Lawrence Solomon and quotes in full the lead section of Wikipedia's article on its rival Conservapedia as evidence of an underlying bias.
Shane Greenstein and Feng Zhu analyzed 2012 era Wikipedia articles on U.S. politics , going back a decade, and wrote a study [84] arguing the more contributors there were to an article, the less biased the article would be, and that — based on a study of frequent collocations — fewer articles "leaned Democrat" than was the case in Wikipedia's early years. [85] [86]

American and corporate bias
In 2008, Tim Anderson, a senior lecturer in political economy at the University of Sydney , said that Wikipedia administrators display an American-focused bias in their interactions with editors and their determinations of which sources are appropriate for use on the site. Anderson was outraged after several of the sources he used in his edits to the Hugo Chávez article, including Venezuela Analysis and Z Magazine , were disallowed as "unusable". Anderson also described Wikipedia's neutral point of view policy to ZDNet Australia as "a facade" and that Wikipedia "hides behind a reliance on corporate media editorials". [87]

Racial bias
Wikipedia has been criticized for having a systemic racial bias in its coverage, due to an under-representation of people of colour within its editor base. [88] The President of Wikimedia D.C. , James Hare, noted that "a lot of black history is left out" of Wikipedia, due to articles predominately being written by white editors. [89] Articles that do exist on African topics are, according to some critics, largely edited by editors from Europe and North America and thus reflect their knowledge and consumption of media, which "tend to perpetuate a negative image" of Africa. [90] Maira Liriano of the Schomburg Center for Research in Black Culture , has argued that the lack of information regarding black history on Wikipedia "makes it seem like it's not important." [91] San Francisco Poet Laureate Alejandro Murguía has stressed how it is important for Latinos to be part of Wikipedia "because it is a major source of where people get their information." [92]

Gender bias and sexism
Wikipedia has a longstanding controversy concerning gender bias and sexism. [94] [95] [96] [97] [98] [99] Gender bias on Wikipedia refers to the finding that between 84 and 91 percent of Wikipedia editors are male, [100] [101] which allegedly leads to systemic bias . [102] Wikipedia has been criticized [94] by some journalists and academics for lacking not only women contributors but also extensive and in-depth encyclopedic attention to many topics regarding gender. Sue Gardner , the former executive director of the foundation, said that increasing diversity was about making the encyclopedia "as good as it could be". Factors the article cited as possibly discouraging women from editing included the "obsessive fact-loving realm", associations with the "hard-driving hacker crowd", and the necessity to be "open to very difficult, high-conflict people, even misogynists." [95] In 2011, the Wikimedia Foundation set a goal of increasing the proportion of female contributors to 25 percent by 2015. [95] In August 2013, Gardner conceded defeat: "I didn't solve it. We didn't solve it. The Wikimedia Foundation didn't solve it. The solution won't come from the Wikimedia Foundation." [103] In August 2014, Wikipedia co-founder Jimmy Wales acknowledged in a BBC interview the failure of Wikipedia to fix the gender gap and announced the Wikimedia Foundation 's plans for "doubling down" on the issue. Wales said the Foundation would be open to more outreach and more software changes. [104]

Sexual content
Wikipedia has been criticized for allowing graphic sexual content such as images and videos of masturbation and ejaculation as well as photos from hardcore pornographic films found on its articles. Child protection campaigners say graphic sexual content appears on many Wikipedia entries, displayed without any warning or age verification. [105]
The Wikipedia article Virgin Killer —a 1976 album from German heavy metal band Scorpions —features a picture of the album's original cover, which depicts a naked prepubescent girl. In December 2008, the Internet Watch Foundation , a nonprofit, nongovernment-affiliated organization, added the article to its blacklist, criticizing the inclusion of the picture as "distasteful". As a result, access to the article was blocked for four days by most Internet service providers in the United Kingdom. [106]
In April 2010, Larry Sanger , a co-founder of Wikipedia who had left the organization eight years previously, wrote a letter to the Federal Bureau of Investigation , outlining his concerns that two categories of images on Wikimedia Commons contained child pornography, and were in violation of United States federal obscenity law. Sanger also expressed concerns about access to the images on Wikipedia in schools. [107] Sanger later said that it was probably not correct to call it "child pornography", which most people associate with images of real children, and that he should have said "depictions of child sexual abuse". [108] Wikimedia Foundation spokesman Jay Walsh said that Wikipedia doesn't have "material we would deem to be illegal. If we did, we would remove it." [107] Following the complaint by Larry Sanger, Jimmy Wales deleted many sexual images without consulting the community; some were reinstated following discussion. [109] Critics, including Wikipediocracy , noticed that many of the sexual images deleted from Wikipedia since 2010 have reappeared. [110]

Exposure to vandals
As an online encyclopedia which almost anyone can edit, Wikipedia has long had problems with vandalism of articles, which range from "blanking" articles to inserting profanities, hoaxes or nonsense. Wikipedia has a range of tools available to users and administrators in order to fight against vandalism, including blocking and banning of vandals and automated bots that detect and repair vandalism. Supporters of the project argue that the vast majority of vandalism on Wikipedia is reverted within a short time, and a study by Fernanda Viégas of the MIT Media Lab and Martin Wattenberg and Kushal Dave of IBM Research found that most vandal edits were reverted within around five minutes; however they state that "it is essentially impossible to find a crisp definition of vandalism". [111] While most instances of page blanking or the addition of offensive material are soon reverted, less obvious vandalism, or vandalism to a little viewed article, has remained for longer periods.
A 2007 peer-reviewed study [112] that measured the actual number of page views with "damaged" content, concluded:

Privacy concerns
Most privacy concerns refer to cases of government or employer data gathering; or to computer or electronic monitoring; or to trading data between organizations. "The Internet has created conflicts between personal privacy, commercial interests and the interests of society at large" warn James Donnelly and Jenifer Haeckl. [113] Balancing the rights of all concerned as technology alters the social landscape will not be easy. It "is not yet possible to anticipate the path of the common law or governmental regulation" regarding this problem. [113]
The concern in the case of Wikipedia is the right of a private citizen to remain private; to remain a "private citizen" rather than a " public figure " in the eyes of the law. [114] It is somewhat of a battle between the right to be anonymous in cyberspace and the right to be anonymous in real life (" meatspace "). [ citation needed ] Wikipedia Watch argues that "Wikipedia is a potential menace to anyone who values privacy" and that "a greater degree of accountability in the Wikipedia structure" would be "the very first step toward resolving the privacy problem." [115] A particular problem occurs in the case of an individual who is relatively unimportant and for whom there exists a Wikipedia page against their wishes.
In 2005 Agence France-Presse quoted Daniel Brandt, the Wikipedia Watch owner, as saying that "the basic problem is that no one, neither the trustees of Wikimedia Foundation, nor the volunteers who are connected with Wikipedia, consider themselves responsible for the content." [116]
In January 2006, a German court ordered the German Wikipedia shut down within Germany because it stated the full name of Boris Floricic , aka "Tron", a deceased hacker who was formerly with the Chaos Computer Club . More specifically, the court ordered that the URL within the German .de domain ( http://www.wikipedia.de/ ) may no longer redirect to the encyclopedia's servers in Florida at http://de.wikipedia.org although German readers were still able to use the US-based URL directly, and there was virtually no loss of access on their part. The court order arose out of a lawsuit filed by Floricic's parents, demanding that their son's surname be removed from Wikipedia. [ citation needed ] On February 9, 2006, the injunction against Wikimedia Deutschland was overturned, with the court rejecting the notion that Tron's right to privacy or that of his parents were being violated. [117]

Criticism of the community

Role of Jimmy Wales
The community of Wikipedia editors has been criticized for placing an irrational emphasis on Jimmy Wales as a person. Wales's role in personally determining the content of some articles has also been criticized as contrary to the independent spirit that Wikipedia supposedly has gained. [118] [119] In early 2007, Wales dismissed the criticism of the Wikipedia model: "I am unaware of any problems with the quality of discourse on the site. I don't know of any higher-quality discourse anywhere." [120] [121] [122] [123] [124]

Conflict of interest cases
A Business Insider article wrote about a controversy in September 2012 where two Wikimedia Foundation employees were found to have been "running a PR business on the side and editing Wikipedia on behalf of their clients." [125]

Unfair treatment of female contributors
Some female editors have stated that they have been harassed by male editors. [126]
The English Wikipedia Arbitration Committee has been criticized as unfairly targeting female and feminist editors. [126]
In an article for Slate , David Auerbach criticized the decisions made by the Arbitration Committee, in a December 2014 case centered around the site's Gender Gap Task Force . Auerbach was critical of the committee's decision to permanently ban a female editor involved in the case, while not banning her male "chief antagonists", stating "With the Arbitration Committee opting only to ban the one woman in the dispute despite her behavior being no worse than that of the men, it's hard not to see this as a setback to Wikipedia's efforts to rectify its massive gender gap." [127]
In January 2015, The Guardian reported that the Arbitration Committee had banned five feminist editors from gender-related articles on a case related to the Gamergate controversy , while including quotes from a Wikipedia editor alleging unfair treatment. [128] [129] Other commentators, including from Gawker and ThinkProgress , provided additional analysis while sourcing from The Guardian ' s story. [129] [130] [131] [132] [133] Reports in The Washington Post , Slate and Social Text described these articles as "flawed" or factually inaccurate, pointing out that the Arbitration case had not concluded as at the time of publishing; no editor had been banned. [129] [134] [135] After the result was published, Gawker wrote that "ArbCom ruled to punish six editors who could be broadly classified as 'anti-Gamergate' and five who are 'pro-Gamergate'." All of the supposed "Five Horsemen" were among the editors punished, with one of them being the sole editor banned due to this case. [136] An article called "ArbitrationGate" regarding this situation was created (and quickly deleted) on Wikipedia, while The Guardian later issued a correction to their article. [129] The Committee and the Wikimedia Foundation issued press statements that the Gamergate case was in response to the atmosphere of the Gamergate article resembling a "battlefield" due to "various sides of the discussion [having] violated community policies and guidelines on conduct", and that the Committee was fulfilling its role to "uphold a civil, constructive atmosphere" on Wikipedia. The Committee also wrote that it "does not rule on the content of articles, or make judgements on the personal views of parties to the case". [134] [137] Michael Mandiberg , writing in Social Text , remained unconvinced. [135]

Lack of verifiable identities

Scandals involving administrators and arbitrators
David Boothroyd, a Wikipedia editor and a Labour Party ( United Kingdom ) member, created controversy in 2009, when Wikipedia Review contributor "Tarantino" discovered that he committed sockpuppeting , editing under the accounts "Dbiv", "Fys", and "Sam Blacketer", none of which acknowledged his real identity. After earning Administrator status with one account, then losing it for inappropriate use of the administrative tools, Boothroyd regained Administrator status with the Sam Blacketer sockpuppet account in April 2007. [ citation needed ] Later in 2007, Boothroyd's Sam Blacketer account became part of the English Wikipedia's Arbitration Committee. [ citation needed ] Under the Sam Blacketer account, Boothroyd edited many articles related to United Kingdom politics, including that of rival Conservative Party leader David Cameron . [138] [139] Boothroyd then resigned as an administrator and as an arbitrator. [140] [141]

Essjay controversy
In July 2006 The New Yorker ran a feature about Wikipedia by Stacy Schiff about a highly credentialed Wikipedia editor. [142] The initial version of the article included an interview with a Wikipedia administrator known by the pseudonym Essjay, who was described as a tenured professor of theology. [143] Essjay's Wikipedia user page, now removed, said the following:
Essjay also stated that he held four academic degrees: Bachelor of Arts in Religious Studies (B.A.), Master of Arts in Religion (M.A.R.), Doctorate of Philosophy in Theology (Ph.D.), and Doctorate in Canon Law (JCD). Essjay specialized in editing articles about religion on Wikipedia, including subjects such as "the penitential rite, transubstantiation, the papal tiara"; [142] on one occasion he was called in to give some "expert testimony" on the status of Mary in the Roman Catholic Church . [144] In January 2007, Essjay was hired as a manager with Wikia , a wiki-hosting service founded by Wales and Angela Beesley. In February, Wales appointed Essjay as a member of the Wikipedia Arbitration Committee , a group with powers to issue binding rulings in disputes relating to Wikipedia. [145]
In late February 2007 The New Yorker added an editorial note to its article on Wikipedia stating that it had learned that Essjay was Ryan Jordan, a 24-year-old college dropout from Kentucky with no advanced degrees and no teaching experience. [146] Initially Jimmy Wales commented on the issue of Essjay's identity: "I regard it as a pseudonym and I don't really have a problem with it." Larry Sanger , co-founder [147] [148] [149] of Wikipedia, responded to Wales on his Citizendium blog by calling Wales' initial reaction "utterly breathtaking, and ultimately tragic." Sanger said the controversy "reflects directly on the judgment and values of the management of Wikipedia." [150]
Wales later issued a new statement saying he had not previously understood that "EssJay used his false credentials in content disputes." He added: "I have asked EssJay to resign his positions of trust within the [Wikipedia] community." [151] Sanger responded the next day: "It seems Jimmy finds nothing wrong, nothing trust-violating, with the act itself of openly and falsely touting many advanced degrees on Wikipedia. But there most obviously is something wrong with it, and it's just as disturbing for Wikipedia's head to fail to see anything wrong with it." [152]
On March 4, Essjay wrote on his user page that he was leaving Wikipedia, and he also resigned his position with Wikia. [153] A subsequent article in The Courier-Journal ( Louisville ) suggested that the new résumé he had posted at his Wikia page was exaggerated. [154] The March 19, 2007 issue of The New Yorker published a formal apology by Wales to the magazine and Stacy Schiff for Essjay's false statements. [155]
Discussing the incident, the New York Times noted that the Wikipedia community had responded to the affair with "the fury of the crowd", and observed:
The Essjay incident received extensive media coverage, including a national United States television broadcast on ABC's World News with Charles Gibson [157] and the March 7, 2007, Associated Press story. [158] The controversy has led to a proposal that users who say that they possess academic qualifications should have to provide evidence before citing them in Wikipedia content disputes. [159] The proposal was not accepted. [160]

Anonymity
Wikipedia has been criticised for allowing editors to contribute anonymously (without a registered account and using an auto-generated IP -labeled account) or pseudonymously (using a registered account), with critics saying that this leads to a lack of accountability. [124] [161] This also sometimes leads to uncivil conduct in debates between Wikipedians. [124] [161] For privacy reasons, Wikipedia even forbids editors to reveal information about an anonymous editor on Wikipedia. [ citation needed ]

Editorial process

Level of debate, edit wars and harassment
The standard of debate on Wikipedia has been called into question by persons who have noted that contributors can make a long list of salient points and pull in a wide range of empirical observations to back up their arguments, only to have them ignored completely on the site. [162] An academic study of Wikipedia articles found that the level of debate among Wikipedia editors on controversial topics often degenerated into counterproductive squabbling:
In 2008, a team from the Palo Alto Research Center found that for editors that make between two and nine edits a month, the percentage of their edits being reverted had gone from 5% in 2004 to about 15%, and people who only make one edit a month were being reverted at a 25% rate. [164] According to The Economist magazine (2008), "The behaviour of Wikipedia's self-appointed deletionist guardians, who excise anything that does not meet their standards, justifying their actions with a blizzard of acronyms, is now known as "wiki-lawyering". [165] In regards to the decline in the number of Wikipedia editors since the 2007 policy changes, another study stated this was partly down to the way "in which newcomers are rudely greeted by automated quality control systems and are overwhelmed by the complexity of the rule system." [166]
Another complaint about Wikipedia focuses on the efforts of contributors with idiosyncratic beliefs , who push their point of view in an effort to dominate articles, especially controversial ones. [167] [168] This sometimes results in revert wars and pages being locked down. In response, an Arbitration Committee has been formed on the English Wikipedia that deals with the worst alleged offenders—though a conflict resolution strategy is actively encouraged before going to this extent. Also, to stop the continuous reverting of pages, Jimmy Wales introduced a "three-revert rule", whereby those users who reverse the effect of others' contributions to one article more than three times in a 24-hour period may be blocked. [ citation needed ]
In a 2008 article in The Brooklyn Rail , Wikipedia contributor David Shankbone contended that he had been harassed and stalked because of his work on Wikipedia, had received no support from the authorities or the Wikimedia Foundation, and only mixed support from the Wikipedia community. Shankbone wrote, "If you become a target on Wikipedia, do not expect a supportive community." [169]
David Auerbach , writing in Slate magazine, said:

Consensus and the "hive mind"
Oliver Kamm , in an article for The Times , said that Wikipedia's reliance on consensus in forming its content was dubious: [2]
Wikimedia advisor Benjamin Mako Hill also talked about Wikipedia's disproportional representation of viewpoints, saying:
Wikimedia steward Dariusz Jemielniak says:
In his article, " Digital Maoism: The Hazards of the New Online Collectivism " (first published online by Edge: The Third Culture , May 30, 2006), computer scientist and digital theorist Jaron Lanier describes Wikipedia as a "hive mind" that is "for the most part stupid and boring", and asks, rhetorically, "why pay attention to it?" His thesis says:
Lanier also says the economic trend to reward entities that aggregate information, rather than those that actually generate content. [ incomprehensible ] In the absence of "new business models", the popular demand for content will be sated by mediocrity, thus reducing or even eliminating any monetary incentives for the production of new knowledge. [173]
Lanier's opinions produced some strong disagreement. Internet consultant Clay Shirky noted that Wikipedia has many internal controls in place and is not a mere mass of unintelligent collective effort:

Excessive rule-making
Various figures involved with the Wikimedia Foundation have argued that Wikipedia's increasingly complex policies and guidelines are driving away new contributors to the site . Former chair Kat Walsh has criticized the project in recent years, saying, "It was easier when I joined in 2004... Everything was a little less complicated.... It's harder and harder for new people to adjust." [175] Top Wikipedia administrator Oliver Moran also views "policy creep" as the major barrier, writing that "the loose collective running the site today, estimated to be 90 percent male, operates a crushing bureaucracy with an often abrasive atmosphere that deters newcomers who might increase participation in Wikipedia and broaden its coverage". [176]
In his 2014 book, Common Knowledge?: An Ethnography of Wikipedia , Jemielniak, the Wikimedia steward, states similarly that the sheer complexity of the rules and laws governing content and editor behavior has become excessive and creates a learning burden for new editors. [6] [7] In a 2013 study, Aaron Halfaker of the University of Minnesota concluded the same thing. [8] Jemielniak suggests actively abridging and rewriting the rules and laws to fall within a fixed and reasonable limit of size and complexity to remedy their excessive complexity and size. [6] [7]

Social stratification
Despite the perception that the Wikipedia process is democratic, "a small number of people are running the show", [177] including administrators, bureaucrats, stewards, checkusers, mediators, arbitrators, and oversighters. [10] In an article on Wikipedia conflicts in 2007, The Guardian discussed "a backlash among some editors, who say that blocking users compromises the supposedly open nature of the project and the imbalance of power between users and administrators may even be a reason some users choose to vandalize in the first place" based on the experiences of one editor who became a vandal after his edits were reverted and he was blocked for edit warring. [178]

See also
WebPage index: 00048
Oliver Kamm
Oliver Kamm (born 1963) is a British journalist and writer. Since 2008 he has been a leader writer and columnist for The Times . Before that he had a 20-year career in the financial sector.
Predominantly identifying with the left and liberal issues, he is a prominent supporter of former British Prime Minister Tony Blair . An advocate of the foreign policies pursued by the Blair government, Kamm wrote a short book, Anti-Totalitarianism: The Left-wing Case for a Neoconservative Foreign Policy (2005), which puts forward the case for an interventionist neoconservative foreign policy.

Early life
The son of translator Anthea Bell , [1] and Antony Kamm , [2] he was educated at New College, Oxford and Birkbeck College , University of London .
Kamm embarked on to a career in the financial sector, working for 20 years in the City of London as an economist and investment strategist. [3] He had posts in the Bank of England and the securities industry, including as European Equity Strategist and European Quantitative Strategist at HSBC Securities [3] and Head of Strategic Research at Commerzbank Global Equities in London. [4] He helped start a pan-European investment bank in 1997. [5] [6]

Opinions
Kamm describes his politics as left-wing. [7] His early activities in Labour included canvassing in Leicester South in the 1979 general election , which saw Margaret Thatcher become Prime Minister. While he continued to vote Labour into the 1980s, [8] he eventually became dissatisfied with the party's leadership and policies, particularly its stance on nuclear disarmament , and left the party in 1988, [9] but has continued to vote for the party on the majority of occasions. [10] He worked for the 1997 election campaign of Martin Bell , who is his uncle, [11] against incumbent Neil Hamilton , drafting a manifesto "so right-wing that Hamilton was incapable of outflanking it." [12]
That year saw the election of the 'New Labour' government of Tony Blair , which Kamm strongly supported, particularly its foreign policy and 'liberal interventionism'. [13] Although generally supportive of the Labour Party in the 2005 general election , Kamm stated that he could not support Celia Barlow , the Labour candidate in his local constituency, Hove , because of her opposition to Blair's foreign policies. Instead, he stated that he would vote for the Conservative candidate, Nicholas Boles , who supported the Iraq war. [14] Despite believing the Labour Prime Minister Gordon Brown was unsuited for office, he voted for the party at the 2010 general election . [10]
Kamm supported the 2003 Invasion of Iraq , and asserted that "the world is a safer place for the influence" George W. Bush had during his presidency. [15] Although critical of George W. Bush linking Saddam, Iran and North Korea in a combined "axis of evil", [15] in 2004, he outlined a case for supporting the re-election of George W. Bush. [13] Kamm was a patron of the Henry Jackson Society at its inception in 2005, [16] but is no longer connected to, or a member of HJS. [17] In 2006, he was a signatory to the Euston Manifesto , arguing for a reorientation of the left around what its creators termed 'anti-totalitarian' principles. He favourably commented on Peter Beinart 's The Good Fight: Why Liberals—and Only Liberals—Can Win the War on Terror and Make America Great Again , which has similar themes to Kamm's own book, arguing that the left should look to the policies of Clement Attlee and Harry S. Truman in the early days of the Cold War as a model for response to Islamism and totalitarianism. [18]
Because of Kamm's position on war and terrorism, the commentator Peter Wilby asserted that while he claims "to be left-wing" Kamm" holds "no discernible left-wing views". [19] Kamm rejects this criticism, saying that he "claim[s] to be left-wing, for the straightforward reason that it's true". He elaborates on his support for left-wing policies such as economic redistribution, progressive taxation and a welfare state. He also supports legal abortion and gay marriage. [7] When interviewed by politics academic Norman Geras in 2003, he said that he wrote to "express a militant liberalism that I feel ought to be part of public debate but which isn't often articulated, or at least not where I can find it, in the communications media that I read or listen to" and that he felt that "the crucial distinction in politics is not between Left and Right, as I had once tribally thought, but between the defenders and the enemies of an open society." [5] Kamm wrote that former Prime Minister James Callaghan 's "greatest single achievement" was to "destroy socialism as a serious proposition in British politics." [20] In 2008, he supported the rendition of suspected terrorists. [21]
Kamm wrote an article for Index on Censorship following the 2009 visit of Geert Wilders arguing that "No one has a right in a free society to be protected from anguish". [22]
Regarding the bombing of Dresden , he has asserted that the bombing of the city "was not a crime. It was a terrible act in a just and necessary war." [23]
In September 2011, Kamm wrote in the New Statesman that he supports the Euro and admonishes Labour's recent criticisms of it: "Monetary union is not the cause of the crisis. Done properly, it may help insulate member states from disruptive volatility in the international capital markets". [24] He criticised Ed Miliband 's stand on immigration before the 2015 general election , finding the Labour leader's position decidedly illiberal. [10] He believes current controls are far too tight, that immigration is economically beneficial, and such arguments against incomers are based on the Lump of labour fallacy . [10]
Other publications Kamm has contributed to include The Jewish Chronicle , for which he writes most months, [25] Prospect magazine, [26] and The Guardian . [27] In Prospect in February 2016, he wrote that he had resigned from the Frontline Club after founder Vaughan Smith had given refuge to Julian Assange at the club. Kamm wrote that "Smith’s statement in defence of his decision tellingly made not a single reference to the women Assange is alleged to have attacked." [28]

Criticism of Noam Chomsky
Kamm has criticised the linguist and political writer Noam Chomsky . His thoughts on Chomsky are summarised in an article [29] for Prospect magazine opposing a readers' poll choice of Chomsky in the top position for its 2005 Global Intellectuals Poll . [30]
Chomsky in turn accused Kamm of "transparent falsification" and claimed that Kamm's article demonstrated "the lengths to which some will go to prevent exposure of state crimes and their own complicity in them". [31] Kamm replied by accusing Chomsky of "polemical distortions" by selective self-quotation. [32]

Bibliography
WebPage index: 00049
Accountability
In ethics and governance , accountability is answerability, blameworthiness , liability , and the expectation of account-giving. [1] As an aspect of governance , it has been central to discussions related to problems in the public sector , nonprofit and private ( corporate ) and individual contexts. In leadership roles, [2] accountability is the acknowledgment and assumption of responsibility for actions, products , decisions, and policies including the administration , governance, and implementation within the scope of the role or employment position and encompassing the obligation to report, explain and be answerable for resulting consequences.
In governance, accountability has expanded beyond the basic definition of "being called to account for one's actions". [3] [4] It is frequently described as an account-giving relationship between individuals, e.g. "A is accountable to B when A is obliged to inform B about A's (past or future) actions and decisions, to justify them, and to suffer punishment in the case of eventual misconduct". [5] Accountability cannot exist without proper accounting practices; in other words, an absence of accounting means an absence of accountability.
Accountability is an element of a RACI to indicate who (or group) is ultimately answerable for the correct and thorough completion of the deliverable or task, and the one who delegates the work to those responsible.
There are various reasons (legitimate or excuses) why accountability fails. [6]

History and etymology
"Accountability" stems from late Latin accomptare (to account), a prefixed form of computare (to calculate), which in turn derived from putare (to reckon). [7] While the word itself does not appear in English until its use in 13th century Norman England, [8] [9] the concept of account-giving has ancient roots in record keeping activities related to governance and money-lending systems that first developed in Ancient Egypt , [10] Israel , [11] Babylon , [12] Greece , [13] and later, Rome . [14]

Types
Bruce Stone, O.P. Dwivedi, and Joseph G. Jabbra list 8 types of accountability, namely: moral, administrative, political, managerial, market, legal/judicial, constituency relation, and professional. [15] Leadership accountability cross cuts many of these distinctions.

Political
Political accountability is the accountability of the government , civil servants and politicians to the public and to legislative bodies such as a congress or a parliament .
Hirschman makes substantial contributions to accountability theory, positing exit or voice as pivotal accountability mechanisms. [16] The relationship between the governor and governed (i.e., [autonomous (auto) - dependent (dep)], [auto-auto], [dep-dep], [dep-auto] ) functiolns such that the prospect of citizen exit can, not only disciplines ex ante, but also ex post if the state is dependent on the citizens. The literature connects this disposition of autonomy or dependence to its fiscal capacity . States that are most responsive adjust to exit or voice. Clark & Golder model the dynamics of Hirschman's theory and elaborate important aspects like those that operationalize loyalty as an active choice. [17] Exit, voice, and loyalty will be expressed in different ways under differing regimes and given the relevant assumptions. All three of these sufficiently broad categories present ways and means of holding the state accountable.
Recall elections can be used to revoke the office of an elected official. Generally, however, voters do not have any direct way of holding elected representatives to account during the term for which they have been elected. Additionally, some officials and legislators may be appointed rather than elected. Constitution , or statute , can empower a legislative body to hold their own members, the government, and government bodies to account. This can be through holding an internal or independent inquiry . Inquiries are usually held in response to an allegation of misconduct or corruption. The powers, procedures and sanctions vary from country to country. The legislature may have the power to impeach the individual, remove them, or suspend them from office for a period of time. The accused person might also decide to resign before trial. Impeachment in the United States has been used both for elected representatives and other civil offices, such as district court judges .
In parliamentary systems, the government relies on the support or parliament, which gives parliament power to hold the government to account. For example, some parliaments can pass a vote of no confidence in the government.
Belsky et. al point out, whereas, under more democratic governance accountability is built into the institution of the state by a habit of regular elections, accountability in autocratic regimes [18] relies on a selectorate; a group that legitimizes or delegitimizes the autocrats powers according to selectorate theory . The primary mechanism at a selectorate's disposal is deposition, which is a form of exit. Beyond that institutions can act as credible restraints on autocracy as well.
Researchers at the Overseas Development Institute found that empowering citizens in developing countries to be able to hold their domestic governments to account was incredibly complex in practice. However, by developing explicit processes that generate change from individuals, groups or communities ( Theories of Change ), and by fusing political economy analysis and outcome mapping tools, the complex state-citizen dynamics can be better understood. As such, more effective ways to achieve outcomes can hence be generated. [19]
Researchers at the International Budget Partnership (IBP) found that civil society organizations play an important role in achieving accountability outcomes. The IBP case studies showed that CSOs can have an impact in a broad array of political and economic contexts. The researchers concluded that CSOs are most effective when they draw in a broad web of actors from across the accountability system, including the media, auditors, donors, the legislature, executive insiders, and political parties. [20]

Ethical
Within an organization, the principles and practices of ethical accountability aim to improve both the internal standard of individual and group conduct as well as external factors, such as sustainable economic and ecologic strategies. Also, ethical accountability plays a progressively important role in academic fields, such as laboratory experiments and field research. Debates around the practice of ethical accountability on the part of researchers in the social field – whether professional or others – have been thoroughly explored by Norma R.A. Romm in her work on Accountability in Social Research, [21] including her book on New Racism: Revisiting Researcher Accountabilities, reviewed by Carole Truman in the journal Sociological Research Online. [22] Here it is suggested that researcher accountability implies that researchers are cognizant of, and take some responsibility for, the potential impact of their ways of doing research – and of writing it up – on the social fields of which the research is part. That is, accountability is linked to considering carefully, and being open to challenge in relation to, one's choices concerning how research agendas are framed and the styles in which write-ups of research "results" are created.

Administrative
Internal rules and norms as well as some independent commission are mechanisms to hold civil servants within the administration of government accountable. Within department or ministry, firstly, behavior is bound by rules and regulations; secondly, civil servants are subordinates in a hierarchy and accountable to superiors. Nonetheless, there are independent "watchdog" units to scrutinize and hold departments accountable; legitimacy of these commissions is built upon their independence, as it avoids any conflicts of interests. The accountability is defined as "an element which is part of a unique responsibility and which represents an obligation of an actor to achieve the goal, or to perform the procedure of a task, and the justification that it is done to someone else, under threat of sanction". [23]

Individuals within organizations
Because many different individuals in large organizations contribute in many ways to the decisions and policies, it is difficult even in principle to identify who should be accountable for the results. This is what is known, following Thompson, as the problem of many hands. [24] It creates a dilemma for accountability. If individuals are held accountable or responsible, individuals who could not have prevented the results are either unfairly punished, or they "take responsibility" in a symbolic ritual without suffering any consequences. If only organizations are held accountable, then all individuals in the organization are equally blameworthy or all are excused. Various solutions have been proposed. One is to broaden the criteria for individual responsibility so that individuals are held accountable for not anticipating failures in the organization. Another solution, recently proposed by Thompson, is to hold individuals accountable for the design of the organization, both retrospectively and prospectively. [25]

Constituency relations
Within this perspective, a particular agency of the government is accountable if voices are heard from agencies, groups or institutions outside the public sector representing citizens' interests from a particular constituency or field. Moreover, the government is obliged to empower members of agencies with political rights to run for elections and be elected; or, appoint them into the public sector as a way to make the government representative and to ensure that voices from all constituencies are included in policy-making.

Public/private overlap
With the increase over the last several decades in public service provided by private entities, especially in Britain and the United States, some have called for increased political accountability mechanisms for otherwise non-political entities. Legal scholar Anne Davies, for instance, argues that the line between public institutions and private entities like corporations is becoming blurred in certain areas of public service in the United Kingdom, and that this can compromise political accountability in those areas. She and others argue that some administrative law reform is necessary to address this accountability gap. [26]
With respect to the public/private overlap in the United States, public concern over the contracting of government services (including military) and the resulting accountability gap has been highlighted recently following the shooting incident involving the Blackwater security firm in Iraq. [27]

Contemporary studies
Accountability involves either the expectation or assumption of account-giving behavior. The study of account giving as a sociological act was articulated in a 1968 article on "Accounts" by Marvin Scott and Stanford Lyman, [28] although it can be traced as well to J. L. Austin 's 1956 essay "A Plea for Excuses", [29] in which he used excuse-making as an example of speech acts .
Communications scholars have extended this work through the examination of strategic uses of excuses, justifications, rationalizations, apologies and other forms of account giving behavior by individuals and corporations, and Philip Tetlock and his colleagues have applied experimental design techniques to explore how individuals behave under various scenarios and situations that demand accountability.
Recently, accountability has become an important topic in the discussion about the legitimacy of international institutions. [30] Because there is no global democratically elected body to which organizations must account, global organizations from all sectors bodies are often criticized as having large accountability gaps. The Charter 99 for Global Democracy, [31] spearheaded by the One World Trust , first proposed that cross-sector principles of accountability be researched and observed by institutions that affect people, independent of their legal status. One paradigmatic problem arising in the global context is that of institutions such as the World Bank and the International Monetary Fund who are founded and supported by wealthy nations or individuals and provide grants and loans, to developing nations. Should those institutions be accountable to their founders and investors or to the persons and nations they lend money to? In the debate over global justice and its distributional consequences, Cosmopolitans tend to advocate greater accountability to the disregarded interests of traditionally marginalized populations and developing nations. On the other hand, those in the Nationalism and Society of States traditions deny the tenets of moral universalism and argue that beneficiaries of global development initiatives have no substantive entitlement to call international institutions to account. The One World Trust Global Accountability Report, published in a first full cycle 2006 to 2008, [32] is one attempt to measure the capability of global organizations to be accountable to their stakeholders.

Accountability in education
Student accountability is traditionally based on having school and classroom rules, combined with sanctions for infringement. As defined by National Council on Measurement in Education (NCME), accountability is "A program, often legislated, that attributes the responsibility for student learning to teachers, school administrators, and/or students. Test results typically are used to judge accountability, and often consequences are imposed for shortcomings." [33]
In contrast, some educational establishments such as Sudbury schools believe that students are personally responsible for their acts, and that traditional schools do not permit students to choose their course of action fully; they do not permit students to embark on the course, once chosen; and they do not permit students to suffer the consequences of the course, once taken. Freedom of choice, freedom of action, freedom to bear the results of action are considered the three great freedoms that constitute personal responsibility. Sudbury schools claim that " ' Ethics ' is a course taught by life experience". They adduce that the essential ingredient for acquiring values—and for moral action is personal responsibility, that schools will become involved in the teaching of morals when they become communities of people who fully respect each other's right to make choices, and that the only way the schools can become meaningful purveyors of ethical values is if they provide students and adults with real-life experiences that are bearers of moral import. Students are given complete responsibility for their own education and the school is run by a direct democracy in which students and staff are equals. [34] [35] [36] [37] [38] [39]

Media and accountability
Econometric research has found that countries with greater press freedom tend to have less corruption. [40] Greater political accountability and lower corruption were more likely where newspaper consumption was higher in data from roughly 100 countries and from different states in the US. [41] A "poor fit between newspaper markets and political districts reduces press coverage of politics. ... Congressmen who are less covered by the local press work less for their constituencies: they are less likely to stand witness before congressional hearings ... . Federal spending is lower in areas where there is less press coverage of the local members of congress." [42] This was supported by an analysis of the consequences of the closure of the Cincinnati Post in 2007. The following year, "fewer candidates ran for municipal office in the Kentucky suburbs most reliant on the Post , incumbents became more likely to win reelection, and voter turnout and campaign spending fell." [43]
An analysis of the evolution of mass media in the US and Europe since World War II noted mixed results from the growth of the Internet: "The digital revolution has been good for freedom of expression [and] information [but] has had mixed effects on freedom of the press": It has disrupted traditional sources of funding, and new forms of Internet journalism have replaced only a tiny fraction of what's been lost. [44] Various systems have been proposed for increasing the funds available for investigative journalism that allow individual citizens to direct small amounts of government funds to news outlets or investigative journalism projects of their choice.
To train people to conduct these kinds of investigations, Charles Lewis has proposed "the creation of a new multidisciplinary academic field called Accountability Studies. ... [S]tudents from widely different academic backgrounds are excited about the prospect of learning exactly how to investigate those in power and hold them accountable." [45]

Standards
Accountability standards have been set up, and organizations can voluntarily commit to them. Standards apply in particular to the non-profit world and to Corporate Social Responsibility (CSR) initiatives. Accountability standards include:
In addition, some non-profit organizations set up their own commitments to accountability:

Proposed symbolism
Viktor Frankl , neurologist, psychiatrist, author, and founder of logotherapy and one of the key figures in existential therapy , in his book Man's Search for Meaning recommended "that the Statue of Liberty on the East Coast (that has become a symbol of Liberty and Freedom ) should be supplemented by a Statue of Responsibility on the West Coast." Frankl stated: "Freedom, however, is not the last word. Freedom is only part of the story and half of the truth. Freedom is but the negative aspect of the whole phenomenon whose positive aspect is responsibleness. In fact, freedom is in danger of degenerating into mere arbitrariness unless it is lived in terms of responsibleness." [50] [51]

See also

Footnotes
WebPage index: 00050
Spamming
Electronic spamming is the use of electronic messaging systems to send an unsolicited message ( spam ), especially advertising, as well as sending messages repeatedly on the same site. While the most widely recognized form of spam is email spam , the term is applied to similar abuses in other media: instant messaging spam , Usenet newsgroup spam , Web search engine spam , spam in blogs , wiki spam , online classified ads spam, mobile phone messaging spam , Internet forum spam , junk fax transmissions , social spam , spam mobile apps, [1] television advertising and file sharing spam. It is named after Spam , a luncheon meat, by way of a Monty Python sketch about a menu that includes Spam in every dish. [2] [3]
Spamming remains economically viable because advertisers have no operating costs beyond the management of their mailing lists, servers, infrastructures, IP ranges, and domain names, and it is difficult to hold senders accountable for their mass mailings. Because the barrier to entry is so low, spammers are numerous, and the volume of unsolicited mail has become very high. In the year 2011, the estimated figure for spam messages is around seven trillion. The costs, such as lost productivity and fraud, are borne by the public and by Internet service providers , which have been forced to add extra capacity to cope with the deluge. Spamming has been the subject of legislation in many jurisdictions. [4]
A person who creates electronic spam is called a spammer . [5]

Etymology
The term spam is derived from the 1970 Spam sketch of the BBC television comedy series Monty Python's Flying Circus . [6] [3] The sketch is set in a cafe where nearly every item on the menu includes Spam canned luncheon meat. As the waiter recites the Spam-filled menu, a chorus of Viking patrons drowns out all conversations with a song repeating "Spam, Spam, Spam, Spam… Spammity Spam! Wonderful Spam!", hence spamming the dialogue. [7] The excessive amount of Spam mentioned references the preponderance of it and other imported canned meat products in the United Kingdom after World War II, as the country struggled to rebuild its agricultural base. Spam captured a large slice of the British market within the lower classes, and became a byword among British children of the 1960s for low-grade fodder due to its commonality, monotonous taste and low price, leading to the humour of the Python sketch. [ citation needed ]
In the 1980s the term was adopted to describe certain abusive users who frequented BBSs and MUDs , who would repeat "Spam" a huge number of times to scroll other users' text off the screen. [8] In early chat rooms services like PeopleLink and the early days of Online America (later known as America Online or AOL), they actually flooded the screen with quotes from the Monty Python Spam sketch. [ citation needed ] With internet connections over phone lines, typically running at 1200 or even 300 bit/s , it could take an enormous amount of time for a spammy logo, drawn in ASCII art to scroll to completion on a viewer's terminal. Sending an irritating, large, meaningless block of text in this way was called spamming. This was used as a tactic by insiders of a group that wanted to drive newcomers out of the room so the usual conversation could continue. It was also used to prevent members of rival groups from chatting—for instance, Star Wars fans often invaded Star Trek chat rooms, filling the space with blocks of text until the Star Trek fans left. [9] This act, previously called flooding or trashing , later became known as spamming . [10] The term was soon applied to a large amount of text broadcast by many users.
It later came to be used on Usenet to mean excessive multiple posting —the repeated posting of the same message. The unwanted message would appear in many, if not all newsgroups, just as Spam appeared in nearly all the menu items in the Monty Python sketch. The first usage of this sense was by Joel Furr [11] in the aftermath of the ARMM incident of March 31, 1993, in which a piece of experimental software released dozens of recursive messages onto the news.admin.policy newsgroup. [12] This use had also become established—to spam Usenet was flooding newsgroups with junk messages. The word was also attributed to the flood of " Make Money Fast " messages that clogged many newsgroups during the 1990s. [ citation needed ] In 1998, the New Oxford Dictionary of English , which had previously only defined "spam" in relation to the trademarked food product, added a second definition to its entry for "spam": "Irrelevant or inappropriate messages sent on the Internet to a large number of newsgroups or users."
There was also an effort to differentiate between types of newsgroup spam. Messages that were crossposted to too many newsgroups at once – as opposed to those that were posted too frequently – were called velveeta (after a cheese product). But this term didn't persist. [13]

History

Pre-Internet
In the late 19th Century Western Union allowed telegraphic messages on its network to be sent to multiple destinations. The first recorded instance of a mass unsolicited commercial telegram is from May 1864, when some British politicians received an unsolicited telegram advertising a dentistry shop. [14]

History
The earliest documented spam (although the term had not yet been coined [15] ) was a message advertising the availability of a new model of Digital Equipment Corporation computers sent by Gary Thuerk to 393 recipients on ARPANET in 1978. [11] Rather than send a separate message to each person, which was the standard practice at the time, he had an assistant, Carl Gartley, write a single mass email. Reaction from the net community was fiercely negative, but the spam did generate some sales. [16] [17]
Spamming had been practiced as a prank by participants in multi-user dungeon games, to fill their rivals' accounts with unwanted electronic junk. [17] The first known electronic chain letter , titled Make Money Fast , was released in 1988.
The first major commercial spam incident started on March 5, 1994, when a husband and wife team of lawyers, Laurence Canter and Martha Siegel , began using bulk Usenet posting to advertise immigration law services. The incident was commonly termed the " Green Card spam", after the subject line of the postings. Defiant in the face of widespread condemnation, the attorneys claimed their detractors were hypocrites or "zealouts", claimed they had a free speech right to send unwanted commercial messages, and labeled their opponents "anti-commerce radicals." The couple wrote a controversial book entitled How to Make a Fortune on the Information Superhighway . [17]
Within a few years, the focus of spamming (and anti-spam efforts) moved chiefly to email, where it remains today. [8] Arguably, the aggressive email spamming by a number of high-profile spammers such as Sanford Wallace of Cyber Promotions in the mid-to-late 1990s contributed to making spam predominantly an email phenomenon in the public mind. [ citation needed ] By 1999, Khan C. Smith, a well known hacker at the time, had begun to commercialize the bulk email industry and rallied thousands into the business by building more friendly bulk email software and providing internet access illegally hacked from major ISPs such as Earthlink and Botnets. [18]
By 2009, the majority of spam sent around the World was in the English language ; spammers began using automatic translation services to send spam in other languages. [19]

In different media

Email
Email spam, also known as unsolicited bulk email (UBE), junk mail, or unsolicited commercial email (UCE), is the practice of sending unwanted email messages, frequently with commercial content, in large quantities to an indiscriminate set of recipients. Spam in email started to become a problem when the Internet was opened up to the general public in the mid-1990s. It grew exponentially over the following years, and today composes some 80 to 85 percent of all the e-mail in the World, by a "conservative estimate". [20] Pressure to make email spam illegal has been successful in some jurisdictions, but less so in others. The efforts taken by governing bodies, security systems and email service providers seem to be helping to reduce the onslaught of email spam. According to "2014 Internet Security Threat Report, Volume 19" published by Symantec Corporation , spam volume dropped to 66% of all email traffic. [21] Spammers take advantage of this fact, [ clarification needed ] and frequently outsource parts of their operations to countries where spamming will not get them into legal trouble.
Increasingly, e-mail spam today is sent via " zombie networks", networks of virus - or worm -infected personal computers in homes and offices around the globe. Many modern worms install a backdoor that allows the spammer to access the computer and use it for malicious purposes. This complicates attempts to control the spread of spam, as in many cases the spam does not obviously originate from the spammer. In November 2008 an ISP, McColo , which was providing service to botnet operators, was depeered and spam dropped 50 to 75 percent Internet-wide. At the same time, it is becoming clear that malware authors, spammers, and phishers are learning from each other, and possibly forming various kinds of partnerships.
An industry of email address harvesting is dedicated to collecting email addresses and selling compiled databases. [22] Some of these address-harvesting approaches rely on users not reading the fine print of agreements, resulting in their agreeing to send messages indiscriminately to their contacts. This is a common approach in social networking spam such as that generated by the social networking site Quechup . [23]

Instant messaging
Instant messaging spam makes use of instant messaging systems. Although less ubiquitous than its e-mail counterpart, according to a report from Ferris Research, 500 million spam IMs were sent in 2003, twice the level of 2002. As instant messaging tends to not be blocked by firewalls, it is an especially useful channel for spammers. This is very common on many instant messaging systems such as Skype .

Newsgroup and forum
Newsgroup spam is a type of spam where the targets are Usenet newsgroups. Spamming of Usenet newsgroups actually pre-dates e-mail spam. Usenet convention defines spamming as excessive multiple posting, that is, the repeated posting of a message (or substantially similar messages). The prevalence of Usenet spam led to the development of the Breidbart Index as an objective measure of a message's "spamminess".
Forum spam is the creation of advertising messages on Internet forums. It is generally done by automated spambots. Most forum spam consists of links to external sites, with the dual goals of increasing search engine visibility in highly competitive areas such as weight loss, pharmaceuticals, gambling, pornography, real estate or loans, and generating more traffic for these commercial websites. Some of these links contain code to track the spambot's identity; if a sale goes through, the spammer behind the spambot works on commission.

Mobile phone
Mobile phone spam is directed at the text messaging service of a mobile phone . This can be especially irritating to customers not only for the inconvenience, but also because of the fee they may be charged per text message received in some markets. The term "SpaSMS" was coined at the adnews website Adland in 2000 to describe spam SMS. To comply with CAN-SPAM regulations in the US, SMS messages now must provide options of HELP and STOP, the latter to end communication with the advertiser via SMS altogether.
Despite the high number of phone users, there has not been so much phone spam, because there is a charge for sending SMS, and installing trojans into other's phones that send spam (common for e-mail spam) is hard because applications normally must be downloaded from a central database.

Social networking spam
Facebook and Twitter are not immune to messages containing spam links. Most insidiously, spammers hack into accounts and send false links under the guise of a user's trusted contacts such as friends and family. [24] As for Twitter, spammers gain credibility by following verified accounts such as that of Lady Gaga; when that account owner follows the spammer back, it legitimizes the spammer and allows him or her to proliferate. [25] Twitter has studied what interest structures allow their users to receive interesting tweets and avoid spam, despite the site using the broadcast model, in which all tweets from a user are broadcast to all followers of the user. [26]

Social spam
Spreading beyond the centrally managed social networking platforms, user-generated content increasingly appears on business, government, and nonprofit websites worldwide. Fake accounts and comments planted by computers programmed to issue social spam can infiltrate these websites. [27] Well-meaning and malicious human users can break websites' policies by submitting profanity, [28] insults, [29] hate speech , and violent messages.

Online game messaging
Many online games allow players to contact each other via player-to-player messaging, chat rooms, or public discussion areas. What qualifies as spam varies from game to game, but usually this term applies to all forms of message flooding, violating the terms of service contract for the website. This is particularly common in MMORPGs where the spammers are trying to sell game-related "items" for real-world money, chiefly among them being in-game currency. In gameplay terms, spamming also refers to the repetitive use of the same combat skills as a cheap tactic (e.g. "to defeat the blue dragon, just spam fireballs").

Spam targeting search engines (spamdexing)
Spamdexing (a portmanteau of spamming and indexing ) refers to a practice on the World Wide Web of modifying HTML pages to increase their chances of high placement on search engine relevancy lists. These sites use "black-hat" search engine optimization techniques to deliberately manipulate their rank in search engines. Many modern search engines modified their search algorithms to try to exclude web pages utilizing spamdexing tactics. For example, the search bots will detect repeated keywords as spamming by using a grammar analysis. If a website owner is found to have spammed the webpage to falsely increase its page rank, the website may be penalized by search engines.

Blog, wiki, and guestbook
Blog spam , or "blam" for short, is spamming on weblogs . In 2003, this type of spam took advantage of the open nature of comments in the blogging software Movable Type by repeatedly placing comments to various blog posts that provided nothing more than a link to the spammer's commercial web site. [30] Similar attacks are often performed against wikis and guestbooks , both of which accept user contributions. Another possible form of spam in blogs is the spamming of a certain tag on websites such as Tumblr.

Spam targeting video sharing sites
Video sharing sites, such as YouTube , are now frequently targeted by spammers. The most common technique involves spammers (or spambots ) posting links to sites, most likely pornographic or dealing with online dating , on the comments section of random videos or user profiles. With the addition of a "thumbs up/thumbs down" feature, groups of spambots may constantly "thumbs up" a comment, getting it into the top comments section and making the message more visible. Another frequently used technique is using bots to post messages on random users' profiles to a spam account's channel page, along with enticing text and images, usually of a sexually suggestive nature. These pages may include their own or other users' videos, again often suggestive. The main purpose of these accounts is to draw people to the link in the home page section of their profile. YouTube has blocked the posting of such links. In addition, YouTube has implemented a CAPTCHA system that makes rapid posting of repeated comments much more difficult than before, because of abuse in the past by mass spammers who would flood individuals' profiles with thousands of repetitive comments.
Yet another kind is actual video spam, giving the uploaded movie a name and description with a popular figure or event that is likely to draw attention, or within the video has a certain image timed to come up as the video's thumbnail image to mislead the viewer, such as a still image from a feature film, purporting to be a part-by-part piece of a movie being pirated, e.g. Big Buck Bunny Full Movie Online - Part 1/10 HD , a link to a supposed keygen , trainer, ISO file for a video game , or something similar. The actual content of the video ends up being totally unrelated, a Rickroll , offensive, or simply on-screen text of a link to the site being promoted. [31] In some cases, the link in question may lead to an online survey site, a password-protected archive file with instructions leading to the aforementioned survey (though the survey, and the archive file itself, is worthless and doesn't contain the file in question at all), or in extreme cases, malware . [32] Others may upload videos presented in an infomercial -like format selling their product which feature actors and paid testimonials , though the promoted product or service is of dubious quality and would likely not pass the scrutiny of a standards and practices department at a television station or cable network .

SPIT
SPIT (SPam over Internet Telephony) is VoIP (Voice over Internet Protocol) spam, usually using SIP (Session Initiation Protocol) . This is nearly identical to telemarketing calls over traditional phone lines. When the user chooses to receive the spam call, a pre-recorded spam message or advertisement is usually played back. This is generally easier for the spammer as VoIP services are cheap and easy to anonymize over the Internet, and there are many options for sending mass amounts of calls from a single location. Accounts or IP addresses being used for VoIP spam can usually be identified by a large number of outgoing calls, low call completion and short call length.

Academic search
Academic search engines enable researchers to find academic literature and are used to obtain citation data for calculating performance metrics such as the H-index and impact factor . Researchers from the University of California, Berkeley and OvGU demonstrated that most (web-based) academic search engines, especially Google Scholar , are not capable of identifying spam attacks. [33] The researchers manipulated the citation counts of articles, and managed to make Google Scholar index complete fake articles, some containing advertising. [33]

Noncommercial forms
E-mail and other forms of spamming have been used for purposes other than advertisements. Many early Usenet spams were religious or political. Serdar Argic , for instance, spammed Usenet with historical revisionist screeds. A number of evangelists have spammed Usenet and e-mail media with preaching messages. A growing number of criminals are also using spam to perpetrate various sorts of fraud. [34]

Geographical origins
In 2011 the origins of spam were analyzed by Cisco Systems . They provided a report that shows spam volume originating from countries worldwide. [35]

Trademark issues
Hormel Foods Corporation , the maker of SPAM luncheon meat, does not object to the Internet use of the term "spamming". However, they did ask that the capitalized word "Spam" be reserved to refer to their product and trademark. [36] By and large, this request is obeyed in forums that discuss spam. In Hormel Foods v. SpamArrest , Hormel attempted to assert its trademark rights against SpamArrest, a software company, from using the mark "spam", since Hormel owns the trademark. In a dilution claim, Hormel argued that SpamArrest's use of the term "spam" had endangered and damaged "substantial goodwill and good reputation" in connection with its trademarked lunch meat and related products. Hormel also asserted that SpamArrest's name so closely resembles its luncheon meat that the public might become confused, or might think that Hormel endorses SpamArrest's products.
Hormel did not prevail. Attorney Derek Newman responded on behalf of SpamArrest: "Spam has become ubiquitous throughout the [w]orld to describe unsolicited commercial email. No company can claim trademark rights on a generic term." Hormel stated on its website: "Ultimately, we are trying to avoid the day when the consuming public asks, 'Why would Hormel Foods name its product after junk email?'". [37]
Hormel also made two attempts that were dismissed in 2005 to revoke the marks "SPAMBUSTER" [38] and Spam Cube . [39] Hormel's corporate attorney Melanie J. Neumann also sent SpamCop 's Julian Haight a letter on August 27, 1999 requesting that he delete an objectionable image (a can of Hormel's Spam luncheon meat product in a trash can), change references to UCE spam to all lower case letters, and confirm his agreement to do so. [40]

Cost–benefit analyses
The European Union 's Internal Market Commission estimated in 2001 that "junk email" cost Internet users €10 billion per year worldwide. [41] The California legislature found that spam cost United States organizations alone more than $13 billion in 2007, including lost productivity and the additional equipment, software, and manpower needed to combat the problem. [42] Spam's direct effects include the consumption of computer and network resources, and the cost in human time and attention of dismissing unwanted messages. [43] Large companies who are frequent spam targets utilize numerous techniques to detect and prevent spam. [44]
In addition, spam has costs stemming from the kinds of spam messages sent, from the ways spammers send them, and from the arms race between spammers and those who try to stop or control spam. In addition, there are the opportunity cost of those who forgo the use of spam-afflicted systems. There are the direct costs, as well as the indirect costs borne by the victims—both those related to the spamming itself, and to other crimes that usually accompany it, such as financial theft, identity theft , data and intellectual property theft, virus and other malware infection, child pornography, fraud , and deceptive marketing .
The cost to providers of search engines is not insignificant: "The secondary consequence of spamming is that search engine indexes are inundated with useless pages, increasing the cost of each processed query". [5] The methods of spammers are likewise costly. Because spamming contravenes the vast majority of ISPs' acceptable-use policies, most spammers have for many years gone to some trouble to conceal the origins of their spam. Email, Usenet, and instant-message spam are often sent through insecure proxy servers belonging to unwilling third parties. Spammers frequently use false names, addresses, phone numbers, and other contact information to set up "disposable" accounts at various Internet service providers. In some cases, they have used falsified or stolen credit card numbers to pay for these accounts. This allows them to quickly move from one account to the next as each one is discovered and shut down by the host ISPs.
The costs of spam also include the collateral costs of the struggle between spammers and the administrators and users of the media threatened by spamming. [45] Many users are bothered by spam because it impinges upon the amount of time they spend reading their email. Many also find the content of spam frequently offensive, in that pornography is one of the most frequently advertised products. Spammers send their spam largely indiscriminately, so pornographic ads may show up in a work place email inbox—or a child's, the latter of which is illegal in many jurisdictions. Recently, there has been a noticeable increase in spam advertising websites that contain child pornography . [ citation needed ]
Some spammers argue that most of these costs could potentially be alleviated by having spammers reimburse ISPs and persons for their material. [ citation needed ] There are three problems with this logic: first, the rate of reimbursement they could credibly budget is not nearly high enough to pay the direct costs [ citation needed ] , second, the human cost (lost mail, lost time, and lost opportunities) is basically unrecoverable, and third, spammers often use stolen bank accounts and credit cards to finance their operations, and would conceivably do so to pay off any fines imposed.
Email spam exemplifies a tragedy of the commons : spammers use resources (both physical and human), without bearing the entire cost of those resources. In fact, spammers commonly do not bear the cost at all. This raises the costs for everyone. In some ways spam is even a potential threat to the entire email system, as operated in the past. Since email is so cheap to send, a tiny number of spammers can saturate the Internet with junk mail. Although only a tiny percentage of their targets are motivated to purchase their products (or fall victim to their scams), the low cost may provide a sufficient conversion rate to keep the spamming alive. Furthermore, even though spam appears not to be economically viable as a way for a reputable company to do business, it suffices for professional spammers to convince a tiny proportion of gullible advertisers that it is viable for those spammers to stay in business. Finally, new spammers go into business every day, and the low costs allow a single spammer to do a lot of harm before finally realizing that the business is not profitable.
Some companies and groups "rank" spammers; spammers who make the news are sometimes referred to by these rankings. [46] [47] The secretive nature of spamming operations makes it difficult to determine how prolific an individual spammer is, thus making the spammer hard to track, block or avoid. Also, spammers may target different networks to different extents, depending on how successful they are at attacking the target. Thus considerable resources are employed to actually measure the amount of spam generated by a single person or group. For example, victims that use common anti-spam hardware, software or services provide opportunities for such tracking. Nevertheless, such rankings should be taken with a grain of salt.

General costs
In all cases listed above, including both commercial and non-commercial, "spam happens" because of a positive cost-benefit analysis result; if the cost to recipients is excluded as an externality the spammer can avoid paying.
Cost is the combination of
Benefit is the total expected profit from spam, which may include any combination of the commercial and non-commercial reasons listed above. It is normally linear, based on the incremental benefit of reaching each additional spam recipient, combined with the conversion rate . The conversion rate for botnet -generated spam has recently been measured to be around one in 12,000,000 for pharmaceutical spam and one in 200,000 for infection sites as used by the Storm botnet . [48] The authors of the study calculating those conversion rates noted, "After 26 days, and almost 350 million e-mail messages, only 28 sales resulted."

In crime
Spam can be used to spread computer viruses , trojan horses or other malicious software. The objective may be identity theft , or worse (e.g., advance fee fraud ). Some spam attempts to capitalize on human greed, while some attempts to take advantage of the victims' inexperience with computer technology to trick them (e.g., phishing ). On May 31, 2007, one of the world's most prolific spammers, Robert Alan Soloway , was arrested by US authorities. [49] Described as one of the top ten spammers in the world, Soloway was charged with 35 criminal counts, including mail fraud, wire fraud, e-mail fraud , aggravated identity theft, and money laundering. [49] Prosecutors allege that Soloway used millions of "zombie" computers to distribute spam during 2003. [50] This is the first case in which US prosecutors used identity theft laws to prosecute a spammer for taking over someone else's Internet domain name. [ citation needed ]
In an attempt to assess potential legal and technical strategies for stopping illegal spam, a study from the University of California, San Diego, and the University of California, Berkeley, "Click Trajectories: End-to-End Analysis of the Spam Value Chain", cataloged three months of online spam data and researched website naming and hosting infrastructures. The study concluded that: 1) half of all spam programs have their domains and servers distributed over just eight percent or fewer of the total available hosting registrars and autonomous systems, with 80 percent of spam programs overall being distributed over just 20 percent of all registrars and autonomous systems; 2) of the 76 purchases for which the researchers received transaction information, there were only 13 distinct banks acting as credit card acquirers and only three banks provided the payment servicing for 95 percent of the spam-advertised goods in the study; and, 3) a "financial blacklist" of banking entities that do business with spammers would dramatically reduce monetization of unwanted e-mails. Moreover, this blacklist could be updated far more rapidly than spammers could acquire new banking resources, an asymmetry favoring anti-spam efforts. [51]

Political issues
Spamming remains a hot discussion topic. In 2004, the seized Porsche of an indicted spammer was advertised on the Internet; [52] this revealed the extent of the financial rewards available to those who are willing to commit duplicitous acts online. However, some of the possible means used to stop spamming may lead to other side effects, such as increased government control over the Internet, loss of privacy, barriers to free expression, and the commercialization of e-mail. [ citation needed ]
One of the chief values favored by many long-time Internet users and experts, as well as by many members of the public, is the free exchange of ideas. Many have valued the relative anarchy of the Internet, and bridle at the idea of restrictions placed upon it. [ citation needed ] A common refrain from spam-fighters is that spamming itself abridges the historical freedom of the Internet, by attempting to force users to carry the costs of material that they would not choose. [ citation needed ]
An ongoing concern expressed by parties such as the Electronic Frontier Foundation and the American Civil Liberties Union has to do with so-called "stealth blocking", a term for ISPs employing aggressive spam blocking without their users' knowledge. These groups' concern is that ISPs or technicians seeking to reduce spam-related costs may select tools that (either through error or design) also block non-spam e-mail from sites seen as "spam-friendly". Spam Prevention Early Warning System (SPEWS) is a common target of these criticisms. Few object to the existence of these tools; it is their use in filtering the mail of users who are not informed of their use that draws fire. [ citation needed ]
Some see spam-blocking tools as a threat to free expression—and laws against spamming as an untoward precedent for regulation or taxation of e-mail and the Internet at large. Even though it is possible in some jurisdictions to treat some spam as unlawful merely by applying existing laws against trespass and conversion , some laws specifically targeting spam have been proposed. In 2004, United States passed the CAN-SPAM Act of 2003 that provided ISPs with tools to combat spam. This act allowed Yahoo! to successfully sue Eric Head, reportedly one of the biggest spammers in the World, who settled the lawsuit for several thousand U.S. dollars in June 2004. But the law is criticized by many for not being effective enough. Indeed, the law was supported by some spammers and organizations that support spamming, and opposed by many in the anti-spam community. Examples of effective anti-abuse laws that respect free speech rights include those in the U.S. against unsolicited faxes and phone calls, and those in Australia and a few U.S. states against spam. [ citation needed ]
In November 2004, Lycos Europe released a screen saver called make LOVE not SPAM that made Distributed Denial of Service attacks on the spammers themselves. It met with a large amount of controversy and the initiative ended in December 2004. [53] [54] [55]

Court cases

United States
Earthlink won a $25 million judgment against one of the most notorious and active "spammers" Khan C. Smith in 2001 for his role in founding the modern spam industry which dealt billions in economic damage and established thousands of spammers into the industry. [56] His email efforts were said to make up more than a third of all Internet email being sent from 1999 until 2002.
Sanford Wallace and Cyber Promotions were the target of a string of lawsuits, many of which were settled out of court, up through a 1998 Earthlink settlement [ citation needed ] that put Cyber Promotions out of business. Attorney Laurence Canter was disbarred by the Tennessee Supreme Court in 1997 for sending prodigious amounts of spam advertising his immigration law practice. In 2005, Jason Smathers , a former America Online employee, pleaded guilty to charges of violating the CAN-SPAM Act . In 2003, he sold a list of approximately 93 million AOL subscriber e-mail addresses to Sean Dunaway who, in turn, sold the list to spammers. [57] [58]
In 2007, Robert Soloway lost a case in a federal court against the operator of a small Oklahoma-based Internet service provider who accused him of spamming. U.S. Judge Ralph G. Thompson granted a motion by plaintiff Robert Braver for a default judgment and permanent injunction against him. The judgment includes a statutory damages award of $10,075,000 under Oklahoma law. [59]
In June 2007, two men were convicted of eight counts stemming from sending millions of e-mail spam messages that included hardcore pornographic images. Jeffrey A. Kilbride, 41, of Venice, California was sentenced to six years in prison, and James R. Schaffer, 41, of Paradise Valley, Arizona , was sentenced to 63 months. In addition, the two were fined $100,000, ordered to pay $77,500 in restitution to AOL , and ordered to forfeit more than $1.1 million, the amount of illegal proceeds from their spamming operation. [60] The charges included conspiracy , fraud , money laundering , and transportation of obscene materials. The trial, which began on June 5, was the first to include charges under the CAN-SPAM Act of 2003 , according to a release from the Department of Justice . The specific law that prosecutors used under the CAN-Spam Act was designed to crack down on the transmission of pornography in spam. [61]
In 2005, Scott J. Filary and Donald E. Townsend of Tampa, Florida were sued by Florida Attorney General Charlie Crist for violating the Florida Electronic Mail Communications Act. [62] The two spammers were required to pay $50,000 USD to cover the costs of investigation by the state of Florida , and a $1.1 million penalty if spamming were to continue, the $50,000 was not paid, or the financial statements provided were found to be inaccurate. The spamming operation was successfully shut down. [63]
Edna Fiedler, 44, of Olympia, Washington , on June 25, 2008, pleaded guilty in a Tacoma court and was sentenced to 2 years imprisonment and 5 years of supervised release or probation in an Internet $1 million "Nigerian check scam." She conspired to commit bank, wire and mail fraud, against US citizens, specifically using Internet by having had an accomplice who shipped counterfeit checks and money orders to her from Lagos , Nigeria, the previous November. Fiedler shipped out $609,000 fake check and money orders when arrested and prepared to send additional $1.1 million counterfeit materials. Also, the U.S. Postal Service recently intercepted counterfeit checks, lottery tickets and eBay overpayment schemes with a face value of $2.1 billion. [64] [65]
In a 2009 opinion, Gordon v. Virtumundo, Inc. , 575 F.3d 1040, the Ninth Circuit assessed the standing requirements necessary for a private plaintiff to bring a civil cause of action against spam senders under the CAN-SPAM Act of 2003, as well as the scope of the CAN-SPAM Act's federal preemption clause. [66]

United Kingdom
In the first successful case of its kind, Nigel Roberts from the Channel Islands won £270 against Media Logistics UK who sent junk e-mails to his personal account. [67]
In January 2007, a Sheriff Court in Scotland awarded Mr. Gordon Dick £750 (the then maximum sum that could be awarded in a Small Claim action) plus expenses of £618.66, a total of £1368.66 against Transcom Internet Services Ltd. [68] for breaching anti-spam laws. [69] Transcom had been legally represented at earlier hearings, but were not represented at the proof, so Gordon Dick got his decree by default. It is the largest amount awarded in compensation in the United Kingdom since Roberts v Media Logistics case in 2005.
Despite the statutory tort that is created by the Regulations implementing the EC Directive, few other people have followed their example. As the Courts engage in active case management, such cases would probably now be expected to be settled by mediation and payment of nominal damages.

New Zealand
In October 2008, a vast international internet spam operation run from New Zealand was cited by American authorities as one of the world’s largest, and for a time responsible for up to a third of all unwanted e-mails. In a statement the US Federal Trade Commission (FTC) named Christchurch’s Lance Atkinson as one of the principals of the operation. New Zealand’s Internal Affairs announced it had lodged a $200,000 claim in the High Court against Atkinson and his brother Shane Atkinson and courier Roland Smits, after raids in Christchurch. This marked the first prosecution since the Unsolicited Electronic Messages Act (UEMA) was passed in September 2007. The FTC said it had received more than three million complaints about spam messages connected to this operation, and estimated that it may be responsible for sending billions of illegal spam messages. The US District Court froze the defendants’ assets to preserve them for consumer redress pending trial. [70] U.S. co-defendant Jody Smith forfeited more than $800,000 and faces up to five years in prison for charges to which he pleaded guilty. [71]

Bulgaria
While most countries either outlaw or at least ignore spam, Bulgaria is the first and until now [ when? ] only one [ citation needed ] to legalize it. According to the Bulgarian E-Commerce act [72] (Чл.5,6) anyone can send spam to mailboxes published as owned by a company or organization, as long as there is a "clear and straight indication that the message is unsolicited commercial e-mail" ("да осигури ясното и недвусмислено разпознаване на търговското съобщение като непоискано") in the message body.
This made lawsuits against Bulgarian ISP's and public e-mail providers with antispam policy possible, as they are obstructing legal commerce activity and thus violate Bulgarian antitrust acts. While there are no such lawsuits until now, several cases of spam obstruction are currently awaiting decision in the Bulgarian Antitrust Commission (Комисия за защита на конкуренцията) and can end with serious fines for the ISP's in question. [ when? ] [ citation needed ]
The law contains other dubious provisions — for example, the creation of a nationwide public electronic register of e-mail addresses that do not want to receive spam. [73] It is usually abused as the perfect source for e-mail address harvesting , because publishing invalid or incorrect information in such a register is a criminal offense in Bulgaria.

Newsgroups

See also
WebPage index: 00051
Lecturer
In the United Kingdom and Ireland , a lecturer holds an open-ended, tenure-track or tenured position at a university or similar institution, and is often an academic at an early career stage who teaches, conducts research, and leads research groups. Most lecturers typically hold permanent contracts at their academic institution. In terms of responsibilities and recognition, the position of an open-ended lecturer on a permanent contract is equivalent to assistant professor or associate professor in the North American academic system. This is a tenure-track or tenured position, although UK tenure has eroded since 1988.
In other countries, the term lecturer generally denotes an academic expert without tenure in the university, who is hired to teach on a full- or part-time basis, but who is not paid to conduct research. In most research universities in the United States, the title of lecturer requires a doctorate or equivalent degree.

Australia
In Australia, the term lecturer may be used informally to refer to anyone who conducts lectures at a university or elsewhere, but formally refers to a specific academic rank. The academic ranks in Australia are similar to those in the UK, with the rank of associate professor roughly equivalent to reader in UK universities. The academic levels in Australia are (in ascending academic level): A) associate lecturer, (B) lecturer, (C) senior lecturer, (D) associate professor, and (E) professor. [1]

India
In India, one can appear for interviews for a post of a lecturer after passing the competitive exam of National Eligibility Test conducted by the University Grants Commission .
The position is equivalent to assistant professor in the US system. The term is not universally applied, with some universities preferring the lecturer/reader /professor titles, while others work with the assistant professor/associate professor/professor title.
As such, most lecturers' position can be considered tenure track.
In many states of India, the term lecturer or Post Graduate Teacher (PGT) [2] is also used for the intermediate college teachers. [3] The intermediate colleges are equivalent to higher secondary schools. Such lecturers are subject experts specifically engaged to teach a subject in higher classes. [4]

United Kingdom
In the UK, the term lecturer is ambiguous and covers several academic ranks. The key distinction is between permanent/open-ended or temporary/fixed-term lectureships.
A permanent lecturer in UK universities usually holds an open-ended position that covers teaching, research, and administrative responsibilities. Permanent lectureships are tenure-track or tenured positions that are equivalent to an assistant or associate professorship in North America. After a number of years, a lecturer may be promoted based on his or her research record to become a senior lecturer . This position is below reader and professor .
Research lecturers (where they are permanent appointments) are the equivalent in rank of lecturers and senior lecturers, but reflect a research-intensive orientation. Research lecturers are common in fields such as medicine, engineering, and biological and physical sciences.
In contrast, fixed-term or temporary lecturers are appointed for specific short-term teaching needs. These positions are often non-renewable and are common post-doctoral appointments. In North American terms, a fixed-term lecturer can hold an equivalent rank to assistant professor without tenure. Typically, longer contracts denote greater seniority or higher rank. Teaching fellows may also sometimes be referred to as lecturers—for example, Exeter named some of that group as education and scholarship lecturers (E & S) to recognise the contribution of teaching, and elevate the titles of teaching fellows to lecturers. Some universities also refer to graduate students or others, who undertake ad-hoc teaching for a department sessional lecturers . Like adjunct professors and sessional lecturers in North America, these non-permanent teaching staff are often very poorly paid (as little as £6000 p.a. in 2011-12). These varying uses of the term lecturer cause confusion for non-UK academics.
As a proportion of UK academic staff, the proportion of permanent lectureships has fallen considerably. This is one reason why permanent lectureships are usually secured only after several years of post-doctoral experience. Data from the Higher Education Statistics Agency show that in 2013-14, 36 per cent of full- and part-time academic staff were on fixed-term contracts, down from 45 per cent a decade earlier. Over the same period, the proportion of academic staff on permanent contracts rose from 55 per cent to 64 per cent. Others were on contracts classed as “atypical”.' [5]
In summary, most UK academics with the rank of lecturer have open-ended or permanent positions, can lead research groups, can apply for external funding for research as principal investigators, and have teaching responsibilities.

Historical use
Historically in the UK, promotion to a senior lectureship reflected prowess in teaching or administration rather than research, and the position was much less likely to lead direct to promotion to professor. [6]
In contrast, promotion to senior lecturer nowadays is based on research achievements (for research-intensive universities), and is an integral part of the promotion path to a full chair. Promotion to reader is sometimes still necessary before promotion to a full chair; however, some universities no longer make appointments at the level of reader (for instance, the University of Leeds and the University of Oxford). Senior lecturers and readers are sometimes paid on the same salary scale, although readers are recognized as more senior. Readers in pre-1992 universities are generally considered at least the equivalent, in terms of status, of (full) professors in post-1992 universities. Many academics consider it more prestigious to have been a reader in a pre-1992 university than a professor in a post-1992 university.
Many open-ended lecturers in the UK have a doctorate (50.1% in 2009-2010) and often have postdoctoral research experience. [7] In almost all fields, a doctorate is a prerequisite, although historically this was not the case. Some academic positions could have been held on the basis of research merit alone, without a higher degree. [8]

Current uses
The new universities (that is universities that were until recently termed polytechnics ) have a slightly different ranking naming scheme from the older universities. Whereas many pre-1992 universities use the grades: Lecturer (A), Lecturer (B), Sr Lecturer, Reader, Professor, post-1992 grades are normally Lecturer, Senior Lecturer, Principal Lecturer, Reader, Professor. Much confusion surrounds the differing use of the "Sr. Lecturer" title. A Sr. Lecturer in a post-1992 university is equivalent to a Lecturer (B) in a pre-1992 university, whereas a Sr. Lecturer in a pre-1992 university is most often equivalent to a Principal Lecturer in a post-1992 university. [9]
According to the Times Higher Education, the University of Warwick decided in 2006 "to break away from hundreds of years of academic tradition, renaming lecturers 'assistant professors', senior lecturers and readers 'associate professors' while still calling professors 'professors'. The radical move will horrify those who believe the "professor" title should be reserved for an academic elite." [10] Nottingham has a mixture of the standard UK system, and the system at Warwick, with both lecturers and assistant professors. At Reading, job advertisements and academic staff web pages use the title associate professor, but the ordinances of the university make no reference to these titles. They address only procedures for conferring the traditional UK academic ranks. [11]

Tenure and permanent lectureships
Since the Conservatives' 1988 Education Reform Act, the ironclad tenure that used to exist in the UK has given way to a less secure form of tenure. [12] Technically, university vice-chancellors can make individual faculty members redundant for poor performance or institute departmental redundancies, but in practice, this is rare. The most noted use of this policy happened in 2012 at Queen Mary University where lecturers on permanent contracts were fired. The institutions now has a stated policy of firing and replacing under-performing teaching staff members. This policy is complicated by the 2008 Ball vs Aberdeen tribunal decision, the distinction between teaching and research faculty is blurring- with implications for who can and cannot be made redundant at UK universities, and under what conditions.
Despite this recent erosion of tenure in the UK, it is still practiced in most universities. Permanent contracts use the word "tenure" for lecturers who are "reappointed to the retiring age". This is equivalent to a US tenure decision—references are sought from world-leading academics and tenure and promotions committees meet to decide "tenure" cases. There is normally no title elevation is such instances—tenure and title are independent.

United States
As different US academic institutions use the term lecturer in various ways, there is sometimes confusion. On a generic level, the term broadly denotes one who teaches at a university but is not eligible for tenure and has no research obligations . At non-research colleges, the latter distinction is less meaningful, making the absence of tenure the main difference between lecturers and other academic faculty. Unlike the adjective adjunct (which can modify most academic titles, from professor to lecturer to instructor, etc. and refers to part-time status), the title of lecturer at most schools does not address the issue of full-time vs. part-time status.
Lecturers are almost always required to have at least a master's degree and quite often have earned doctorates. (For example, at Columbia University in New York, the title of lecturer actually requires a doctorate or its professional equivalent; they also use the term for "instructors in specialized programs." [13] ) Sometimes the title is used as an equivalent-alternative to instructor , but schools that use both titles tend to provide relatively more advancement potential (e.g. multiple ranks of progression, at least some of which entail faculty voting privileges and/or faculty committee service) to their lecturers. [14]
Major research universities are more frequently hiring full-time lecturers, whose responsibilities tend to focus primarily in undergraduate education, especially for introductory/survey courses. In addition to the reason of higher-ranking faculty tending to prefer higher-level courses, part of the reason is also cost-savings, as non-tenure-track faculty tend to have lower salaries. [15] When a lecturer is part-time, there is little practical distinction in the position from an adjunct professor /instructor/etc., since all non-tenure-track faculty by definition are not on the tenure-track. However, for full-time lecturers (or those regularly salaried above some stated level, such as half-time [16] ), many institutions now incorporate the role quite formally—managing it with performance reviews, promotional tracks, administrative service responsibilities, and many faculty privileges (e.g. voting, use of resources, etc.). [14]
An emerging alternative to using full-time lecturers at research institutions is to create a parallel professorship track that is focused on teaching. It may offer tenure, and typically has a title series such as teaching professor. (This is analogous to the research-only faculty tracks at some universities, which typically have title series such as research professor/scientist/scholar.) A related concept—at least in professional fields—is the clinical professor or professor of practice, which in addition to a teaching focus (vs. research), also tend to have a practical/professional/skills oriented focus (vs. theory and scholarship, etc.).
In some institutions, the position of lecturer, especially "distinguished lecturer", may also refer to a position somewhat similar to emeritus professor and/or a temporary post used for visiting academics of considerable prominence—e.g. a famous writer may serve for a term or a year, for instance. When confusion arose about President Barack Obama 's status on the law faculty at the University of Chicago , the institution stated that although his title was "senior lecturer", the university considered him to be a "professor" and further noted that it uses that title for notable people, such as federal judges and politicians, who are deemed of high prestige but lack the time to commit to a traditional tenure-track position. [17] While other universities instead use the term "senior" as simply a matter of rank or promotion, all such references to lecturers of any rank are consistent with the normal U.S. practice of using lower-case-p "professor" as a common noun for anyone who teaches college, as well as a pre-nominal title of address (e.g. "Professor Smith") without necessarily referring to job title or position rank (e.g. "John Smith, Assistant/Associate/Full Professor of X").

Other countries
In other countries, usage varies. In Israel , the term has a meaning in academia similar to that in the UK.
In France , the title maître de conférences ("lecture master") is the lower of the two possible academic ranks (the other being professeur des universités or "university professor").
In German-speaking countries, the term lektor historically denoted a teaching position below a professor, primarily responsible for delivering and organizing lectures. The contemporary equivalent is dozent or hochschuldozent . Nowadays, the German term lektor exists only in philology or modern-language departments at German-speaking universities, for positions that primarily involve teaching a foreign language.
In Poland , the related term lektor is used for a teaching-only position, generally for teaching foreign languages.
In Norway , a lektor is an academic rank, usually reached after three or five years of post-secondary education, which enables a teacher to lecture at Ungdomsskole (secondary school) or Videregående skole (high school) level.
In Sweden and Denmark , a lektor or universitetslektor is an academic rank similar to that of senior lecturer in Great Britain and associate professor in USA. The lektor holds the position below professor in rank.
In South Korea , the term gangsa is the literal translation of "part-time lecturer". A gangsa is usually part-time, paid by the number of hours of teaching. No research or administrative obligation is attached. In most disciplines, gangsa is regarded as a first step in one's academic career. In Korea, the tenure position started from "full-time lecturer". The tenure track positions in South Korea are "full-time lecturer (JunImGangSa)", "assistant professor (JoKyoSu)", "associate professor (BuKyosu)", and "professor (KyoSu)". Therefore, "full-time lecturer" is the same position as "assistant professor" in other countries, including the USA.
In the Netherlands, a "lector" used to be equivalent to the rank of associate professor at universities. Nowadays, it is the highest rank at so-called "applied universities" (i.e., school providing higher vocational/professional training to their students). At regular universities, this rank does not exist anymore.
WebPage index: 00052
Harvard University
Harvard University is a private Ivy League research university in Cambridge, Massachusetts , established in 1636, whose history, influence, and wealth have made it one of the world's most prestigious universities. [7]
Established originally by the Massachusetts legislature and soon thereafter named for John Harvard (its first benefactor), Harvard is the United States' oldest institution of higher learning , [8] and the Harvard Corporation (formally, the President and Fellows of Harvard College ) is its first chartered corporation . Although never formally affiliated with any denomination , the early College primarily trained Congregationalist and Unitarian clergy. Its curriculum and student body were gradually secularized during the 18th century, and by the 19th century Harvard had emerged as the central cultural establishment among Boston elites . [9] [10] Following the American Civil War , President Charles W. Eliot 's long tenure (1869–1909) transformed the college and affiliated professional schools into a modern research university ; Harvard was a founding member of the Association of American Universities in 1900. [11] James Bryant Conant led the university through the Great Depression and World War II and began to reform the curriculum and liberalize admissions after the war. The undergraduate college became coeducational after its 1977 merger with Radcliffe College .
The university is organized into eleven separate academic units—ten faculties and the Radcliffe Institute for Advanced Study —with campuses throughout the Boston metropolitan area: [12] its 209-acre (85 ha) main campus is centered on Harvard Yard in Cambridge, approximately 3 miles (5 km) northwest of Boston; the business school and athletics facilities, including Harvard Stadium , are located across the Charles River in the Allston neighborhood of Boston and the medical , dental , and public health schools are in the Longwood Medical Area . [13] Harvard's $34.5 billion financial endowment is the largest of any academic institution . [6]
Harvard is a large, highly residential research university. [14] The nominal cost of attendance is high, but the university's large endowment allows it to offer generous financial aid packages. [15] It operates several arts, cultural, and scientific museums, alongside the Harvard Library , which is the world's largest academic and private library system, comprising 79 individual libraries with over 18 million volumes. [16] [17] [18] Harvard's alumni include eight U.S. presidents , several foreign heads of state, 62 living billionaires , 359 Rhodes Scholars , and 242 Marshall Scholars . [19] [20] [21] To date, some 130 Nobel laureates , 18 Fields Medalists , and 13 Turing Award winners have been affiliated as students, faculty, or staff. [22]

History

Colonial
Harvard was established in 1636 by vote of the Great and General Court of the Massachusetts Bay Colony . In 1638, it obtained British North America's first known printing press. [23] [24] In 1639 it was named Harvard College after deceased clergyman John Harvard an alumnus of the University of Cambridge who had left the school £ 779 and his scholar's library of some 400 volumes. [25] The charter creating the Harvard Corporation was granted in 1650.
A 1643 publication gave the school's purpose as "to advance learning and perpetuate it to posterity, dreading to leave an illiterate ministry to the churches when our present ministers shall lie in the dust"; [26] in its early years trained many Puritan ministers. [27] It offered a classic curriculum on the English university model‍—‌many leaders in the colony had attended the University of Cambridge ‍—‌but conformed to the tenets of Puritanism . It was never affiliated with any particular denomination, but many of its earliest graduates went on to become clergymen in Congregational and Unitarian churches. [28]
The leading Boston divine Increase Mather served as president from 1685 to 1701. In 1708, John Leverett became the first president who was not also a clergyman, marking a turning of the college from Puritanism and toward intellectual independence.

19th century
Throughout the 18th century, Enlightenment ideas of the power of reason and free will became widespread among Congregationalist ministers, putting those ministers and their congregations in tension with more traditionalist, Calvinist parties. [29] :1–4 When the Hollis Professor of Divinity David Tappan died in 1803 and the president of Harvard Joseph Willard died a year later, in 1804, a struggle broke out over their replacements. Henry Ware was elected to the chair in 1805, and the liberal Samuel Webber was appointed to the presidency of Harvard two years later, which signaled the changing of the tide from the dominance of traditional ideas at Harvard to the dominance of liberal, Arminian ideas (defined by traditionalists as Unitarian ideas). [29] :4–5 [30] :24
In 1846, the natural history lectures of Louis Agassiz were acclaimed both in New York and on the campus at Harvard College. Agassiz's approach was distinctly idealist and posited Americans' "participation in the Divine Nature" and the possibility of understanding "intellectual existences". Agassiz's perspective on science combined observation with intuition and the assumption that a person can grasp the "divine plan" in all phenomena. When it came to explaining life-forms, Agassiz resorted to matters of shape based on a presumed archetype for his evidence. This dual view of knowledge was in concert with the teachings of Common Sense Realism derived from Scottish philosophers Thomas Reid and Dugald Stewart , whose works were part of the Harvard curriculum at the time. The popularity of Agassiz's efforts to "soar with Plato" probably also derived from other writings to which Harvard students were exposed, including Platonic treatises by Ralph Cudworth , John Norris and, in a Romantic vein, Samuel Taylor Coleridge . The library records at Harvard reveal that the writings of Plato and his early modern and Romantic followers were almost as regularly read during the 19th century as those of the "official philosophy" of the more empirical and more deistic Scottish school. [31]
Charles W. Eliot , president 1869–1909, eliminated the favored position of Christianity from the curriculum while opening it to student self-direction. While Eliot was the most crucial figure in the secularization of American higher education, he was motivated not by a desire to secularize education, but by Transcendentalist Unitarian convictions. Derived from William Ellery Channing and Ralph Waldo Emerson , these convictions were focused on the dignity and worth of human nature, the right and ability of each person to perceive truth, and the indwelling God in each person. [32]

20th century
During the 20th century, Harvard's international reputation grew as a burgeoning endowment and prominent professors expanded the university's scope. Rapid enrollment growth continued as new graduate schools were begun and the undergraduate College expanded. Radcliffe College , established in 1879 as sister school of Harvard College, became one of the most prominent schools for women in the United States. Harvard became a founding member of the Association of American Universities in 1900. [11]
In the early 20th century, the student body was predominately "old-stock, high-status Protestants, especially Episcopalians, Congregationalists, and Presbyterians"—a group later called "WASPs" ( White Anglo-Saxon Protestants ). By the 1970s it was much more diversified. [34]
James Bryant Conant (president, 1933–1953) reinvigorated creative scholarship to guarantee its preeminence among research institutions. He saw higher education as a vehicle of opportunity for the talented rather than an entitlement for the wealthy, so Conant devised programs to identify, recruit, and support talented youth. In 1943, he asked the faculty make a definitive statement about what general education ought to be, at the secondary as well as the college level. The resulting Report , published in 1945, was one of the most influential manifestos in the history of American education in the 20th century. [35]
In 1945–1960 admissions policies were opened up to bring in students from a more diverse applicant pool. No longer drawing mostly from rich alumni of select New England prep schools , the undergraduate college was now open to striving middle class students from public schools; many more Jews and Catholics were admitted, but few blacks, Hispanics or Asians. [36]
Harvard graduate schools began admitting women in small numbers in the late 19th century, and during World War II, students at Radcliffe College (which since 1879 had been paying Harvard professors to repeat their lectures for women students) began attending Harvard classes alongside men, [37] The first class of women was admitted to Harvard Medical School in 1945. [38] Since the 1970s Harvard has been responsible for essentially all aspects of admission, instruction, and undergraduate life for women, and Radcliffe was formally merged into Harvard in 1999. [39]

21st century
Drew Gilpin Faust , the Dean at Radcliffe, became the first female president of Harvard in 2007. [40] [41]

Campuses

Cambridge
Harvard's 209-acre (85 ha) main campus is centered on Harvard Yard in Cambridge, about 3 miles (5 km) west-northwest of downtown Boston, and extends into the surrounding Harvard Square neighborhood. Harvard Yard itself contains the central administrative offices and main libraries of the university , academic buildings including Sever Hall and University Hall , Memorial Church, and the majority of the freshman dormitories . Sophomore, junior, and senior undergraduates live in twelve residential Houses , nine of which are south of Harvard Yard along or near the Charles River . The other three are located in a residential neighborhood half a mile northwest of the Yard at the Quadrangle (commonly referred to as the Quad), which formerly housed Radcliffe College students until Radcliffe merged its residential system with Harvard. Each residential house contains rooms for undergraduates, House masters, and resident tutors, as well as a dining hall and library. The facilities were made possible by a gift from Yale University alumnus Edward Harkness . [42]
Radcliffe Yard, formerly the center of the campus of Radcliffe College and now home of the Radcliffe Institute for Advanced Study at Harvard , [43] is adjacent to the Graduate School of Education and the Cambridge Common .
Between 2011 and 2013, Harvard University reported crime statistics for its main Cambridge campus that included 104 forcible sex offenses, 55 robberies, 83 aggravated assaults, 89 burglaries, and 43 cases of motor vehicle theft. [44]
Harvard also has commercial real estate holdings in Cambridge and Allston, on which it pays property taxes. [45] This includes the Allston Doubletree Hotel , The Inn at Harvard, and the Harvard Square Hotel. [46]

Allston
The Harvard Business School and many of the university's athletics facilities, including Harvard Stadium , are located on a 358-acre (145 ha) campus in Allston , [47] a Boston neighborhood across the Charles River from the Cambridge campus. The John W. Weeks Bridge , a pedestrian bridge over the Charles River , connects the two campuses. Intending a major expansion, Harvard now owns more land in Allston than it does in Cambridge. [48] A ten-year plan [49] calls for 1.4 million square feet (130,000 square meters) of new construction and 500,000 square feet (50,000 square meters) of renovations, including new and renovated buildings at Harvard Business School ; a hotel and conference center; a multipurpose institutional building; renovations to graduate student housing and to Harvard Stadium ; new athletic facilities; new laboratories and classrooms for the John A. Paulson School of Engineering and Applied Sciences ; expansion of the Harvard Education Portal; and a district energy facility.

Longwood
Further south, the Harvard Medical School , Harvard School of Dental Medicine , and the Harvard School of Public Health are located on a 21-acre (8.5 ha) campus in the Longwood Medical and Academic Area about 3.3 miles (5.3 km) south of the Cambridge campus, and the same distance southwest of downtown Boston. [13] The Arnold Arboretum , in the Jamaica Plain neighborhood of Boston, is also owned and operated by Harvard.

Other
Harvard also owns and operates the Dumbarton Oaks Research Library and Collection , in Washington, D.C. ; the Harvard Forest in Petersham, Massachusetts ; the Concord Field Station in Estabrook Woods in Concord, Massachusetts [50] and the Villa I Tatti research center [51] in Florence , Italy. Harvard also operates the Harvard Shanghai Center in China. [52]

Organization and administration

Governance
Harvard is governed by a combination of its Board of Overseers and the President and Fellows of Harvard College (also known as the Harvard Corporation), which in turn appoints the President of Harvard University . [53] There are 16,000 staff and faculty, [54] including 2,400 professors, lecturers, and instructors [55] teaching 7,200 undergraduates and 14,000 graduate students. [56]
The Faculty of Arts and Sciences has primary responsibility for instruction in Harvard College , Graduate School of Arts and Sciences , and the Harvard Division of Continuing Education , which includes Harvard Summer School and Harvard Extension School . There are ten other graduate and professional school faculties, in addition to the Radcliffe Institute for Advanced Study . [ clarification needed ]
Joint programs with the Massachusetts Institute of Technology include the Harvard-MIT Division of Health Sciences and Technology , the Broad Institute , The Observatory of Economic Complexity , and edX .

Endowment
Harvard has the largest university endowment in the world. As of September 2011 [update] , it had nearly regained the loss suffered during the 2008 recession. It was worth $32 billion in 2011, up from $28 billion in September 2010 [57] and $26 billion in 2009. It suffered about 30% loss in 2008–09. [58] [59] In December 2008, Harvard announced that its endowment had lost 22% (approximately $8 billion) from July to October 2008, necessitating budget cuts. [60] Later reports [61] suggest the loss was actually more than double that figure, a reduction of nearly 50% of its endowment in the first four months alone. Forbes in March 2009 estimated the loss to be in the range of $12 billion. [62] One of the most visible results of Harvard's attempt to re-balance its budget was their halting [61] of construction of the $1.2 billion Allston Science Complex that had been scheduled to be completed by 2011, resulting in protests from local residents. [63] As of 2012 [update] , Harvard University had a total financial aid reserve of $159 million for students, and a Pell Grant reserve of $4.093 million available for disbursement. [64]

Divestment
Since the 1970s, several campaigns have sought to divest Harvard's endowment from holdings the campaigns opposed, including investments in apartheid South Africa , the tobacco industry , Sudan during the Darfur genocide , and the fossil fuel industry. [65]
During the divestment from South Africa movement in the late 1980s, student activists erected a symbolic "shantytown" on Harvard Yard and blockaded a speech given by South African Vice Consul Duke Kent-Brown. [66] [67] The Harvard Management Company repeatedly refused to divest, stating that "operating expenses must not be subject to financially unrealistic strictures or carping by the unsophisticated or by special interest groups." [68] However, the university did eventually reduce its South African holdings by $230 million (out of $400 million) in response to the pressure. [66] [69]

Academics

Admission
Undergraduate admission to Harvard is characterized by the Carnegie Foundation as "more selective, lower transfer-in". [14] Harvard College accepted 5.2% of applicants for the class of 2021, a record low and the second lowest acceptance rate among all national universities. [70] [71] Harvard College ended its early admissions program in 2007 as the program was believed to disadvantage low-income and under-represented minority applicants applying to selective universities, but for the class of 2016, an early action program was reintroduced. [72]
The undergraduate admissions office's policies on preference for children of alumni have been the subject of scrutiny and debate as it primarily aids Caucasians and the wealthy and seems to conflict with the concept of meritocratic admissions. [73] [74]

Teaching and learning
Harvard is a large, highly residential research university. [14] The university has been accredited by the New England Association of Schools and Colleges since 1929. [76] The university offers 46 undergraduate concentrations (majors), [77] 134 graduate degrees, [78] and 32 professional degrees. [79] For the 2008–2009 academic year, Harvard granted 1,664 baccalaureate degrees, 400 master's degrees, 512 doctoral degrees, and 4,460 professional degrees. [79]
The four-year, full-time undergraduate program comprises a minority of enrollments at the university and emphasizes instruction with an "arts and sciences focus". [14] Between 1978 and 2008, entering students were required to complete a core curriculum of seven classes outside of their concentration. [80] Since 2008, undergraduate students have been required to complete courses in eight General Education categories: Aesthetic and Interpretive Understanding, Culture and Belief, Empirical and Mathematical Reasoning, Ethical Reasoning, Science of Living Systems, Science of the Physical Universe, Societies of the World, and United States in the World. [81] Harvard offers a comprehensive doctoral graduate program, and there is a high level of coexistence [ further explanation needed ] between graduate and undergraduate degrees. [14] The Carnegie Foundation for the Advancement of Teaching , The New York Times , and some students have criticized Harvard for its reliance on teaching fellows for some aspects of undergraduate education; they consider this to adversely affect the quality of education. [82] [83]
Harvard's academic programs operate on a semester calendar beginning in early September and ending in mid-May. [84] Undergraduates typically take four half-courses per term and must maintain a four-course rate average to be considered full-time. [85] In many concentrations, students can elect to pursue a basic program or an honors-eligible program requiring a senior thesis and/or advanced course work. [86] Students graduating in the top 4–5% of the class are awarded degrees summa cum laude , students in the next 15% of the class are awarded magna cum laude , and the next 30% of the class are awarded cum laude . [87] Harvard has chapters of academic honor societies such as Phi Beta Kappa and various committees and departments also award several hundred named prizes annually. [88] Harvard, along with other universities, has been accused of grade inflation , [89] although there is evidence that the quality of the student body and its motivation have also increased. [90] Harvard College reduced the number of students who receive Latin honors from 90% in 2004 to 60% in 2005. Moreover, the honors of "John Harvard Scholar" and "Harvard College Scholar" will now be given only to the top 5 percent and the next 5 percent of each class. [91] [92] [93] [94]
University policy is to expel students engaging in academic dishonesty to discourage a "culture of cheating." [95] [96] [97] In 2012, dozens of students were expelled for cheating after an investigation of more than 120 students. [98] In 2013, there was a report that as many as 42% of incoming freshmen had cheated on homework prior to entering the university, [99] and these incidents have prompted the university to consider adopting an honor code . [97] [100]
For the 2012–13 school year, annual tuition was $38,000, with a total cost of attendance of $57,000. [101] Beginning in 2007, families with incomes below $60,000 pay nothing for their children to attend, including room and board. Families with incomes between $60,000 to $80,000 pay only a few thousand dollars per year, and families earning between $120,000 and $180,000 pay no more than 10% of their annual incomes. [15] In 2009, Harvard offered grants totaling $414 million across all eleven divisions; [ further explanation needed ] $340 million came from institutional funds, $35 million from federal support, and $39 million from other outside support. Grants total 88% of Harvard's aid for undergraduate students, with aid also provided by loans (8%) and work-study (4%). [102]

Research
Harvard is a founding member of the Association of American Universities [103] and remains a research university with "very high" research activity and a "comprehensive" doctoral program across the arts, sciences, engineering, and medicine. [14] Research and development expenditures in 2011 totaled $649.7 million, 27th among American universities. [104]

Libraries and museums
The Harvard University Library System is centered in Widener Library in Harvard Yard and comprises nearly 80 individual libraries holding over 18 million volumes. [16] [17] According to the American Library Association , this makes it the largest academic library in the United States, and one of the largest in the world. [17] Cabot Science Library, Lamont Library, and Widener Library are three of the most popular libraries for undergraduates to use, with easy access and central locations. There are rare books, manuscripts and other special collections throughout Harvard's libraries; [105] Houghton Library, the Arthur and Elizabeth Schlesinger Library on the History of Women in America, and the Harvard University Archives consist principally of rare and unique materials. America's oldest collection of maps, gazetteers, and atlases both old and new is stored in Pusey Library and open to the public. The largest collection of East-Asian language material outside of East Asia is held in the Harvard-Yenching Library .
Harvard operates several arts, cultural, and scientific museums. The Harvard Art Museums comprises three museums. The Arthur M. Sackler Museum includes collections of ancient, Asian, Islamic and later Indian art, the Busch-Reisinger Museum , formerly the Germanic Museum, covers central and northern European art, and the Fogg Museum of Art , covers Western art from the Middle Ages to the present emphasizing Italian early Renaissance , British pre-Raphaelite , and 19th-century French art. The Harvard Museum of Natural History includes the Harvard Mineralogical Museum , Harvard University Herbaria featuring the Blaschka Glass Flowers exhibit, and the Museum of Comparative Zoology . Other museums include the Carpenter Center for the Visual Arts , designed by Le Corbusier , housing the film archive, the Peabody Museum of Archaeology and Ethnology , specializing in the cultural history and civilizations of the Western Hemisphere, and the Semitic Museum featuring artifacts from excavations in the Middle East.

University rankings
Among overall rankings, both Academic Ranking of World Universities ( ARWU ) and THE World Reputation Rankings have consecutively ranked Harvard the best since the time when they were first released. [114] [115] When QS and THE were published in partnership as the THE-QS World University Rankings during 2004-2009, Harvard had also held the top spot every year. [116]
Regarding rankings of specific indicators, Harvard topped both University Ranking by Academic Performance 2015-2016 and Mines ParisTech: Professional Ranking of World Universities (2011), which measured universities' numbers of alumni holding CEO positions in Fortune Global 500 companies. [117] According to the 2016 poll done by The Princeton Review , Harvard is the second most commonly named "dream college" in the United States, both for students and parents. [118] College ROI Report: Best Value Colleges by PayScale puts Harvard 22nd nationwide in the most recent 2016 edition. [119]

Student life

Student body
In the last six years, Harvard's student population ranged from 19,000 to 21,000, across all programs. [123] Harvard enrolled 6,655 students in undergraduate programs, 3,738 students in graduate programs, and 10,722 students in professional programs. [120] The undergraduate population is 51% female, the graduate population is 48% female, and the professional population is 49% female. [120]

Athletics
The Harvard Crimson competes in 42 intercollegiate sports in the NCAA Division I Ivy League . Harvard has an intense athletic rivalry with Yale University culminating in The Game , although the Harvard–Yale Regatta predates the football game. This rivalry, though, is put aside every two years when the Harvard and Yale Track and Field teams come together to compete against a combined Oxford University and Cambridge University team, a competition that is the oldest continuous international amateur competition in the world. [124]
Harvard's athletic rivalry with Yale is intense in every sport in which they meet, coming to a climax each fall in the annual football meeting, which dates back to 1875 and is usually called simply " The Game ". While Harvard's football team is no longer one of the country's best as it often was a century ago during football's early days (it won the Rose Bowl in 1920), both it and Yale have influenced the way the game is played. In 1903, Harvard Stadium introduced a new era into football with the first-ever permanent reinforced concrete stadium of its kind in the country. The stadium's structure actually played a role in the evolution of the college game. Seeking to reduce the alarming number of deaths and serious injuries in the sport, Walter Camp (former captain of the Yale football team), suggested widening the field to open up the game. But the stadium was too narrow to accommodate a wider playing surface. So, other steps had to be taken. Camp would instead support revolutionary new rules for the 1906 season. These included legalizing the forward pass , perhaps the most significant rule change in the sport's history. [125] [126]
Harvard has several athletic facilities, such as the Lavietes Pavilion , a multi-purpose arena and home to the Harvard basketball teams. The Malkin Athletic Center, known as the "MAC", serves both as the university's primary recreation facility and as a satellite location for several varsity sports. The five-story building includes two cardio rooms, an Olympic-size swimming pool , a smaller pool for aquaerobics and other activities, a mezzanine, where all types of classes are held, an indoor cycling studio, three weight rooms, and a three-court gym floor to play basketball. The MAC offers personal trainers and specialty classes. It is home to Harvard volleyball, fencing and wrestling. The offices of several of the school's varsity coaches are also in the MAC.
Weld Boathouse and Newell Boathouse house the women's and men's rowing teams, respectively. The men's crew also uses the Red Top complex in Ledyard, Connecticut, as their training camp for the annual Harvard-Yale Regatta . The Bright Hockey Center hosts the Harvard hockey teams, and the Murr Center serves both as a home for Harvard's squash and tennis teams as well as a strength and conditioning center for all athletic sports.
As of 2013 [update] , there were 42 Division I intercollegiate varsity sports teams for women and men at Harvard, more than at any other NCAA Division I college in the country. [127] As with other Ivy League universities, Harvard does not offer athletic scholarships . [128]
Older than The Game by 23 years, the Harvard-Yale Regatta was the original source of the athletic rivalry between the two schools. It is held annually in June on the Thames River in eastern Connecticut. The Harvard crew is typically considered to be one of the top teams in the country in rowing . Today, Harvard fields top teams in several other sports, such as the Harvard Crimson men's ice hockey team (with a strong rivalry against Cornell ), squash , and even recently won NCAA titles in Men's and Women's Fencing . Harvard also won the Intercollegiate Sailing Association National Championships in 2003.
Harvard's men's ice hockey team won the school's first NCAA Championship in any team sport in 1989. Harvard was also the first Ivy League institution to win a NCAA championship title in a women's sport when its women's lacrosse team won the NCAA Championship in 1990.
Harvard Undergraduate Television has footage from historical games and athletic events including the 2005 pep-rally before the Harvard-Yale Game.
The school color is crimson , which is also the name of the Harvard sports teams and the daily newspaper, The Harvard Crimson . The color was unofficially adopted (in preference to magenta ) by an 1875 vote of the student body, although the association with some form of red can be traced back to 1858, when Charles William Eliot , a young graduate student who would later become Harvard's 21st and longest-serving president (1869–1909), bought red bandanas for his crew so they could more easily be distinguished by spectators at a regatta.

Song
Harvard has several fight songs, the most played of which, especially at football, are " Ten Thousand Men of Harvard " and " Harvardiana ." While " Fair Harvard " is actually the alma mater , "Ten Thousand Men" is better known outside the university. The Harvard University Band performs these fight songs, and other cheers, at football and hockey games. These were parodied by Harvard alumnus Tom Lehrer in his song " Fight Fiercely, Harvard ," which he composed while an undergraduate.

Notable people

Alumni

Faculty
Harvard's faculty includes scholars such as biologist E. O. Wilson , cognitive scientist Steven Pinker , physicists Lisa Randall and Roy Glauber , chemists Elias Corey , Dudley R. Herschbach and George M. Whitesides , computer scientists Michael O. Rabin and Leslie Valiant , Shakespeare scholar Stephen Greenblatt , writer Louis Menand , critic Helen Vendler , historians Henry Louis Gates, Jr. and Niall Ferguson , economists Amartya Sen , N. Gregory Mankiw , Robert Barro , Stephen A. Marglin , Don M. Wilson III and Martin Feldstein , political philosophers Harvey Mansfield , Baroness Shirley Williams and Michael Sandel , Fields Medalist mathematician Shing-Tung Yau , political scientists Robert Putnam , Joseph Nye , and Stanley Hoffmann , scholar/composers Robert Levin and Bernard Rands , astrophysicist Alyssa A. Goodman , and legal scholars Alan Dershowitz and Lawrence Lessig .
Past faculty members include Stephen Jay Gould , Robert Nozick , Stephan Thernstrom , Michael Walzer , and Cornel West .

Literature and popular culture
Harvard's legacy as a leading research and educational institution has a significant impact in both academy and popular culture. Furthermore, the perception of Harvard as a center of either elite achievement, or elitist privilege, has made it a frequent literary and cinematic backdrop. "In the grammar of film, Harvard has come to mean both tradition, and a certain amount of stuffiness," film critic Paul Sherman has said. [129]

Literature

Film
Harvard's policy since 1970 has been to permit filming on its property only rarely, so most scenes set at Harvard (especially indoor shots, but excepting aerial footage and shots of public areas such as Harvard Square) are in fact shot elsewhere. [135] [136]

See also
WebPage index: 00053
Amin Azzam
Amin Azzam is a clinical professor in the department of psychiatry at the University of California, San Francisco School of Medicine. He is also a clinical professor at the University of California, Berkeley , the associate director of the UC Berkeley—UCSF Joint Medical Program, and the director of the program's "Problem-Based Learning" curriculum. [1] [2] He is known for teaching an elective class for fourth-year medical students that consists entirely of editing Wikipedia articles about medical topics. [3] He originally got the idea from one of his students, Michael Turken, in 2012, and was skeptical at first, but later became convinced that it could be a good idea. He then developed the class with Turken. [4] [5] He first taught the monthlong course in December 2013. [6] With regard to the class, he has said, "It is part of our social contract with society, as physicians, to be contributing to Wikipedia and other open-access repositories because that is where the world reads about health information.” [5] His class continues to the present.

Education
Azzam received his undergraduate degree from the University of Rochester and his medical degree from the Medical College of Virginia . [1] He then completed his general adult psychiatry residency at the University of California, San Francisco, followed by a masters in education from the University of California, Berkeley. [1]
WebPage index: 00054
Roy Rosenzweig
Roy Alan Rosenzweig (August 6, 1950 – October 11, 2007) was an American historian at George Mason University in Virginia . He was the founder and director of the Center for History and New Media from 1994 until his death in October 2007 from lung cancer , aged 57. [1]

Career
Rosenzweig was the co-author, with Elizabeth Blackmar, of The Park and the People: A History of Central Park , which won several awards including the 1993 Historic Preservation Book Award and the 1993 Urban History Association Prize for Best Book on North American Urban History. He also co-authored (with David Thelen) The Presence of the Past: Popular Uses of History in American Life , which won prizes from the Center for Historic Preservation and the American Association for State and Local History. He was co-author, with Steve Brier and Joshua Brown, of the American Social History Project's CD-ROM, Who Built America? , which won James Harvey Robinson Prize of American Historical Association for its “outstanding contribution to the teaching and learning of history.”
Rosenzweig's other books include Eight Hours for What We Will: Workers and Leisure in an Industrial City, 1870–1920 and edited volumes on history museums ( History Museums in the United States: A Critical Assessment ), history and the public (Presenting the Past: Essays on History and the Public), history teaching (Experiments in History Teaching), oral history ( Government and the Arts in 1930s America ), and recent history ( A Companion to Post-1945 America ). His most recent book (co-authored with Daniel Cohen) is Digital History: A Guide to Gathering, Preserving, and Presenting the Past on the Web , He has been the recipient of a Guggenheim Fellowship and has lectured in Australia as a Fulbright Professor. He recently served as Vice-President for Research of the American Historical Association .
As founder and director of the Center for History and New Media (CHNM), he was involved in a number of different digital history projects including websites on U.S. history, historical thinking, the French Revolution , the history of science and technology, world history, and the September 11, 2001, attacks. All of these are available through the CHNM web site. His work in digital history was recognized in 2003 with the Richard W. Lyman Award (awarded by the National Humanities Center and the Rockefeller Foundation ) for “outstanding achievement in the use of information technology to advance scholarship and teaching in the humanities.”
In June 2006 he published an article about English Wikipedia in the Journal of American History , "Can History Be Open Source? Wikipedia and the Future of the Past". The article discusses the pros and cons of using Wikipedia as a historical, reliable source and attempts to answer questions on Wikipedia's history and its impact on historical writing.

Selected bibliography
WebPage index: 00055
Cancer
Cancer is a group of diseases involving abnormal cell growth with the potential to invade or spread to other parts of the body. [2] [8] Not all tumors are cancerous; benign tumors do not spread to other parts of the body. [8] Possible signs and symptoms include a lump, abnormal bleeding, prolonged cough, unexplained weight loss and a change in bowel movements . [1] While these symptoms may indicate cancer, they may have other causes. [1] Over 100 types of cancers affect humans. [8]
Tobacco use is the cause of about 22% of cancer deaths. [2] Another 10% is due to obesity , poor diet , lack of physical activity , and excessive drinking of alcohol . [2] [9] [10] Other factors include certain infections , exposure to ionizing radiation and environmental pollutants. [3] In the developing world nearly 20% of cancers are due to infections such as hepatitis B , hepatitis C and human papillomavirus (HPV). [2] These factors act, at least partly, by changing the genes of a cell. [11] Typically many genetic changes are required before cancer develops. [11] Approximately 5–10% of cancers are due to inherited genetic defects from a person's parents. [12] Cancer can be detected by certain signs and symptoms or screening tests . [2] It is then typically further investigated by medical imaging and confirmed by biopsy . [13]
Many cancers can be prevented by not smoking , maintaining a healthy weight, not drinking too much alcohol , eating plenty of vegetables , fruits and whole grains , vaccination against certain infectious diseases, not eating too much processed and red meat, and avoiding too much sunlight exposure. [14] [15] Early detection through screening is useful for cervical and colorectal cancer. [16] The benefits of screening in breast cancer are controversial. [16] [17] Cancer is often treated with some combination of radiation therapy , surgery , chemotherapy , and targeted therapy . [2] [4] Pain and symptom management are an important part of care. Palliative care is particularly important in people with advanced disease. [2] The chance of survival depends on the type of cancer and extent of disease at the start of treatment. [11] In children under 15 at diagnosis the five-year survival rate in the developed world is on average 80%. [18] For cancer in the United States the average five-year survival rate is 66%. [5]
In 2015 about 90.5 million people had cancer. [6] About 14.1 million new cases occur a year (not including skin cancer other than melanoma ). [11] It caused about 8.8 million deaths (15.7%) of human deaths . [7] The most common types of cancer in males are lung cancer , prostate cancer , colorectal cancer and stomach cancer . In females, the most common types are breast cancer , colorectal cancer, lung cancer and cervical cancer . [11] If skin cancer other than melanoma were included in total new cancers each year it would account for around 40% of cases. [19] [20] In children, acute lymphoblastic leukaemia and brain tumors are most common except in Africa where non-Hodgkin lymphoma occurs more often. [18] In 2012, about 165,000 children under 15 years of age were diagnosed with cancer. The risk of cancer increases significantly with age and many cancers occur more commonly in developed countries . [11] Rates are increasing as more people live to an old age and as lifestyle changes occur in the developing world. [21] The financial costs of cancer were estimated at $1.16 trillion US dollars per year as of 2010. [22]

Definitions
Cancers are a large family of diseases that involve abnormal cell growth with the potential to invade or spread to other parts of the body. [2] [8] They form a subset of neoplasms . A neoplasm or tumor is a group of cells that have undergone unregulated growth and will often form a mass or lump, but may be distributed diffusely. [23] [24]
All tumor cells show the six hallmarks of cancer . These characteristics are required to produce a malignant tumor. They include: [25]
The progression from normal cells to cells that can form a detectable mass to outright cancer involves multiple steps known as malignant progression. [26] [27]

Signs and symptoms
When cancer begins, it produces no symptoms. Signs and symptoms appear as the mass grows or ulcerates . The findings that result depend on the cancer's type and location. Few symptoms are specific . Many frequently occur in individuals who have other conditions. Cancer is a " great imitator ". Thus, it is common for people diagnosed with cancer to have been treated for other diseases, which were hypothesized to be causing their symptoms. [28]
People may become anxious or depressed post-diagnosis. The risk of suicide in people with cancer is approximately double. [29]

Local symptoms
Local symptoms may occur due to the mass of the tumor or its ulceration. For example, mass effects from lung cancer can block the bronchus resulting in cough or pneumonia ; esophageal cancer can cause narrowing of the esophagus , making it difficult or painful to swallow; and colorectal cancer may lead to narrowing or blockages in the bowel , affecting bowel habits. Masses in breasts or testicles may produce observable lumps. Ulceration can cause bleeding that, if it occurs in the lung, will lead to coughing up blood , in the bowels to anemia or rectal bleeding , in the bladder to blood in the urine and in the uterus to vaginal bleeding. Although localized pain may occur in advanced cancer, the initial swelling is usually painless. Some cancers can cause a buildup of fluid within the chest or abdomen . [28]

Systemic symptoms
General symptoms occur due to effects that are not related to direct or metastatic spread. These may include: unintentional weight loss , fever , excessive fatigue and changes to the skin. [30] Hodgkin disease , leukemias and cancers of the liver or kidney can cause a persistent fever . [28]
Some cancers may cause specific groups of systemic symptoms, termed paraneoplastic phenomena . Examples include the appearance of myasthenia gravis in thymoma and clubbing in lung cancer . [28]

Metastasis
Cancer can spread from its original site by local spread, lymphatic spread to regional lymph nodes or by hematogenous spread via the blood to distant sites, known as metastasis. When cancer spreads by a hematogenous route, it usually spreads all over the body. However, cancer 'seeds' grow in certain selected site only ('soil') as hypothesized in the soil and seed hypothesis of cancer metastasis. The symptoms of metastatic cancers depend on the tumor location and can include enlarged lymph nodes (which can be felt or sometimes seen under the skin and are typically hard), enlarged liver or enlarged spleen , which can be felt in the abdomen , pain or fracture of affected bones and neurological symptoms. [28]

Causes
The majority of cancers, some 90–95% of cases, are due to environmental factors . The remaining 5–10% are due to inherited genetics . [3] Environmental , as used by cancer researchers, means any cause that is not inherited genetically , such as lifestyle, economic and behavioral factors and not merely pollution . [31] Common environmental factors that contribute to cancer death include tobacco (25–30%), diet and obesity (30–35%), infections (15–20%), radiation (both ionizing and non-ionizing, up to 10%), stress, lack of physical activity and environmental pollutants . [3]
It is not generally possible to prove what caused a particular cancer because the various causes do not have specific fingerprints. For example, if a person who uses tobacco heavily develops lung cancer, then it was probably caused by the tobacco use, but since everyone has a small chance of developing lung cancer as a result of air pollution or radiation, the cancer may have developed for one of those reasons. Excepting the rare transmissions that occur with pregnancies and occasional organ donors , cancer is generally not a transmissible disease . [32]

Chemicals
Exposure to particular substances have been linked to specific types of cancer. These substances are called carcinogens .
Tobacco smoke , for example, causes 90% of lung cancer . [33] It also causes cancer in the larynx , head, neck, stomach, bladder, kidney, esophagus and pancreas . [34] Tobacco smoke contains over fifty known carcinogens, including nitrosamines and polycyclic aromatic hydrocarbons . [35]
Tobacco is responsible for about one in five cancer deaths worldwide [35] and about one in three in the developed world. [36] Lung cancer death rates in the United States have mirrored smoking patterns, with increases in smoking followed by dramatic increases in lung cancer death rates and, more recently, decreases in smoking rates since the 1950s followed by decreases in lung cancer death rates in men since 1990. [37] [38]
In Western Europe, 10% of cancers in males and 3% of cancers in females are attributed to alcohol exposure, especially liver and digestive tract cancers. [39] Cancer from work-related substance exposures may cause between 2 and 20% of cases, [40] causing at least 200,000 deaths. [41] Cancers such as lung cancer and mesothelioma can come from inhaling tobacco smoke or asbestos fibers, or leukemia from exposure to benzene . [41]

Diet and exercise
Diet, physical inactivity and obesity are related to up to 30–35% of cancer deaths. [3] [42] In the United States excess body weight is associated with the development of many types of cancer and is a factor in 14–20% of cancer deaths. [42] A UK study including data on over 5 million people showed higher body mass index to be related to at least 10 types of cancer and responsible for around 12,000 cases each year in that country. [43] Physical inactivity is believed to contribute to cancer risk, not only through its effect on body weight but also through negative effects on the immune system and endocrine system . [42] More than half of the effect from diet is due to overnutrition (eating too much), rather than from eating too few vegetables or other healthful foods.
Some specific foods are linked to specific cancers. A high- salt diet is linked to gastric cancer . [44] Aflatoxin B1 , a frequent food contaminant, causes liver cancer. [44] Betel nut chewing can cause oral cancer. [44] National differences in dietary practices may partly explain differences in cancer incidence. For example, gastric cancer is more common in Japan due to its high-salt diet [45] while colon cancer is more common in the United States. Immigrant cancer profiles develop mirror that of their new country, often within one generation. [46]

Infection
Worldwide approximately 18% of cancer deaths are related to infectious diseases . [3] This proportion ranges from a high of 25% in Africa to less than 10% in the developed world. [3] Viruses are the usual infectious agents that cause cancer but cancer bacteria and parasites may also play a role.
Oncovirus es (viruses that can cause cancer) include human papillomavirus ( cervical cancer ), Epstein–Barr virus ( B-cell lymphoproliferative disease and nasopharyngeal carcinoma ), Kaposi's sarcoma herpesvirus ( Kaposi's sarcoma and primary effusion lymphomas), hepatitis B and hepatitis C viruses ( hepatocellular carcinoma ) and human T-cell leukemia virus-1 (T-cell leukemias). Bacterial infection may also increase the risk of cancer, as seen in Helicobacter pylori -induced gastric carcinoma . [47] [48] Parasitic infections associated with cancer include Schistosoma haematobium ( squamous cell carcinoma of the bladder ) and the liver flukes , Opisthorchis viverrini and Clonorchis sinensis ( cholangiocarcinoma ). [49]

Radiation
Up to 10% of invasive cancers are related to radiation exposure, including both ionizing radiation and non-ionizing ultraviolet radiation . [3] Additionally, the majority of non-invasive cancers are non-melanoma skin cancers caused by non-ionizing ultraviolet radiation , mostly from sunlight. Sources of ionizing radiation include medical imaging and radon gas.
Ionizing radiation is not a particularly strong mutagen . [50] Residential exposure to radon gas, for example, has similar cancer risks as passive smoking . [50] Radiation is a more potent source of cancer when combined with other cancer-causing agents, such as radon plus tobacco smoke. [50] Radiation can cause cancer in most parts of the body, in all animals and at any age. Children and adolescents are twice as likely to develop radiation-induced leukemia as adults; radiation exposure before birth has ten times the effect. [50]
Medical use of ionizing radiation is a small but growing source of radiation-induced cancers. Ionizing radiation may be used to treat other cancers, but this may, in some cases, induce a second form of cancer. [50] It is also used in some kinds of medical imaging . [51]
Prolonged exposure to ultraviolet radiation from the sun can lead to melanoma and other skin malignancies. [52] Clear evidence establishes ultraviolet radiation, especially the non-ionizing medium wave UVB , as the cause of most non-melanoma skin cancers , which are the most common forms of cancer in the world. [52]
Non-ionizing radio frequency radiation from mobile phones , electric power transmission and other similar sources have been described as a possible carcinogen by the World Health Organization 's International Agency for Research on Cancer . [53] However, studies have not found a consistent link between mobile phone radiation and cancer risk. [54]

Heredity
The vast majority of cancers are non-hereditary (sporadic). Hereditary cancers are primarily caused by an inherited genetic defect. Less than 0.3% of the population are carriers of a genetic mutation that has a large effect on cancer risk and these cause less than 3–10% of cancer. [55] Some of these syndromes include: certain inherited mutations in the genes BRCA1 and BRCA2 with a more than 75% risk of breast cancer and ovarian cancer , [55] and hereditary nonpolyposis colorectal cancer (HNPCC or Lynch syndrome), which is present in about 3% of people with colorectal cancer , [56] among others.

Physical agents
Some substances cause cancer primarily through their physical, rather than chemical, effects. [57] A prominent example of this is prolonged exposure to asbestos , naturally occurring mineral fibers that are a major cause of mesothelioma (cancer of the serous membrane ) usually the serous membrane surrounding the lungs. [57] Other substances in this category, including both naturally occurring and synthetic asbestos-like fibers, such as wollastonite , attapulgite , glass wool and rock wool , are believed to have similar effects. [57] Non-fibrous particulate materials that cause cancer include powdered metallic cobalt and nickel and crystalline silica ( quartz , cristobalite and tridymite ). [57] Usually, physical carcinogens must get inside the body (such as through inhalation) and require years of exposure to produce cancer. [57]
Physical trauma resulting in cancer is relatively rare. [58] Claims that breaking bones resulted in bone cancer, for example, have not been proven. [58] Similarly, physical trauma is not accepted as a cause for cervical cancer, breast cancer or brain cancer. [58] One accepted source is frequent, long-term application of hot objects to the body. It is possible that repeated burns on the same part of the body, such as those produced by kanger and kairo heaters (charcoal hand warmers ), may produce skin cancer, especially if carcinogenic chemicals are also present. [58] Frequent consumption of scalding hot tea may produce esophageal cancer. [58] Generally, it is believed that cancer arises, or a pre-existing cancer is encouraged, during the process of healing, rather than directly by the trauma. [58] However, repeated injuries to the same tissues might promote excessive cell proliferation, which could then increase the odds of a cancerous mutation.
Chronic inflammation has been hypothesized to directly cause mutation. [58] [59] Inflammation can contribute to proliferation, survival, angiogenesis and migration of cancer cells by influencing the tumor microenvironment . [60] [61] Oncogenes build up an inflammatory pro-tumorigenic microenvironment. [62]

Hormones
Some hormones play a role in the development of cancer by promoting cell proliferation . [63] Insulin-like growth factors and their binding proteins play a key role in cancer cell proliferation, differentiation and apoptosis , suggesting possible involvement in carcinogenesis. [64]
Hormones are important agents in sex-related cancers, such as cancer of the breast, endometrium , prostate, ovary and testis and also of thyroid cancer and bone cancer . [63] For example, the daughters of women who have breast cancer have significantly higher levels of estrogen and progesterone than the daughters of women without breast cancer. These higher hormone levels may explain their higher risk of breast cancer, even in the absence of a breast-cancer gene. [63] Similarly, men of African ancestry have significantly higher levels of testosterone than men of European ancestry and have a correspondingly higher level of prostate cancer. [63] Men of Asian ancestry, with the lowest levels of testosterone-activating androstanediol glucuronide , have the lowest levels of prostate cancer. [63]
Other factors are relevant: obese people have higher levels of some hormones associated with cancer and a higher rate of those cancers. [63] Women who take hormone replacement therapy have a higher risk of developing cancers associated with those hormones. [63] On the other hand, people who exercise far more than average have lower levels of these hormones and lower risk of cancer. [63] Osteosarcoma may be promoted by growth hormones . [63] Some treatments and prevention approaches leverage this cause by artificially reducing hormone levels and thus discouraging hormone-sensitive cancers. [63]

Autoimmune diseases
There is an association between celiac disease and an increased risk of all cancers. People with untreated celiac disease have a higher risk, but this risk decreases with time after diagnosis and strict treatment, probably due to the adoption of a gluten-free diet , which seems to have a protective role against development of malignancy in people with celiac disease. However, the delay in diagnosis and initiation of a gluten-free diet seems to increase the risk of malignancies. [65] Rates of gastrointestinal cancers are increased in people with Crohn's disease and ulcerative colitis , due to chronic inflammation. Also, immunomodulators and biologic agents used to treat these diseases may promote developing extra-intestinal malignancies. [66]

Pathophysiology

Genetics
Cancer is fundamentally a disease of tissue growth regulation. In order for a normal cell to transform into a cancer cell, the genes that regulate cell growth and differentiation must be altered. [67]
The affected genes are divided into two broad categories. Oncogenes are genes that promote cell growth and reproduction. Tumor suppressor genes are genes that inhibit cell division and survival. Malignant transformation can occur through the formation of novel oncogenes, the inappropriate over-expression of normal oncogenes, or by the under-expression or disabling of tumor suppressor genes. Typically, changes in multiple genes are required to transform a normal cell into a cancer cell. [68]
Genetic changes can occur at different levels and by different mechanisms. The gain or loss of an entire chromosome can occur through errors in mitosis . More common are mutations , which are changes in the nucleotide sequence of genomic DNA.
Large-scale mutations involve the deletion or gain of a portion of a chromosome. Genomic amplification occurs when a cell gains copies (often 20 or more) of a small chromosomal locus, usually containing one or more oncogenes and adjacent genetic material. Translocation occurs when two separate chromosomal regions become abnormally fused, often at a characteristic location. A well-known example of this is the Philadelphia chromosome , or translocation of chromosomes 9 and 22, which occurs in chronic myelogenous leukemia and results in production of the BCR - abl fusion protein , an oncogenic tyrosine kinase .
Small-scale mutations include point mutations, deletions, and insertions, which may occur in the promoter region of a gene and affect its expression , or may occur in the gene's coding sequence and alter the function or stability of its protein product. Disruption of a single gene may also result from integration of genomic material from a DNA virus or retrovirus , leading to the expression of viral oncogenes in the affected cell and its descendants.
Replication of the data contained within the DNA of living cells will probabilistically result in some errors (mutations). Complex error correction and prevention is built into the process and safeguards the cell against cancer. If a significant error occurs, the damaged cell can self-destruct through programmed cell death, termed apoptosis . If the error control processes fail, then the mutations will survive and be passed along to daughter cells .
Some environments make errors more likely to arise and propagate. Such environments can include the presence of disruptive substances called carcinogens , repeated physical injury, heat, ionising radiation or hypoxia . [69]
The errors that cause cancer are self-amplifying and compounding, for example:
The transformation of a normal cell into cancer is akin to a chain reaction caused by initial errors, which compound into more severe errors, each progressively allowing the cell to escape more controls that limit normal tissue growth. This rebellion-like scenario is an undesirable survival of the fittest , where the driving forces of evolution work against the body's design and enforcement of order. Once cancer has begun to develop, this ongoing process, termed clonal evolution , drives progression towards more invasive stages . [70] Clonal evolution leads to intra- tumour heterogeneity (cancer cells with heterogeneous mutations) that complicates designing effective treatment strategies.
Characteristic abilities developed by cancers are divided into categories, specifically evasion of apoptosis, self-sufficiency in growth signals, insensitivity to anti-growth signals, sustained angiogenesis, limitless replicative potential, metastasis, reprogramming of energy metabolism and evasion of immune destruction. [26] [27]

Epigenetics
The classical view of cancer is a set of diseases that are driven by progressive genetic abnormalities that include mutations in tumor-suppressor genes and oncogenes and chromosomal abnormalities. Later epigenetic alterations ' role was identified. [71]
Epigenetic alterations refer to functionally relevant modifications to the genome that do not change the nucleotide sequence. Examples of such modifications are changes in DNA methylation (hypermethylation and hypomethylation), histone modification [72] and changes in chromosomal architecture (caused by inappropriate expression of proteins such as HMGA2 or HMGA1 ). [73] Each of these alterations regulates gene expression without altering the underlying DNA sequence . These changes may remain through cell divisions , last for multiple generations and can be considered to be epimutations (equivalent to mutations).
Epigenetic alterations occur frequently in cancers. As an example, one study listed protein coding genes that were frequently altered in their methylation in association with colon cancer. These included 147 hypermethylated and 27 hypomethylated genes. Of the hypermethylated genes, 10 were hypermethylated in 100% of colon cancers and many others were hypermethylated in more than 50% of colon cancers. [74]
While epigenetic alterations are found in cancers, the epigenetic alterations in DNA repair genes, causing reduced expression of DNA repair proteins, may be of particular importance. Such alterations are thought to occur early in progression to cancer and to be a likely cause of the genetic instability characteristic of cancers. [75] [76] [77] [78]
Reduced expression of DNA repair genes disrupts DNA repair. This is shown in the figure at the 4th level from the top. (In the figure, red wording indicates the central role of DNA damage and defects in DNA repair in progression to cancer.) When DNA repair is deficient DNA damage remains in cells at a higher than usual level (5th level) and cause increased frequencies of mutation and/or epimutation (6th level). Mutation rates increase substantially in cells defective in DNA mismatch repair [79] [80] or in homologous recombinational repair (HRR). [81] Chromosomal rearrangements and aneuploidy also increase in HRR defective cells. [82]
Higher levels of DNA damage cause increased mutation (right side of figure) and increased epimutation. During repair of DNA double strand breaks, or repair of other DNA damage, incompletely cleared repair sites can cause epigenetic gene silencing. [83] [84]
Deficient expression of DNA repair proteins due to an inherited mutation can increase cancer risks. Individuals with an inherited impairment in any of 34 DNA repair genes (see article DNA repair-deficiency disorder ) have increased cancer risk, with some defects ensuring a 100% lifetime chance of cancer (e.g. p53 mutations). [85] Germ line DNA repair mutations are noted on the figure's left side. However, such germline mutations (which cause highly penetrant cancer syndromes) are the cause of only about 1 percent of cancers. [86]
In sporadic cancers, deficiencies in DNA repair are occasionally caused by a mutation in a DNA repair gene but are much more frequently caused by epigenetic alterations that reduce or silence expression of DNA repair genes. This is indicated in the figure at the 3rd level. Many studies of heavy metal-induced carcinogenesis show that such heavy metals cause a reduction in expression of DNA repair enzymes, some through epigenetic mechanisms. DNA repair inhibition is proposed to be a predominant mechanism in heavy metal-induced carcinogenicity. In addition, frequent epigenetic alterations of the DNA sequences code for small RNAs called microRNAs (or miRNAs). miRNAs do not code for proteins, but can "target" protein-coding genes and reduce their expression.
Cancers usually arise from an assemblage of mutations and epimutations that confer a selective advantage leading to clonal expansion (see Field defects in progression to cancer ). Mutations, however, may not be as frequent in cancers as epigenetic alterations. An average cancer of the breast or colon can have about 60 to 70 protein-altering mutations, of which about three or four may be "driver" mutations and the remaining ones may be "passenger" mutations. [87]

Metastasis
Metastasis is the spread of cancer to other locations in the body. The dispersed tumors are called metastatic tumors, while the original is called the primary tumor. Almost all cancers can metastasize. [88] Most cancer deaths are due to cancer that has metastasized. [89]
Metastasis is common in the late stages of cancer and it can occur via the blood or the lymphatic system or both. The typical steps in metastasis are local invasion, intravasation into the blood or lymph, circulation through the body, extravasation into the new tissue, proliferation and angiogenesis . Different types of cancers tend to metastasize to particular organs, but overall the most common places for metastases to occur are the lungs , liver , brain and the bones . [88]

Diagnosis
Most cancers are initially recognized either because of the appearance of signs or symptoms or through screening . Neither of these leads to a definitive diagnosis, which requires the examination of a tissue sample by a pathologist . People with suspected cancer are investigated with medical tests . These commonly include blood tests , X-rays , CT scans and endoscopy .
The tissue diagnosis from the biopsy indicates the type of cell that is proliferating, its histological grade , genetic abnormalities and other features. Together, this information is useful to evaluate the prognosis and to choose the best treatment.
Cytogenetics and immunohistochemistry are other types of tissue tests. These tests provide information about molecular changes (such as mutations , fusion genes and numerical chromosome changes) and may thus also indicate the prognosis and best treatment.

Classification
Cancers are classified by the type of cell that the tumor cells resemble and is therefore presumed to be the origin of the tumor. These types include:
Cancers are usually named using -carcinoma , -sarcoma or -blastoma as a suffix, with the Latin or Greek word for the organ or tissue of origin as the root. For example, cancers of the liver parenchyma arising from malignant epithelial cells is called hepatocarcinoma , while a malignancy arising from primitive liver precursor cells is called a hepatoblastoma and a cancer arising from fat cells is called a liposarcoma . For some common cancers, the English organ name is used. For example, the most common type of breast cancer is called ductal carcinoma of the breast . Here, the adjective ductal refers to the appearance of cancer under the microscope, which suggests that it has originated in the milk ducts.
Benign tumors (which are not cancers) are named using -oma as a suffix with the organ name as the root. For example, a benign tumor of smooth muscle cells is called a leiomyoma (the common name of this frequently occurring benign tumor in the uterus is fibroid ). Confusingly, some types of cancer use the -noma suffix, examples including melanoma and seminoma .
Some types of cancer are named for the size and shape of the cells under a microscope, such as giant cell carcinoma , spindle cell carcinoma and small-cell carcinoma .

Prevention
Cancer prevention is defined as active measures to decrease cancer risk. [91] The vast majority of cancer cases are due to environmental risk factors. Many of these environmental factors are controllable lifestyle choices. Thus, cancer is generally preventable. [92] Between 70% and 90% of common cancers are due to environmental factors and therefore potentially preventable. [93]
Greater than 30% of cancer deaths could be prevented by avoiding risk factors including: tobacco , excess weight / obesity , insufficient diet, physical inactivity , alcohol , sexually transmitted infections and air pollution . [94] Not all environmental causes are controllable, such as naturally occurring background radiation and cancers caused through hereditary genetic disorders and thus are not preventable via personal behavior.

Dietary
While many dietary recommendations have been proposed to reduce cancer risks, the evidence to support them is not definitive. [14] [95] The primary dietary factors that increase risk are obesity and alcohol consumption. Diets low in fruits and vegetables and high in red meat have been implicated but reviews and meta-analyses do not come to a consistent conclusion. [96] [97] A 2014 meta-analysis find no relationship between fruits and vegetables and cancer. [98] Coffee is associated with a reduced risk of liver cancer . [99] Studies have linked excess consumption of red or processed meat to an increased risk of breast cancer, colon cancer and pancreatic cancer , a phenomenon that could be due to the presence of carcinogens in meats cooked at high temperatures. [100] [101] In 2015 the IARC reported that eating processed meat (e.g., bacon, ham, hot dogs, sausages) and, to a lesser degree, red meat was linked to some cancers. [102] [103]
Dietary recommendations for cancer prevention typically include an emphasis on vegetables, fruit, whole grains and fish and an avoidance of processed and red meat (beef, pork, lamb), animal fats and refined carbohydrates. [14] [95]

Medication
Medications can be used to prevent cancer in a few circumstances. [104] In the general population, NSAIDs reduce the risk of colorectal cancer ; however, due to cardiovascular and gastrointestinal side effects, they cause overall harm when used for prevention. [105] Aspirin has been found to reduce the risk of death from cancer by about 7%. [106] COX-2 inhibitors may decrease the rate of polyp formation in people with familial adenomatous polyposis ; however, it is associated with the same adverse effects as NSAIDs. [107] Daily use of tamoxifen or raloxifene reduce the risk of breast cancer in high-risk women. [108] The benefit versus harm for 5-alpha-reductase inhibitor such as finasteride is not clear. [109]
Vitamins are not effective at preventing cancer, [110] although low blood levels of vitamin D are correlated with increased cancer risk. [111] [112] People who have cancer are also at a high risk of developing vitamin D deficiency. [113] Whether this relationship is causal and vitamin D supplementation is protective is not determined. [114] Beta-carotene supplementation increases lung cancer rates in those who are high risk. [115] Folic acid supplementation is not effective in preventing colon cancer and may increase colon polyps. [116] It is unclear if selenium supplementation has an effect. [117]

Vaccination
Vaccines have been developed that prevent infection by some carcinogenic viruses. [118] Human papillomavirus vaccine ( Gardasil and Cervarix ) decrease the risk of developing cervical cancer . [118] The hepatitis B vaccine prevents infection with hepatitis B virus and thus decreases the risk of liver cancer. [118] The administration of human papillomavirus and hepatitis B vaccinations is recommended when resources allow. [119]

Screening
Unlike diagnostic efforts prompted by symptoms and medical signs , cancer screening involves efforts to detect cancer after it has formed, but before any noticeable symptoms appear. [120] This may involve physical examination , blood or urine tests or medical imaging . [120]
Cancer screening is not available for many types of cancers. Even when tests are available, they may not be recommended for everyone. Universal screening or mass screening involves screening everyone. [121] Selective screening identifies people who are at higher risk, such as people with a family history. [121] Several factors are considered to determine whether the benefits of screening outweigh the risks and the costs of screening. [120] These factors include:

Recommendations

U.S. Preventive Services Task Force
The U.S. Preventive Services Task Force (USPSTF) issues recommendations for various cancers:

Japan
Screens for gastric cancer using photofluorography due to the high incidence there. [21]

Genetic testing
Genetic testing for individuals at high-risk of certain cancers is recommended by unofficial groups. [119] [135] Carriers of these mutations may then undergo enhanced surveillance, chemoprevention, or preventative surgery to reduce their subsequent risk. [135]

Management
Many treatment options for cancer exist. The primary ones include surgery , chemotherapy , radiation therapy , hormonal therapy , targeted therapy and palliative care . Which treatments are used depends on the type, location and grade of the cancer as well as the patient's health and preferences. The treatment intent may or may not be curative.

Chemotherapy
Chemotherapy is the treatment of cancer with one or more cytotoxic anti- neoplastic drugs ( chemotherapeutic agents ) as part of a standardized regimen . The term encompasses a variety of drugs, which are divided into broad categories such as alkylating agents and antimetabolites . [136] Traditional chemotherapeutic agents act by killing cells that divide rapidly, a critical property of most cancer cells.
Targeted therapy is a form of chemotherapy that targets specific molecular differences between cancer and normal cells. The first targeted therapies blocked the estrogen receptor molecule, inhibiting the growth of breast cancer. Another common example is the class of Bcr-Abl inhibitors , which are used to treat chronic myelogenous leukemia (CML). [137] Currently, targeted therapies exist for breast cancer , multiple myeloma , lymphoma , prostate cancer , melanoma and other cancers. [138]
The efficacy of chemotherapy depends on the type of cancer and the stage. In combination with surgery, chemotherapy has proven useful in cancer types including breast cancer , colorectal cancer, pancreatic cancer , osteogenic sarcoma , testicular cancer , ovarian cancer and certain lung cancers. [139] Chemotherapy is curative for some cancers, such as some leukemias , [140] [141] ineffective in some brain tumors , [142] and needless in others, such as most non-melanoma skin cancers . [143] The effectiveness of chemotherapy is often limited by its toxicity to other tissues in the body. Even when chemotherapy does not provide a permanent cure, it may be useful to reduce symptoms such as pain or to reduce the size of an inoperable tumor in the hope that surgery will become possible in the future.

Radiation
Radiation therapy involves the use of ionizing radiation in an attempt to either cure or improve symptoms. It works by damaging the DNA of cancerous tissue, killing it. To spare normal tissues (such as skin or organs, which radiation must pass through to treat the tumor), shaped radiation beams are aimed from multiple exposure angles to intersect at the tumor, providing a much larger dose there than in the surrounding, healthy tissue. As with chemotherapy, cancers vary in their response to radiation therapy. [144] [145] [146]
Radiation therapy is used in about half of cases. The radiation can be either from internal sources ( brachytherapy ) or external sources. The radiation is most commonly low energy x-rays for treating skin cancers, while higher energy x-rays are used for cancers within the body. [147] Radiation is typically used in addition to surgery and or chemotherapy. For certain types of cancer, such as early head and neck cancer , it may be used alone. [148] For painful bone metastasis , it has been found to be effective in about 70% of patients. [148]

Surgery
Surgery is the primary method of treatment for most isolated, solid cancers and may play a role in palliation and prolongation of survival. It is typically an important part of definitive diagnosis and staging of tumors, as biopsies are usually required. In localized cancer, surgery typically attempts to remove the entire mass along with, in certain cases, the lymph nodes in the area. For some types of cancer this is sufficient to eliminate the cancer. [139]

Palliative care
Palliative care refers to treatment that attempts to help the patient feel better and may be combined with an attempt to treat the cancer. Palliative care includes action to reduce physical, emotional, spiritual and psycho-social distress. Unlike treatment that is aimed at directly killing cancer cells, the primary goal of palliative care is to improve quality of life .
People at all stages of cancer treatment typically receive some kind of palliative care. In some cases, medical specialty professional organizations recommend that patients and physicians respond to cancer only with palliative care. [149] This applies to patients who: [150]
Palliative care may be confused with hospice and therefore only indicated when people approach end of life . Like hospice care, palliative care attempts to help the patient cope with their immediate needs and to increase comfort. Unlike hospice care, palliative care does not require people to stop treatment aimed at the cancer.
Multiple national medical guidelines recommend early palliative care for patients whose cancer has produced distressing symptoms or who need help coping with their illness. In patients first diagnosed with metastatic disease, palliative care may be immediately indicated. Palliative care is indicated for patients with a prognosis of less than 12 months of life even given aggressive treatment. [151] [152] [153]

Immunotherapy
A variety of therapies using immunotherapy , stimulating or helping the immune system to fight cancer, have come into use since 1997. Approaches include antibodies , checkpoint therapy and adoptive cell transfer . [154]

Alternative medicine
Complementary and alternative cancer treatments are a diverse group of therapies, practices and products that are not part of conventional medicine. [155] "Complementary medicine" refers to methods and substances used along with conventional medicine, while "alternative medicine" refers to compounds used instead of conventional medicine. [156] Most complementary and alternative medicines for cancer have not been studied or tested using conventional techniques such as clinical trials. Some alternative treatments have been investigated and shown to be ineffective but still continue to be marketed and promoted. Cancer researcher Andrew J. Vickers stated, "The label 'unproven' is inappropriate for such therapies; it is time to assert that many alternative cancer therapies have been 'disproven'." [157]

Prognosis
Survival rates vary by cancer type and by the stage at which it is diagnosed, ranging from majority survival to complete mortality five years after diagnosis. Once a cancer has metastasized, prognosis normally becomes much worse. About half of patients receiving treatment for invasive cancer (excluding carcinoma in situ and non-melanoma skin cancers) die from that cancer or its treatment. [21]
Survival is worse in the developing world , [21] partly because the types of cancer that are most common there are harder to treat than those associated with developed countries . [158]
Those who survive cancer develop a second primary cancer at about twice the rate of those never diagnosed. [159] The increased risk is believed to be due to the random chance of developing any cancer, the likelihood of surviving the first cancer, the same risk factors that produced the first cancer, unwanted side effects of treating the first cancer (particularly radiation therapy), and to better compliance with screening. [159]
Predicting short- or long-term survival depends on many factors. The most important are the cancer type and the patient's age and overall health. Those who are frail with other health problems have lower survival rates than otherwise healthy people. Centenarians are unlikely to survive for five years even if treatment is successful. People who report a higher quality of life tend to survive longer. [160] People with lower quality of life may be affected by depression and other complications and/or disease progression that both impairs quality and quantity of life. Additionally, patients with worse prognoses may be depressed or report poorer quality of life because they perceive that their condition is likely to be fatal.
Cancer patients have an increased risk of blood clots in veins . The use of heparin appears to improve survival and decrease the risk of blood clots. [161]

Epidemiology
In 2008, approximately 12.7 million cancers were diagnosed (excluding non-melanoma skin cancers and other non-invasive cancers) [21] and in 2010 nearly 7.98 million people died. [162] Cancers account for approximately 13% of deaths. The most common are lung cancer (1.4 million deaths), stomach cancer (740,000), liver cancer (700,000), colorectal cancer (610,000) and breast cancer (460,000). [163] This makes invasive cancer the leading cause of death in the developed world and the second leading in the developing world . [21] Over half of cases occur in the developing world. [21]
Deaths from cancer were 5.8 million in 1990. [162] Deaths have been increasing primarily due to longer lifespans and lifestyle changes in the developing world. [21] The most significant risk factor for developing cancer is age. [164] Although it is possible for cancer to strike at any age, most patients with invasive cancer are over 65. [164] According to cancer researcher Robert A. Weinberg , "If we lived long enough, sooner or later we all would get cancer." [165] Some of the association between aging and cancer is attributed to immunosenescence , [166] errors accumulated in DNA over a lifetime [167] and age-related changes in the endocrine system . [168] Aging's effect on cancer is complicated by factors such as DNA damage and inflammation promoting it and factors such as vascular aging and endocrine changes inhibiting it. [169]
Some slow-growing cancers are particularly common, but often are not fatal. Autopsy studies in Europe and Asia showed that up to 36% of people have undiagnosed and apparently harmless thyroid cancer at the time of their deaths and that 80% of men develop prostate cancer by age 80. [170] [171] As these cancers do not cause the patient's death, identifying them would have represented overdiagnosis rather than useful medical care.
The three most common childhood cancers are leukemia (34%), brain tumors (23%) and lymphomas (12%). [172] In the United States cancer affects about 1 in 285 children. [173] Rates of childhood cancer increased by 0.6% per year between 1975 and 2002 in the United States [174] and by 1.1% per year between 1978 and 1997 in Europe. [172] Death from childhood cancer decreased by half since 1975 in the United States. [173]

History
Cancer has existed for all of human history. [175] The earliest written record regarding cancer is from circa 1600 BC in the Egyptian Edwin Smith Papyrus and describes breast cancer. [175] Hippocrates (ca. 460 BC – ca. 370 BC) described several kinds of cancer, referring to them with the Greek word καρκίνος karkinos ( crab or crayfish ). [175] This name comes from the appearance of the cut surface of a solid malignant tumor, with "the veins stretched on all sides as the animal the crab has its feet, whence it derives its name". [176] Galen stated that "cancer of the breast is so called because of the fancied resemblance to a crab given by the lateral prolongations of the tumor and the adjacent distended veins". [177] :738 Celsus (ca. 25 BC – 50 AD) translated karkinos into the Latin cancer , also meaning crab and recommended surgery as treatment. [175] Galen (2nd century AD) disagreed with the use of surgery and recommended purgatives instead. [175] These recommendations largely stood for 1000 years. [175]
In the 15th, 16th and 17th centuries, it became acceptable for doctors to dissect bodies to discover the cause of death. [178] The German professor Wilhelm Fabry believed that breast cancer was caused by a milk clot in a mammary duct. The Dutch professor Francois de la Boe Sylvius , a follower of Descartes , believed that all disease was the outcome of chemical processes and that acidic lymph fluid was the cause of cancer. His contemporary Nicolaes Tulp believed that cancer was a poison that slowly spreads and concluded that it was contagious . [179]
The physician John Hill described tobacco snuff as the cause of nose cancer in 1761. [178] This was followed by the report in 1775 by British surgeon Percivall Pott that chimney sweeps' carcinoma , a cancer of the scrotum , was a common disease among chimney sweeps . [180] With the widespread use of the microscope in the 18th century, it was discovered that the 'cancer poison' spread from the primary tumor through the lymph nodes to other sites (" metastasis "). This view of the disease was first formulated by the English surgeon Campbell De Morgan between 1871 and 1874. [181]

Society and culture
Though many diseases (such as heart failure ) may have a worse prognosis than most cases of cancer, cancer is the subject of widespread fear and taboos . The euphemism "after a long illness" is still commonly used (2012), reflecting an apparent stigma . [182] This deep belief that cancer is necessarily a difficult and usually deadly disease is reflected in the systems chosen by society to compile cancer statistics: the most common form of cancer—non-melanoma skin cancers , accounting for about one-third of cancer cases worldwide, but very few deaths [183] [184] —are excluded from cancer statistics specifically because they are easily treated and almost always cured, often in a single, short, outpatient procedure. [185]
Cancer is regarded as a disease that must be "fought" to end the "civil insurrection"; a War on Cancer was declared in the US. Military metaphors are particularly common in descriptions of cancer's human effects and they emphasize both the state of the patient's health and the need to take immediate, decisive actions himself, rather than to delay, to ignore, or to rely entirely on others. The military metaphors also help rationalize radical, destructive treatments. [186] [187]
In the 1970s, a relatively popular alternative cancer treatment in the US was a specialized form of talk therapy , based on the idea that cancer was caused by a bad attitude. [188] People with a "cancer personality"—depressed, repressed, self-loathing and afraid to express their emotions—were believed to have manifested cancer through subconscious desire. Some psychotherapists said that treatment to change the patient's outlook on life would cure the cancer. [188] Among other effects, this belief allowed society to blame the victim for having caused the cancer (by "wanting" it) or having prevented its cure (by not becoming a sufficiently happy, fearless and loving person). [189] It also increased patients' anxiety, as they incorrectly believed that natural emotions of sadness, anger or fear shorten their lives. [189] The idea was ridiculed by Susan Sontag , who published Illness as Metaphor while recovering from treatment for breast cancer in 1978. [188] Although the original idea is now generally regarded as nonsense, the idea partly persists in a reduced form with a widespread, but incorrect, belief that deliberately cultivating a habit of positive thinking will increase survival. [189] This notion is particularly strong in breast cancer culture . [189]
One idea about why people with cancer are blamed or stigmatized, called the just-world hypothesis , is that blaming cancer on the patient's actions or attitudes allows the blamers to regain a sense of control. This is based upon the blamers' belief that the world is fundamentally just and so any dangerous illness, like cancer, must be a type of punishment for bad choices, because in a just world, bad things would not happen to good people. [190]

Economic effect
In 2007, the overall costs of cancer in the US—including treatment and indirect mortality expenses (such as lost productivity in the workplace)—was estimated to be $226.8 billion. In 2009, 32% of Hispanics and 10% of children 17 years old or younger lacked health insurance; "uninsured patients and those from ethnic minorities are substantially more likely to be diagnosed with cancer at a later stage, when treatment can be more extensive and more costly." [191]

Research
Because cancer is a class of diseases, [192] [193] it is unlikely that there will ever be a single " cure for cancer " any more than there will be a single treatment for all infectious diseases . [194] Angiogenesis inhibitors were once incorrectly thought to have potential as a " silver bullet " treatment applicable to many types of cancer. [195] Angiogenesis inhibitors and other cancer therapeutics are used in combination to reduce cancer morbidity and mortality. [196]
Experimental cancer treatments are studied in clinical trials to compare the proposed treatment to the best existing treatment. Treatments that succeeded in one cancer type can be tested against other types. [197] Diagnostic tests are under development to better target the right therapies to the right patients, based on their individual biology. [198]
Cancer research focuses on the following issues:
The improved understanding of molecular biology and cellular biology due to cancer research has led to new treatments for cancer since US President Richard Nixon declared the " War on Cancer " in 1971. Since then, the country has spent over $200 billion on cancer research, including resources from public and private sectors. [199] The cancer death rate (adjusting for size and age of the population) declined by five percent between 1950 and 2005. [200]
Competition for financial resources appears to have suppressed the creativity, cooperation, risk-taking and original thinking required to make fundamental discoveries, unduly favoring low-risk research into small incremental advancements over riskier, more innovative research. Other consequences of competition appear to be many studies with dramatic claims whose results cannot be replicated and perverse incentives that encourage grantee institutions to grow without making sufficient investments in their own faculty and facilities. [201] [202] [203] [204]

Pregnancy
Cancer affects approximately 1 in 1,000 pregnant women. The most common cancers found during pregnancy are the same as the most common cancers found in non-pregnant women during childbearing ages: breast cancer, cervical cancer, leukemia, lymphoma, melanoma, ovarian cancer and colorectal cancer. [205]
Diagnosing a new cancer in a pregnant woman is difficult, in part because any symptoms are commonly assumed to be a normal discomfort associated with pregnancy. As a result, cancer is typically discovered at a somewhat later stage than average. Some imaging procedures, such as MRIs (magnetic resonance imaging), CT scans , ultrasounds and mammograms with fetal shielding are considered safe during pregnancy; some others, such as PET scans , are not. [205]
Treatment is generally the same as for non-pregnant women. However, radiation and radioactive drugs are normally avoided during pregnancy, especially if the fetal dose might exceed 100 cGy. In some cases, some or all treatments are postponed until after birth if the cancer is diagnosed late in the pregnancy. Early deliveries are often used to advance the start of treatment. Surgery is generally safe, but pelvic surgeries during the first trimester may cause miscarriage. Some treatments, especially certain chemotherapy drugs given during the first trimester , increase the risk of birth defects and pregnancy loss (spontaneous abortions and stillbirths). [205]
Elective abortions are not required and, for the most common forms and stages of cancer, do not improve the mother's survival. In a few instances, such as advanced uterine cancer, the pregnancy cannot be continued and in others, the patient may end the pregnancy so that she can begin aggressive chemotherapy. [205]
Some treatments can interfere with the mother's ability to give birth vaginally or to breastfeed. [205] Cervical cancer may require birth by Caesarean section . Radiation to the breast reduces the ability of that breast to produce milk and increases the risk of mastitis . Also, when chemotherapy is given after birth, many of the drugs appear in breast milk, which could harm the baby. [205]

Other animals
Veterinary oncology , concentrating mainly on cats and dogs, is a growing specialty in wealthy countries and the major forms of human treatment such as surgery and radiotherapy may be offered. The most common types of cancer differ, but the cancer burden seems at least as high in pets as in humans. Animals, typically rodents, are often used in cancer research and studies of natural cancers in larger animals may benefit research into human cancer. [206]
In non-humans, a few types of transmissible cancer have been described, wherein the cancer spreads between animals by transmission of the tumor cells themselves. This phenomenon is seen in dogs with Sticker's sarcoma (also known as canine transmissible venereal tumor), and in Tasmanian devils with devil facial tumour disease (DFTD). [207]

Notes

Further reading

External links
WebPage index: 00056
Depictions of Muhammad
The permissibility of depictions of Muhammad in Islam has been a contentious issue. Oral and written descriptions of Muhammad are readily accepted by all traditions of Islam, but there is disagreement about visual depictions. [1] [2] The Quran does not explicitly forbid images of Muhammad, but there are a few hadith (supplemental teachings) which have explicitly prohibited Muslims from creating visual depictions of figures. [3] It is agreed on all sides that there is no authentic visual tradition as to the appearance of Muhammad, although there are early legends of portraits of him, and written physical descriptions whose authenticity is often accepted.
The question of whether images in Islamic art , including those depicting Muhammad, can be considered as religious art remains a matter of contention among scholars. [4] They appear in illustrated books that are normally works of history or poetry, including those with religious subjects; the Quran is never illustrated: "context and intent are essential to understanding Islamic pictorial art. The Muslim artists creating images of Muhammad, and the public who beheld them, understood that the images were not objects of worship. Nor were the objects so decorated used as part of religious worship". [5]
However, scholars concede that such images have "a spiritual element", and were also sometimes used in informal religious devotions celebrating the day of the Mi'raj . [6] Many visual depictions only show Muhammad with his face veiled, or symbolically represent him as a flame; other images, notably from before about 1500, show his face. [7] [8] [9] With the notable exception of modern-day Iran, [10] depictions of Muhammad were rare, never numerous in any community or era throughout Islamic history, [11] [12] and appeared almost exclusively in the private medium of Persian and other miniature book illustration. [13] [14] The key medium of public religious art in Islam was and is calligraphy . [12] [13] In Ottoman Turkey the hilya developed as a decorated visual arrangement of texts about Muhammad that was displayed as a portrait might be.
Visual images of Muhammad in the non-Islamic West have always been infrequent. In the Middle Ages they were mostly hostile, and most often appear in illustrations of Dante 's poetry. In the Renaissance and Early Modern period, Muhammad was sometimes depicted, typically in a more neutral or heroic light. These depictions began to encounter protests from Muslims, and in the age of the internet, a handful of caricature depictions printed in the European press have caused global protests and controversy, and been associated with violence.

Background
In Islam, although nothing in the Quran explicitly bans images, some supplemental hadith explicitly ban the drawing of images of any living creature; other hadith tolerate images, but never encourage them. Hence, most Muslims avoid visual depictions of Muhammad or any other prophet such as Moses or Abraham . [1] [15] [16]
Most Sunni Muslims believe that visual depictions of all the prophets of Islam should be prohibited [17] and are particularly averse to visual representations of Muhammad. [18] The key concern is that the use of images can encourage idolatry . [19] In Shia Islam, however, images of Muhammad are quite common nowadays, even though Shia scholars historically were against such depictions. [18] [20] Still, many Muslims who take a stricter view of the supplemental traditions will sometimes challenge any depiction of Muhammad, including those created and published by non-Muslims. [21]
Some major religions have experienced times during their history when images of their religious figures were forbidden . In Judaism, one of the Ten Commandments forbids "graven images". In Byzantine Christianity during the periods of Iconoclasm in the 8th century, and again during the 9th century, visual representations of sacred figures were forbidden, and only the Cross could be depicted in churches. The visual representation of Jesus and other religious figures remains a concern in parts of Protestant Christianity . [22]

Portraiture of Muhammad in Islamic literature
A number of hadith and other writings of the early Islamic period include stories in which portraits of Muhammad appear. Abu Hanifa Dinawari , Ibn al-Faqih , Ibn Wahshiyya and Abu Nu`aym tell versions of a story in which the Byzantine Emperor Heraclius is visited by two Meccans. He shows them a cabinet, handed down to him from Alexander the Great and originally created by God for Adam, each of whose drawers contains a portrait of a prophet. They are astonished to see a portrait of Muhammad in the final drawer. Sadid al-Din al-Kazaruni tells a similar story in which the Meccans are visiting the king of China. Kisa'i tells that God did indeed give portraits of the prophets to Adam. [23]
Ibn Wahshiyya and Abu Nu'ayn tell a second story in which a Meccan merchant visiting Syria is invited to a Christian monastery where a number of sculptures and paintings depict prophets and saints. There he sees the images of Muhammad and Abu Bakr, as yet unidentified by the Christians. [24] In an 11th-century story, Muhammad is said so have sat for a portrait by an artist retained by Sassanid king Kavadh II . The king liked the portrait so much that he placed it on his pillow. [23]
Later, Al-Maqrizi tells a story in which Muqawqis , ruler of Egypt, meets with Muhammad's envoy. He asks the envoy to describe Muhammad and checks the description against a portrait of an unknown prophet which he has on a piece of cloth. The description matches the portrait. [23]
In a 17th-century Chinese story, the king of China asks to see Muhammad, but Muhammad instead sends his portrait. The king is so enamoured of the portrait that he is converted to Islam, at which point the portrait, having done its job, disappears. [25]

Depiction by Muslims

Verbal descriptions
In one of the earliest sources, Ibn Sa'd 's Kitab al-Tabaqat al-Kabir , there are numerous verbal descriptions of Muhammad. One description sourced to Ali ibn Abi Talib is as follows:
From the Ottoman period onwards such texts have been presented on calligraphic hilya panels (Turkish: hilye , pl. hilyeler ), commonly surrounded by an elaborate frame of illuminated decoration and either included in books or, more often, muraqqas or albums, or sometimes placed in wooden frames so that they can hang on a wall. [27] The elaborated form of the calligraphic tradition was founded in the 17th century by the Ottoman calligrapher Hâfiz Osman . While containing a concrete and artistically appealing description of Muhammad's appearance, they complied with the strictures against figurative depictions of Muhammad, leaving his appearance to the viewer's imagination. Several parts of the complex design were named after parts of the body, from the head downwards, indicating the explicit intention of the hilya as a substitute for a figurative depiction. [28] [29]
The Ottoman hilye format customarily starts with a basmala , shown on top, and is separated in the middle by Quran 21:107 : "And We have not sent you but as a mercy to the worlds". [29] Four compartments set around the central one often contain the names of the Rightly-Guided Caliphs , Abu Bakr , Umar , Uthman , and Ali , each followed by "radhi Allahu anhu" ("may God be pleased with him").

Calligraphic representations
The most common visual representation of the Muhammad in Islamic art, especially in Arabic-speaking areas, is by a calligraphic representation of his name, a sort of monogram in roughly circular form, often given a decorated frame. Such inscriptions are normally in Arabic, and may rearrange or repeat forms, or add a blessing or honorific, or for example the word "messenger" or a contraction of it. The range of ways of representing Muhammad's name is considerable, including ambigrams ; he is also frequently symbolised by a rose.
The more elaborate versions relate to other Islamic traditions of special forms of calligraphy such as those writing the names of God , and the secular tughra or elaborate monogram of Ottoman rulers.

Figurative visual depictions
Throughout Islamic history, depictions of Muhammad in Islamic art were rare. [11] Even so, there exists a "notable corpus of images of Muhammad produced, mostly in the form of manuscript illustrations, in various regions of the Islamic world from the thirteenth century through modern times". [30] Depictions of Muhammad date back to the start of the tradition of Persian miniatures as illustrations in books. The illustrated book from the Persianate world ( Warka and Gulshah , Topkapi Palace Library H. 841, attributed to Konya 1200–1250) contains the two earliest known Islamic depictions of Muhammad. [31]
This book dates to before or just around the time of the Mongol invasion of Anatolia in the 1240s, and before the campaigns against Persia and Iraq of the 1250s, which destroyed great numbers of books in libraries. Recent scholarship has noted that, although surviving early examples are now uncommon, generally human figurative art was a continuous tradition in Islamic lands (such as in literature, science, and history); as early as the 8th century, such art flourished during the Abbasid Caliphate (c. 749 - 1258, across Spain, North Africa, Egypt, Syria, Turkey, Mesopotamia, and Persia). [32]
Christiane Gruber traces a development from "veristic" images showing the whole body and face, in the 13th to 15th centuries, to more "abstract" representations in the 16th to 19th centuries, the latter including the representation of Muhammad by a special type of calligraphic representation, with the older types also remaining in use. [33] An intermediate type, first found from about 1400, is the "inscribed portrait" where the face of Muhammad is blank, with "Ya Muhammad" ("O Muhammad") or a similar phrase written in the space instead; these may be related to Sufi thought. In some cases the inscription appears to have been an underpainting that would later be covered by a face or veil, so a pious act by the painter, for his eyes alone, but in others it was intended to be seen. [30] According to Gruber, a good number of these paintings later underwent iconoclastic mutilations, in which the facial features of Muhammad were scratched or smeared, as Muslim views on the acceptability of veristic images changed. [34]
A number of extant Persian manuscripts representing Muhammad date from the Ilkhanid period under the new Mongol rulers, including a Marzubannama dating to 1299. The Ilkhanid MS Arab 161 of 1307/8 contains 25 illustrations found in an illustrated version of Al-Biruni 's The Remaining Signs of Past Centuries , of which five include depictions Muhammad, including the two concluding images, the largest and most accomplished in the manuscript, which emphasize the relation of Muhammad and `Ali according to Shi`ite doctrine. [35] According to Christiane Gruber, other works use images to promote Sunni Islam, such as a set of Mi'raj illustrations (MS H 2154) in the early 14th century, [36] although other historians have dated the same illustrations to the Jalayrid period of Shia rulers. [37]
Depictions of Muhammad are also found in Persian manuscripts in the following Timurid and Safavid dynasties, and Turkish Ottoman art in the 14th to 17th centuries, and beyond. Perhaps the most elaborate cycle of illustrations of Muhammad's life is the copy, completed in 1595, of the 14th-century biography Siyer-i Nebi commissioned by the Ottoman sultan Murat III for his son, the future Mehmed III , containing over 800 illustrations. [38]
Probably the commonest narrative scene represented is the Mi'raj ; according to Gruber, "There exist countless single-page paintings of the meʿrāj included in the beginnings of Persian and Turkish romances and epic stories produced from the beginning of the 15th century to the 20th century". [39] These images were also used in celebrations of the anniversary of the Mi'raj on 27 Rajab , when the accounts were recited aloud to male groups: "Didactic and engaging, oral stories of the ascension seem to have had the religious goal of inducing attitudes of praise among their audiences". Such practices are most easily documented in the 18th and 19th centuries, but manuscripts from much earlier appear to have fulfilled the same function. [40] Otherwise a large number of different scenes may be represented at times, from Muhammad's birth to the end of his life, and his existence in Paradise. [41]

Halo
In the earliest depictions Muhammad may be shown with or without a halo , the earliest halos being round in the style of Christian art, [42] but before long a flaming halo or aureole in the Buddhist or Chinese tradition becomes more common than the circular form found in the West, when a halo is used. A halo or flame may surround only his head, but often his whole body, and in some images the body itself cannot be seen for the halo. This "luminous" form of representation avoided the issues caused by "veristic" images, and could be taken to convey qualities of Muhammad's person described in texts. [43] If the body is visible, the face may be covered with a veil (see gallery for examples of both types). This form of representation, which began at the start of the Safavid period in Persia, [44] was done out of reverence and respect. [11] Other prophets of Islam , and Muhammad's wives and relations, may be treated in similar ways if they also appear.
T. W. Arnold (1864–1930), an early historian of Islamic art, stated that "Islam has never welcomed painting as a handmaid of religion as both Buddhism and Christianity have done. Mosques have never been decorated with religious pictures, nor has a pictorial art been employed for the instruction of the heathen or for the edification of the faithful." [11] Comparing Islam to Christianity, he also writes: "Accordingly, there has never been any historical tradition in the religious painting of Islam – no artistic development in the representation of accepted types – no schools of painters of religious subjects; least of all has there been any guidance on the part of leaders of religious thought corresponding to that of ecclesiastical authorities in the Christian Church." [11]
Images of Muhammad remain controversial to the present day, and are not considered acceptable in many countries in the Middle East. For example, in 1963 an account by a Turkish author of a Hajj pilgrimage to Mecca was banned in Pakistan because it contained reproductions of miniatures showing Muhammad unveiled. [45]

Contemporary Iran
Despite the ban on the representation of Muhammad, images of Muhammed are not uncommon in Iran. The Iranian Shi'ism seems more tolerant on this point than Sunnite orthodoxy. [48] In Iran, depictions have considerable acceptance to the present day, and may be found in the modern forms of the poster and postcard . [10] [49]
Since the late 1990s, experts in Islamic iconography discovered images, printed on paper in Iran, portraying Mohammed as a teenager wearing a turban. [48] There are several variants, all show the same juvenile face, identified by an inscription such as "Muhammad, the Messenger of God", or a more detailed legend referring to an episode in the life of Muhammad and the supposed origin of the image. [48] Some Iranian versions of these posters attributed the original depiction to a Bahira , a Christian monk who met the young Muhammad in Syria. By crediting the image to a Christian and predating it to the time before Muhammad became a prophet, the manufacturers of the image exonerate themselves from any wrongdoing. [50]
The motif was taken from a photograph of a young Tunisian taken by the Germans Rudolf Franz Lehnert and Ernst Heinrich Landrock in 1905 or 1906, which had been printed in high editions on picture post cards till 1921. [48] This depiction has been popular in Iran as a form of curiosity. [50]
In Tehran , a mural depicting the prophet – his face veiled – riding Buraq was installed at a public road intersection in 2008, the only mural of its kind in a Muslim-majority country. [10]

Cinema
Very few films have been made about Muhammad. The 1976 film The Message , also known as Mohammad, Messenger of God , focused on other persons and never directly showed Muhammad or most members of his family. A devotional cartoon called Muhammad: The Last Prophet was released in 2004. [51] An Iranian film directed by Majid Majidi was released in 2015 named Muhammad . It is the first part of the trilogy film series on Muhammad by Majid Majidi.
While Sunni Muslims have always explicitly prohibited the depiction of Muhammad on film, [52] contemporary Shi'a scholars have taken a more relaxed attitude, stating that it is permissible to depict Muhammad, even in television or movies, if done with respect. [53]

Depiction by non-Muslims
Western representations of Muhammad were very rare until the explosion of images following the invention of the printing press ; he is shown in a few medieval images, normally in an unflattering manner, often influenced by his brief mention in Dante's Divine Comedy . Muhammad sometimes figures in Western depictions of groups of influential people in world history. Such depictions tend to be favourable or neutral in intent; one example can be found at the United States Supreme Court building in Washington, D.C. . Created in 1935, the frieze includes major historical lawgivers, and places Muhammad alongside Hammurabi , Moses , Confucius , and others. In 1997, a controversy erupted surrounding the frieze, and tourist materials have since been edited to describe the depiction as "a well-intentioned attempt by the sculptor to honor Muhammad" that "bears no resemblance to Muhammad." [54]
In 1955, a statue of Muhammad was removed from a courthouse in New York City after the ambassadors of Indonesia , Pakistan, and Egypt requested its removal. [55] The extremely rare representations of Muhammad in monumental sculpture are especially likely to be offensive to Muslims, as the statue is the classic form for idols, and a fear of any hint of idolatry is the basis of Islamic prohibitions. Islamic art has almost always avoided large sculptures of any subject, especially free-standing ones; only a few animals are known, mostly fountain-heads, like those in the Lion Court of the Alhambra ; the Pisa Griffin is perhaps the largest.
In 1997, the Council on American–Islamic Relations , a Muslim advocacy group in the United States, wrote to United States Supreme Court Chief Justice William Rehnquist requesting that the sculpted representation of the Prophet Muhammad on the north frieze inside the Supreme Court building be removed or sanded down. The court rejected CAIR's request. [56]
There have also been numerous book illustrations showing Muhammad.
Dante , in The Divine Comedy: Inferno , placed Muhammad in Hell, with his entrails hanging out (Canto 28):
This scene was sometimes shown in illustrations of the Divina Commedia before modern times. Muhammad is represented in a 15th-century fresco Last Judgement by Giovanni da Modena and drawing on Dante, in the Church of San Petronio , Bologna, Italy . [58] and artwork by Salvador Dalí , Auguste Rodin , William Blake , and Gustave Doré . [59]

Controversies in the 21st century
The start of the 21st century has been marked by controversies over depictions of Muhammad, not only for recent caricatures or cartoons, but also regarding the display of historical artwork.
In a story on morals at the end of the millennium in December 1999, the German news magazine Der Spiegel printed on the same page pictures of “moral apostles” Muhammad, Jesus , Confucius , and Immanuel Kant . In the subsequent weeks, the magazine received protests, petitions and threats against publishing the picture of Muhammad. The Turkish TV-station Show TV broadcast the telephone number of an editor who then received daily calls. [60]
Nadeem Elyas, leader of the Central Council of Muslims in Germany said that the picture should not be printed again in order to avoid hurting the feelings of Muslims intentionally. Elyas recommended to whiten the face of Muhammad instead. [61]
In June 2001, the Spiegel with consideration of Islamic laws published a picture of Muhammed with a whitened face on its title page. [62] The same picture of Muhammad by Hosemann had been published by the magazine once before in 1998 in a special edition on Islam, but then without evoking similar protests. [63]
In 2002, Italian police reported that they had disrupted a terrorist plot to destroy a church in Bologna , which contains a 15th-century fresco depicting an image of Muhammad (see above). [58] [64]

Cartoons
In 2005, Danish newspaper Jyllands-Posten published a set of editorial cartoons , many of which depicted Muhammad. In late 2005 and early 2006, Danish Muslim organizations ignited a controversy through public protests and by spreading knowledge of the publication of the cartoons. [22] According to John Woods, Islamic history professor at the University of Chicago, it was not simply the depiction of Muhammad that was offensive, but the implication that Muhammad was somehow a supporter of terrorism. [16] In Sweden, an online caricature competition was announced in support of Jyllands-Posten , but Foreign Affairs Minister Laila Freivalds and the Swedish Security Service pressured the internet service provider to shut the page down. In 2006, when her involvement was revealed to the public, she had to resign. [65] On 12 February 2008 the Danish police arrested three men alleged to be involved in a plot to assassinate Kurt Westergaard , one of the cartoonists. [66]
In 2006, the controversial American animated television comedy program South Park , which had previously depicted Muhammad as a superhero character in the July 4, 2001 episode " Super Best Friends " [67] and has depicted Muhammad in the opening sequence since that episode, [68] attempted to satirize the Danish newspaper incident. In the episode, " Cartoon Wars Part II ", they intended to show Muhammad handing a salmon helmet to Peter Griffin , a character from the Fox animated series Family Guy . However, Comedy Central , who airs South Park , rejected the scene, citing concerns of violent protests in the Islamic world . The creators of South Park reacted by instead satirizing Comedy Central's double standard for broadcast acceptability by including a segment of "Cartoon Wars Part II" in which American president George W. Bush and Jesus defecate on the flag of the United States .
The Lars Vilks Muhammad drawings controversy began in July 2007 with a series of drawings by Swedish artist Lars Vilks which depicted Muhammad as a roundabout dog . Several art galleries in Sweden declined to show the drawings, citing security concerns and fear of violence. The controversy gained international attention after the Örebro -based regional newspaper Nerikes Allehanda published one of the drawings on August 18 to illustrate an editorial on self-censorship and freedom of religion . [69]
While several other leading Swedish newspapers had published the drawings already, this particular publication led to protests from Muslims in Sweden as well as official condemnations from several foreign governments including Iran , [70] Pakistan , [71] Afghanistan , [72] Egypt [73] and Jordan , [74] as well as by the inter-governmental Organisation of the Islamic Conference (OIC). [75] The controversy occurred about one and a half years after the Jyllands-Posten Muhammad cartoons controversy in Denmark in early 2006.
Another controversy emerged in September 2007 when Bangladeshi cartoonist Arifur Rahman was detained on suspicion of showing disrespect to Muhammad. The interim government confiscated copies of the Bengali -language Prothom Alo in which the drawings appeared. The cartoon consisted of a boy holding a cat conversing with an elderly man. The man asks the boy his name, and he replies "Babu". The older man chides him for not mentioning the name of Muhammad before his name.
He then points to the cat and asks the boy what it is called, and the boy replies "Muhammad the cat". The cartoon caused a firestorm in Bangladesh, with militant Islamists demanding that Rahman be executed for blasphemy . A group of people torched copies of the paper and several Islamic groups protested, saying the drawings ridiculed Mohammad and his companions. They demanded "exemplary punishment" for the paper's editor and the cartoonist. Bangladesh does not have a blasphemy law , although one had been demanded by the same extremist Islamic groups.

Charlie Hebdo
On November 2, 2010, the office of the French satirical weekly newspaper Charlie Hebdo at Paris was attacked with a firebomb and its website hacked, after it had announced plans to publish a special edition with Muhammad as its “chief editor”, and the title page with a cartoon of Muhammad had been pre-issued on social media.
In September 2012, the newspaper published a series of satirical cartoons of Muhammad, some of which feature nude caricatures of him. In January 2013, Charlie Hebdo announced that they would make a comic book on the life of Muhammad. [77] In March 2013, Al-Qaeda's branch in Yemen, commonly known as Al-Qaeda in the Arabian Peninsula (AQAP), released a hit list in an edition of their English-language magazine Inspire . The list included Stéphane Charbonnier , Lars Vilks , three Jyllands-Posten employees involved in the Muhammad cartoon controversy, Molly Norris from the Everybody Draw Mohammed Day and others whom AQAP accused of insulting Islam. [78] [79]
On January 7, 2015, the office was attacked again with 12 shot dead including Stéphane Charbonnier.

Wikipedia article
In 2008, several Muslims protested against the inclusion of Muhammad's depictions in the English Wikipedia 's Muhammad article. [80] [81] An online petition claims to have collected over 450,000 signatures in three months (December 2007 to February 2008). The petition was started by Faraz Ahmad of Daska, Pakistan , resident in Glasgow, Scotland , formerly editing Wikipedia as "Farazilu". [ citation needed ]
The petition opposed a reproduction of a 17th-century Ottoman copy of a 14th-century Ilkhanate manuscript image ( MS Arabe 1489 ) depicting Muhammad as he prohibited Nasīʾ . [82] Jeremy Henzell-Thomas of The American Muslim deplored the petition as one of "these mechanical knee-jerk reactions [which] are gifts to those who seek every opportunity to decry Islam and ridicule Muslims and can only exacerbate a situation in which Muslims and the Western media seem to be locked in an ever-descending spiral of ignorance and mutual loathing." [83]
Wikipedia considered but rejected a compromise that would allow visitors to choose whether to view the page with images. [81] The Wikipedia community has not acted upon the petition. [84] The site's answers to frequently asked questions about these images state that Wikipedia does not censor itself for the benefit of any one group. [85]

Metropolitan Museum of Art
The Metropolitan Museum of Art in January 2010 confirmed to the New York Post that it had quietly removed all historic paintings which contained depictions of Muhammad from public exhibition. The Museum quoted objections on the part of conservative Muslims which were "under review." The museum's action was criticized as excessive political correctness , as were other decisions taken close to the same time, including the renaming of the "Primitive Art Galleries" to the "Arts of Africa, Oceania and the Americas" and the projected "Islamic Galleries" to "Arab Lands, Turkey, Iran, Central Asia and Later South Asia". [86]

Everybody Draw Mohammed Day
Everybody Draw Mohammed Day was a protest against those who threatened violence against artists who drew representations of Muhammad. It began as a protest against the action of Comedy Central in forbidding the broadcast of the South Park episode " 201 " in response to death threats against some of those responsible for the segment. Observance of the day began with a drawing posted on the Internet on April 20, 2010, accompanied by text suggesting that "everybody" create a drawing representing Muhammad, on May 20, 2010, as a protest against efforts to limit freedom of speech .

Muhammad Art Exhibit & Contest
A May 3, 2015, event held in Garland, Texas, held by American activists Pamela Geller and Robert Spencer , was the scene of a shooting by two individuals who were later themselves shot and killed outside the event. [87] Police officers assisting in security at the event returned fire and killed the two gunmen. The event offered a $10,000 prize and was said to be in response to the January 2015 attacks on the French magazine Charlie Hebdo . One of the gunmen was identified as a former terror suspect, known to the FBI . [88] [89]

See also
General:

Notes
WebPage index: 00057
United Kingdom
WebPage index: 00058
Cyberculture
Cyberculture or computer culture is the culture that has emerged, or is emerging, from the use of computer networks for communication , entertainment , and business . Internet culture is also the study of various social phenomena associated with the Internet and other new forms of the network communication, such as online communities, online multi-player gaming, wearable computing, social gaming, social media, mobile apps, augmented reality, and texting, [1] and includes issues related to identity, privacy, and network formation.

Overview
Since the boundaries of cyberculture are difficult to define, the term is used flexibly, and its application to specific circumstances can be controversial. It generally refers at least to the cultures of virtual communities , but extends to a wide range of cultural issues relating to " cyber -topics", e.g. cybernetics , and the perceived or predicted cyborgization of the human body and human society itself. It can also embrace associated intellectual and cultural movements, such as cyborg theory and cyberpunk . The term often incorporates an implicit anticipation of the future .
The Oxford English Dictionary lists the earliest usage of the term "cyberculture" in 1963, when A.M. Hilton wrote the following, "In the era of cyberculture, all the plows pull themselves and the fried chickens fly right onto our plates." [3] This example, and all others, up through 1995 are used to support the definition of cyberculture as "the social conditions brought about by automation and computerization." [3] The American Heritage Dictionary broadens the sense in which "cyberculture" is used by defining it as, "The culture arising from the use of computer networks, as for communication, entertainment, work, and business". [4] However, what both the OED and the American Heritage Dictionary miss is that cyberculture is the culture within and among users of computer networks. This cyberculture may be purely an online culture or it may span both virtual and physical worlds. This is to say, that cyberculture is a culture endemic to online communities; it is not just the culture that results from computer use, but culture that is directly mediated by the computer. Another way to envision cyberculture is as the electronically enabled linkage of like-minded, but potentially geographically disparate (or physically disabled and hence less mobile) persons. [ original research? ]
Cyberculture is a wide social and cultural movement closely linked to advanced information science and information technology , their emergence, development and rise to social and cultural prominence between the 1960s and the 1990s. Cyberculture was influenced at its genesis by those early users of the internet, frequently including the architects of the original project. These individuals were often guided in their actions by the hacker ethic . While early cyberculture was based on a small cultural sample, and its ideals, the modern cyberculture is a much more diverse group of users and the ideals that they espouse.
Numerous specific concepts of cyberculture have been formulated by such authors as Lev Manovich , [5] [6] Arturo Escobar and Fred Forest . [7] However, most of these concepts concentrate only on certain aspects, and they do not cover these in great detail. Some authors aim to achieve a more comprehensive understanding distinguished between early and contemporary cyberculture ( Jakub Macek ), [8] or between cyberculture as the cultural context of information technology and cyberculture (more specifically cyberculture studies) as "a particular approach to the study of the 'culture + technology' complex" (David Lister et al.). [9]

Manifestations
Manifestations of cyberculture include various human interactions mediated by computer networks. They can be activities, pursuits, games, place's and metaphors, and include a diverse base of applications. Some are supported by specialized software and others work on commonly accepted internet protocols. Example include but are not limited to:

Qualities
First and foremost, cyberculture derives from traditional notions of culture, as the roots of the word imply. In non-cyberculture, it would be odd to speak of a single, monolithic culture. In cyberculture, by extension, searching for a single thing that is cyberculture would likely be problematic. The notion that there is a single, definable cyberculture is likely the complete dominance of early cyber territory by affluent North Americans. Writing by early proponents of cyberspace tends to reflect this assumption (see Howard Rheingold ). [10]
The ethnography of cyberspace is an important aspect of cyberculture that does not reflect a single unified culture. It "is not a monolithic or placeless 'cyberspace'; rather, it is numerous new technologies and capabilities, used by diverse people, in diverse real-world locations." It is malleable, perishable, and can be shaped by the vagaries of external forces on its users. For example, the laws of physical world governments, social norms, the architecture of cyberspace, and market forces shape the way cybercultures form and evolve. As with physical world cultures, cybercultures lend themselves to identification and study.
There are several qualities that cybercultures share that make them warrant the prefix "cyber-". Some of those qualities are that cyberculture:
Thus, cyberculture can be generally defined as the set of technologies (material and intellectual), practices, attitudes, modes of thought, and values that developed with cyberspace. [12]

Identity – "Architectures of credibility"
Cyberculture, like culture in general, relies on establishing identity and credibility. However, in the absence of direct physical interaction, it could be argued that the process for such establishment is more difficult.
How does cyberculture rely on and establish identity and credibility? This relationship is two-way, with identity and credibility being both used to define the community in cyberspace and to be created within and by online communities.
In some senses, online credibility is established in much the same way that it is established in the offline world; however, since these are two separate worlds, it is not surprising that there are differences in their mechanisms and interactions of the markers found in each.
Following the model put forth by Lawrence Lessig in Code: Version 2.0 , [13] the architecture of a given online community may be the single most important factor regulating the establishment of credibility within online communities. Some factors may be:

Anonymous versus known
Many sites allow anonymous commentary, where the user-id attached to the comment is something like "guest" or "anonymous user". In an architecture that allows anonymous posting about other works, the credibility being impacted is only that of the product for sale, the original opinion expressed, the code written, the video, or other entity about which comments are made (e.g., a Slashdot post). Sites that require "known" postings can vary widely from simply requiring some kind of name to be associated with the comment to requiring registration, wherein the identity of the registrant is visible to other readers of the comment. These "known" identities allow and even require commentators to be aware of their own credibility, based on the fact that other users will associate particular content and styles with their identity. By definition, then, all blog postings are "known" in that the blog exists in a consistently defined virtual location, which helps to establish an identity, around which credibility can gather. Conversely, anonymous postings are inherently incredible. Note that a "known" identity need have nothing to do with a given identity in the physical world.

Linked to physical identity versus internet-based identity only
Architectures can require that physical identity be associated with commentary, as in Lessig's example of Counsel Connect. [13] :94–97 However, to require linkage to physical identity, many more steps must be taken (collecting and storing sensitive information about a user) and safeguards for that collected information must be established-the users must have more trust of the sites collecting the information (yet another form of credibility). Irrespective of safeguards, as with Counsel Connect, [13] :94–97 using physical identities links credibility across the frames of the internet and real space, influencing the behaviors of those who contribute in those spaces. However, even purely internet-based identities have credibility. Just as Lessig describes linkage to a character or a particular online gaming environment, nothing inherently links a person or group to their internet-based persona, but credibility (similar to "characters") is "earned rather than bought, and because this takes time and (credibility is) not fungible, it becomes increasingly hard" to create a new persona. [13] :113

Unrated commentary system versus rated commentary system
In some architectures those who review or offer comments can, in turn, be rated by other users. This technique offers the ability to regulate the credibility of given authors by subjecting their comments to direct "quantifiable" approval ratings.

Positive feedback-oriented versus mixed feedback (positive and negative) oriented
Architectures can be oriented around positive feedback or a mix of both positive and negative feedback. While a particular user may be able to equate fewer stars with a "negative" rating, the semantic difference is potentially important. The ability to actively rate an entity negatively may violate laws or norms that are important in the jurisdiction in which the internet property is important. The more public a site, the more important this concern may be, as noted by Goldsmith & Wu regarding eBay. [14]

Moderated versus unmoderated
Architectures can also be oriented to give editorial control to a group or individual. Many email lists are worked in this fashion (e.g., Freecycle). In these situations, the architecture usually allows, but does not require that contributions be moderated. Further, moderation may take two different forms: reactive or proactive. In the reactive mode, an editor removes posts, reviews, or content that is deemed offensive after it has been placed on the site or list. In the proactive mode, an editor must review all contributions before they are made public.
In a moderated setting, credibility is often given to the moderator. However, that credibility can be damaged by appearing to edit in a heavy-handed way, whether reactive or proactive (as experienced by digg.com). In an unmoderated setting, credibility lies with the contributors alone. It should be noted that the very existence of an architecture allowing moderation may lend credibility to the forum being used (as in Howard Rheingold's examples from the WELL), [10] or it may take away credibility (as in corporate web sites that post feedback, but edit it highly).

Cyberculture studies
The field of cyberculture studies examines the topics explained above, including the communities emerging within the networked spaces sustained by the use of modern technology. Students of cyberculture engage with political, philosophical, sociological, and psychological issues that arise from the networked interactions of human beings by humans who act in various relations to information science and technology.
Donna Haraway , Sadie Plant , Manuel De Landa , Bruce Sterling , Kevin Kelly , Wolfgang Schirmacher , Pierre Levy , David Gunkel , Victor J.Vitanza , Gregory Ulmer , Charles D. Laughlin , and Jean Baudrillard are among the key theorists and critics who have produced relevant work that speaks to, or has influenced studies in, cyberculture. Following the lead of Rob Kitchin, in his work Cyberspace: The World in the Wires , we might view cyberculture from different critical perspectives. These perspectives include futurism or techno-utopianism , technological determinism , social constructionism , postmodernism , poststructuralism , and feminist theory . [11] :56–72

See also
WebPage index: 00059
Anarchism
Anarchism is a political philosophy that advocates self-governed societies based on voluntary institutions. These are often described as stateless societies , [1] [2] [3] [4] although several authors have defined them more specifically as institutions based on non- hierarchical free associations . [5] [6] [7] [8] Anarchism holds the state to be undesirable, unnecessary, and harmful. [9] [10]
While anti-statism is central, [11] anarchism generally entails opposing authority or hierarchical organisation in the conduct of all human relations, including, but not limited to, the state system. Anarchism is usually considered a radical left-wing ideology, [12] [13] and much of anarchist economics and anarchist legal philosophy reflects anti-authoritarian interpretations of communism , collectivism , syndicalism , mutualism , or participatory economics . [14]
Anarchism does not offer a fixed body of doctrine from a single particular world view, instead fluxing and flowing as a philosophy. [15] Many types and traditions of anarchism exist, not all of which are mutually exclusive. [16] Anarchist schools of thought can differ fundamentally, supporting anything from extreme individualism to complete collectivism. [10] Strains of anarchism have often been divided into the categories of social and individualist anarchism or similar dual classifications. [17] [18]

Etymology and terminology
The term anarchism is a compound word composed from the word anarchy and the suffix -ism , [19] themselves derived respectively from the Greek ἀναρχία , [20] i.e. anarchy [21] [22] [23] (from ἄναρχος , anarchos , meaning "one without rulers"; [24] from the privative prefix ἀν - ( an- , i.e. "without") and ἀρχός , archos , i.e. "leader", "ruler"; [25] (cf. archon or ἀρχή , arkhē , i.e. "authority", "sovereignty", "realm", "magistracy") [26] ) and the suffix -ισμός or -ισμα ( -ismos , -isma , from the verbal infinitive suffix -ίζειν, -izein ). [27] The first known use of this word was in 1539. [28] Various factions within the French Revolution labelled opponents as anarchists (as Robespierre did the Hébertists ) [29] although few shared many views of later anarchists. There would be many revolutionaries of the early nineteenth century who contributed to the anarchist doctrines of the next generation, such as William Godwin and Wilhelm Weitling , but they did not use the word anarchist or anarchism in describing themselves or their beliefs. [30]
The first political philosopher to call himself an anarchist was Pierre-Joseph Proudhon , marking the formal birth of anarchism in the mid-nineteenth century. Since the 1890s, and beginning in France, [31] the term libertarianism has often been used as a synonym for anarchism [32] and was used almost exclusively in this sense until the 1950s in the United States; [33] its use as a synonym is still common outside the United States. [34] On the other hand, some use libertarianism to refer to individualistic free-market philosophy only, referring to free-market anarchism as libertarian anarchism . [35] [36]

History

Origins
The earliest [37] anarchist themes can be found in the 6th century BC, among the works of Taoist philosopher Laozi , [38] and in later centuries by Zhuangzi and Bao Jingyan. [39] Zhuangzi's philosophy has been described by various sources as anarchist. [40] [41] [42] [43] Zhuangzi wrote, "A petty thief is put in jail. A great brigand becomes a ruler of a Nation." [44] Diogenes of Sinope and the Cynics , and their contemporary Zeno of Citium , the founder of Stoicism , also introduced similar topics. [38] [45] Jesus is sometimes considered the first anarchist in the Christian anarchist tradition. Georges Lechartier wrote that "The true founder of anarchy was Jesus Christ and ... the first anarchist society was that of the apostles ." [46] In early Islamic history , some manifestations of anarchic thought are found during the Islamic civil war over the Caliphate , where the Kharijites insisted that the imamate is a right for each individual within the Islamic society. [47] Later, some Muslim scholars, such as Amer al-Basri [48] and Abu Hanifa , [49] led movements of boycotting the rulers, paving the way to the waqf (endowments) tradition, which served as an alternative to and asylum from the centralised authorities of the emirs. But such interpretations reverberate subversive religious conceptions like the aforementioned seemingly anarchistic Taoist teachings and that of other anti-authoritarian religious traditions creating a complex relationship regarding the question as to whether or not anarchism and religion are compatible . [ clarification needed ] This is exemplified when the glorification of the state is viewed as a form of sinful idolatry . [50] [51]
The French renaissance political philosopher Étienne de La Boétie wrote in his most famous work the Discourse on Voluntary Servitude what some historians consider an important anarchist precedent. [52] [53] The radical Protestant Christian Gerrard Winstanley and his group the Diggers are cited by various authors as proposing anarchist social measures in the 17th century in England. [54] [55] [56] The term "anarchist" first entered the English language in 1642, during the English Civil War , as a term of abuse , used by Royalists against their Roundhead opponents. [57] By the time of the French Revolution some, such as the Enragés , began to use the term positively, [58] in opposition to Jacobin centralisation of power, seeing "revolutionary government" as oxymoronic . [57] By the turn of the 19th century, the English word "anarchism" had lost its initial negative connotation. [57]
Modern anarchism emerged from the secular or religious thought of the Enlightenment , particularly Jean-Jacques Rousseau 's arguments for the moral centrality of freedom. [59]
As part of the political turmoil of the 1790s in the wake of the French Revolution, William Godwin developed the first expression of modern anarchist thought. [60] [61] Godwin was, according to Peter Kropotkin , "the first to formulate the political and economical conceptions of anarchism, even though he did not give that name to the ideas developed in his work", [38] while Godwin attached his anarchist ideas to an early Edmund Burke . [62]
Godwin is generally regarded as the founder of the school of thought known as 'philosophical anarchism'. He argued in Political Justice (1793) [61] [63] that government has an inherently malevolent influence on society, and that it perpetuates dependency and ignorance. He thought that the spread of the use of reason to the masses would eventually cause government to wither away as an unnecessary force. Although he did not accord the state with moral legitimacy, he was against the use of revolutionary tactics for removing the government from power. Rather, he advocated for its replacement through a process of peaceful evolution. [61] [64]
His aversion to the imposition of a rules-based society led him to denounce, as a manifestation of the people's 'mental enslavement', the foundations of law, property rights and even the institution of marriage. He considered the basic foundations of society as constraining the natural development of individuals to use their powers of reasoning to arrive at a mutually beneficial method of social organisation. In each case, government and its institutions are shown to constrain the development of our capacity to live wholly in accordance with the full and free exercise of private judgement.
The French Pierre-Joseph Proudhon is regarded as the first self-proclaimed anarchist, a label he adopted in his groundbreaking work, What is Property? , published in 1840. It is for this reason that some claim Proudhon as the founder of modern anarchist theory. [65] He developed the theory of spontaneous order in society, where organisation emerges without a central coordinator imposing its own idea of order against the wills of individuals acting in their own interests; his famous quote on the matter is, "Liberty is the mother, not the daughter, of order." In What is Property? Proudhon answers with the famous accusation " Property is theft ." In this work, he opposed the institution of decreed "property" ( propriété ), where owners have complete rights to "use and abuse" their property as they wish. [66] He contrasted this with what he called "possession," or limited ownership of resources and goods only while in more or less continuous use. Later, however, Proudhon added that "Property is Liberty," and argued that it was a bulwark against state power. [67] His opposition to the state, organised religion, and certain capitalist practices inspired subsequent anarchists, and made him one of the leading social thinkers of his time.
The anarcho-communist Joseph Déjacque was the first person to describe himself as "libertarian". [68] Unlike Pierre-Joseph Proudhon, he argued that, "it is not the product of his or her labour that the worker has a right to, but to the satisfaction of his or her needs, whatever may be their nature." [69] In 1844 in Germany the post-hegelian philosopher Max Stirner published the book, The Ego and Its Own , which would later be considered an influential early text of individualist anarchism. [70] French anarchists active in the 1848 Revolution included Anselme Bellegarrigue , Ernest Coeurderoy, Joseph Déjacque [68] and Pierre Joseph Proudhon . [71] [72]

First International and the Paris Commune
In Europe, harsh reaction followed the revolutions of 1848 , during which ten countries had experienced brief or long-term social upheaval as groups carried out nationalist uprisings. After most of these attempts at systematic change ended in failure, conservative elements took advantage of the divided groups of socialists, anarchists, liberals, and nationalists, to prevent further revolt. [73] In Spain Ramón de la Sagra established the anarchist journal El Porvenir in La Coruña in 1845 which was inspired by Proudhon´s ideas. [74] The Catalan politician Francesc Pi i Margall became the principal translator of Proudhon's works into Spanish [75] and later briefly became president of Spain in 1873 while being the leader of the Democratic Republican Federal Party. According to George Woodcock "These translations were to have a profound and lasting effect on the development of Spanish anarchism after 1870, but before that time Proudhonian ideas, as interpreted by Pi, already provided much of the inspiration for the federalist movement which sprang up in the early 1860's." [76] According to the Encyclopædia Britannica "During the Spanish revolution of 1873, Pi y Margall attempted to establish a decentralised, or "cantonalist," political system on Proudhonian lines." [74]
In 1864 the International Workingmen's Association (sometimes called the "First International") united diverse revolutionary currents including French followers of Proudhon, [77] Blanquists , Philadelphes , English trade unionists, socialists and social democrats . Due to its links to active workers' movements, the International became a significant organisation. Karl Marx became a leading figure in the International and a member of its General Council. Proudhon's followers, the mutualists, opposed Marx's state socialism , advocating political abstentionism and small property holdings. [78] [79] Woodcock also reports that the American individualist anarchists Lysander Spooner and William B. Greene had been members of the First International . [80] In 1868, following their unsuccessful participation in the League of Peace and Freedom (LPF), Russian revolutionary Mikhail Bakunin and his collectivist anarchist associates joined the First International (which had decided not to get involved with the LPF). [81] They allied themselves with the federalist socialist sections of the International, [82] who advocated the revolutionary overthrow of the state and the collectivisation of property.
At first, the collectivists worked with the Marxists to push the First International in a more revolutionary socialist direction. Subsequently, the International became polarised into two camps, with Marx and Bakunin as their respective figureheads. [83] Mikhail Bakunin characterised Marx's ideas as centralist and predicted that, if a Marxist party came to power, its leaders would simply take the place of the ruling class they had fought against. [84] [85] Anarchist historian George Woodcock reports that "The annual Congress of the International had not taken place in 1870 owing to the outbreak of the Paris Commune, and in 1871 the General Council called only a special conference in London. One delegate was able to attend from Spain and none from Italy, while a technical excuse – that they had split away from the Fédération Romande – was used to avoid inviting Bakunin's Swiss supporters. Thus only a tiny minority of anarchists was present, and the General Council's resolutions passed almost unanimously. Most of them were clearly directed against Bakunin and his followers." [86] In 1872, the conflict climaxed with a final split between the two groups at the Hague Congress , where Bakunin and James Guillaume were expelled from the International and its headquarters were transferred to New York. In response, the federalist sections formed their own International at the St. Imier Congress , adopting a revolutionary anarchist programme. [87]
The Paris Commune was a government that briefly ruled Paris from 18 March (more formally, from 28 March) to 28 May 1871. The Commune was the result of an uprising in Paris after France was defeated in the Franco-Prussian War. Anarchists participated actively in the establishment of the Paris Commune. They included Louise Michel , the Reclus brothers, and Eugene Varlin (the latter murdered in the repression afterwards). As for the reforms initiated by the Commune, such as the re-opening of workplaces as co-operatives, anarchists can see their ideas of associated labour beginning to be realised ... Moreover, the Commune's ideas on federation obviously reflected the influence of Proudhon on French radical ideas. Indeed, the Commune's vision of a communal France based on a federation of delegates bound by imperative mandates issued by their electors and subject to recall at any moment echoes Bakunin's and Proudhon's ideas (Proudhon, like Bakunin, had argued in favour of the "implementation of the binding mandate" in 1848 ... and for federation of communes). Thus both economically and politically the Paris Commune was heavily influenced by anarchist ideas. [88] George Woodcock states:

Organised labour
The anti-authoritarian sections of the First International were the precursors of the anarcho-syndicalists, seeking to "replace the privilege and authority of the State" with the "free and spontaneous organization of labour." [89] In 1886, the Federation of Organized Trades and Labor Unions (FOTLU) of the United States and Canada unanimously set 1 May 1886, as the date by which the eight-hour work day would become standard. [90]
In response, unions across the United States prepared a general strike in support of the event. [90] On 3 May, in Chicago, a fight broke out when strikebreakers attempted to cross the picket line, and two workers died when police opened fire upon the crowd. [91] The next day, 4 May, anarchists staged a rally at Chicago's Haymarket Square. [92] A bomb was thrown by an unknown party near the conclusion of the rally, killing an officer. [93] In the ensuing panic, police opened fire on the crowd and each other. [94] Seven police officers and at least four workers were killed. [95] Eight anarchists directly and indirectly related to the organisers of the rally were arrested and charged with the murder of the deceased officer. The men became international political celebrities among the labour movement. Four of the men were executed and a fifth committed suicide prior to his own execution. The incident became known as the Haymarket affair, and was a setback for the labour movement and the struggle for the eight-hour day. In 1890 a second attempt, this time international in scope, to organise for the eight-hour day was made. The event also had the secondary purpose of memorialising workers killed as a result of the Haymarket affair. [96] Although it had initially been conceived as a once-off event, by the following year the celebration of International Workers' Day on May Day had become firmly established as an international worker's holiday. [90]
In 1907, the International Anarchist Congress of Amsterdam gathered delegates from 14 different countries, among which important figures of the anarchist movement, including Errico Malatesta , Pierre Monatte , Luigi Fabbri , Benoît Broutchoux , Emma Goldman , Rudolf Rocker , and Christiaan Cornelissen . Various themes were treated during the Congress, in particular concerning the organisation of the anarchist movement, popular education issues, the general strike or antimilitarism . A central debate concerned the relation between anarchism and syndicalism (or trade unionism ). Malatesta and Monatte were in particular disagreement themselves on this issue, as the latter thought that syndicalism was revolutionary and would create the conditions of a social revolution , while Malatesta did not consider syndicalism by itself sufficient. [97] He thought that the trade-union movement was reformist and even conservative , citing as essentially bourgeois and anti-worker the phenomenon of professional union officials. Malatesta warned that the syndicalists aims were in perpetuating syndicalism itself, whereas anarchists must always have anarchy as their end and consequently refrain from committing to any particular method of achieving it. [98]
The Spanish Workers Federation in 1881 was the first major anarcho-syndicalist movement; anarchist trade union federations were of special importance in Spain. The most successful was the Confederación Nacional del Trabajo (National Confederation of Labour: CNT), founded in 1910. Before the 1940s, the CNT was the major force in Spanish working class politics, attracting 1.58 million members at one point and playing a major role in the Spanish Civil War . [99] The CNT was affiliated with the International Workers Association , a federation of anarcho-syndicalist trade unions founded in 1922, with delegates representing two million workers from 15 countries in Europe and Latin America. In Latin America in particular "The anarchists quickly became active in organising craft and industrial workers throughout South and Central America, and until the early 1920s most of the trade unions in Mexico , Brazil , Peru, Chile, and Argentina were anarcho-syndicalist in general outlook; the prestige of the Spanish C.N.T. as a revolutionary organisation was undoubtedly to a great extent responsible for this situation. The largest and most militant of these organisations was the Federación Obrera Regional Argentina ... it grew quickly to a membership of nearly a quarter of a million, which dwarfed the rival socialdemocratic unions." [86]

Propaganda of the deed and illegalism
Some anarchists, such as Johann Most , advocated publicising violent acts of retaliation against counter-revolutionaries because "we preach not only action in and for itself, but also action as propaganda." [100] Scholars such as Beverly Gage contend that this was not advocacy of mass murder, but targeted killings of members of the ruling class at times when such actions might garner sympathy from the population, such as during periods of heightened government repression or labor conflicts where workers were killed. [101] However, Most himself once boasted that "the existing system will be quickest and most radically overthrown by the annihilation of its exponents. Therefore, massacres of the enemies of the people must be set in motion." [102] Most is best known for a pamphlet published in 1885: The Science of Revolutionary Warfare , a how-to manual on the subject of making explosives, based on knowledge he acquired while working at an explosives plant in New Jersey. [103]
By the 1880s, people inside and outside the anarchist movement began to use the slogan, "propaganda of the deed" to refer to individual bombings, regicides , and tyrannicides . From 1905 onwards, the Russian counterparts of these anti-syndicalist anarchist-communists become partisans of economic terrorism and illegal ' expropriations '." [104] Illegalism as a practice emerged and within it "The acts of the anarchist bombers and assassins (" propaganda by the deed ") and the anarchist burglars (" individual reappropriation ") expressed their desperation and their personal, violent rejection of an intolerable society. Moreover, they were clearly meant to be exemplary invitations to revolt.". [105] France's Bonnot Gang was the most famous group to embrace illegalism.
However, as soon as 1887, important figures in the anarchist movement distanced themselves from such individual acts. Peter Kropotkin thus wrote that year in Le Révolté that "a structure based on centuries of history cannot be destroyed with a few kilos of dynamite". [106] A variety of anarchists advocated the abandonment of these sorts of tactics in favour of collective revolutionary action, for example through the trade union movement. The anarcho-syndicalist, Fernand Pelloutier , argued in 1895 for renewed anarchist involvement in the labour movement on the basis that anarchism could do very well without "the individual dynamiter." [107]
State repression (including the infamous 1894 French lois scélérates ) of the anarchist and labour movements following the few successful bombings and assassinations may have contributed to the abandonment of these kinds of tactics, although reciprocally state repression, in the first place, may have played a role in these isolated acts. The dismemberment of the French socialist movement , into many groups and, following the suppression of the 1871 Paris Commune, the execution and exile of many communards to penal colonies , favoured individualist political expression and acts. [108]
Numerous heads of state were assassinated between 1881 and 1914 by members of the anarchist movement, including Tsar Alexander II of Russia , President Sadi Carnot of France, Empress Elisabeth of Austria , King Umberto I of Italy , President William McKinley of the United States, King Carlos I of Portugal and King George I of Greece . McKinley's assassin Leon Czolgosz claimed to have been influenced by anarchist and feminist Emma Goldman. [109]
Propaganda of the deed was abandoned by the vast majority of the anarchist movement after World War I (1914–1918) and the 1917 October Revolution . [ citation needed ]

Russian Revolution and other uprisings of the 1910s
Anarchists participated alongside the Bolsheviks in both February and October revolutions , and were initially enthusiastic about the Bolshevik revolution. [110] However, following a political falling out with the Bolsheviks by the anarchists and other left-wing opposition, the conflict culminated in the 1921 Kronstadt rebellion , which the new government repressed. Anarchists in central Russia were either imprisoned, driven underground or joined the victorious Bolsheviks; the anarchists from Petrograd and Moscow fled to Ukraine . [111] There, in the Free Territory , they fought in the civil war against the Whites (a grouping of monarchists and other opponents of the October Revolution) and then the Bolsheviks as part of the Revolutionary Insurrectionary Army of Ukraine led by Nestor Makhno, who established an anarchist society in the region for a number of months.
Expelled American anarchists Emma Goldman and Alexander Berkman were among those agitating in response to Bolshevik policy and the suppression of the Kronstadt uprising, before they left Russia. Both wrote accounts of their experiences in Russia, criticising the amount of control the Bolsheviks exercised. For them, Bakunin's predictions about the consequences of Marxist rule that the rulers of the new "socialist" Marxist state would become a new elite had proved all too true. [84] [112]
The victory of the Bolsheviks in the October Revolution and the resulting Russian Civil War did serious damage to anarchist movements internationally. Many workers and activists saw Bolshevik success as setting an example; Communist parties grew at the expense of anarchism and other socialist movements. In France and the United States, for example, members of the major syndicalist movements of the CGT and IWW left the organisations and joined the Communist International . [113]
The revolutionary wave of 1917–23 saw the active participation of anarchists in varying degrees of protagonism. In the German uprising known as the German Revolution of 1918–1919 which established the Bavarian Soviet Republic the anarchists Gustav Landauer , Silvio Gesell and Erich Mühsam had important leadership positions within the revolutionary councilist structures. [114] [115] In the Italian events known as the biennio rosso [116] the anarcho-syndicalist trade union Unione Sindacale Italiana "grew to 800,000 members and the influence of the Italian Anarchist Union (20,000 members plus Umanita Nova , its daily paper) grew accordingly ... Anarchists were the first to suggest occupying workplaces. [117] In the Mexican Revolution the Mexican Liberal Party was established and during the early 1910s it led a series of military offensives leading to the conquest and occupation of certain towns and districts in Baja California with the leadership of anarcho-communist Ricardo Flores Magón . [118]
In Paris, the Dielo Truda group of Russian anarchist exiles, which included Nestor Makhno, concluded that anarchists needed to develop new forms of organisation in response to the structures of Bolshevism. Their 1926 manifesto, called the Organisational Platform of the General Union of Anarchists (Draft) , [119] was supported. Platformist groups active today include the Workers Solidarity Movement in Ireland and the North Eastern Federation of Anarchist Communists of North America. Synthesis anarchism emerged as an organisational alternative to platformism that tries to join anarchists of different tendencies under the principles of anarchism without adjectives . [120] In the 1920s this form found as its main proponents Volin and Sebastien Faure . [120] It is the main principle behind the anarchist federations grouped around the contemporary global International of Anarchist Federations . [120]

Conflicts with European fascist regimes
In the 1920s and 1930s, the rise of fascism in Europe transformed anarchism's conflict with the state. Italy saw the first struggles between anarchists and fascists. Italian anarchists played a key role in the anti-fascist organisation Arditi del Popolo , which was strongest in areas with anarchist traditions, and achieved some success in their activism, such as repelling Blackshirts in the anarchist stronghold of Parma in August 1922. [121] The veteran Italian anarchist, Luigi Fabbri, was one of the first critical theorists of fascism, describing it as "the preventive counter-revolution." [39] In France, where the far right leagues came close to insurrection in the February 1934 riots , anarchists divided over a united front policy. [122]
Anarchists in France [123] and Italy [124] were active in the Resistance during World War II . In Germany the anarchist Erich Mühsam was arrested on charges unknown in the early morning hours of 28 February 1933, within a few hours after the Reichstag fire in Berlin. Joseph Goebbels , the Nazi propaganda minister , labelled him as one of "those Jewish subversives." Over the next seventeen months, he would be imprisoned in the concentration camps at Sonnenburg , Brandenburg and finally, Oranienburg . On 2 February 1934, Mühsam was transferred to the concentration camp at Oranienburg when finally on the night of 9 July 1934, Mühsam was tortured and murdered by the guards, his battered corpse found hanging in a latrine the next morning. [125]

Spanish Revolution
In Spain, the national anarcho-syndicalist trade union Confederación Nacional del Trabajo initially refused to join a popular front electoral alliance, and abstention by CNT supporters led to a right wing election victory. But in 1936, the CNT changed its policy and anarchist votes helped bring the popular front back to power. Months later, conservative members of the military, with the support of minority extreme-right parties, responded with an attempted coup, causing the Spanish Civil War (1936–1939). [126] In response to the army rebellion, an anarchist-inspired movement of peasants and workers, supported by armed militias, took control of Barcelona and of large areas of rural Spain where they collectivised the land. [127] [128] But even before the fascist victory in 1939, the anarchists were losing ground in a bitter struggle with the Stalinists , who controlled much of the distribution of military aid to the Republican cause from the Soviet Union . According to Noam Chomsky , "the communists were mainly responsible for the destruction of the Spanish anarchists. Not just in Catalonia—the communist armies mainly destroyed the collectives elsewhere. The communists basically acted as the police force of the security system of the Republic and were very much opposed to the anarchists, partially because Stalin still hoped at that time to have some kind of pact with Western countries against Hitler. That, of course, failed and Stalin withdrew the support to the Republic. They even withdrew the Spanish gold reserves." [129] The events known as the Spanish Revolution was a workers' social revolution that began during the outbreak of the Spanish Civil War in 1936 and resulted in the widespread implementation of anarchist and more broadly libertarian socialist organisational principles throughout various portions of the country for two to three years, primarily Catalonia , Aragon , Andalusia , and parts of the Levante . Much of Spain's economy was put under worker control; in anarchist strongholds like Catalonia, the figure was as high as 75%, but lower in areas with heavy Communist Party of Spain influence, as the Soviet-allied party actively resisted attempts at collectivisation enactment. Factories were run through worker committees, agrarian areas became collectivised and run as libertarian communes . Anarchist historian Sam Dolgoff estimated that about eight million people participated directly or at least indirectly in the Spanish Revolution, [130] which he claimed "came closer to realising the ideal of the free stateless society on a vast scale than any other revolution in history." [131] Spanish Communist Party -led troops suppressed the collectives and persecuted both dissident Marxists and anarchists. [132] The prominent Italian anarchist Camillo Berneri , who volunteered to fight against Franco was killed instead in Spain by gunmen associated with the Spanish Communist Party. [133] [134] [135] The city of Madrid was turned over to the francoist forces by the last non-francoist mayor of the city, the anarchist Melchor Rodríguez García . [136]

Post-war years
Anarchism sought to reorganise itself after the war and in this context the organisational debate between synthesis anarchism and platformism took importance once again especially in the anarchist movements of Italy and France . The Mexican Anarchist Federation was established in 1945 after the Anarchist Federation of the Centre united with the Anarchist Federation of the Federal District. [137] In the early 1940s, the Antifascist International Solidarity and the Federation of Anarchist Groups of Cuba merged into the large national organisation Asociación Libertaria de Cuba (Cuban Libertarian Association). [138] From 1944 to 1947, the Bulgarian Anarchist Communist Federation reemerged as part of a factory and workplace committee movement, but was repressed by the new Communist regime. [139] In 1945 in France the Fédération Anarchiste and the anarchosyndicalist trade union Confédération nationale du travail was established in the next year while the also synthesist Federazione Anarchica Italiana was founded in Italy. Korean anarchists formed the League of Free Social Constructors in September 1945 [139] and in 1946 the Japanese Anarchist Federation was founded. [140] An International Anarchist Congress with delegates from across Europe was held in Paris in May 1948. [139] After World War II, an appeal in the Fraye Arbeter Shtime detailing the plight of German anarchists and called for Americans to support them. By February 1946, the sending of aid parcels to anarchists in Germany was a large-scale operation. The Federation of Libertarian Socialists was founded in Germany in 1947 and Rudolf Rocker wrote for its organ, Die Freie Gesellschaft , which survived until 1953. [141] In 1956 the Uruguayan Anarchist Federation was founded. [142] In 1955 the Anarcho-Communist Federation of Argentina renamed itself as the Argentine Libertarian Federation . The Syndicalist Workers' Federation was a syndicalist group in active in post-war Britain, [143] and one of Solidarity Federation 's earliest predecessors. It was formed in 1950 by members of the dissolved Anarchist Federation of Britain. [143] Unlike the AFB, which was influenced by anarcho-syndicalist ideas but ultimately not syndicalist itself, the SWF decided to pursue a more definitely syndicalist, worker-centred strategy from the outset. [143]
Anarchism continued to influence important literary and intellectual personalities of the time, such as Albert Camus , Herbert Read , Paul Goodman, Dwight Macdonald, Allen Ginsberg , George Woodcock, Leopold Kohr , [144] [145] Julian Beck , John Cage [146] and the French Surrealist group led by André Breton , which now openly embraced anarchism and collaborated in the Fédération Anarchiste. [147]
Anarcho-pacifism became influential in the Anti-nuclear movement and anti war movements of the time [148] [149] as can be seen in the activism and writings of the English anarchist member of Campaign for Nuclear Disarmament Alex Comfort or the similar activism of the American catholic anarcho-pacifists Ammon Hennacy and Dorothy Day . Anarcho-pacifism became a "basis for a critique of militarism on both sides of the Cold War ." [150] The resurgence of anarchist ideas during this period is well documented in Robert Graham's Anarchism: A Documentary History of Libertarian Ideas , Volume Two: The Emergence of the New Anarchism (1939–1977) . [139]

Contemporary anarchism
A surge of popular interest in anarchism occurred in western nations during the 1960s and 1970s. [151] Anarchism was influential in the Counterculture of the 1960s [152] [153] [154] and anarchists actively participated in the late sixties students and workers revolts . [155] In 1968 in Carrara , Italy the International of Anarchist Federations was founded during an international anarchist conference held there in 1968 by the three existing European federations of France (the Fédération Anarchiste ), the Federazione Anarchica Italiana of Italy and the Iberian Anarchist Federation as well as the Bulgarian federation in French exile. [156] [157]
In the United Kingdom in the 1970s this was associated with the punk rock movement, as exemplified by bands such as Crass and the Sex Pistols . [158] The housing and employment crisis in most of Western Europe led to the formation of communes and squatter movements like that of Barcelona, Spain. In Denmark, squatters occupied a disused military base and declared the Freetown Christiania , an autonomous haven in central Copenhagen. Since the revival of anarchism in the mid-20th century, [159] a number of new movements and schools of thought emerged. Although feminist tendencies have always been a part of the anarchist movement in the form of anarcha-feminism , they returned with vigour during the second wave of feminism in the 1960s. Anarchist anthropologist David Graeber and anarchist historian Andrej Grubacic have posited a rupture between generations of anarchism, with those "who often still have not shaken the sectarian habits" of the 19th century contrasted with the younger activists who are "much more informed, among other elements, by indigenous , feminist , ecological and cultural-critical ideas", and who by the turn of the 21st century formed "by far the majority" of anarchists. [160]
Around the turn of the 21st century, anarchism grew in popularity and influence as part of the anti-war, anti-capitalist, and anti-globalisation movements . [161] Anarchists became known for their involvement in protests against the meetings of the World Trade Organization (WTO), Group of Eight , and the World Economic Forum . Some anarchist factions at these protests engaged in rioting, property destruction, and violent confrontations with police. These actions were precipitated by ad hoc, leaderless, anonymous cadres known as black blocs ; other organisational tactics pioneered in this time include security culture , affinity groups and the use of decentralised technologies such as the internet. [161] A significant event of this period was the confrontations at WTO conference in Seattle in 1999 . [161] According to anarchist scholar Simon Critchley , "contemporary anarchism can be seen as a powerful critique of the pseudo-libertarianism of contemporary neo-liberalism ... One might say that contemporary anarchism is about responsibility, whether sexual, ecological or socio-economic; it flows from an experience of conscience about the manifold ways in which the West ravages the rest; it is an ethical outrage at the yawning inequality, impoverishment and disenfranchisment that is so palpable locally and globally." [162]
International anarchist federations in existence include the International of Anarchist Federations , the International Workers' Association , and International Libertarian Solidarity . The largest organised anarchist movement today is in Spain, in the form of the Confederación General del Trabajo (CGT) and the CNT. CGT membership was estimated at around 100,000 for 2003. [163] Other active syndicalist movements include in Sweden the Central Organisation of the Workers of Sweden and the Swedish Anarcho-syndicalist Youth Federation ; the CNT-AIT in France; the Unione Sindicale Italiana in Italy; in the US Workers Solidarity Alliance and the UK Solidarity Federation and Anarchist Federation . The revolutionary industrial unionist Industrial Workers of the World, claiming 3,000 paying members, and the International Workers Association, an anarcho-syndicalist successor to the First International, also remain active. [ citation needed ]

Anarchist schools of thought
Anarchist schools of thought had been generally grouped in two main historical traditions, individualist anarchism and social anarchism , which have some different origins, values and evolution. [10] [17] [164] [165] The individualist wing of anarchism emphasises negative liberty , i.e. opposition to state or social control over the individual, while those in the social wing emphasise positive liberty to achieve one's potential and argue that humans have needs that society ought to fulfil, "recognising equality of entitlement". [166] In a chronological and theoretical sense, there are classical – those created throughout the 19th century – and post-classical anarchist schools – those created since the mid-20th century and after.
Beyond the specific factions of anarchist thought is philosophical anarchism , which embodies the theoretical stance that the state lacks moral legitimacy without accepting the imperative of revolution to eliminate it. A component especially of individualist anarchism [167] [168] philosophical anarchism may accept the existence of a minimal state as unfortunate, and usually temporary, "necessary evil" but argue that citizens do not have a moral obligation to obey the state when its laws conflict with individual autonomy. [169] One reaction against sectarianism within the anarchist milieu was "anarchism without adjectives", a call for toleration first adopted by Fernando Tarrida del Mármol in 1889 in response to the "bitter debates" of anarchist theory at the time. [170] In abandoning the hyphenated anarchisms (i.e. collectivist-, communist-, mutualist– and individualist-anarchism), it sought to emphasise the anti-authoritarian beliefs common to all anarchist schools of thought. [171]

Classical anarchist schools of thought

Mutualism
Mutualism began in 18th-century English and French labour movements before taking an anarchist form associated with Pierre-Joseph Proudhon in France and others in the United States. [172] Proudhon proposed spontaneous order, whereby organisation emerges without central authority, a "positive anarchy" where order arises when everybody does "what he wishes and only what he wishes" [173] and where "business transactions alone produce the social order." [174] Proudhon distinguished between ideal political possibilities and practical governance. For this reason, much in contrast to some of his theoretical statements concerning ultimate spontaneous self-governance, Proudhon was heavily involved in French parliamentary politics and allied himself not with anarchist but socialist factions of workers' movements and, in addition to advocating state-protected charters for worker-owned cooperatives, promoted certain nationalisation schemes during his life of public service.
Mutualist anarchism is concerned with reciprocity , free association, voluntary contract, federation, and credit and currency reform. According to the American mutualist William Batchelder Greene , each worker in the mutualist system would receive "just and exact pay for his work; services equivalent in cost being exchangeable for services equivalent in cost, without profit or discount." [175] Mutualism has been retrospectively characterised as ideologically situated between individualist and collectivist forms of anarchism. [176] Proudhon first characterised his goal as a "third form of society, the synthesis of communism and property." [177]

Individualist anarchism
Individualist anarchism refers to several traditions of thought within the anarchist movement that emphasise the individual and their will over any kinds of external determinants such as groups, society, traditions, and ideological systems. [178] [179] Individualist anarchism is not a single philosophy but refers to a group of individualistic philosophies that sometimes are in conflict.
In 1793, William Godwin, who has often [60] been cited as the first anarchist, wrote Political Justice , which some consider the first expression of anarchism. [61] [63] Godwin, a philosophical anarchist, from a rationalist and utilitarian basis opposed revolutionary action and saw a minimal state as a present "necessary evil" that would become increasingly irrelevant and powerless by the gradual spread of knowledge. [61] [180] Godwin advocated individualism, proposing that all cooperation in labour be eliminated on the premise that this would be most conducive with the general good. [181] [182]
An influential form of individualist anarchism, called "egoism," [183] or egoist anarchism , was expounded by one of the earliest and best-known proponents of individualist anarchism, the German Max Stirner. [70] Stirner's The Ego and Its Own , published in 1844, is a founding text of the philosophy. [70] According to Stirner, the only limitation on the rights of individuals is their power to obtain what they desire, [184] without regard for God, state, or morality. [185] To Stirner, rights were spooks in the mind, and he held that society does not exist but "the individuals are its reality". [186] Stirner advocated self-assertion and foresaw unions of egoists , non-systematic associations continually renewed by all parties' support through an act of will, [187] which Stirner proposed as a form of organisation in place of the state. [188] Egoist anarchists argue that egoism will foster genuine and spontaneous union between individuals. [189] "Egoism" has inspired many interpretations of Stirner's philosophy. It was re-discovered and promoted by German philosophical anarchist and homosexual activist John Henry Mackay .
Josiah Warren is widely regarded as the first American anarchist, [190] and the four-page weekly paper he edited during 1833, The Peaceful Revolutionist , was the first anarchist periodical published. [191] For American anarchist historian Eunice Minette Schuster "It is apparent ... that Proudhonian Anarchism was to be found in the United States at least as early as 1848 and that it was not conscious of its affinity to the Individualist Anarchism of Josiah Warren and Stephen Pearl Andrews ... William B. Greene presented this Proudhonian Mutualism in its purest and most systematic form.". [192] Henry David Thoreau (1817–1862) was an important early influence in individualist anarchist thought in the United States and Europe. Thoreau was an American author, poet, naturalist, tax resister, development critic , surveyor, historian, philosopher, and leading transcendentalist . He is best known for his books Walden , a reflection upon simple living in natural surroundings, and his essay, Civil Disobedience , an argument for individual resistance to civil government in moral opposition to an unjust state. Later Benjamin Tucker fused Stirner's egoism with the economics of Warren and Proudhon in his eclectic influential publication Liberty .
From these early influences individualist anarchism in different countries attracted a small but diverse following of bohemian artists and intellectuals, [193] free love and birth control advocates (see Anarchism and issues related to love and sex ), [194] [195] individualist naturists nudists (see anarcho-naturism ), [195] [196] [197] freethought and anti-clerical activists [198] [199] as well as young anarchist outlaws in what became known as illegalism and individual reclamation [105] [200] (see European individualist anarchism and individualist anarchism in France ). These authors and activists included Oscar Wilde , Emile Armand , Han Ryner , Henri Zisly , Renzo Novatore , Miguel Gimenez Igualada , Adolf Brand and Lev Chernyi among others.

Social anarchism
Social anarchism calls for a system with common ownership of means of production and democratic control of all organisations, without any government authority or coercion . It is the largest school of thought in anarchism. [201] Social anarchism rejects private property, seeing it as a source of social inequality (while retaining respect for personal property ), [202] and emphasises cooperation and mutual aid . [203]

Collectivist anarchism
Collectivist anarchism, also referred to as "revolutionary socialism" or a form of such, [204] [205] is a revolutionary form of anarchism, commonly associated with Mikhail Bakunin and Johann Most. [206] [207] Collectivist anarchists oppose all private ownership of the means of production, instead advocating that ownership be collectivised. This was to be achieved through violent revolution, first starting with a small cohesive group through acts of violence, or propaganda by the deed , which would inspire the workers as a whole to revolt and forcibly collectivise the means of production. [206]
However, collectivisation was not to be extended to the distribution of income, as workers would be paid according to time worked, rather than receiving goods being distributed "according to need" as in anarcho-communism. This position was criticised by anarchist communists as effectively "uphold[ing] the wages system". [208] Collectivist anarchism arose contemporaneously with Marxism but opposed the Marxist dictatorship of the proletariat, despite the stated Marxist goal of a collectivist stateless society. [209] Anarchist, communist and collectivist ideas are not mutually exclusive ; although the collectivist anarchists advocated compensation for labour, some held out the possibility of a post-revolutionary transition to a communist system of distribution according to need. [210]

Anarcho-communism
Anarchist communism (also known as anarcho-communism, libertarian communism [211] [212] [213] [214] and occasionally as free communism) is a theory of anarchism that advocates abolition of the state, markets , money, private property (while retaining respect for personal property), [202] and capitalism in favour of common ownership of the means of production , [215] [216] direct democracy and a horizontal network of voluntary associations and workers' councils with production and consumption based on the guiding principle: " from each according to his ability, to each according to his need ". [217] [218]
Some forms of anarchist communism such as insurrectionary anarchism are strongly influenced by egoism and radical individualism, believing anarcho-communism is the best social system for the realisation of individual freedom. [219] [220] [221] [222] Most anarcho-communists view anarcho-communism as a way of reconciling the opposition between the individual and society. [223] [224] [225]
Anarcho-communism developed out of radical socialist currents after the French revolution [226] [227] but was first formulated as such in the Italian section of the First International. [228] The theoretical work of Peter Kropotkin took importance later as it expanded and developed pro-organisationalist and insurrectionary anti-organisationalist sections. [229] To date, the best known examples of an anarchist communist society (i.e., established around the ideas as they exist today and achieving worldwide attention and knowledge in the historical canon), are the anarchist territories during the Spanish Revolution [230] and the Free Territory during the Russian Revolution . Through the efforts and influence of the Spanish Anarchists during the Spanish Revolution within the Spanish Civil War, starting in 1936 anarchist communism existed in most of Aragon, parts of the Levante and Andalusia, as well as in the stronghold of Anarchist Catalonia before being crushed by the combined forces of the regime that won the war , Hitler , Mussolini, Spanish Communist Party repression (backed by the USSR) as well as economic and armaments blockades from the capitalist countries and the Spanish Republic itself. [231] During the Russian Revolution, anarchists such as Nestor Makhno worked to create and defend – through the Revolutionary Insurrectionary Army of Ukraine – anarchist communism in the Free Territory of the Ukraine from 1919 before being conquered by the Bolsheviks in 1921.

Anarcho-syndicalism
Anarcho-syndicalism is a branch of anarchism that focuses on the labour movement. [232] Anarcho-syndicalists view labour unions as a potential force for revolutionary social change, replacing capitalism and the state with a new society democratically self-managed by workers. The basic principles of anarcho-syndicalism are: Workers' solidarity , Direct action and Workers' self-management
Anarcho-syndicalists believe that only direct action – that is, action concentrated on directly attaining a goal, as opposed to indirect action, such as electing a representative to a government position – will allow workers to liberate themselves. [233] Moreover, anarcho-syndicalists believe that workers' organisations (the organisations that struggle against the wage system, which, in anarcho-syndicalist theory, will eventually form the basis of a new society) should be self-managing. They should not have bosses or "business agents"; rather, the workers should be able to make all the decisions that affect them themselves. Rudolf Rocker was one of the most popular voices in the anarcho-syndicalist movement. He outlined a view of the origins of the movement, what it sought, and why it was important to the future of labour in his 1938 pamphlet Anarcho-Syndicalism . The International Workers Association is an international anarcho-syndicalist federation of various labour unions from different countries. The Spanish Confederación Nacional del Trabajo played and still plays a major role in the Spanish labour movement. It was also an important force in the Spanish Civil War.

Syncretic anarchism
The term syncretic anarchism was first coined by Alberto Frigo in relation to his reading of Jacques Ellul. Rephrasing the latter, Frigo observed that, if on one hand new technologies creates new form of power, on the other, new technologies are accompanied by the rise of what Marcel Mauss defines as magic. By developing the techniques to perform new magic and by adhering to it, marginal individuals come to create forms of syncretism which brings together the different dogmas and cultures a power structures uses to put humans against one another. The 19th century French postman Ferdinand Cheval for example, has intuitively experimented with the, at that time, new medium of cement, and created, after 33 years of adherence to certain rituals, a monument blending religions from around the world.

Post-classical schools of thought
Anarchism continues to generate many philosophies and movements, at times eclectic, drawing upon various sources, and syncretic , combining disparate concepts to create new philosophical approaches. [234]
Green anarchism (or eco-anarchism) [235] is a school of thought within anarchism that emphasises environmental issues, [236] with an important precedent in anarcho-naturism, [195] [237] [238] and whose main contemporary currents are anarcho-primitivism and social ecology .
Anarcha-feminism (also called anarchist feminism and anarcho-feminism) combines anarchism with feminism. It generally views patriarchy as a manifestation of involuntary coercive hierarchy that should be replaced by decentralised free association. Anarcha-feminists believe that the struggle against patriarchy is an essential part of class struggle , and the anarchist struggle against the state. In essence, the philosophy sees anarchist struggle as a necessary component of feminist struggle and vice versa. L. Susan Brown claims that "as anarchism is a political philosophy that opposes all relationships of power, it is inherently feminist". [239] Anarcha-feminism began with the late 19th-century writings of early feminist anarchists such as Emma Goldman and Voltairine de Cleyre .
Anarcho-pacifism is a tendency that rejects violence in the struggle for social change (see non-violence ). [86] [240] It developed "mostly in the Netherlands , Britain, and the United States, before and during the Second World War". [86] Christian anarchism is a movement in political theology that combines anarchism and Christianity. [241] Its main proponents included Leo Tolstoy , Dorothy Day, Ammon Hennacy, and Jacques Ellul .
Platformism is a tendency within the wider anarchist movement based on the organisational theories in the tradition of Dielo Truda's Organisational Platform of the General Union of Anarchists (Draft) . [119] The document was based on the experiences of Russian anarchists in the 1917 October Revolution, which led eventually to the victory of the Bolsheviks over the anarchists and other groups. The Platform attempted to address and explain the anarchist movement's failures during the Russian Revolution.
Synthesis anarchism is a form of anarchism that tries to join anarchists of different tendencies under the principles of anarchism without adjectives. [242] In the 1920s, this form found as its main proponents the anarcho-communists Voline and Sébastien Faure . [120] [243] It is the main principle behind the anarchist federations grouped around the contemporary global International of Anarchist Federations . [242]
Post-left anarchy is a recent current in anarchist thought that promotes a critique of anarchism's relationship to traditional Left-wing politics . Some post-leftists seek to escape the confines of ideology in general also presenting a critique of organisations and morality . [244] Influenced by the work of Max Stirner [244] and by the Marxist Situationist International , [244] post-left anarchy is marked by a focus on social insurrection and a rejection of leftist social organisation. [245]
Insurrectionary anarchism is a revolutionary theory, practice, and tendency within the anarchist movement which emphasises insurrection within anarchist practice. [246] [247] It is critical of formal organisations such as labour unions and federations that are based on a political programme and periodic congresses. [246] Instead, insurrectionary anarchists advocate informal organisation and small affinity group based organisation. [246] [247] Insurrectionary anarchists put value in attack, permanent class conflict , and a refusal to negotiate or compromise with class enemies. [246] [247]
Post-anarchism is a theoretical move towards a synthesis of classical anarchist theory and poststructuralist thought, drawing from diverse ideas including post-modernism , autonomist marxism , post-left anarchy, Situationist International, and postcolonialism .
Left-wing market anarchism strongly affirm the classical liberal ideas of self-ownership and free markets, while maintaining that, taken to their logical conclusions, these ideas support strongly anti-corporatist, anti-hierarchical, pro-labour positions and anti-capitalism in economics and anti-imperialism in foreign policy. [248] [249] [250] [251]
Anarcho-capitalism advocates the elimination of the state in favour of individual sovereignty in a free market . [252] [253] Anarcho-capitalism developed from radical anti-state libertarianism and individualist anarchism, [254] [255] [256] [257] [258] [259] [260] drawing from Austrian School economics, study of law and economics , and public choice theory . [261] There is a strong current within anarchism which believes that anarcho-capitalism cannot be considered a part of the anarchist movement, due to the fact that anarchism has historically been an anti-capitalist movement and for definitional reasons which see anarchism as incompatible with capitalist forms. [262] [263] [264] [265] [266] [267]

Internal issues and debates
Anarchism is a philosophy that embodies many diverse attitudes, tendencies and schools of thought; as such, disagreement over questions of values, ideology and tactics is common. The compatibility of capitalism, [268] nationalism , and religion with anarchism is widely disputed. Similarly, anarchism enjoys complex relationships with ideologies such as Marxism , communism , collectivism , syndicalism / trade unionism , and capitalism. Anarchists may be motivated by humanism , divine authority, enlightened self-interest , veganism or any number of alternative ethical doctrines.
Phenomena such as civilisation , technology (e.g. within anarcho-primitivism), and the democratic process may be sharply criticised within some anarchist tendencies and simultaneously lauded in others.
On a tactical level, while propaganda of the deed was a tactic used by anarchists in the 19th century (e.g. the Nihilist movement ), some contemporary anarchists espouse alternative direct action methods such as nonviolence , counter-economics and anti-state cryptography to bring about an anarchist society. About the scope of an anarchist society, some anarchists advocate a global one, while others do so by local ones. [269] The diversity in anarchism has led to widely different use of identical terms among different anarchist traditions, which has led to many definitional concerns in anarchist theory .

Topics of interest
Intersecting and overlapping between various schools of thought, certain topics of interest and internal disputes have proven perennial within anarchist theory.

Free love
An important current within anarchism is free love. [270] Free love advocates sometimes traced their roots back to Josiah Warren and to experimental communities, viewed sexual freedom as a clear, direct expression of an individual's sovereignty. Free love particularly stressed women's rights since most sexual laws discriminated against women: for example, marriage laws and anti-birth control measures. [194] The most important American free love journal was Lucifer the Lightbearer (1883–1907) edited by Moses Harman and Lois Waisbrooker , [271] but also there existed Ezra Heywood and Angela Heywood's The Word (1872–1890, 1892–1893). [194] Free Society (1895–1897 as The Firebrand ; 1897–1904 as Free Society ) was a major anarchist newspaper in the United States at the end of the 19th and beginning of the 20th centuries. [272] The publication advocated free love and women's rights, and critiqued " Comstockery " – censorship of sexual information. Also M. E. Lazarus was an important American individualist anarchist who promoted free love. [194]
In New York City's Greenwich Village , bohemian feminists and socialists advocated self-realisation and pleasure for women (and also men) in the here and now. They encouraged playing with sexual roles and sexuality, [273] and the openly bisexual radical Edna St. Vincent Millay and the lesbian anarchist Margaret Anderson were prominent among them. Discussion groups organised by the Villagers were frequented by Emma Goldman, among others. Magnus Hirschfeld noted in 1923 that Goldman "has campaigned boldly and steadfastly for individual rights, and especially for those deprived of their rights. Thus it came about that she was the first and only woman, indeed the first and only American, to take up the defence of homosexual love before the general public." [274] In fact, before Goldman, heterosexual anarchist Robert Reitzel (1849–1898) spoke positively of homosexuality from the beginning of the 1890s in his Detroit-based German language journal Der arme Teufel (English: The Poor Devil). In Argentina anarcha-feminist Virginia Bolten published the newspaper called La Voz de la Mujer (English: The Woman's Voice), which was published nine times in Rosario between 8 January 1896 and 1 January 1897, and was revived, briefly, in 1901. [275]
In Europe the main propagandist of free love within individualist anarchism was Emile Armand. [276] He proposed the concept of la camaraderie amoureuse to speak of free love as the possibility of voluntary sexual encounter between consenting adults. He was also a consistent proponent of polyamory . [276] In Germany the stirnerists Adolf Brand and John Henry Mackay were pioneering campaigners for the acceptance of male bisexuality and homosexuality . Mujeres Libres was an anarchist women's organisation in Spain that aimed to empower working class women. It was founded in 1936 by Lucía Sánchez Saornil , Mercedes Comaposada and Amparo Poch y Gascón and had approximately 30,000 members. The organisation was based on the idea of a "double struggle" for women's liberation and social revolution and argued that the two objectives were equally important and should be pursued in parallel. In order to gain mutual support, they created networks of women anarchists. [277] Lucía Sánchez Saornil was a main founder of the Spanish anarcha-feminist federation Mujeres Libres who was open about her lesbianism . [278] She was published in a variety of literary journals where working under a male pen name, she was able to explore lesbian themes [279] at a time when homosexuality was criminalised and subject to censorship and punishment.
More recently, the British anarcho-pacifist Alex Comfort gained notoriety during the sexual revolution for writing the bestseller sex manual The Joy of Sex . The issue of free love has a dedicated treatment in the work of French anarcho- hedonist philosopher Michel Onfray in such works as Théorie du corps amoureux : pour une érotique solaire (2000) and L'invention du plaisir : fragments cyréaniques (2002).

Libertarian education and freethought
For English anarchist William Godwin education was "the main means by which change would be achieved." [280] Godwin saw that the main goal of education should be the promotion of happiness. [280] For Godwin education had to have "A respect for the child's autonomy which precluded any form of coercion," "A pedagogy that respected this and sought to build on the child's own motivation and initiatives," and "A concern about the child's capacity to resist an ideology transmitted through the school." [280] In his Political Justice he criticises state sponsored schooling "on account of its obvious alliance with national government". [281] Early American anarchist Josiah Warren advanced alternative education experiences in the libertarian communities he established. [282] Max Stirner wrote in 1842 a long essay on education called The False Principle of our Education . In it Stirner names his educational principle "personalist," explaining that self-understanding consists in hourly self-creation. Education for him is to create "free men, sovereign characters," by which he means "eternal characters ... who are therefore eternal because they form themselves each moment". [283]
In the United States "freethought was a basically anti-christian , anti-clerical movement, whose purpose was to make the individual politically and spiritually free to decide for himself on religious matters. A number of contributors to Liberty (anarchist publication) were prominent figures in both freethought and anarchism. The individualist anarchist George MacDonald was a co-editor of Freethought and, for a time, The Truth Seeker . E.C. Walker was co-editor of the excellent free-thought / free love journal Lucifer, the Light-Bearer ". [198] "Many of the anarchists were ardent freethinkers; reprints from freethought papers such as Lucifer, the Light-Bearer , Freethought and The Truth Seeker appeared in Liberty ... The church was viewed as a common ally of the state and as a repressive force in and of itself". [198]
In 1901, Catalan anarchist and free-thinker Francesc Ferrer i Guàrdia established "modern" or progressive schools in Barcelona in defiance of an educational system controlled by the Catholic Church. [284] The schools' stated goal was to "educate the working class in a rational, secular and non-coercive setting". Fiercely anti-clerical, Ferrer believed in "freedom in education", education free from the authority of church and state. [285] Murray Bookchin wrote: "This period [1890s] was the heyday of libertarian schools and pedagogical projects in all areas of the country where Anarchists exercised some degree of influence. Perhaps the best-known effort in this field was Francisco Ferrer's Modern School (Escuela Moderna), a project which exercised a considerable influence on Catalan education and on experimental techniques of teaching generally." [286] La Escuela Moderna, and Ferrer's ideas generally, formed the inspiration for a series of Modern Schools in the United States, [284] Cuba , South America and London. The first of these was started in New York City in 1911. It also inspired the Italian newspaper Università popolare , founded in 1901. Russian christian anarchist Leo Tolstoy established a school for peasant children on his estate. [287] Tolstoy's educational experiments were short-lived due to harassment by the Tsarist secret police. [288] Tolstoy established a conceptual difference between education and culture. [287] He thought that "Education is the tendency of one man to make another just like himself ... Education is culture under restraint, culture is free. [Education is] when the teaching is forced upon the pupil, and when then instruction is exclusive, that is when only those subjects are taught which the educator regards as necessary". [287] For him "without compulsion, education was transformed into culture". [287]
A more recent libertarian tradition on education is that of unschooling and the free school in which child-led activity replaces pedagogic approaches. Experiments in Germany led to A. S. Neill founding what became Summerhill School in 1921. [289] Summerhill is often cited as an example of anarchism in practice. [290] [291] However, although Summerhill and other free schools are radically libertarian, they differ in principle from those of Ferrer by not advocating an overtly political class struggle-approach. [292] In addition to organising schools according to libertarian principles, anarchists have also questioned the concept of schooling per se. The term deschooling was popularised by Ivan Illich , who argued that the school as an institution is dysfunctional for self-determined learning and serves the creation of a consumer society instead. [293]

Criticisms
Criticisms of anarchism include moral criticisms and pragmatic criticisms. Anarchism is often evaluated as unfeasible or utopian by its critics.

See also
WebPage index: 00060
Filter bubble
Filter bubbles result from personalized searches when a website algorithm selectively guesses what information a user would like to see based on information about the user (such as location, past click-behavior and search history). [1] [2] [3] As a result, users become separated from information that disagrees with their viewpoints, effectively isolating them in their own cultural or ideological bubbles . [4] The choices made by these algorithms are not transparent. Prime examples include Google Personalized Search results and Facebook 's personalized news-stream . The bubble effect may have negative implications for civic discourse , according to Pariser, but contrasting views regard the effect as minimal [5] and addressable. [6] The surprising results of the U.S. presidential election in 2016 have been associated with the influence of social media platforms such as Twitter and Facebook , [7] [8] and as a result have called into question the effects of the "filter bubble" phenomenon on user exposure to fake news and echo chambers , [9] spurring new interest in the term, [10] with many concerned that the phenomenon may harm democracy. [11] [12] [10]

Concept
The term was coined by internet activist Eli Pariser in his book by the same name; according to Pariser, users get less exposure to conflicting viewpoints and are isolated intellectually in their own informational bubble. He related an example in which one user searched Google for "BP" and got investment news about British Petroleum while another searcher got information about the Deepwater Horizon oil spill and that the two search results pages were "strikingly different". [14] [15] [16] [5]
Pariser defined his concept of filter bubble in more formal terms as "that personal ecosystem of information that's been catered by these algorithms". [14] Other terms have been used to describe this phenomenon, including " ideological frames " [15] or a "figurative sphere surrounding you as you search the Internet". [17] The past search history is built up over time when an Internet user indicates interest in topics by "clicking links, viewing friends, putting movies in your queue, reading news stories" and so forth. [17] An Internet firm then uses this information to target advertising to the user or make it appear more prominently in a search results query page . [17]
Pariser’s idea of the ‘filter bubble’ was popularized after the Ted Talk he gave in May 2011. [18] These ‘bubbles’ are created by algorithms that use 57 different signals to determine search results. These signals include “[the] computer being used,” “where you’re sitting,” “the browser doing the surfing,” and more. Pariser gives examples of how ‘filter bubbles’ work and where they can be seen. In an attempt to test the validity of ‘filter bubbles’ Pariser asked two of his friends to search the word ‘Egypt’ on Google and send him the search results. What each of them found were two completely different search results, one focusing on the political tensions in the country at the time, and one with vacation advertisements.
In The Filter Bubble , Pariser warns that a potential downside to filtered searching is that it "closes us off to new ideas, subjects, and important information" [19] and "creates the impression that our narrow self-interest is all that exists". [15] It is potentially harmful to both individuals and society, in his view. He criticized Google and Facebook for offering users "too much candy, and not enough carrots". [20] He warned that "invisible algorithmic editing of the web" may limit our exposure to new information and narrow our outlook. [20] According to Pariser, the detrimental effects of filter bubbles include harm to the general society in the sense that it has the possibility of "undermining civic discourse" and making people more vulnerable to "propaganda and manipulation". [15] He wrote:
A filter bubble has been described as exacerbating a phenomenon that has been called splinternet or cyberbalkanization , [22] which happens when the Internet becomes divided up into sub-groups of like-minded people who become insulated within their own online community and fail to get exposure to different views; the term cyberbalkanization was coined in 1996. [23] [24] [25]
Although his speech did not employ the adjective "filter", President Obama 's farewell address identified a similar concept to filter bubbles as a "threat to [Americans'] democracy", i.e., the "retreat into our own bubbles, ...especially our social media feeds, surrounded by people who look like us and share the same political outlook and never challenge our assumptions... And increasingly we become so secure in our bubbles that we start accepting only information, whether it’s true or not, that fits our opinions, instead of basing our opinions on the evidence that is out there." [26]

Reactions
There are conflicting reports about the extent to which personalized filtering is happening and whether such activity is beneficial or harmful. Analyst Jacob Weisberg writing in Slate did a small non-scientific experiment to test Pariser's theory which involved five associates with different ideological backgrounds conducting exactly the same search—the results of all five search queries were nearly identical across four different searches, suggesting that a filter bubble was not in effect, which led him to write that a situation in which all people are "feeding at the trough of a Daily Me " was overblown. [15] A scientific study from Wharton that analyzed personalized recommendations also found that these filters can actually create commonality, not fragmentation, in online music taste. [27] Consumers apparently use the filter to expand their taste, not limit it. [27] Book reviewer Paul Boutin did a similar experiment among people with differing search histories, and found results similar to Weisberg's with nearly identical search results. [5] Harvard law professor Jonathan Zittrain disputed the extent to which personalisation filters distort Google search results; he said "the effects of search personalization have been light". [15] Further, there are reports that users can shut off personalisation features on Google if they choose [28] by deleting the Web history and by other methods. [5] A spokesperson for Google suggested that algorithms were added to Google search engines to deliberately "limit personalization and promote variety". [15]
While algorithms do limit political diversity, some of the filter bubble is the result of user choice. [29] In a study by data scientists at Facebook, they found that for every four Facebook friends that share ideology, users have one friend with contrasting views. [30] [31] No matter what Facebook’s algorithm for its News Feed is, people are simply more likely to befriend/follow people who share similar beliefs. [30] The nature of the algorithm is that it ranks stories based on a user’s history, resulting in a reduction of the “politically cross-cutting content by 5 percent for conservatives and 8 percent for liberals.” [30] However, even when people are given the option to click on a link offering contrasting views, they still default to their most viewed sources. [30] “[U]ser choice decreases the likelihood of clicking on a cross-cutting link by 17 percent for conservatives and 6 percent for liberals.” [30]
There are reports that Google and other sites have vast information which might enable them to further personalise a user's Internet experience if they chose to do so. One account suggested that Google can keep track of user past histories even if they don't have a personal Google account or are not logged into one. [5] One report was that Google has collected "10 years worth" of information amassed from varying sources, such as Gmail , Google Maps , and other services besides its search engine, [16] although a contrary report was that trying to personalise the Internet for each user was technically challenging for an Internet firm to achieve despite the huge amounts of available web data. Analyst Doug Gross of CNN suggested that filtered searching seemed to be more helpful for consumers than for citizens , and would help a consumer looking for "pizza" find local delivery options based on a personalized search and appropriately filter out distant pizza stores. [16] There is agreement that sites within the Internet, such as the Washington Post , The New York Times , and others are pushing efforts towards creating personalized information engines, with the aim of tailoring search results to those that users are likely to like or agree with. [15]
Several designers developed tools to counteract the effects of filter bubbles. [32] Swiss radio station SRF voted the word filterblase (the German translation of filter bubble) word of the year 2016. [33]

Counter Measures
Users can take actions to burst through their filter bubbles. Some make a conscious effort to evaluate what information they are exposing themselves to, thinking critically about whether they are engaging with a broad range of content. [34] Websites such as allsides.com [35] and hifromtheotherside.com [36] aim to expose readers to different perspectives with diverse content.
Some additional plug-ins aimed to help us step out of our filter bubbles and make us aware of our personal perspectives; thus, these media show content that contradicts with our beliefs and opinions. For instance, Escape Your Bubble asks users to indicate a specific political party they want to be more informed about. [37] The plug-in will then suggest articles from well-established sources for you to read relating to that political party, encouraging users to become more educated about the other party. [37] In addition to plug-ins, there are apps created with the mission of encouraging us to open our echo chambers. Read Across the Aisle is a news app that reveals whether or not users are reading from diverse new sources that include multiple perspectives. [38] Each source is color coordinated, representing the political leaning of each article. [38] When users only read news from one perspective, the app communicates that to the user and encourages readers to explore other sources with opposing viewpoints. [38] Although apps and plug-ins are tools humans can use, Eli Pariser stated “certainly, there is some individual responsibility here to really seek out new sources and people who aren’t like you.” [29]
Since web-based advertising can further the effect of the filter bubbles by exposing users to more of the same content, users can block much advertising by deleting their search history, turning off targeted ads, and downloading browser extensions. [39] [40] Extensions such as Escape your Bubble [41] for Google Chrome aim to help curate content and prevent users from only being exposed to biased information, while Mozilla Firefox extensions such as Lightbeam [42] and Self-Destructing Cookies [43] enable users to visualize how their data is being tracked, and lets them remove some of the tracking cookies . Some use anonymous web browsers such as duckduckgo , [44] StartPage , [45] and Disconnect [46] in order to prevent companies from gathering their web-search data. Swiss daily Neue Zürcher Zeitung is beta-testing a personalised news engine app which uses machine learning to guess what content a user is interested in, while "always including an element of surprise"; the idea is to mix in stories which a user is unlikely to have followed in the past. [47]
The European Union is taking measures to lessen the impact of the filter bubble. The European Parliament is sponsoring inquiries into how filter bubbles affect people’s ability to access diverse news. [48] Additionally, it introduced a program aimed to educate citizens about social media. [49]

In practice
In January 2017, Facebook removed personalization from its Trending Topics list in response to problems with some users not seeing highly talked-about events there. [50]

Ethical Implications
As the popularity of cloud services increases, personalized algorithms used to construct filter bubbles are expected to become more widespread. [51] Scholars have begun considering the effect of filter bubbles on the users of social media from an ethical standpoint , particularly concerning the areas of personal freedom , security , and information bias . [52]
Filter bubbles in popular social media and personalized search sites can determine the particular content seen by users, often without their direct consent or cognizance, [51] due to the algorithms used to curate that content. Critics of the use of filter bubbles speculate that individuals may lose autonomy over their own social media experience and have their identities socially constructed as a result of the pervasiveness of filter bubbles. [51]
Technologists, social media engineers, and computer specialists have also examined the prevalence of filter bubbles. [53] Mark Zuckerberg , founder of Facebook , and Eli Pariser , author of “The Filter Bubble,” have even expressed concerns regarding the risks of privacy and information polarization. [54] [55] The information of the users of personalized search engines and social media platforms is not private, though some people believe it should be. [54] The concern over privacy has resulted in a debate as to whether or not it is moral for information technologists to take users’ online activity and manipulate future exposure to related information. [55]
Since the content seen by individual social media users is influenced by algorithms that produce filter bubbles, users of social media platforms are more susceptible to confirmation bias , [56] and may be exposed to biased, misleading information. [57] Social sorting and other unintentional discriminatory practices are also anticipated as a result of personalized filtering. [58]

See also
WebPage index: 00061
Feces
Feces or faeces (British and Latin) are the solid or semisolid metabolic waste from an animal 's digestive tract , discharged through the anus or cloaca during a process called defecation . Urine and feces together are called excreta .
Collected feces has various uses, namely as fertilizer or soil conditioner in agriculture, as a fuel source , or for medicinal purposes (fecal transplants or fecal bacteriotherapy , in the case of human feces ).

Ecology
After an animal has digested eaten material, the remains of that material are discharged from its body as waste. Although it is lower in energy than the food from which it is derived, feces may retain a large amount of energy, often 50% of that of the original food. [1] This means that of all food eaten, a significant amount of energy remains for the decomposers of ecosystems. Many organisms feed on feces, from bacteria to fungi to insects such as dung beetles , who can sense odors from long distances. [2] Some may specialize in feces, while others may eat other foods as well. Feces serve not only as a basic food, but also as a supplement to the usual diet of some animals. This is known as coprophagia , and occurs in various animal species such as young elephants eating the feces of their mothers in order to gain essential gut flora , or by other animals such as dogs, rabbits, and monkeys.
Feces and urine, which reflect ultraviolet light, are important to raptors such as kestrels , who can see the near ultraviolet and thus find their prey by their middens and territorial markers. [3]
Seeds also may be found in feces. Animals who eat fruit are known as frugivores . An advantage for a plant in having fruit is that animals will eat the fruit and unknowingly disperse the seed in doing so. This mode of seed dispersal is highly successful, as seeds dispersed around the base of a plant are unlikely to succeed and often are subject to heavy predation . Provided the seed can withstand the pathway through the digestive system, it is not only likely to be far away from the parent plant, but is even provided with its own fertilizer.
Organisms that subsist on dead organic matter or detritus are known as detritivores , and play an important role in ecosystems by recycling organic matter back into a simpler form that plants and other autotrophs may absorb once again. This cycling of matter is known as the biogeochemical cycle . To maintain nutrients in soil it is therefore important that feces return to the area from which they came, which is not always the case in human society where food may be transported from rural areas to urban populations and then feces disposed of into a river or sea.

Characteristics
The distinctive odor of feces is due to bacterial action. Gut flora produce compounds such as indole , skatole , and thiols ( sulfur -containing compounds), as well as the inorganic gas hydrogen sulfide . These are the same compounds that are responsible for the odor of flatulence . Consumption of foods prepared with spices may result in the spices being undigested and adding to the odor of feces.
The perceived bad odor of feces has been hypothesized to be a deterrent for humans, as consuming or touching it may result in sickness or infection. [4] Human perception of the odor may be contrasted by a non-human animal's perception of it; for example, an animal who eats feces may be attracted to its odor.

Human feces
In humans and depending on the individual and the circumstances, defecation may occur daily, or once every two or three days to several times a day. Extensive hardening of the feces may cause prolonged interruption in the routine and is called constipation .
Human fecal matter varies significantly in appearance, depending on diet and health. [5] Normally it is semisolid, with a mucus coating. The brown coloration comes from a combination of bile and bilirubin , which comes from dead red blood cells .
In newborn babies, initially fecal matter is yellow-green after the meconium . This coloration comes from the presence of bile alone. In time, as the body starts expelling bilirubin from dead red blood cells, it acquires its familiar brown appearance, unless the baby is breast feeding , in which case it remains soft, pale yellowish, and not completely malodorous, until the baby begins to eat significant amounts of other food.
Throughout the life of an ordinary human, one may experience many types of feces. A "green" stool is from rapid transit of feces through the intestines (or the consumption of certain blue or green food dyes in quantity), and "clay-like" appearance to the feces is the result of a lack of bilirubin .

Pets
Pets can be trained to use litter boxes or wait to be allowed outside to defecate . Training can be done in several ways, especially dependent on species. An example is crate training for dogs. Several companies market cleaning products for pet owners whose pets have soiled carpets in the home.

Uses of animal feces

Fertilizer
The feces of animals often are used as fertilizer ; see guano and manure .

Energy source
Dry animal dung is used as a fuel source in many countries around the world by burning it. Some animal feces, especially those of camel , bison , and cattle , are used as fuel when dried . [6]
Animals such as the giant panda [7] and zebra [8] possess gut bacteria capable of producing biofuel. The bacteria, Brocadia anammoxidans , can create the rocket fuel hydrazine from feces. [9] [10]

Coprolites and paleofeces
A coprolite is fossilized feces and is classified as a trace fossil . In paleontology they give evidence about the diet of an animal. They were first described by William Buckland in 1829. Prior to this they were known as "fossil fir cones " and " bezoar stones". They serve a valuable purpose in paleontology because they provide direct evidence of the predation and diet of extinct organisms. [11] Coprolites may range in size from a few millimetres to more than 60 centimetres.
Paleofeces are ancient human feces , often found as part of archaeological excavations or surveys. Intact feces of ancient people may be found in caves in arid climates and in other locations with suitable preservation conditions. These are studied to determine the diet and health of the people who produced them through the analysis of seeds, small bones, and parasite eggs found inside. These feces may contain information about the person excreting the material as well as information about the material. They also may be analyzed chemically for more in-depth information on the individual who excreted them, using lipid analysis and ancient DNA analysis. The success rate of usable DNA extraction is relatively high in paleofeces, making it more reliable than skeletal DNA retrieval. [12]
The reason this analysis is possible at all is due to the digestive system not being entirely efficient, in the sense that not everything that passes through the digestive system is destroyed. Not all of the surviving material is recognizable, but some of it is. Generally, this material is the best indicator archaeologists can use to determine ancient diets, as no other part of the archaeological record is so direct an indicator. [13]
A process that preserves feces in a way that they may be analyzed later is called the Maillard reaction . This reaction creates a casing of sugar that preserves the feces from the elements. To extract and analyze the information contained within, researchers generally have to freeze the feces and grind it up into powder for analysis. [14]

Other uses
Animal dung occasionally is used as a cement to make adobe mudbrick huts, [15] or even in throwing sports such as cow pat throwing or camel dung throwing contests. [16]
Kopi Luwak (pronounced [ˈkopi ˈlu.aʔ] ), or civet coffee, is coffee made from coffee berries that have been eaten by and passed through the digestive tract of the Asian palm civet ( Paradoxurus hermaphroditus ). Giant pandas provide fertilizer for the world's most expensive green tea . [17] In Malaysia, tea is made from the droppings of stick insects fed on guava leaves.
In northern Thailand , elephants are used to digest coffee beans in order to make Black Ivory coffee , which is among the world's most expensive coffees. [17]
Dog feces were used in the tanning process of leather during the Victorian era. Collected dog feces, known as "pure", "puer", or "pewer", [18] were mixed with water to form a substance known as "bate." Enzymes in the dog feces helped to relax the fibrous structure of the hide before the final stages of tanning. [19]
Elephants , hippos , koalas and pandas are born with sterile intestines, and require bacteria obtained from eating the feces of their mothers to digest vegetation.

Terminology
Feces is the scientific terminology, while the term stool is also commonly used in medical contexts. [20] Outside of scientific contexts, these terms are less common, with the most common layman's term being poo (or poop in North American English). The term shit is also in common use, although is widely considered vulgar or offensive. There are many other terms, with some of the more widely used being crap , dump , load and turd . [21]

Etymology
The word faeces is the plural of the Latin word faex meaning "dregs". In most English-language usage , there is no singular form, making the word a plurale tantum ; [22] out of various major dictionaries, only one enters variation from plural agreement . [23]

Synonyms
"Feces" is used more in biology and medicine than in other fields (reflecting science 's tradition of classical Latin and New Latin )
As with urine, there are many synonyms in informal registers for feces. Many are euphemismistic , colloquial , or both; some are profane (such as shit ), whereas most belong chiefly to child-directed speech (such as poo or poop ) or to crude humor (such as deuce or turd ). It is also represented in emoji form in the Miscellaneous Symbols and Pictographs block of Unicode as U+1F4A9 💩 pile of poo , called unchi or unhci-kun in Japan. [25] [26]
The feces of animals often have special names, for example:

Society and culture

Feelings of disgust
In all human cultures, feces elicit varying degrees of disgust , a basic human emotion. [ citation needed ] Disgust is experienced primarily in relation to the sense of taste (either perceived or imagined) and, secondarily to anything that causes a similar feeling by sense of smell, touch, or vision.

See also
WebPage index: 00062
Sexual content
In media discourse , sexual content is material depicting sexual behavior . The sexual behavior involved may be explicit, implicit sexual behavior such as flirting , [1] or include sexual language and euphemisms . [2]
Sexual content is a large factor in most content rating systems , such as those used for television programs , films , and video games . Its increasing availability, especially the Internet , has increased people's exposure to sexual content. Such exposure is not always wanted. [1]
Research has suggested that exposure to sexual content affects people's thoughts and behavior, though there is disagreement as to the extent of the effect. [2] Gert Martin Hald, a psychologist at the University of Copenhagen, who authored a study which found that watching "sexually explicit media" only accounted for 0.3 to 4.0 percent of behavior changes, said, "Our data suggest that other factors such as personal dispositions — specifically sensation-seeking — rather than consumption of sexually explicit material may play a more important role in a range of sexual behaviors of adolescents and young adults." [3]

See also
WebPage index: 00063
Hardcore pornography
Hardcore pornography , or hardcore porn , is still photography or video footage that contains explicit forms of pornography , most commonly including depictions of sexual acts such as vaginal , anal or oral intercourse, cunnilingus , fellatio , anilingus , ejaculation , and fetish play . Hardcore pornography usually takes the form of photographs , often displayed in magazines or on the Internet , or films and cartoons . Since the 1990s it has been distributed widely over the Internet.

Etymology
A distinction between "hardcore pornography" and "borderline pornography" (or "borderline obscenity") was made in the 1950s and 1960s by American jurists discussing obscenity laws. "Borderline pornography" appealed to sexual prurience, but had positive qualities, such as literary or artistic merit, and so was arguably permitted by obscenity laws; "hardcore pornography" lacked such merits and was definitely prohibited. [1] In Roth v. United States (1957) the government brief distinguished three classes of sexual material: "novels of apparently serious literary intent"; "borderline entertainment ... magazines, cartoons, nudist publications, etc."; and "hard core pornography, which no one would suggest had literary merit". [2] Eberhard and Phyllis Kronhausen in 1959 distinguished "erotic realism" from "pornography"; in the latter "the main purpose is to stimulate erotic response in the reader. And that is all." [3] Most famously, in Jacobellis v. Ohio (1964), Potter Stewart wrote :
In Jacobellis v. Ohio and other cases, the United States Supreme Court ruled that only "hardcore" pornography could be prohibited by obscenity laws, with the rest protected by the First Amendment . The category of "borderline obscenity" thus became obsolete. The 1970 report of the President's Commission on Obscenity and Pornography said: [4]
From the 1970s, the salient distinction was between hardcore pornography and softcore pornography , which may use simulated sex and limits the range and intensity of depictions of sexual activities. For example, William Rotsler 's 1973 classification subdivided the X rating for erotic films: "The XXX-rating means hard-core, the XX-rating is for simulation, and an X-rating is for comparatively cool films." [5]

History
The prehistory of modern pornography is the classical American stag film , also known as blue movies , a body of clandestine short pornographic films produced during the first two-thirds of the 20th century. While the exact corpus of the distinctive stag film remains unknown, scholars at the Kinsey Institute believe there are approximately 2000 films produced between 1915-1968. [6] Stag cinema is a form of hardcore film and is characterized as silent, usually filling a single reel or less, and was illegally made and exhibited because of censorship laws in America. Women were excluded from these private screenings that were shown in American "smoker" houses such as fraternities or other exclusive institutions. In Europe, films of the same kind were screened within brothels. The mode of reception of the all-male audience of stag films was raucous, collective sexual banter [7] and sexual arousal . Film historians describe stag films as a primitive form of cinema because they were produced by anonymous and amateur male artists who failed in achieving narrative coherence and continuity within their diegesis. Today, many of these films have been archived by the Kinsey Institute, however most are in a state of decay and have no copyright , real credits, or acknowledged authorship. The stag film era inevitably ended due to the beginnings of the sexual revolution in the fifties in combination with the new technologies of the post war era, such as 16mm, 8mm, and the Super 8 . American stag cinema in general has received scholarly attention first in the mid-seventies by heterosexual males such as in Di Lauro and Gerald Rabkin's Dirty Movies (1976) and more recently by feminist and queer cultural historians such as in Linda M. Williams ' Hard Core: Power Pleasure, and the "Frenzy of the Visible" (1989) and Thomas Waugh's Homosociality in the Classical American Stag Film: Off-Screen, On-screen (2001).

Legality
The distribution of hardcore pornography had been widely prohibited in many countries until the second half of the 20th century when many countries began to allow some dissemination of softcore material. Supply is now usually regulated by a motion picture rating system as well as by direct regulation of points of sale . Restrictions, as applicable, apply to the screening, or rental, sale, or giving of a movie, in the form of a DVD, video, computer file, etc. Public display and advertising of hardcore pornography is often prohibited, as is its supply to minors .
Most countries have eased the restrictions on the distribution of pornography , either by general or restricted legalization or by failure to enforce prohibitive legislation. Most easing of restrictions has been by way of changes to the criteria of a country's movie classification system. The anti-pornography movement often vigorously opposes legalization. In 1969, Denmark became the first country in the world to legalize pornography. [8] In the U.S., legal interpretations of pornography in relation to the constitutional right to free speech differ from state to state and from city to city. Hardcore pornography was legalized in the UK in 2000. [9]

Impact on society
Berl Kutchinsky's Studies on Pornography and Sex Crimes in Denmark (1970), a scientific report commissioned by the Presidential Commission on Obscenity and Pornography , found that the legalizing of pornography in Denmark had not (as had been expected) resulted in an increase of sex crimes. [10]
A study conducted in Denmark in 2003 and later published in Archives of Sexual Behavior found that men and women generally believe that hardcore pornography has a positive influence on their lives. [11]

United Kingdom
The Independent reported in 2006 that Nielsen NetRatings found that more than nine million British male adults used Internet porn services. [12] The study also reported a one-third rise in the number of women visiting X-rated sites, from 1.05 million to 1.38 million. A 2003 study found that one third of all British Internet users accessed hardcore porn. [13]

United States
A 2005 study by Eric Schlosser estimated that revenues from hardcore porn match Hollywood's domestic box office takings. Hardcore porn videos, Internet sites, live sex acts and cable TV programming generate US$10 billion, roughly equal to U.S. domestic box office receipts. [14]

See also
WebPage index: 00064
Scorpions (band)
Scorpions are a German rock band formed in 1965 in Hanover by Rudolf Schenker . [2] Since the band's inception, its musical style has ranged from hard rock [3] [4] to heavy metal . [5] [6] The lineup from 1978–92 was the most successful incarnation of the group, and included singer Klaus Meine (vocals), Rudolf Schenker (rhythm guitar), Matthias Jabs (lead guitar), Francis Buchholz (bass guitar), and Herman Rarebell (drums). The band's only constant member has been Schenker, although Meine has been the lead singer for all of the band's studio albums, and Jabs has been a consistent member since 1979. [7]
During the mid-1970s, with guitarist Uli Jon Roth part of the line-up, the music of the Scorpions was defined as hard rock. [3] [4] After the departure of Roth in 1978, Matthias Jabs joined and, following the guidance of producer Dieter Dierks , the Scorpions changed their sound towards melodic heavy metal, mixed with rock power ballads . Throughout the 1980s the group received positive reviews and critical acclaim from music critics, and experienced commercial success with the albums Animal Magnetism (1980), Blackout (1982), Love at First Sting (1984), World Wide Live (1985) and Savage Amusement (1988). [8]
Their best-selling album Crazy World (1990) includes the song " Wind of Change ", [9] a symbolic anthem of the political changes in Eastern Europe in the late 1980s and early 1990s and the fall of the Berlin Wall . It is one of the best-selling singles in the world with over fourteen million copies sold. [10] Scorpions have sold over 100 million records in total. [11] They have released 18 studio albums, 27 compilation albums and 74 singles. Six of their singles have reached number one on the charts in different countries. Their albums, singles, compilations and video releases have reached 200 times gold, platinum and multi-platinum status in different countries. [11]
Rolling Stone Magazine described the Scorpions as "the heroes of heavy metal", [12] and MTV called them "Ambassadors of Rock". The band was ranked number 46 on VH1 's Greatest Artists of Hard Rock programme, [13] with " Rock You Like a Hurricane " at number 18 on VH1's list of the 100 Greatest Hard Rock Songs . [14] " Still Loving You " ranked 22nd place among the greatest ballads. The Scorpions have received prestigious awards such as three World Music Awards , [15] a star on the Hollywood Rock wall, [16] and a presence in the permanent exhibition of the Rock and Roll Hall of Fame . [17] In 2015 the group celebrated its 50th anniversary. [18]

History

Formation and early history (1965–1973)
Rudolf Schenker , the band's rhythm guitarist launched the band in 1965. At first, the band had beat influences and Schenker himself handled the vocals. [19] Things began to come together in 1970 when Schenker's younger brother Michael and vocalist Klaus Meine joined the band. In 1972 the group recorded and released their debut album Lonesome Crow , with Lothar Heimberg on bass and Wolfgang Dziony on drums. [20] During the Lonesome Crow tour, the Scorpions opened for upcoming British band UFO . Near the end of the tour, guitarist Michael Schenker accepted an offer of lead guitar for UFO. Uli Roth , a friend of Michael's, was then introduced to the band and he helped them to finish off the tour.
The departure of Michael Schenker led to the breakup of the band. In 1973, Uli Roth , who had helped the Scorpions complete the Lonesome Crow tour, was offered the role as lead guitarist, but turned the band down, preferring instead to remain in the band Dawn Road. Rudolf Schenker eventually decided that he wanted to work with Roth, but did not want to resurrect the last Scorpions lineup. He attended some of Dawn Road's rehearsals and ultimately decided to join the band, which consisted of Roth, Francis Buchholz (bass), Achim Kirschning (keyboards) and Jürgen Rosenthal (drums). Uli Roth and Buchholz persuaded Rudolf Schenker to invite Klaus Meine to join on vocals, which he soon did. While there were more members of Dawn Road than Scorpions in the band, they decided to use the Scorpions name because it was well known in the German hard rock scene and an album had been released under that name. [21]

Rise to fame (1974–1978)
In 1974, the new line-up released Fly to the Rainbow . The album proved to be more successful than Lonesome Crow and songs such as "Speedy's Coming" and the title track established the band's sound. Achim Kirschning decided to leave after the recordings. Soon after, Jürgen Rosenthal had to leave as he was being drafted into the army. In 1976, he would join a German progressive rock band called Eloy recording three albums. He was replaced in July 1974 by Jurgen Fechter. In 1975 Rudy Lenners from Belgium became the next drummer.
That year (1975) the band released In Trance , which marked the beginning of their long collaboration with German producer Dieter Dierks . The album was a huge step forward for the Scorpions and established their heavy metal formula. It garnered a fan base at home and abroad with cuts such as "In Trance", "Dark Lady" and "Robot Man".
Meanwhile, as "The Hunters", the band recorded "Fuchs geh' voran" and "Wenn es richtig losgeht", German cover versions of " Action " and " Fox on the Run " by Sweet for EMI's Electrola label. [22]
In 1976, the Scorpions released Virgin Killer , the album cover of which featured a nude prepubescent girl behind a broken pane of glass. The cover art was designed by Stefan Bohle who was the product manager for RCA Records , [23] their label at the time. The cover brought the band considerable market exposure but was subsequently pulled or replaced in other countries. The album itself garnered widespread praise for its music from select critics and fan base. In 2008, the cover art on the English Wikipedia was blacklisted by the Internet Watch Foundation .
The following year, Rudy Lenners resigned for personal reasons and was replaced by Herman Rarebell .
For the follow-up Taken by Force , RCA Records made a determined effort to promote the album in stores and on the radio. The album's single, "Steamrock Fever", was added to some of RCA's radio promotional records. Roth was not happy with the commercial direction the band was taking. Although he performed on the band's Japan tour, he departed to form his own band, Electric Sun prior to the release of the resultant double live album Tokyo Tapes . Tokyo Tapes was released in the US and Europe six months after its Japanese release. By that time in mid 1978, after auditioning around 140 guitarists, the Scorpions recruited guitarist Matthias Jabs.

Commercial success (1978-1992)
Following the addition of Jabs, Scorpions left RCA for Mercury Records in the United States and Harvest /EMI Electrola worldwide to record their next album Lovedrive . Just weeks after quitting UFO, Michael Schenker returned to the group for a short period during the recordings for the album. This gave the band three guitarists (though Schenker's contribution to the final release was limited to only three songs). The result was Lovedrive , an album which some critics consider to be the pinnacle of their career. [24] Containing such fan favourites as "Loving You Sunday Morning", "Always Somewhere", "Holiday" and the instrumental "Coast to Coast", it firmly cemented the "Scorpions formula" of hard rock songs mixed with melodic ballads. The album's provocative artwork was named "Best album sleeve of 1979" by Playboy magazine , yet ultimately changed for American release. Lovedrive reached No. 55 on the US charts, demonstrating that the band was gathering an international following. After the completion and release of the album the band decided to retain Michael in the band, forcing Jabs to leave. However, in April 1979, during their tour in France, Michael quit and Jabs was brought in permanently to replace him.
In 1980 the band released Animal Magnetism , again with a provocative cover this time showing a girl kneeling and a Doberman Pinscher sitting in front of a man. Animal Magnetism contained classics such as " The Zoo " and "Make It Real". Soon after the album's release, Meine began experiencing throat problems. He required surgery on his vocal cords and doubts were raised about whether he would ever sing again.
Meanwhile, the band began working on their next album, Blackout in 1981. Don Dokken was brought in to provide guide and backing vocals while Meine recovered. [25] Meine eventually healed completely and was able to finish the album. Blackout was released in 1982 and quickly became the band's best selling album to date, eventually going platinum. Meine's voice showed no signs of weakness and fan response to the album was good. Blackout spawned two singles: " No One Like You " and " Can't Live Without You ".
Gaining in popularity from the success of Blackout , the Scorpions performed to over 375,000 fans on Day 2 at the three-day US Festival concert held in San Bernardino, California during Memorial Day Weekend of 1983. The concert was aired live on MTV, giving the band wide exposure in a live show.
The 1984 album Love at First Sting cemented the Scorpions' status as an internationally popular band. Propelled by the single " Rock You Like a Hurricane ", Love at First Sting climbed the charts and went double platinum in the USA a few months after its release.
MTV gave the album's videos "Rock You Like a Hurricane", "Bad Boys Running Wild", "Big City Nights", and the power ballad " Still Loving You " significant airtime greatly contributing to the album's success. The channel even supplied Scorpions with the nickname "The Ambassadors of Rock" to the chagrin of industry insiders who recognised the executive influence behind the scenes. Rolling Stone magazine named them "The Heroes of Heavy Metal".
The band toured extensively behind Love at First Sting and released their second live album, World Wide Live in 1985. Recorded over a year-long world tour and released at the height of their popularity, the album was another success for the band, peaking at No. 14 in the charts in the US and at No. 18 in the UK.
After their extensive world tours, the band finally returned to the studio to record Savage Amusement . Released in 1988, four years after their previous studio album, Savage Amusement represented a more polished and mature sound similar to the style Def Leppard had found success with. The album sold well but was considered somewhat of a critical disappointment. However, British heavy rock magazine Kerrang! did award the album five K's out of five.
On the Savage Amusement tour in 1988, the Scorpions became only the second Western group (not American) to play in the Soviet Union . Uriah Heep had performed in December 1987 in Leningrad . The following year the band returned to perform at the Moscow Music Peace Festival . As a result, the Scorpions developed an extended Russian fan base and still return to perform. [26]
Wishing to distance themselves from the Savage Amusement style, the band separated from their long-time producer and "Sixth Scorpion", Dieter Dierks , replacing him with Keith Olsen when they returned to the studio in 1990. Crazy World was released that same year and displayed a less polished sound. The album was propelled in large part by the massive success of the ballad " Wind of Change ". The song muses on the socio-political changes that were occurring in Eastern Europe and in other parts of the world at the end of the Cold War . On 21 July 1990 they joined many other guests for Roger Waters ' massive performance of The Wall in Berlin. Scorpions performed both versions of " In the Flesh " from The Wall . After the Crazy World tour Francis Buchholz, the band's long-serving bassist, left the group.

Later days (1993–2009)
In 1993, the Scorpions released Face the Heat . Bass was handled by Ralph Rieckermann . For the recording process, the band brought in producer Bruce Fairbairn . The album's sound was more metal than melodic. Neither the heavy metal single "Alien Nation" nor the ballad "Under The Same Sun" came close to matching the success of "Wind of Change". Face the Heat was a moderate success. In 1995, a new album, Live Bites , was produced. The disc documented retro live performances from their Savage Amusement Tour in 1988, all the way through the Face the Heat Tour in 1994. While the album had a technologically cleaner sound in comparison to their best-selling live album, World Wide Live , it was not as successful.
Prior to recording their 13th studio album, 1996s Pure Instinct , drummer Herman Rarebell left the band to set up a recording label. Curt Cress took charge of the drumsticks for the album before Louisville, Kentucky-born James Kottak took over permanently. The album had many ballads. Still, the album's singles "Wild Child" and the soothing ballad "You and I" both enjoyed moderate success.
1999 saw the release of Eye II Eye and a significant change in the band's style, mixing in elements of pop and techno. While the album was slickly produced, it was not received well by fans. The video to the album's first European single, "To Be No. 1", featured a Monica Lewinsky look-alike which did little to improve its popularity.
The following year, the Scorpions had an artistic collaboration with the Berlin Philharmonic that resulted in a 10-song album named Moment of Glory . The album went a long way toward rebuilding the band's reputation after the harsh criticism of Eye II Eye . However, critics accused them of following on the coattails of Metallica 's similar collaboration ( S&M ) with the San Francisco Symphony which had been released the previous year, even though the orchestra had first approached the Scorpions with the idea in 1995.
In 2001, the Scorpions released Acoustica , a live unplugged album featuring acoustic reworkings of the band's biggest hits, plus new tracks. While appreciated by fans, the lack of a new studio album was frustrating to some, and Acoustica did little to return the band to the spotlight.
In 2004, the band released Unbreakable , an album that was hailed by critics as a long-awaited return to form. The album was the heaviest the band had released since Face the Heat . Whether a result of poor promotion by the band's label or the long time between studio releases, Unbreakable received little airplay and did not chart. Scorpions toured extensively behind the album and played as "Special Guests" with Judas Priest during the 2005 British tour—these were the Scorpions' first dates in the UK since 1999.
In early 2006, the Scorpions released the DVD 1 Night in Vienna that included 14 live tracks and a complete rockumentary . In LA , the band spent about four months in the studio with producers James Michael and Desmond Child working on a concept album titled Humanity: Hour I , which was released in late May 2007, [27] and was followed by the " Humanity World Tour ".
In 2007, the band collaborated with two of their signature tracks in the video game series, Guitar Hero . "No One Like You" was featured on the Rocks the '80s version of the game while "Rock You Like A Hurricane" was released on Guitar Hero 3: Legends of Rock .
On 14 May 2007, the Scorpions released Humanity – Hour I in Europe. Humanity – Hour I became available in the U.S. on August 28 on New Door Records , entering the Billboard charts at number No. 63.
In a September 2007 podcast interview, Meine said the album was not so much a "concept album", but rather a collection of songs with a common theme. "We didn't want to make another record with songs about boys chasing girls. I mean, come on, give me a break," Meine said. [28]
Asked in 2007 if the band was planning to release a Humanity – Hour II , Meine replied:
On 20 December 2007, the Scorpions played at a concert for the elite of Russia's security forces in the Kremlin . The concert was a celebration of the 90th anniversary of the founding of the Cheka —predecessor of the KGB . The band has claimed that they thought they were performing a Christmas concert. They have said that their concert was by no means a tribute to the Cheka, communism, or Russia's brutal past . Members of the audience included Vladimir Putin and Dmitry Medvedev . [30]
On 22 February 2009, the band received Germany's ECHO Honorary Award for lifetime achievement at Berlin's O2 World . [31]

Sting in the Tail
In November 2009, the Scorpions announced that their 17th studio album, Sting in the Tail , would be released in early 2010, [32] recorded in Hanover with Swedish producers Mikael "Nord" Andersson and Martin Hansen. Sting in the Tail was released on 23 March 2010. [33]
On 24 January 2010, the band announced their intentions for Sting in the Tail to be their last album, with the tour supporting it being their final tour, although the band later made the decision to continue recording past the end of the tour. [34] [35] Dokken was scheduled to open for them but cancelled after a dispute. [36]
On 6 April 2010, they were enshrined in Hollywood's Rock Walk in a handprint ceremony, [37] with the band members placing their hands in a long slab of wet cement next to other musical artists.
An album of re-recordings of older songs, Comeblack , was released on 7 November 2011. [38]
Meine was asked in a July 2011 interview about the future of the Scorpions. He replied, "Our newest project comes out in the next few months. It gives you a chance to experience the Scorpions in 3D. You can actually feel the smoke string out of the guitar like it is a live show. It is an incredible experience. The DVD features our concerts in 3D in Germany. We are just about to do the mix and it should be in the Middle East and Saudi Arabia hopefully soon. Indeed, the strong 3D technology makes us feel like pioneers after all these years (he says, laughing). We have an album coming out later this year featuring classics. You know our love for them. The '60s was the era for our inspiration. Our movie/documentary also is soon to be released. We have cameras with us on tours, so this documentary is being made during our tours. It also gives you a picture of the Scorpions career and journey." [39]
Almost a year in advance it was announced the Scorpions would headline the Wacken Open Air Festival on 4 August.
Despite ongoing rumours of a break up or retirement, guitarist Matthias Jabs told AZ Central on 12 June 2012 that the Scorpions would not be splitting up. [40] A month later, Jabs told Billboard magazine that the band has been working on an album that will contain unreleased songs they recorded for the albums Blackout , Love at First Sting , Savage Amusement and Crazy World and plan to release it in 2014. [41] In April the Scorpions announced shows in Russia and Belarus with an orchestra in October 2013. On 11, 12, and 14 September 2013, Scorpions played three MTV Unplugged concerts at the Lycabettus-Theatre in Athens . [42] On 6 November 2013 they announce 4 more MTV Unplugged Concert in Germany 2014. In December 2013 in an interview at Rock Show radio program in Greece, Meine said he was not sure if the album with unreleased songs they recorded for the albums Blackout , Love at First Sting , Savage Amusement and Crazy World would be released in 2014 or later on. [43]
In 2014 the Scorpions were nominated for two Echo Awards ("Euro Grammys") for their MTV Unplugged.
On 16 August, they announced that there was a new album in the works, due for release sometime in 2015. [44]

50th anniversary and 
On 23 October 2014 Meine spoke to the band's French fan-club Crazyscorps, and announced that the new record would be published in February or March 2015, to coincide with the band's 50th anniversary. Contrary to what the band said in 2013, the new record would present not only newly recorded versions of never-published songs, but also new material, written between 2011 and 2014. The album is being recorded in Sweden, with producers Martin Hansen and Mikael Nord Andersson. Drummer James Kottak, who left the band in May 2014 for rehab, would return to play drums on the new record. [45] The new album Return to Forever was released on 20 February 2015.
On 29 August 2015, the Scorpions announced 50th anniversary deluxe editions of their albums Taken By Force , Tokyo Tapes , Lovedrive , Animal Magnetism , Blackout , Love At First Sting , World Wide Live , and Savage Amusement which were released 6 November 2015. These deluxe releases include "dozens of unreleased songs, alternate versions of big hits, rough mixes, and rare live concert recordings". On 28 April 2016, it was announced that Motörhead drummer Mikkey Dee would fill in for James Kottak and play drums on 12 North American headlining dates, [46] [47] including a run of shows at the Hard Rock Hotel in Las Vegas dubbed "Scorpions blacked out in Las Vegas" with Queensryche opening the Vegas shows, [48] and dates in São Paulo, Brazil. On 12 September 2016, Dee was officially announced as the band's new permanent drummer. [49]
On January 18, 2017 The Scorpions were inducted into the Hall of Heavy Metal History (created by Pat Gesualdo and Joe Dell) for being in the forefront of the two guitar attack in Heavy Metal. [50]

In other media

Musical theatre
The Scorpions song " Wind of Change " appears in the Off-Broadway production Power Balladz .

Band members

Current members

Former members

Vocalists

Bassists

Drummers

Guitarists

Keyboardists

Manager

Timeline

Awards and honours

Discography

Tours
The Scorpions have played around 5,000 concerts in over 80 countries. [56]

See also
WebPage index: 00065
United States obscenity law
United States obscenity law deals with the regulation or suppression of what is considered obscenity . In the United States, discussion of obscenity revolves around what constitutes pornography and of censorship , but also raises issues of freedom of speech and of the press , otherwise protected by the First Amendment to the Constitution of the United States . Issues of obscenity arise at federal and state levels. The States have a direct interest in public morality and have responsibility in relation to criminal law matters, including the punishment for the production and sale of obscene materials. State laws operate only within the jurisdiction of each state, and there are a wide differences in such laws. The federal government is involved in the issue indirectly, by making it an offense to distribute obscene materials through the post, to broadcast it, [1] as well as in relation to importation of such materials.
Most obscenity cases in the United States in the past century have revolved around images and films, but there have also been many cases that dealt with textual works as well, most infamously that of the 18th century novel Fanny Hill . Because censorship laws enacted to combat obscenity restrict freedom of expression, crafting a legal definition of obscenity presents a civil liberties issue.

Legal issues and definitions
The sale and distribution of obscene materials had been prohibited in most American states since the early 19th century, and by federal law since 1873. Adoption of obscenity laws in the United States at the federal level in 1873 was largely due to the efforts of Anthony Comstock , who created and led the New York Society for the Suppression of Vice . Comstock's intense lobbying led to the passage of an anti-obscenity statute known as the Comstock Act which made it a crime to distribute "obscene" material through the post. It also prohibited the use of the mail for distribution of birth control devices and information. Comstock was appointed postal inspector to enforce the new law. [2] Twenty-four states passed similar prohibitions on materials distributed within the states. [3] The law criminalized not only sexually explicit material, but also material dealing with birth control and abortion. [4] However, the legislation did not define "obscenity", which was left to the courts to determine on a case by case basis.
In the United States, the suppression or limitation of what is claimed to be an obscenity raises issues of rights to freedom of speech and of the press protected by the First Amendment to the Constitution of the United States . The Supreme Court has ruled that obscenity is not protected by the First Amendment, but the courts still need to determine whether material in question in each case is in fact obscene.
Legally, a distinction is made between socially permitted material and discussions that the public can access on the one hand and obscenity , access to which should be denied, on the other. There does exist a classification of those acceptable materials and discussions that the public should be allowed to engage in, and the access to that same permitted material—which in the areas of sexual materials ranges between the permitted areas of erotic art (which usually includes "classic nude forms" such as Michelangelo's David statue) and the generally less respected commercial pornography . The legal distinction between artistic nudity and permitted commercial pornography (which includes sexual penetration) deemed "protected forms of speech", versus "obscene acts", which are illegal acts and separate from those permitted areas, is usually predicated on cultural factors. However, no such specific objective distinction exists outside of legal decisions in federal court cases where a specific action is deemed to fit the classification of obscene and thus illegal. The difference between erotic art and (protected) commercial pornography, vs. that which is legally obscene (and thus not covered by 1st Amendment protection), appears to be subject to decisions within local US federal districts and contemporary moral standards.
In fact, federal obscenity law in the U.S. is highly unusual in that not only is there no uniform national standard, but rather, there is an explicit legal precedent (the "Miller test", below) that all but guarantees that something that is legally obscene in one jurisdiction may not be in another. In effect, the First Amendment protections of free speech vary by location within the U.S., and over time. With the advent of Internet distribution of potentially obscene material, this question of jurisdiction and community standards has created significant controversy in the legal community. (See United States v. Thomas, 74 F.3d 701 (6th Cir. 1996))
Even at the federal level, there does not exist a specific listing of which exact acts are to be classified as obscene outside of the legally determined court cases. Title 18, chapter 71 of the USC deals with obscenity, the workings out of the law described in this article, most notably the aforementioned Miller test.

Definition of obscenity
Although lower courts in the U.S. had used the Hicklin standard sporadically since 1868, it was not until 1879, when prominent federal judge Samuel Blatchford upheld the obscenity conviction of D. M. Bennett using the Hicklin test , that the constitutionality of the Comstock Law became firmly established. [5]
In Rosen v. United States (1896), the Supreme Court adopted the same obscenity standard as had been articulated in a famous British case, Regina v. Hicklin , [1868] L. R. 3 Q. B. 360. The Hicklin test defined material as obscene if it tended "to deprave or corrupt those whose minds are open to such immoral influences, and into whose hands a publication of this sort may fall." [6]
The Court ruled in Roth v. United States , 354 U.S. 476 (1957) that the Hicklin test was inappropriate. Instead, the Roth test for obscenity was
In 1964, in Jacobellis v. Ohio , Justice Potter Stewart in applying the Roth test pointed out that "community standards" applicable to an obscenity are national, not local standards. He found that the material in question is "utterly without redeeming social importance". Also, in attempting to classify what material constituted exactly "what is obscene," infamously wrote, "I shall not today attempt further to define the kinds of material I understand to be embraced…[b]ut I know it when I see it ..." [8] In Memoirs v. Massachusetts (1966) (dealing with the banning of the book Fanny Hill ) the Court applied the Roth-Jacobellis test to determine that though the other aspects of the test were clear, the censor could not prove that Fanny Hill had no redeeming social value. [9]
In 1973, the Supreme Court in Miller v. California established the three-tiered Miller test to determine what was obscene (and thus not protected) versus what was merely erotic and thus protected by the First Amendment. Delivering the opinion of the court, Chief Justice Warren Burger wrote:
Justice Douglas wrote a dissenting opinion that eloquently expressed his dissatisfaction with the ruling:

Past standards
These standards were once used to determine exactly what was obscene. All have been invalidated, overturned, or superseded by the Miller Test .
Under FCC rules and federal law, radio stations and over-the-air television channels cannot air obscene material at any time and cannot air indecent material between 6 a.m. and 10 p.m.: language or material that, in context, depicts or describes, in terms patently offensive as measured by contemporary community standards for the broadcast medium, sexual or excretory organs or activities.
Many historically important works have been described as obscene or prosecuted under obscenity laws, including the works of Charles Baudelaire , Lenny Bruce , William S. Burroughs , Allen Ginsberg , James Joyce , D. H. Lawrence , Henry Miller , Samuel Beckett , and the Marquis de Sade .

Court cases on obscenity

Application of test
In U.S. legal texts, the question of "obscenity" presently always refers to the " Miller test obscenity". As articulated in several sections of 18 USC Chapter 71, the Supreme Court has ruled that it is constitutional to legally limit the sale, transport for personal use or other transmission of obscenity. However, it has ruled unconstitutional the passing of law concerning personal possession of obscenity per se . Federal obscenity laws at present apply to inter-state and foreign obscenity issues such as distribution; intrastate issues are for the most part still governed by state law. "Obscene articles... are generally prohibited entry" to the United States by U.S. Customs and Border Protection . [13]
At present, there are only two legally protected areas of explicit commercial pornography. The first is "mere nudity" as upheld in "Jenkins v. Georgia, 418 U.S. 153 (1974)" whereby the film Carnal Knowledge was deemed not to be obscene under the constitutional standards announced by Miller. As declared by the judge at trial "The film shows occasional nudity, but nudity alone does not render material obscene under Miller's standards." This was upheld time and again in later cases including "Erznoznik v. City of Jacksonville FL, 422 U.S. 205 (1975)" in which the city of Jacksonville stated that showing films containing nudity when the screen is visible from a public street or place is a punishable offense. The law was determined to be invalid as it was an infringement of First Amendment rights of the movie producer and theater owners. The second is single male to female vaginal-only penetration that does NOT show the actual ejaculation of semen, sometimes referred to as "soft-core" pornography wherein the sexual act and its fulfillment (orgasm) are merely implied to happen rather than explicitly shown.
In June 2006, the U.S. Federal government in the district of Arizona brought a case against JM Productions of Chatsworth, California in order to classify commercial pornography that specifically shows actual semen being ejaculated as obscene. The four films that were the subject of the case are entitled American Bukkake 13 , Gag Factor 15 , Gag Factor 18 and Filthy Things 6 . The case also includes charges of distribution of obscene material (a criminal act under 18 USC § 1465 - "Transportation of obscene matters for sale or distribution") against Five Star DVD for the extra-state commercial distribution of the JM Productions films in question. The case was brought to trial on October 16, 2007. At the first date of trial, the US DoJ decided not to pursue the JM obscenity case any further, leaving the matter without resolution. [14] While the US DoJ decided to abandon its legal pursuit of the JM productions, U.S. District Court Judge Roslyn O. Silver has forced the legal case against Five Star DVD distributors to continue, whereby the legal classification of whether "sperm showing through ejaculation" is an obscene act and thus illegal to produce or distribute will be definitely answered in order to convict Five Star of being guilty of "18 USC 1465 - Transportation of obscene matters for sale or distribution". [15] The jury found that Five Star Video LC and Five Star Video Outlet LC were guilty of "18 USC 1465 - Transportation of obscene matters for sale or distribution" for having shipped JM Productions' film Gag Factor 18 . [16] However, the specific content in that film that the jury deemed to actually fulfill the legal qualification of being "obscene" has not been specifically stated at this point.

Obscenity v. indecency
The differentiation between indecent and obscene material is a particularly difficult one, and a contentious First Amendment issue that has not fully been settled. Similarly, the level of offense (if any) generated by a profane word or phrase depends on region, context, and audience.

Non image-based obscenity cases in the USA

Obscene texts
While most of the obscenity cases in the United States in the past century have revolved around images and films, there have been many cases that dealt with textual works as well.
The classification of "obscene" and thus illegal for production and distribution has been judged on printed text-only stories starting with "Dunlop v. U.S., 165 U.S. 486 (1897)" which upheld a conviction for mailing and delivery of a newspaper called the 'Chicago Dispatch,' containing "obscene, lewd, lascivious, and indecent materials", which was later upheld in several cases. One of these was "A Book Named "John Cleland's Memoirs of a Woman of Pleasure" v. Attorney General of Com. of Massachusetts, 383 U.S. 413 (1966)" wherein the book Fanny Hill , written by John Cleland c. 1760, was judged to be obscene in a proceeding that put the book itself on trial rather than its publisher. Another was "Kaplan v. California, 413 U.S. 115 (1973)" whereby the court most famously determined that "Obscene material in book form is not entitled to any First Amendment protection merely because it has no pictorial content."
However, the book was labeled "erotica" in the 1965 case (206 NE 2d 403) and there a division between erotica and obscenity was made—not all items with erotic content were automatically obscene. Further, the 1965 "John Cleland's 'Memoirs'" case added a further qualification for the proving of "obscenity"—the work in question had to inspire or exhibit "prurient" (that is, "shameful or morbid") interest.
In 1964, the U.S. Supreme Court , in Grove Press, Inc. v. Gerstein , cited Jacobellis v. Ohio (which was decided the same day) and overruled state court findings of obscenity against Henry Miller 's Tropic of Cancer . A copyright infringing "Medusa" edition of the novel was published in New York City in 1940 by Jacob Brussel ; its title page claimed its place of publication to be Mexico. Brussel was eventually sent to jail for three years for the edition, [17] a copy of which is in the Library of Congress .
In 2005, the U.S. Department of Justice formed the Obscenity Prosecution Task Force in a push to prosecute obscenity cases. [18] [19] Red Rose Stories (www.red-rose-stories.com, now defunct), a site dedicated to text-only fantasy stories, became one of many sites targeted by the FBI for shutdown. [20] The government alleged that Red Rose Stories contained depictions of child rape. The publisher pleaded guilty. [21]

Obscene devices
Many U.S. states have had bans on the sale of sex toys , regulating them as obscene devices. For instance, the 1999 Law and Government of Alabama (Ala. Code. § 13A-12-200.1) made it " unlawful to produce, distribute or otherwise sell sexual devices that are marketed primarily for the stimulation of human genital organs ." Alabama claimed that these products were obscene, and that there was " no fundamental right to purchase a product to use in pursuit of having an orgasm ." The ACLU challenged the statute, which was overturned in 2002. A federal judge reinstated the law in 2004. The matter was appealed to the US Supreme Court who in 2007 refused to hear the case, thus the decision of the lower court is enforceable within the state of Alabama. [22] In 2007, a federal appeals court upheld Alabama's law prohibiting the sale of sex toys. [23] The law, the Anti-Obscenity Enforcement Act of 1998, was also upheld by the Supreme Court of Alabama on September 11, 2009. [24]
But other states have seen their sex toy bans ruled unconstitutional in the courts. In 2008 the United States Court of Appeals for the Fifth Circuit ruled a similar Texas statute violated the constitutional right to privacy that was recognized by the U.S. Supreme Court in the Lawrence v. Texas decision. [25] That ruling leaves only Mississippi, Alabama, and Virginia with current bans on the sale of obscene devices. [26] Alabama is the only state where a law prohibiting the sale of sex toys remains on the books. [27]

Criticism
Obscenity law has been criticized in the following areas: [28]
It should be noted that in light of the recent en banc decision of the Third Circuit Court of Appeals, as brought by Judge Lancaster in the original US vs. Extreme Associates case, only the US Supreme Court is allowed to revise its earlier decision that established the Miller decision.
The US Supreme Court refused to hear, effectively rejecting, such modification in August 2006 when the same en banc decision by the Third Circuit was sent to the US Supreme Court for review. [1] Thus the open ended conflicting notes above remain in effect for obscenity prosecutions.

Public funding/public places
Congress passed a law in 1990 that required such organizations such as the National Endowment of the Arts (NEA) and National Associations of Artists Organizations (NAAO) to abide by general decency standards for the "diverse beliefs and values of the American public." [29]
In 1998, Congress made a decision in the case of National Endowment for the Arts vs. Karen Finley , which upheld the general standards and decency law within the United States.
Government owned exhibition spaces are available under the Supreme Court's "public forum" doctrine. This doctrine explains that citizens within the United States have access to display in such public places such as lobbies of public buildings, theatrical productions, etc.
Even with this law in place it is hard for artists who have addressed sexually explicit work in work because of complaints which are generally in the form of "inappropriate for children" or seen as a form of "sexual harassment." Therefore, the arts works are removed and at times there are official "no nudity" policies that are put in place. [30]
When these decisions are taken to court on account of free expression the venues are often looked at to see if they are an actual "designated public forum." If it is then public officials have violated the First Amendment rights on the individuals. The other side is if the court finds that there is "no designated public forum" where government officials have the right to exclude and or censor the work. [30]

Additional restrictions on sexual expression
In the Miller decision the use of the words "contemporary community standards" means that the law evolves along with social mores and norms. This has been shown throughout the booming business of the pornography industry along with commercial pornography by people such as amateurs and publishers of personal websites on the World Wide Web. Indirect government control such as restrictive zoning of adult video stores and nude dancing were put in place because obscenity convictions were harder to come by and not protected by the First Amendment. Similarly a set of rules was put in place to control erotic dancing, where legal, so that all dancers must either wear "pasties" or "g-strings" as shown in the 1991 case of Barnes v. Glen Theatre. [29]

State laws
The laws on pornography are regulated by the state, meaning that there is not a national law for pornography. Many states [ which? ] have restrictions on buying books and magazines of pornography. Between 1995 and 2002, almost half of the states were considering bills to control internet pornography, and more than a quarter of states enacted such laws. [31] In many states, [ which? ] other laws controlling access to pornography exist, such as exposing minors to indecent material.

Censorship in schools, universities, and libraries
Schools, universities, and libraries receive government funds for many purposes, and some of these funds go to censorship of obscenity in these institutions. There are a few different ways in which this is done. One way is by not carrying pornographic or what the government deems obscene material in these places; another is for these places to purchase software that filters the internet activity on campus. An example is the federal Children's Internet Protection Act (CIPA). This mandates that all schools and libraries receiving federal aid for internet connections install a "technology protection measure" (filter) on all computers, whether used by children or adults. There are some states that have passed laws mandating censorship in schools, universities, and libraries even if they are not receiving government aid that would fund censorship in these institutions. These include Arizona, Kentucky, Michigan, Minnesota, South Carolina, and Tennessee. Twenty more states were considering such legislation in 2001–2002. [32]

Child pornography
Child pornography refers to images or films (also known as child abuse images [33] [34] [35] ) and in some cases outside of the United States, writings [35] [36] [37] depicting sexually explicit activities involving a child ; as such, child pornography is a record of child sexual abuse . [38] [39] [40] [41] [42] [43] Abuse of the child occurs during the sexual acts which are recorded in the production of child pornography, [38] [39] [41] [42] [43] [44] [45] and several professors of psychology state that memories of the abuse are maintained as long as visual records exist, are accessed, and are "exploited perversely." [43] [44]
Child pornography is widely considered extremely obscene.

Censorship of film
This is most notably shown with the "X" rating that some films are categorized as. The most notable films given an "X" rating were Deep Throat (1972) and The Devil in Miss Jones (1973). These films show explicit, non-simulated, penetrative sex that was presented as part of a reasonable plot with respectable production values. Some state authorities issued injunctions against such films to protect "local community standards"; in New York the print of Deep Throat was seized mid-run, and the film's exhibitors were found guilty of promoting obscenity. [46] This Film Is Not Yet Rated is a 2006 film which discusses disparities the filmmaker sees in ratings and feedback: between Hollywood and independent films , between homosexual and heterosexual sexual situations, between male and female sexual depictions, and between violence and sexual content. They found that films have also been further censored than their heterosexual, male, white counterparts due to gay sex (even if implied), African American sex, or female pleasure as opposed to male pleasure.

Possession of obscene material
In 1969, the Supreme Court held in Stanley v. Georgia that State laws making mere private possession of obscene material a crime are invalid, [47] at least in the absence of an intention to sell, expose or circulate the material.

See also
WebPage index: 00066
Wikipediocracy
Wikipediocracy is a website for discussion and criticism of Wikipedia . [3] [4] Its members have brought information about Wikipedia's controversies to the attention of the media. The site was founded in March 2012 by users of Wikipedia Review , [5] another site critical of Wikipedia. [6] [7]
The site is "known for digging up dirt on Wikipedia's top brass", wrote reporter Kevin Morris in the Daily Dot . [8] Novelist Amanda Filipacchi wrote in The Wall Street Journal that the site "intelligently discusses and entertainingly lambastes Wikipedia’s problematic practices." [9]

Website user activism
Wikipediocracy contributors have investigated problems, conflicts, and controversies associated with Wikipedia, some being reported by mainstream media. The site's stated mission is "to shine the light of scrutiny into the dark crevices of Wikipedia" and related projects. In a doctoral thesis , Heather Ford , a specialist in Internet policy and law, commented on Wikipediocracy's role, saying, "As Wikipedia’s authority grows, and more groups feel disenfranchised by its processes, the growth of watchdog groups like Wikipediocracy who act as translators of Wikipedia’s complex structures, rules and norms for mainstream media and who begin to give voice to those who feel that they have been excluded from Wikipedia’s representational structures will continue." [10]

Revenge editing
In 2013, Wikipediocracy members contacted Salon.com reporter Andrew Leonard to alert him about the "Qworty fiasco." [11] [12] User Qworty had attracted attention for his provocative comments in a debate on Wikipedia's treatment of female writers. [13] It emerged that many of his past contributions affected the site's treatment of (and targeted rivals of) writer Robert Clark Young . [3] [14] This background information led to Leonard's challenging Young, in an article Revenge, Ego, and the Corruption of Wikipedia , which identified Young as Qworty. Just before the publication of Leonard's article, because of this behavior, Qworty had been banned from editing Wikipedia biographies of living persons. [3] [11]

Discussion of governments
Wikipediocracy contributors' criticisms of Wikipedia have been discussed in news stories covering Jimmy Wales 's relationship with the government of Kazakhstan , [15] [16] [17] the Gibraltarpedia controversy , [18] [19] and an anonymous edit made from a U.S. Senate IP address that labelled whistle-blower Edward Snowden a "traitor". [20] [21]
In May 2014, The Telegraph , working with Wikipediocracy, uncovered evidence identifying the civil servant who had allegedly vandalized the Wikipedia articles on the Hillsborough disaster and Anfield . [22]

Wikimedia Foundation
A Wikipediocracy blog post reported in 2013 that Wikipedia was being vandalized from IP addresses assigned to the Wikimedia Foundation (WMF). [8] [23] Responding to the allegations, WMF spokesman Jay Walsh stated that the IP addresses belonged to WMF servers and were not used by the WMF offices. He stated that the addresses were assigned to some edits by IPs due to a misconfiguration, which was corrected. [8]

Other issues
A Wikipediocracy forum discussion identified the Wikipedia account responsible for a hoax article Wikipedia administrators had recently deleted. The " Bicholim conflict " article described a fictitious 1640–41 Indian civil war. It was awarded Wikipedia's " Good article " status in 2007, and retained it until late 2012, when a Wikipedian checked the article's cited sources and found that none of them appeared to exist. [24]
A September 2013 story resulting from a Wikipediocracy tip-off concerned commercial plastic surgeons editing Wikipedia's plastic surgery articles to promote their services. Concerns with violations of conflict of interest guidelines and the provision of misinformation in the relevant articles had also been raised by Wikipediocracy members on Wikipedia itself. [25]
In January 2014, a Wikipediocracy blog post pointed out that according to official Wikimedia statistics, page views for all major language versions of Wikipedia had experienced significant and unprecedented drops over the course of 2013. The blog post linked these falls to the introduction of the Google Knowledge Graph . [26]
In February 2015, Wikipedia's Arbitration Committee banned a user after finding he had edited to promote the Indian Institute of Planning and Management and added negative material to the article on another university. The user's edits had been noted in Wikipediocracy in December 2013. [27]

See also
WebPage index: 00067
Real life
Real life is a phrase used originally in literature to distinguish between actual and fictional or idealized worlds, and in acting to distinguish between performers and the characters they portray. More recently it has become a popular term on the Internet to describe events, people, activities and interactions occurring offline, or otherwise, not primarily through the medium of the internet.

As distinct from fiction
When used to distinguish from fictional worlds or universes against the consensus reality of the reader, the term has a long history:
In her 1788 work, Original Stories from Real Life; with Conversations Calculated to Regulate the Affections, and Form the Mind to Truth and Goodness , author Mary Wollstonecraft employs the term in her title, representing the work's focus on a middle-class ethos which she viewed as superior to the court culture represented by fairy tales and the values of chance and luck found in chapbook stories for the poor. [2] As phrased by Gary Kelly, writing about the work, "The phrase ‘real life’ strengthens ‘original’, excluding both the artificial and the fictional or imaginary." [3]
Similarly, the phrase can be used to distinguish an actor from a character, e.g. "In real life, he has a British accent" or "In real life, he lives in Los Angeles."
There is a related but slightly distinct usage among role-players and historical reenactors , to distinguish the fantasy or historical context from the actual world and the role-player or actor from the character, e.g. "What do you do in real life?" or "Where do you live in real life?"

As distinct from the Internet
On the Internet, "real life" refers to life offline . Online, the acronym "IRL" stands for "in real life", with the meaning "not on the Internet". [4] For example, while Internet users may speak of having "met" someone that they have contacted via online chat or in an online gaming context, to say that they met someone "in real life" is to say that they literally encountered them in a common physical location. Some, arguing that the Internet is part of real life, prefer to use "away from the keyboard" ( AFK ), e.g. the documentary TPB AFK .
Some sociologists engaged in the study of the Internet have predicted that someday, a distinction between online and real-life worlds may seem "quaint", noting that certain types of online activity, such as sexual intrigues, have already made a full transition to complete legitimacy and "reality". [5]

Related terminology
The initialism "RL" stands for "real life" and "IRL" for "in real life." For example, one can speak of "meeting IRL" (LMIRL) someone whom one has met online. It may also be used to express an inability to use the Internet for a time due to "RL problems." Some Internet users use the idioms " face time ", "meatspace" or "meat world", which contrast with the term " cyberspace ". [6] [7] "Meatspace" has appeared in the Financial Times [8] and in science fiction literature. [9] Some early uses of the term include a post to the Usenet newsgroup austin.public-net in 1993 [10] and an article in The Seattle Times about John Perry Barlow in 1995. [11] The term entered the Oxford English Dictionary in 2000. [12]

Other uses
The phrase is also used to distinguish academic life from work in other sectors, in a manner similar to the term "real world". A person with experience in "real life" or the "real world" has experience beyond book-learning. It may also be used, often pejoratively, to distinguish other insular subcultures , work environments, or lifestyles from more traditional social and professional activities. The terms "real life" and "the real world" may also be used to describe adulthood and the adult world as distinct from childhood and adolescence .

See also
WebPage index: 00068
Harassment
Harassment ( / h ə ˈ r æ s m ə n t / or / ˈ h æ r ə s m ə n t / ) covers a wide range of behaviors of an offensive nature. It is commonly understood as behavior that disturbs or upsets, and it is characteristically repetitive. In the legal sense, it is behavior that appears to be disturbing or threatening. Sexual harassment refers to persistent and unwanted sexual advances, typically in the workplace, where the consequences of refusing are potentially very disadvantageous to the victim.

Etymology
The word is based in English since circa 1618 as a loan word from the French, which was in turn already attested in 1572 meaning torment, annoyance, bother, trouble [1] and later as of 1609 was also referred to the condition of being exhausted, overtired . [2] [3] Of the French verb harasser itself there are the first records in a Latin to French translation of 1527 of Thucydides ’ History of the war that was between the Peloponnesians and the Athenians both in the countries of the Greeks and the Romans and the neighbouring places wherein the translator writes harasser allegedly meaning harceler (to exhaust the enemy by repeated raids); and in the military chant Chanson du franc archer [4] of 1562, where the term is referred to a gaunt jument ( de poil fauveau, tant maigre et harassée : of fawn horsehair, so meagre and …) where it is supposed that the verb is used meaning overtired . [5]
A hypothesis about the origin of the verb harasser is harace / harache , which was used in the 14th century in expressions like courre à la harache (to pursue) and prendre aucun par la harache (to take somebody under constraint). [6] The Französisches Etymologisches Wörterbuch , a German etymological dictionary of the French language (1922–2002) compares phonetically and syntactically both harace and harache to the interjection hare and haro by alleging a pejorative and augmentative form. The latter was an exclamation indicating distress and emergency (recorded since 1180) but is also reported later in 1529 in the expression crier haro sur (to arise indignation over somebody). hare 's use is already reported in 1204 as an order to finish public activities as fairs or markets and later (1377) still as command but referred to dogs. This dictionary suggests a relation of haro / hare with the old lower franconian *hara (here) (as by bringing a dog to heel). [7]
While the pejorative of an exclamation and in particular of such an exclamation is theoretically possible for the first word ( harace ) and maybe phonetically plausible for harache , a semantic, syntactic and phonetic similarity of the verb harasser as used in the first popular attestation (the chant mentioned above) with the word haras should be kept in mind: Already in 1160 haras indicated a group of horses constrained together for the purpose of reproduction and in 1280 it also indicated the enclosure facility itself, where those horses are constrained. [8] The origin itself of harass is thought to be the old Scandinavian hârr with the Romanic suffix –as, which meant grey or dimmish horsehair . Controversial is the etymological relation to the Arabic word for horse whose roman transliteration is faras .
Although the French origin of the word harassment is beyond all question in the Oxford English Dictionary and those dictionaries basing on it, a supposed Old French verb harer should be the origin of the French verb harasser , despite the fact that this verb cannot be found in French etymologic dictionaries like that of the Centre national de ressources textuelles et lexicales ( fr ) or the Trésor de la langue française informatisé ( fr ) (see also their corresponding websites as indicated in the interlinks); since the entry further alleges a derivation from hare , like in the mentioned German etymological dictionary of the French language a possible misprint of harer = har/ass/er = harasser is plausible or cannot be excluded. In those dictionaries the relationship with harassment were an interpretation of the interjection hare as to urge/set a dog on , despite the fact that it should indicate a shout to come and not to go ( hare = hara = here ; cf. above). [9] [10] [11] The American Heritage Dictionary prudently indicates this origin only as possible.

Types

Electronic
Electronic harassment is the unproven belief of the use of electromagnetic waves to harass a victim. Psychologists have identified evidence of auditory hallucinations , delusional disorders , [12] or other mental disorders in online communities supporting those who claim to be targeted. [13] [14]

Landlord
Landlord harassment is the willing creation, by a landlord or his agents, of conditions that are uncomfortable for one or more tenants in order to induce willing abandonment of a rental contract . Such a strategy is often sought because it avoids costly legal expenses and potential problems with eviction . This kind of activity is common in regions where rent control laws exist, but which do not allow the direct extension of rent-controlled prices from one tenancy to the subsequent tenancy, thus allowing landlords to set higher prices. Landlord harassment carries specific legal penalties in some jurisdictions , but enforcement can be very difficult or even impossible in many circumstances. However, when a crime is committed in the process and motives similar to those described above are subsequently proven in court, then those motives may be considered an aggravating factor in many jurisdictions, thus subjecting the offender(s) to a stiffer sentence .

Mobile
Mobile harassment refers to the sending of any type of text message , Text , photo message , video message , or voicemail from a mobile phone that threatens, torments, or humiliates the recipient of these messages. It is a form of cyber bullying .

Online
Harassment directs multiple repeating obscenities and derogatory comments at specific individuals focusing, for example, on the targets' race, religion, nationality, or sexual orientation. This often occurs in chat rooms, through newsgroups, and by sending hate e-mail to interested parties. This may also include stealing photos of the victim and their families, doctoring these photos in offensive ways, and then posting them on social media with the aim of causing emotional distress (see cyber bullying , cyber stalking , hate crime , online predator , and stalking ).

Police
Unfair treatment conducted by law officials, including but not limited to excessive force , profiling , threats , coercion , and racial, ethnic, religious, gender/sexual, age, or other forms of discrimination .

Power
Power harassment is harassment or unwelcome attention of a political nature, often occurring in the environment of a workplace including hospitals, schools and universities. It includes a range of behavior from mild irritation and annoyances to serious abuses which can even involve forced activity beyond the boundaries of the job description. Power harassment is considered a form of illegal discrimination and is a form of political and psychological abuse , and bullying .

Psychological
This is humiliating, intimidating or abusive behavior which is often difficult to detect, leaving no evidence other than victim reports or complaints. This characteristically lowers a person’s self-esteem or causes one torment. This can take the form of verbal comments, engineered episodes of intimidation, aggressive actions or repeated gestures. Falling into this category is workplace harassment by individuals or groups mobbing .
Community-based psychological harassment , meanwhile, is stalking by a group [17] against an individual using repeated distractions that the individual is sensitized to. Media reports of large numbers of coordinated groups stalking individual stalking victims, including a press interview given by an active duty police lieutenant, have described this community-based harassment as gang stalking . [18] [19]

Racial
The targeting of an individual because of their race or ethnicity. The harassment may include words, deeds, and actions that are specifically designed to make the target feel degraded due to their race or ethnicity.

Religious
Verbal, psychological or physical harassment is used against targets because they choose to practice a specific religion. Religious harassment can also include forced and involuntary conversions. [20]

Sexual
Harassment that can happen anywhere but is most common in the workplace and schools. It involves unwanted and unwelcome words, deeds, actions, gestures, symbols, or behaviours of a sexual nature that make the target feel uncomfortable. Gender and sexual orientation harassment fall into this family. When involving children , the use of "gay" or "homo" as a common insult would fall into a category punishable by law [ citation needed ] . The main focus of groups working against sexual harassment has been the protection of women, but the protection of men from sexual harassment by other genders has been coming to light in recent years.

Workplace
Workplace harassment is:
Recently, matters of workplace harassment have gained interest among practitioners and researchers as it is becoming one of the most sensitive areas of effective workplace management. In Oriental countries, it attracted lots of attention from researchers and governments since the 1980s, because a significant source of work stress is associated with aggressive behaviors in the workplace. [23] Third world countries are far behind oriental countries in that there are limited efforts to investigate the questions on workplace harassment. It is almost unseen and the executive leaders (managers) are almost reluctant or unconscious about it in the third world countries. [22] Under occupational health and safety laws around the world, [24] workplace harassment and workplace bullying are identified as being core psychosocial hazards. [25]

Laws

United States
Harassment, under the laws of the United States, is defined as any repeated or continuing un-consented contact that serves no useful purpose beyond creating alarm, annoyance, or emotional distress. In 1964, the United States Congress passed Title VII of the Civil Rights Act which prohibited discrimination at work on the basis of race, color, religion, national origin and sex. This later became the legal basis for early harassment law. The practice of developing workplace guidelines prohibiting harassment was pioneered in 1969, when the U.S. Department of Defense drafted a Human Goals Charter, establishing a policy of equal respect for both sexes. In Meritor Savings Bank v. Vinson , 477 U.S. 57 (1986): the U.S. Supreme Court recognized harassment suits against employers for promoting a sexually hostile work environment. In 2006, U.S.A. President George W. Bush signed a law which prohibited the transmission of annoying messages over the Internet ( aka spamming ) without disclosing the sender's true identity. [26]

New Jersey's Law Against Discrimination ("LAD")
The LAD prohibits employers from discriminating in any job-related action, including recruitment, interviewing, hiring, promotions, discharge, compensation and the terms, conditions and privileges of employment on the basis of any of the law's specified protected categories. These protected categories are race, creed, color, national origin, nationality, ancestry, age, sex (including pregnancy and sexual harassment), marital status, domestic partnership status, affectional or sexual orientation, atypical hereditary cellular or blood trait, genetic information, liability for military service, or mental or physical disability, including HIV/AIDS and related illnesses. The LAD prohibits intentional discrimination based on any of these characteristics. Intentional discrimination may take the form of differential treatment or statements and conduct that reflect discriminatory animus or bias.

Canada
In 1984, the Canadian Human Rights Act prohibited sexual harassment in workplaces under federal jurisdiction.

United Kingdom
In the UK, there are a number of laws protecting people from harassment, including the Protection from Harassment Act 1997 and the Criminal Justice and Public Order Act 1994 .

See also
WebPage index: 00069
Lila Tretikov
Lila Tretikov ( English: /ˈlaɪ̯lə ˈtrɛtɪkɔf/ ), born Olga (Lyalya [1] ) Tretyakova ( Russian : Ольга (Ляля) Третьяко́ва ; January 25, 1978 [2] in Moscow [3] [4] ) is a Russian–American engineer and manager, who was executive director of the Wikimedia Foundation from 2014 to 2016. Born in Moscow, she emigrated to the United States as a teenager and in 1999 began working as a software engineer in California, where she co-authored several software patents and also founded a technology marketing company. A specialist in enterprise software , she was chief information officer and vice president of engineering at SugarCRM Inc. before succeeding Sue Gardner at the Wikimedia Foundation in 2014. On February 25, 2016, Tretikov tendered her resignation, effective March 31, [5] as a result of the WMF's handling of the Knowledge Engine project. [6] [7]

Early life and education
Tretikov is of Russian and Swedish heritage. Her father is a mathematician and her mother was a filmmaker. [8] After moving to New York City at age 15, [9] she learned English while waitressing and attended the University of California, Berkeley , but left before completing her degree. [10] Her majors were computer science and art, and she researched machine learning . [10]

Career
In 1999, Tretikov started her professional career at Sun Microsystems , as an engineer at the Sun-Netscape Alliance , where she worked on the Java server [ clarification needed ] . She then founded GrokDigital, a technology marketing company, and was later appointed chief information officer and vice president of engineering at SugarCRM Inc. [11]
In 2012, she was a Stevie Awards bronze winner in the category for "Female Executive of the Year‍—‌Business Services‍—‌11 to 2,500 Employees‍—‌Computer Hardware & Software". [12] She has co-authored several patents in intelligent data mapping and dynamic language applications. [13] [14]
Tretikov was appointed executive director of the Wikimedia Foundation in May 2014 in succession to Sue Gardner , [15] [16] and took up the post on June 1, 2014. She had edited Wikipedia only once before her appointment. [13] [17]
Tretikov is also on the boards of OpenEd [18] and Rackspace [19] and is an advisor to the board of Zamurai Corporation. [ citation needed ]
Tretikov resigned from the Wikimedia Foundation, as a result of the WMF's controversial Knowledge Engine project and disagreements with the staff, [6] [7] with her last day being March 31, 2016 [5] (succeeded by Katherine Maher in March 2016). On March 16, 2016, it was announced that Tretikov had been invited by the World Economic Forum to join its " Young Global Leaders " community. [20]
WebPage index: 00070
Free software
Free software , freedom-respecting software , or software libre [1] [2] is computer software distributed under terms that allow the software users to run the software for any purpose as well as to study, change, and distribute the software and any adapted versions. [3] [4] [5] [6] [7] Free software is a matter of liberty , not price: users, individually or collectively, are free to do what they want with it, including the freedom to redistribute the software free of charge , or to sell it, or charge for related services such as support or warranty for profit. [8]
The right to study and modify software entails availability of the software source code to its users. While this right is often called 'access to source code', the Free Software Foundation recommends to avoid using the word 'access' in this context [9] because it is misleading and may make people believe that they may have a copy of the source code unconditionally. This right is only conditional on the person actually having a copy of the software, i.e. being a software user .
Richard Stallman used the already existing term free software [10] when he launched the GNU Project —a collaborative effort to create a freedom-respecting operating system —and the Free Software Foundation (FSF). The FSF's Free Software Definition [5] states that users of free software are free because they do not need to ask for permission to use the software. [11]

Alternatives and context
Free software thus differs from
For computer programs that are covered by copyright law, software freedom is achieved with a software license , by which the author grants users the aforementioned freedom. Software that is not covered by copyright law, such as software in the public domain , is free if the source code is in the public domain, or otherwise available without restrictions.
Proprietary software, including freeware, use restrictive software licences or EULAs and usually do not provide access to the source code. Users are thus prevented from changing the software, and this results in the user relying on the publisher to provide updates, help, and support. This situation is called vendor lock-in . Users often may not reverse engineer , modify, or redistribute proprietary software. [12] [13] Other legal and technical aspects, such as software patents and digital rights management may restrict users in exercising their rights, and thus prevent software from being free. [14] Free software may be developed collaboratively by volunteer computer programmers or by corporations; as part of a commercial, for-profit activity or not.

History
From the 1950s up until the early 1970s, it was normal for computer users to have the software freedoms associated with free software, which was typically public domain software . [10] Software was commonly shared by individuals who used computers and by hardware manufacturers who welcomed the fact that people were making software that made their hardware useful. Organizations of users and suppliers, for example, SHARE , were formed to facilitate exchange of software. As software was often written in an interpreted language such as BASIC , the source code was distributed to use a software. Software was also shared and distributed as printed source code ( Type-in program ) in computer magazines (like Creative Computing , SoftSide , Compute! , Byte etc) and books, like the bestseller BASIC Computer Games . [15] By the early 1970s, the picture changed: software costs were dramatically increasing, a growing software industry was competing with the hardware manufacturer's bundled software products (free in that the cost was included in the hardware cost), leased machines required software support while providing no revenue for software, and some customers able to better meet their own needs did not want the costs of "free" software bundled with hardware product costs. In United States vs. IBM , filed January 17, 1969, the government charged that bundled software was anti-competitive . [16] While some software might always be free, there would henceforth be a growing amount of software produced primarily for sale. In the 1970s and early 1980s, the software industry began using technical measures (such as only distributing binary copies of computer programs ) to prevent computer users from being able to study or adapt the software as they saw fit. In 1980, copyright law was extended to computer programs.
In 1983, Richard Stallman , one of the original authors of the popular Emacs program and a longtime member of the hacker community at the MIT Artificial Intelligence Laboratory , announced the GNU project , the purpose of which was to produce a completely non-proprietary Unix-compatible operating system, saying that he had become frustrated with the shift in climate surrounding the computer world and its users. In his initial declaration of the project and its purpose, he specifically cited as a motivation his opposition to being asked to agree to non-disclosure agreements and restrictive licenses which prohibited the free sharing of potentially profitable in-development software, a prohibition directly contrary to the traditional hacker ethic . Software development for the GNU operating system began in January 1984, and the Free Software Foundation (FSF) was founded in October 1985. He developed a free software definition and the concept of " copyleft ", designed to ensure software freedom for all. Some non-software industries are beginning to use techniques similar to those used in free software development for their research and development process; scientists, for example, are looking towards more open development processes, and hardware such as microchips are beginning to be developed with specifications released under copyleft licenses (see the OpenCores project, for instance). Creative Commons and the free culture movement have also been largely influenced by the free software movement.

1980s: Foundation of the GNU project
In 1983, Richard Stallman , longtime member of the hacker community at the MIT Artificial Intelligence Laboratory , announced the GNU project, saying that he had become frustrated with the effects of the change in culture of the computer industry and its users. [17] Software development for the GNU operating system began in January 1984, and the Free Software Foundation (FSF) was founded in October 1985. An article outlining the project and its goals was published in March 1985 titled the GNU Manifesto . The manifesto included significant explanation of the GNU philosophy, Free Software Definition and " copyleft " ideas.

1990s: Release of the Linux kernel
The Linux kernel , started by Linus Torvalds , was released as freely modifiable source code in 1991. The first licence was a proprietary software licence. However, with version 0.12 in February 1992, he relicensed the project under the GNU General Public License . [18] Much like Unix, Torvalds' kernel attracted the attention of volunteer programmers. FreeBSD and NetBSD (both derived from 386BSD ) were released as free software when the USL v. BSDi lawsuit was settled out of court in 1993. OpenBSD forked from NetBSD in 1995. Also in 1995, The Apache HTTP Server , commonly referred to as Apache, was released under the Apache License 1.0 .

Naming
The FSF recommends using the term "free software" rather than " open-source software " because, as they state in a paper on Free Software philosophy, the latter term and the associated marketing campaign focuses on the technical issues of software development, avoiding the issue of user freedoms. The FSF also notes that "Open Source" has exactly one specific meaning in common English, namely that "you can look at the source code." Stallman states that while the term "Free Software" can lead to two different interpretations, one of them is consistent with FSF definition of Free Software so there is at least some chance that it could be understood properly, unlike the term "Open Source". [19] Stallman has also stated that considering the practical advantages of free software is like considering the practical advantages of not being handcuffed in that it is not necessary for an individual to consider practical reasons in order to realize that being handcuffed restricts their freedom. [20] " Libre " is often used to avoid the ambiguity of the word "free" in English language and the ambiguity with the older usage of "free software" as public domain software; [10] see Gratis versus libre .

Definition and the Four Freedoms
The first formal definition of free software was published by FSF in February 1986. [21] That definition, written by Richard Stallman, is still maintained today and states that software is free software if people who receive a copy of the software have the following four freedoms. [22] [23] The numbering begins with zero, not only as a spoof on the common usage of zero-based numbering in programming languages, but also because "Freedom 0" was not initially included in the list, but later added first in the list as it was considered very important.
Freedoms 1 and 3 require source code to be available because studying and modifying software without its source code can range from highly impractical to nearly impossible.
Thus, free software means that computer users have the freedom to cooperate with whom they choose, and to control the software they use. To summarize this into a remark distinguishing libre (freedom) software from gratis (zero price) software, the Free Software Foundation says: "Free software is a matter of liberty, not price. To understand the concept, you should think of 'free' as in ' free speech ', not as in 'free beer ' ". [22] See Gratis versus libre .
In the late 1990s, other groups published their own definitions that describe an almost identical set of software. The most notable are Debian Free Software Guidelines published in 1997, [24] and the Open Source Definition , published in 1998.
The BSD -based operating systems, such as FreeBSD , OpenBSD , and NetBSD , do not have their own formal definitions of free software. Users of these systems generally find the same set of software to be acceptable, but sometimes see copyleft as restrictive. They generally advocate permissive free software licenses , which allow others to use the software as they wish, without being legally forced to provide the source code. Their view is that this permissive approach is more free. The Kerberos , X11 , and Apache software licenses are substantially similar in intent and implementation.

Examples
The Free Software Directory maintains a large database of free software packages. Some of the best-known examples include the Linux kernel , the BSD and Linux operating systems, the GNU Compiler Collection and C library ; the MySQL relational database; the Apache web server; and the Sendmail mail transport agent. Other influential examples include the Emacs text editor; the GIMP raster drawing and image editor; the X Window System graphical-display system; the LibreOffice office suite; and the TeX and LaTeX typesetting systems.

Licensing
All free software licenses must grant users all the freedoms discussed above. However, unless the applications' licenses are compatible, combining programs by mixing source code or directly linking binaries is problematic, because of license technicalities . Programs indirectly connected together may avoid this problem.
The majority of free software falls under a small set of licenses. The most popular of these licenses are: [25] [26]
The Free Software Foundation and the Open Source Initiative both publish lists of licenses that they find to comply with their own definitions of free software and open-source software respectively:
The FSF list is not prescriptive: free licenses can exist that the FSF has not heard about, or considered important enough to write about. So it's possible for a license to be free and not in the FSF list. The OSI list only lists licenses that have been submitted, considered and approved. All open-source licenses must meet the Open Source Definition in order to be officially recognized as open source software. Free software on the other hand is a more informal classification that does not rely on official recognition. Nevertheless, software licensed under licenses that do not meet the Free Software Definition cannot rightly be considered free software.
Apart from these two organizations, the Debian project is seen by some to provide useful advice on whether particular licenses comply with their Debian Free Software Guidelines . Debian doesn't publish a list of approved licenses, so its judgments have to be tracked by checking what software they have allowed into their software archives. That is summarized at the Debian web site. [27]
It is rare that a license announced as being in-compliance with the FSF guidelines does not also meet the Open Source Definition , although the reverse is not necessarily true (for example, the NASA Open Source Agreement is an OSI-approved license, but non-free according to FSF).
There are different categories of free software.

Security and reliability
There is debate over the security of free software in comparison to proprietary software, with a major issue being security through obscurity . A popular quantitative test in computer security is to use relative counting of known unpatched security flaws. Generally, users of this method advise avoiding products that lack fixes for known security flaws, at least until a fix is available.
Free software advocates strongly believe that this methodology is biased by counting more vulnerabilities for the free software, since its source code is accessible and its community is more forthcoming about what problems exist, [35] (This is called "Security Through Disclosure" [36] ) and proprietary software can have undisclosed societal drawbacks, such as disenfranchising less fortunate would-be users of free programs. As users can analyse and trace the source code, many more people with no commercial constraints can inspect the code and find bugs and loopholes than a corporation would find practicable. According to Richard Stallman, user access to the source code makes deploying free software with undesirable hidden spyware functionality far more difficult than for proprietary software. [37]
Some quantitative studies have been done on the subject. [38] [39] [40] [41]

Binary blobs and other proprietary software
In 2006, OpenBSD started the first campaign against the use of binary blobs in kernels . Blobs are usually freely distributable device drivers for hardware from vendors that do not reveal driver source code to users or developers. This restricts the users' freedom effectively to modify the software and distribute modified versions. Also, since the blobs are undocumented and may have bugs , they pose a security risk to any operating system whose kernel includes them. The proclaimed aim of the campaign against blobs is to collect hardware documentation that allows developers to write free software drivers for that hardware, ultimately enabling all free operating systems to become or remain blob-free.
The issue of binary blobs in the Linux kernel and other device drivers motivated some developers in Ireland to launch gNewSense , a Linux based distribution with all the binary blobs removed. The project received support from the Free Software Foundation and stimulated the creation, headed by the Free Software Foundation Latin America , of the Linux-libre kernel. [42] As of October 2012, Trisquel is the most popular FSF endorsed Linux distribution ranked by Distrowatch (over 12 months). [43]

Business model
Since free software may be freely redistributed, it is generally available at little or no fee. Free software business models [44] are usually based on adding value such as applications, support, training, customization, integration, or certification. At the same time, some business models that work with proprietary software are not compatible with free software, such as those that depend on the user to pay for a license in order to lawfully use the software product.
Fees are usually charged for distribution on compact discs and bootable USB drives, or for services of installing or maintaining the operation of free software. Development of large, commercially used free software is often funded by a combination of user donations, corporate contributions, and tax money. Depending on the license type, free software can be embedded in commercial products, too. [44] The SELinux project at the United States National Security Agency is an example of a federally funded free software project.
In practice, for software to be distributed as free software, the source code , a human-readable form of the program from which an executable form is produced, must be accessible to the recipient along with a document granting the same rights to free software under which it was published. Such a document is either a free software license or the release of the source code into the public domain .
Selling software under any free software licence is permissible, as is commercial use. This is true for permissive licences , such as the BSD licence, [45] [46] or copyleft licences such as the GNU GPL .
The Free Software Foundation encourages selling free software. As the Foundation has written, "Distributing free software is an opportunity to raise funds for development. Don't waste it!". [47] For example the GNU GPL that is the Free Software Foundation's license states that "[the user] may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee." [48]
Microsoft CEO Steve Ballmer stated in 2001 that "Open source is not available to commercial companies. The way the license is written, if you use any open-source software, you have to make the rest of your software open source." [49] This misunderstanding is based on a requirement of copyleft licenses (like the GPL) that if one distributes modified versions of software, they must release the source and use the same license. This requirement does not extend to other software from the same developer. The claim of incompatibility between commercial companies and Free Software is also a misunderstanding. There are several large companies, e.g. Red Hat and IBM , which do substantial commercial business in the development of Free Software.
Under the free software business model [ further explanation needed ] , free software vendors may charge a fee for distribution and offer pay support and software customization services. Proprietary software uses a different business model, where a customer of the proprietary software pays a fee for a license to use the software. This license may grant the customer the ability to configure some or no parts of the software themselves. Often some level of support is included in the purchase of proprietary software, but additional support services (especially for enterprise applications) are usually available for an additional fee. Some proprietary software vendors will also customize software for a fee. [50]

 Economical aspects and adoption
Free software played a significant part in the development of the Internet, the World Wide Web and the infrastructure of dot-com companies . [53] [54] Free software allows users to cooperate in enhancing and refining the programs they use; free software is a pure public good rather than a private good . Companies that contribute to free software increase commercial innovation . [55]
The economic viability of free software has been recognized by large corporations such as IBM , Red Hat , and Sun Microsystems . [58] [59] [60] [61] [62] Many companies whose core business is not in the IT sector choose free software for their Internet information and sales sites, due to the lower initial capital investment and ability to freely customize the application packages. Most companies in the software business include free software in their commercial products if the licenses allow that. [44]
Free software is generally available at no cost and can result in permanently lower TCO costs compared to proprietary software . [63] With free software, businesses can fit software to their specific needs by changing the software themselves or by hiring programmers to modify it for them. Free software often has no warranty, and more importantly, generally does not assign legal liability to anyone. However, warranties are permitted between any two parties upon the condition of the software and its usage. Such an agreement is made separately from the free software license.
A report by Standish Group estimates that adoption of free software has caused a drop in revenue to the proprietary software industry by about $60 billion per year. [64] In spite of this, Eric S. Raymond argues that the term free software is too ambiguous and intimidating for the business community. Raymond promotes the term open-source software as a friendlier alternative for the business and corporate world. [65]

See also
WebPage index: 00071
Macro (computer science)
A macro (short for "macroinstruction", from Greek μακρός 'long') in computer science is a rule or pattern that specifies how a certain input sequence (often a sequence of characters ) should be mapped to a replacement output sequence (also often a sequence of characters) according to a defined procedure. The mapping process that instantiates (transforms) a macro use into a specific sequence is known as macro expansion . A facility for writing macros may be provided as part of a software application or as a part of a programming language . In the former case, macros are used to make tasks using the application less repetitive. In the latter case, they are a tool that allows a programmer to enable code reuse or even to design domain-specific languages .
Macros are used to make a sequence of computing instructions available to the programmer as a single program statement, making the programming task less tedious and less error-prone. [1] [2] (Thus, they are called "macros" because a "big" block of code can be expanded from a "small" sequence of characters.) Macros often allow positional or keyword parameters that dictate what the conditional assembler program generates and have been used to create entire programs or program suites according to such variables as operating system , platform or other factors. The term derives from " macro instruction ", and such expansions were originally used in generating assembly language code.

Keyboard and mouse macros
Keyboard macros and mouse macros allow short sequences of keystrokes and mouse actions to transform into other, usually more time-consuming, sequences of keystrokes and mouse actions. In this way, frequently used or repetitive sequences of keystrokes and mouse movements can be automated . Separate programs for creating these macros are called macro recorders .
During the 1980s, macro programs – originally SmartKey , then SuperKey , KeyWorks, Prokey – were very popular, first as a means to automatically format screenplays , then for a variety of user input tasks. These programs were based on the TSR ( terminate and stay resident ) mode of operation and applied to all keyboard input, no matter in which context it occurred. They have to some extent fallen into obsolescence following the advent of mouse-driven user interface and the availability of keyboard and mouse macros in applications such as word processors and spreadsheets , making it possible to create application-sensitive keyboard macros.
Keyboard macros have in more recent times come to life as a method of exploiting the economy of massively multiplayer online role-playing games (MMORPGs). By tirelessly performing a boring, repetitive, but low risk action, a player running a macro can earn a large amount of the game's currency or resources. This effect is even larger when a macro-using player operates multiple accounts simultaneously, or operates the accounts for a large amount of time each day. As this money is generated without human intervention, it can dramatically upset the economy of the game. For this reason, use of macros is a violation of the TOS or EULA of most MMORPGs, and administrators of MMORPGs fight a continual war to identify and punish macro users. [3]

Application macros and scripting
Keyboard and mouse macros that are created using an application's built-in macro features are sometimes called application macros . They are created by carrying out the sequence once and letting the application record the actions. An underlying macro programming language, most commonly a scripting language , with direct access to the features of the application may also exist.
The programmers' text editor, Emacs , (short for "editing macros") follows this idea to a conclusion. In effect, most of the editor is made of macros. Emacs was originally devised as a set of macros in the editing language TECO ; it was later ported to dialects of Lisp.
Another programmers' text editor, Vim (a descendant of vi ), also has full implementation of macros. It can record into a register (macro) what a person types on the keyboard and it can be replayed or edited just like VBA macros for Microsoft Office. Vim also has a scripting language called Vimscript [4] to create macros.
Visual Basic for Applications (VBA) is a programming language included in Microsoft Office up to Office 2013. However, its function has evolved from and replaced the macro languages that were originally included in some of these applications.

Macro virus
VBA has access to most Microsoft Windows system calls and executes when documents are opened. This makes it relatively easy to write computer viruses in VBA, commonly known as macro viruses . In the mid-to-late 1990s, this became one of the most common types of computer virus. However, during the late 1990s and to date, Microsoft has been patching and updating their programs. In addition, current anti-virus programs immediately counteract such attacks.

Text-substitution macros
Languages such as C and assembly language have rudimentary macro systems, implemented as preprocessors to the compiler or assembler. C preprocessor macros work by simple textual search-and-replace at the token, rather than the character level. A classic use of macros is in the computer typesetting system TeX and its derivatives, where most of the functionality is based on macros. MacroML is an experimental system that seeks to reconcile static typing and macro systems. Nemerle has typed syntax macros, and one productive way to think of these syntax macros is as a multi-stage computation . Other examples:

Embeddable languages
Some languages, such as PHP , can be embedded in free-format text, or the source code of other languages. The mechanism by which the code fragments are recognised (for instance, being bracketed by <?php and ?> ) is similar to a textual macro language, but they are much more powerful, fully featured languages.

Procedural macros
Macros in the PL/I language are written in a subset of PL/I itself: the compiler executes " preprocessor statements" at compilation time, and the output of this execution forms part of the code that is compiled. The ability to use a familiar procedural language as the macro language gives power much greater than that of text substitution macros, at the expense of a larger and slower compiler.
Frame technology 's frame macros have their own command syntax but can also contain text in any language. Each frame is both a generic component in a hierarchy of nested subassemblies, and a procedure for integrating itself with its subassembly frames (a recursive process that resolves integration conflicts in favor of higher level subassemblies). The outputs are custom documents, typically compilable source modules. Frame technology can avoid the proliferation of similar but subtly different components, an issue that has plagued software development since the invention of macros and subroutines.
Most assembly languages have less powerful procedural macro facilities, for example allowing a block of code to be repeated N times for loop unrolling ; but these have a completely different syntax from the actual assembly language.

Syntactic macros
Macro systems—such as the C preprocessor described earlier—that work at the level of lexical tokens cannot preserve the lexical structure reliably. Syntactic macro systems work instead at the level of abstract syntax trees , and preserve the lexical structure of the original program. The most widely used implementations of syntactic macro systems are found in Lisp -like languages such as Common Lisp , Clojure , Scheme , ISLISP and Racket . These languages are especially suited for this style of macro due to their uniform, parenthesized syntax (known as S-expressions ). In particular, uniform syntax makes it easier to determine the invocations of macros. Lisp macros transform the program structure itself, with the full language available to express such transformations. While syntactic macros are often found in Lisp-like languages, they are also available in other languages such as Prolog , Dylan , Scala , Nemerle , Rust , Elixir , Haxe , [5] and Python . [6] They are also available as third-party extensions to JavaScript [7] and C# . [8]

Early Lisp macros
Before Lisp had macros, it had so-called FEXPRs , function-like operators whose inputs were not the values computed by the arguments but rather the syntactic forms of the arguments, and whose output were values to be used in the computation. In other words, FEXPRs were implemented at the same level as EVAL, and provided a window into the meta-evaluation layer. This was generally found to be a difficult model to reason about effectively. [9]
In 1963 Timothy Hart proposed adding macros to Lisp 1.5 in AI Memo 57: MACRO Definitions for LISP. [10]

Hygienic macros
In the mid-eighties, a number of papers [11] [12] introduced the notion of hygienic macro expansion ( syntax-rules ), a pattern-based system where the syntactic environments of the macro definition and the macro use are distinct, allowing macro definers and users not to worry about inadvertent variable capture (cf. referential transparency ). Hygienic macros have been standardized for Scheme in both the R5RS and R6RS standards. The upcoming R7RS standard will also include hygienic macros. A number of competing implementations of hygienic macros exist such as syntax-rules , syntax-case , explicit renaming, and syntactic closures. Both syntax-rules and syntax-case have been standardized in the Scheme standards.
Recently, Racket has combined the notions of hygienic macros with a " tower of evaluators ", so that the syntactic expansion time of one macro system is the ordinary runtime of another block of code, [13] and showed how to apply interleaved expansion and parsing in a non-parenthesized language. [14]
A number of languages other than Scheme either implement hygienic macros or implement partially hygienic systems. Examples include Scala , Julia, Dylan , and Nemerle .

Applications
Felleisen conjectures [16] that these three categories make up the primary legitimate uses of macros in such a system. Others have proposed alternative uses of macros, such as anaphoric macros in macro systems that are unhygienic or allow selective unhygienic transformation.
The interaction of macros and other language features has been a productive area of research. For example, components and modules are useful for large-scale programming, but the interaction of macros and these other constructs must be defined for their use together. Module and component-systems that can interact with macros have been proposed for Scheme and other languages with macros. For example, the Racket language extends the notion of a macro system to a syntactic tower, where macros can be written in languages including macros, using hygiene to ensure that syntactic layers are distinct and allowing modules to export macros to other modules.

Macros for machine-independent software
Macros are normally used to map a short string (macro invocation) to a longer sequence of instructions. Another, less common, use of macros is to do the reverse: to map a sequence of instructions to a macro string. This was the approach taken by the STAGE2 Mobile Programming System, which used a rudimentary macro compiler (called SIMCMP) to map the specific instruction set of a given computer to counterpart machine-independent macros. Applications (notably compilers) written in these machine-independent macros can then be run without change on any computer equipped with the rudimentary macro compiler. The first application run in such a context is a more sophisticated and powerful macro compiler, written in the machine-independent macro language. This macro compiler is applied to itself, in a bootstrap fashion, to produce a compiled and much more efficient version of itself. The advantage of this approach is that complex applications can be ported from one computer to a very different computer with very little effort (for each target machine architecture, just the writing of the rudimentary macro compiler). [17] [18] The advent of modern programming languages, notably C , for which compilers are available on virtually all computers, has rendered such an approach superfluous. This was, however, one of the first instances (if not the first) of compiler bootstrapping .

See also
WebPage index: 00072
GNU General Public License
The GNU General Public License ( GNU GPL or GPL ) is a widely used free software license , which guarantees end users the freedom to run, study, share and modify the software. [6] The license was originally written by Richard Stallman of the Free Software Foundation (FSF) for the GNU Project , and grants the recipients of a computer program the rights of the Free Software Definition . [7] The GPL is a copyleft license, which means that derivative work can only be distributed under the same license terms. This is in distinction to permissive free software licenses , of which the BSD licenses and the MIT License are widely used examples. GPL was the first copyleft license for general use.
Historically, the GPL license family has been one of the most popular software licenses in the free and open-source software domain. [6] [8] [9] [10] [11] [12] [13] Prominent free software programs licensed under the GPL include the Linux kernel and the GNU Compiler Collection (GCC). David A. Wheeler argues that the copyleft provided by the GPL was crucial to the success of Linux -based systems, giving the programmers who contributed to the kernel the assurance that their work would benefit the whole world and remain free, rather than being exploited by software companies that would not have to give anything back to the community. [14]
In 2007, the third version of the license (GNU GPLv3) was released to address some perceived problems with the second version (GNU GPLv2) that were discovered during its long-time usage. To keep the license up to date, the GPL license includes an optional "any later version" clause, allowing users to choose between the original terms or the terms in new versions as updated by the FSF. Developers can omit it when licensing their software; for instance the Linux kernel is licensed under GPLv2 without the "any later version" clause. [15] [16]

History
The GPL was written by Richard Stallman in 1989, for use with programs released as part of the GNU project. The original GPL was based on a unification of similar licenses used for early versions of GNU Emacs (1985), [17] the GNU Debugger and the GNU C Compiler . [18] These licenses contained similar provisions to the modern GPL, but were specific to each program, rendering them incompatible, despite being the same license. [19] Stallman's goal was to produce one license that could be used for any project, thus making it possible for many projects to share code.
The second version of the license, version 2, was released in 1991. Over the following 15 years, members of the free software community became concerned over problems in the GPLv2 license that could let someone exploit GPL-licensed software in ways contrary to the license's intent. [20] These problems included tivoization (the inclusion of GPL-licensed software in hardware that refuses to run modified versions of its software), compatibility issues similar to those of the Affero General Public License —and patent deals between Microsoft and distributors of free and open source software, which some viewed as an attempt to use patents as a weapon against the free software community.
Version 3 was developed to attempt to address these concerns and was officially released on 29 June 2007. [21]

Version 1
Version 1 of the GNU GPL, [22] released on 25 February 1989, [23] prevented what were then the two main ways that software distributors restricted the freedoms that define free software. The first problem was that distributors may publish binary files only—executable, but not readable or modifiable by humans. To prevent this, GPLv1 stated that any vendor distributing binaries must also make the human-readable source code available under the same licensing terms (Sections 3a and 3b of the license).
The second problem was that distributors might add restrictions, either to the license, or by combining the software with other software that had other restrictions on distribution. The union of two sets of restrictions would apply to the combined work, thus adding unacceptable restrictions. To prevent this, GPLv1 stated that modified versions, as a whole, had to be distributed under the terms in GPLv1 (Sections 2b and 4 of the license). Therefore, software distributed under the terms of GPLv1 could be combined with software under more permissive terms, as this would not change the terms under which the whole could be distributed. However, software distributed under GPLv1 could not be combined with software distributed under a more restrictive license, as this would conflict with the requirement that the whole be distributable under the terms of GPLv1.

Version 2
According to Richard Stallman, the major change in GPLv2 was the "Liberty or Death" clause, as he calls it [19] – Section 7. The section says that licensees may distribute a GPL-covered work only if they can satisfy all of the license's obligations, despite any other legal obligations they might have. In other words, the obligations of the license may not be severed due to conflicting obligations. This provision is intended to discourage any party from using a patent infringement claim or other litigation to impair users' freedom under the license.
By 1990, it was becoming apparent that a less restrictive license would be strategically useful for the C library and for software libraries that essentially did the job of existing proprietary ones; [24] when version 2 of the GPL (GPLv2) was released in June 1991, therefore, a second license – the Library General Public License – was introduced at the same time and numbered with version 2 to show that both were complementary. The version numbers diverged in 1999 when version 2.1 of the LGPL was released, which renamed it the GNU Lesser General Public License to reflect its place in the philosophy.
Most commonly "GPLv2 or any later version" is stated by users of the license, to allow upgrading to GPLv3. See next section for details.

Version 3
In late 2005, the Free Software Foundation (FSF) announced work on version 3 of the GPL (GPLv3). On 16 January 2006, the first "discussion draft" of GPLv3 was published, and the public consultation began. The public consultation was originally planned for nine to fifteen months but finally stretched to eighteen months with four drafts being published. The official GPLv3 was released by FSF on 29 June 2007. GPLv3 was written by Richard Stallman, with legal counsel from Eben Moglen and the Software Freedom Law Center . [25]
According to Stallman, the most important changes are in relation to software patents , free software license compatibility, the definition of "source code", and hardware restrictions on software modification (" tivoization "). [25] [26] Other changes relate to internationalization, how license violations are handled, and how additional permissions can be granted by the copyright holder.
It also adds a provision that "strips" Digital Rights Management (DRM) of its legal value, so people can break anything a court might recognize as DRM on GPL software without breaking laws like the DMCA . [27]
The public consultation process was coordinated by the Free Software Foundation with assistance from Software Freedom Law Center, Free Software Foundation Europe , [28] and other free software groups. Comments were collected from the public via the gplv3.fsf.org web portal. [29] That portal runs purpose-written software called stet .
During the public consultation process, 962 comments were submitted for the first draft. [30] By the end, a total of 2,636 comments had been submitted. [31] [32] [33]
The third draft was released on 28 March 2007. [34] This draft included language intended to prevent patent-related agreements like the controversial Microsoft-Novell patent agreement and restricts the anti-tivoization clauses to a legal definition of a "User" or "consumer product". It also explicitly removed the section on "Geographical Limitations", whose probable removal had been announced at the launch of the public consultation.
The fourth discussion draft, [35] which was the last, was released on 31 May 2007. It introduced Apache License version 2.0 compatibility (prior versions are incompatible), clarified the role of outside contractors, and made an exception to avoid the perceived problems of a Microsoft–Novell style agreement, saying in Section 11 paragraph 6 that:
This aims to make future such deals ineffective. The license is also meant to cause Microsoft to extend the patent licenses it grants to Novell customers for the use of GPLv3 software to all users of that GPLv3 software; this is possible only if Microsoft is legally a "conveyor" of the GPLv3 software. [36] [37]
Also, early drafts of GPLv3 let licensors add an Affero -like requirement that would have plugged the ASP loophole in the GPL . [38] [39] As there were concerns expressed about the administrative costs of checking code for this additional requirement, it was decided to keep the GPL and the Affero license separated. [40]
Others, notably some high-profile developers of the Linux kernel , for instance Linus Torvalds , Greg Kroah-Hartman and Andrew Morton , commented to the mass media and made public statements about their objections to parts of discussion drafts 1 and 2. [41] The kernel developers referred to GPLv3 draft clauses regarding DRM / Tivoization , patents and "additional restrictions" and warned a Balkanisation of the "Open Source Universe". [41] [42] Linus Torvalds, who decided to not adopt the GPLv3 for the Linux kernel, [15] reiterated his criticism even years later. [43] [44]
GPLv3 improves compatibility with several open source software licenses such as Apache License, version 2.0, and the GNU Affero General Public License, which GPLv2 could not be combined with. [45] But on the downside, GPLv3 software can only be combined and share code with GPLv2 software if the used GPLv2 license has the optional "or later" clause and the software is upgraded to GPLv3. While the "GPLv2 or any later version" clause is considered by FSF as the most common form of licensing GPLv2 software, [46] for example Toybox developer Rob Landley described it as a lifeboat clause . [47] [48] Software projects licensed with the optional "or later" clause include the GNU Project , while a prominent example without the clause is the Linux kernel. [15]
The final version of the license text was published on 29 June 2007. [49]

Terms and conditions
The terms and conditions of the GPL must be made available to anybody receiving a copy of the work that has a GPL applied to it ("the licensee"). Any licensee who adheres to the terms and conditions is given permission to modify the work, as well as to copy and redistribute the work or any derivative version. The licensee is allowed to charge a fee for this service, or do this free of charge. This latter point distinguishes the GPL from software licenses that prohibit commercial redistribution. The FSF argues that free software should not place restrictions on commercial use, [50] and the GPL explicitly states that GPL works may be sold at any price.
The GPL additionally states that a distributor may not impose "further restrictions on the rights granted by the GPL". This forbids activities such as distributing of the software under a non-disclosure agreement or contract.
The fourth section for version 2 of the license and the seventh section of version 3 require that programs distributed as pre-compiled binaries be accompanied by a copy of the source code, a written offer to distribute the source code via the same mechanism as the pre-compiled binary, or the written offer to obtain the source code that the user got when they received the pre-compiled binary under the GPL. The second section of version 2 and the fifth section of version 3 also require giving "all recipients a copy of this License along with the Program". Version 3 of the license allows making the source code available in additional ways in fulfillment of the seventh section. These include downloading source code from an adjacent network server or by peer-to-peer transmission, provided that is how the compiled code was available and there are "clear directions" on where to find the source code.
The FSF does not hold the copyright for a work released under the GPL, unless an author explicitly assigns copyrights to the FSF (which seldom happens except for programs that are part of the GNU project). Only the individual copyright holders have the authority to sue when a license violation takes place.

Use of licensed software
Software under the GPL may be run for all purposes, including commercial purposes and even as a tool for creating proprietary software , for example when using GPL-licensed compilers . [51] Users or companies who distribute GPL-licensed works (e.g. software), may charge a fee for copies or give them free of charge. This distinguishes the GPL from shareware software licenses that allow copying for personal use but prohibit commercial distribution, or proprietary licenses where copying is prohibited by copyright law . The FSF argues that freedom-respecting free software should also not restrict commercial use and distribution (including redistribution): [50] the GPL explicitly states that GPL works may be sold at any price.
In purely private (or internal) use—with no sales and no distribution—the software code may be modified and parts reused without requiring the source code to be released. For sales or distribution, the entire source code need to be made available to end users, including any code changes and additions—in that case, copyleft is applied to ensure that end users retain the freedoms defined above. [52]
However, software running as an application program under a GPL-licensed operating system such as Linux is not required to be licensed under GPL or to be distributed with source-code availability—the licensing depends only on the used libraries and software components and not on the underlying platform. [53] For example, if a program consists only of own original custom software, or is combined with source code from other software components , [54] then the own custom software components need not be licensed under GPL and need not make their code available; even if the underlying operating system used is licensed under the GPL, applications running on it are not considered derivative works. [53] Only if GPLed parts are used in a program (and the program is distributed), then all other source code of the program needs to be made available under the same license terms. The GNU Lesser General Public license (LGPL) was created to have a weaker copyleft than the GPL, in that it does not require own custom-developed source code (distinct from the LGPLed parts) to be made available under the same license terms.

Copyleft
The distribution rights granted by the GPL for modified versions of the work are not unconditional. When someone distributes a GPL'd work plus his/her own modifications, the requirements for distributing the whole work cannot be any greater than the requirements that are in the GPL.
This requirement is known as copyleft. It earns its legal power from the use of copyright on software programs. Because a GPL work is copyrighted, a licensee has no right to redistribute it, not even in modified form (barring fair use ), except under the terms of the license. One is only required to adhere to the terms of the GPL if one wishes to exercise rights normally restricted by copyright law, such as redistribution. Conversely, if one distributes copies of the work without abiding by the terms of the GPL (for instance, by keeping the source code secret), he or she can be sued by the original author under copyright law.
Copyleft thus uses copyright law to accomplish the opposite of its usual purpose: instead of imposing restrictions, it grants rights to other people, in a way that ensures the rights cannot subsequently be taken away. It also ensures that unlimited redistribution rights are not granted, should any legal flaw be found in the copyleft statement. [ citation needed ]
Many distributors of GPL'ed programs bundle the source code with the executables . An alternative method of satisfying the copyleft is to provide a written offer to provide the source code on a physical medium (such as a CD) upon request. In practice, many GPL'ed programs are distributed over the Internet, and the source code is made available over FTP or HTTP . For Internet distribution, this complies with the license.
Copyleft applies only when a person seeks to redistribute the program. Developers may make private modified versions with no obligation to divulge the modifications, as long as they don't distribute the modified software to anyone else. Note that copyleft applies only to the software, and not to its output (unless that output is itself a derivative work of the program [55] ). For example, a public web portal running a modified derivative of a GPL'ed content management system is not required to distribute its changes to the underlying software, because its output is not a derivative.
There has been debate on whether it is a violation of the GPL to release the source code in obfuscated form, such as in cases in which the author is less willing to make the source code available. The consensus was that while unethical, it was not considered a violation. The issue was clarified when the license was altered with v2 to require that the "preferred" version of the source code be made available. [56]

License versus contract
The GPL was designed as a license , rather than a contract. [57] [58] In some Common Law jurisdictions, the legal distinction between a license and a contract is an important one: contracts are enforceable by contract law , whereas licenses are enforced under copyright law . However, this distinction is not useful in the many jurisdictions where there are no differences between contracts and licenses, such as Civil Law systems. [59]
Those who do not accept the GPL's terms and conditions do not have permission, under copyright law, to copy or distribute GPL licensed software or derivative works. However, if they do not redistribute the GPL'd program, they may still use the software within their organization however they like, and works (including programs) constructed by the use of the program are not required to be covered by this license.
Allison Randal argued that the GPLv3 as a license is unnecessarily confusing for lay readers, and could be simplified while retaining the same conditions and legal force. [60]
In April 2017 a US federal court ruled that an open-source license is an enforceable contract. [61]

Derivations
The text of the GPL is itself copyrighted , and the copyright is held by the Free Software Foundation.
The FSF permits people to create new licenses based on the GPL, as long as the derived licenses do not use the GPL preamble without permission. This is discouraged, however, since such a license might be incompatible with the GPL [62] and causes a perceived license proliferation .
Other licenses created by the GNU project include the GNU Lesser General Public License , the GNU Free Documentation License and Affero General Public License .
The text of the GPL is not itself under the GPL. The license's copyright disallows modification of the license. Copying and distributing the license is allowed since the GPL requires recipients to get "a copy of this License along with the Program". [63] According to the GPL FAQ, anyone can make a new license using a modified version of the GPL as long as he or she uses a different name for the license, does not mention "GNU", and removes the preamble, though the preamble can be used in a modified license if permission to use it is obtained from the Free Software Foundation (FSF) .

Linking and derived works

Libraries
According to the FSF , "The GPL does not require you to release your modified version, or any part of it. You are free to make modifications and use them privately, without ever releasing them." [64] However, if one releases a GPL-licensed entity to the public, there is an issue regarding linking: namely, whether a proprietary program that uses a GPL library is in violation of the GPL.
This key dispute is whether non-GPL software can legally statically link or dynamically link to GPL libraries. Different opinions exist on this issue. The GPL is clear in requiring that all derivative works of code under the GPL must themselves be under the GPL. Ambiguity arises with regards to using GPL libraries, and bundling GPL software into a larger package (perhaps mixed into a binary via static linking). This is ultimately a question not of the GPL per se , but of how copyright law defines derivative works. The following points of view exist:

Point of view: dynamic and static linking violate GPL
The Free Software Foundation (which holds the copyright of several notable GPL-licensed software products and of the license text itself) asserts that an executable which uses a dynamically linked library is indeed a derivative work. This does not however apply to separate programs communicating with one another. [65]
The Free Software Foundation also created the LGPL , which is nearly identical to the GPL, but with additional permissions to allow linking for the purposes of "using the library".
Richard Stallman and the FSF specifically encourage library-writers to license under the GPL so that proprietary programs cannot use the libraries, in an effort to protect the free-software world by giving it more tools than the proprietary world. [66]

Point of view: static linking violates GPL but unclear as of dynamic linking
Some people believe that while static linking produces derivative works, it is not clear whether an executable that dynamically links to a GPL code should be considered a derivative work (see Weak Copyleft ). Linux author Linus Torvalds agrees that dynamic linking can create derived works but disagrees over the circumstances. [67]
A Novell lawyer has written that dynamic linking not being derivative "makes sense" but is not "clear-cut", and that evidence for good-intentioned dynamic linking can be seen by the existence of proprietary Linux kernel drivers. [68]
In Galoob v. Nintendo the United States Ninth Circuit Court of Appeals defined a derivative work as having "'form' or permanence" and noted that "the infringing work must incorporate a portion of the copyrighted work in some form", [69] but there have been no clear court decisions to resolve this particular conflict.

Point of view: linking is irrelevant
According to an article in the Linux Journal , Lawrence Rosen (a one-time Open Source Initiative general counsel) argues that the method of linking is mostly irrelevant to the question about whether a piece of software is a derivative work ; more important is the question about whether the software was intended to interface with client software and/or libraries. [70] He states, "The primary indication of whether a new program is a derivative work is whether the source code of the original program was used [in a copy-paste sense], modified, translated or otherwise changed in any way to create the new program. If not, then I would argue that it is not a derivative work," [70] and lists numerous other points regarding intent, bundling, and linkage mechanism. He further argues on his firm's website [71] that such "market-based" factors are more important than the linking technique.
There is also the specific issue of whether a plugin or module (such as the NVidia or ATI graphics card kernel modules ) must also be GPL, if it could reasonably be considered its own work. This point of view suggests that reasonably separate plugins, or plugins for software designed to use plugins, could be licensed under an arbitrary license if the work is GPLv2. Of particular interest is the GPLv2 paragraph:
The GPLv3 has a different clause:
As a case study, some supposedly proprietary plugins and themes / skins for GPLv2 CMS software such as Drupal and WordPress have come under fire, with both sides of the argument taken. [72] [73]
The FSF differentiates on how the plugin is being invoked. If the plugin is invoked through dynamic linkage and it performs function calls to the GPL program then it is most likely a derivative work. [74]

Communicating and bundling with non-GPL programs
The mere act of communicating with other programs does not, by itself, require all software to be GPL; nor does distributing GPL software with non-GPL software. However, minor conditions must be followed that ensures the rights of GPL software is not restricted. The following is a quote from the gnu.org GPL FAQ , which describes to what extent software is allowed to communicate with and be-bundled-with GPL programs:
[75]
The FSF thus draws the line between "library" and "other program" via 1) "complexity" and "intimacy" of information exchange, and 2) mechanism (rather than semantics), but resigns that the question is not clear-cut and that in complex situations, case law will decide.

Legal status
The first known violation of the GPL was in 1989, when NeXT extended the GCC compiler to support Objective-C , but did not publicly release the changes. [76] After an inquiry they created a public patch . There was no lawsuit filed for this violation. [77]
In 2002, MySQL AB sued Progress NuSphere for copyright and trademark infringement in United States district court . NuSphere had allegedly violated MySQL's copyright by linking MySQL's GPL'ed code with NuSphere Gemini table without being in compliance with the license. After a preliminary hearing before Judge Patti Saris on 27 February 2002, the parties entered settlement talks and eventually settled. [78] After the hearing, FSF commented that "Judge Saris made clear that she sees the GNU GPL to be an enforceable and binding license." [79]
In August 2003, the SCO Group stated that they believed the GPL to have no legal validity, and that they intended to pursue lawsuits over sections of code supposedly copied from SCO Unix into the Linux kernel . This was a problematic stand for them, as they had distributed Linux and other GPL'ed code in their Caldera OpenLinux distribution, and there is little evidence that they had any legal right to do so except under the terms of the GPL. For more information, see SCO-Linux controversies and SCO v. IBM .
In April 2004, the netfilter / iptables project was granted a preliminary injunction against Sitecom Germany by Munich District Court after Sitecom refused to desist from distributing Netfilter's GPL'ed software in violation of the terms of the GPL. Harald Welte , of Netfilter, was represented by ifrOSS co-founder Till Jaeger. In July 2004, the German court confirmed this injunction as a final ruling against Sitecom. [80] The court's justification was that:
This exactly mirrored the predictions given previously by the FSF's Eben Moglen . This ruling was important because it was the first time that a court had confirmed that violating terms of the GPL could be a copyright violation and established jurisprudence as to the enforceability of the GPL version 2 under German law. [81]
In May 2005, Daniel Wallace filed suit against the Free Software Foundation in the Southern District of Indiana , contending that the GPL is an illegal attempt to fix prices (at zero). The suit was dismissed in March 2006, on the grounds that Wallace had failed to state a valid anti-trust claim; the court noted that "the GPL encourages, rather than discourages, free competition and the distribution of computer operating systems, the benefits of which directly pass to consumers". [82] Wallace was denied the possibility of further amending his complaint, and was ordered to pay the FSF's legal expenses.
On 8 September 2005, the Seoul Central District Court ruled that the GPL was not material to a case dealing with trade secrets derived from GPL-licensed work. [83] Defendants argued that since it is impossible to maintain trade secrets while being compliant with GPL and distributing the work, they are not in breach of trade secrets. This argument was considered without ground.
On 6 September 2006, the gpl-violations.org project prevailed in court litigation against D-Link Germany GmbH regarding D-Link's copyright-infringing use of parts of the Linux Kernel in storage devices they distributed. [84] The judgment stated that the GPL is valid, legally binding, and stands in German court. [85]
In late 2007, the BusyBox developers and the Software Freedom Law Center embarked upon a program to gain GPL compliance from distributors of BusyBox in embedded systems , suing those who would not comply. These were claimed to be the first US uses of courts for enforcement of GPL obligations. See BusyBox GPL lawsuits .
On 11 December 2008, the Free Software Foundation sued Cisco Systems, Inc. for copyright violations by its Linksys division, of the FSF's GPL-licensed coreutils , readline , Parted , Wget , GNU Compiler Collection , binutils , and GNU Debugger software packages, which Linksys distributes in the Linux firmware [86] of its WRT54G wireless routers , as well as numerous other devices including DSL and Cable modems, Network Attached Storage devices, Voice-Over-IP gateways, virtual private network devices and a home theater/media player device. [87]
After six years of repeated complaints to Cisco by the FSF, claims by Cisco that they would correct, or were correcting, their compliance problems (not providing complete copies of all source code and their modifications), of repeated new violations being discovered and reported with more products, and lack of action by Linksys (a process described on the FSF blog as a "five-years-running game of Whack-a-Mole" [87] ) the FSF took them to court.
Cisco settled the case six months later by agreeing "to appoint a Free Software Director for Linksys" to ensure compliance, "to notify previous recipients of Linksys products containing FSF programs of their rights under the GPL," to make source code of FSF programs freely available on its website, and to make a monetary contribution to the FSF. [88]
In 2011 it was noticed that GNU Emacs had violated its GPL license for the previous two years, by distributing binaries, but not the source code. [89] [90] [91] Richard Stallman described this incident as "a very bad mistake" , [92] no lawsuit was filed and the mistake was promptly resolved.

Compatibility and multi-licensing
Code licensed under several other licenses can be combined with a program under the GPL without conflict, as long as the combination of restrictions on the work as a whole does not put any additional restrictions beyond what GPL allows. [93] In addition to the regular terms of the GPL, there are additional restrictions and permissions one can apply:
FSF maintains a list [99] of GPL- compatible free software licenses [100] [101] with many of the most common free software licenses, such as the original MIT/X license , the BSD license (in its current 3-clause form) and the Artistic License 2.0. [102]
David A. Wheeler has advocated that free/open source software developers use only GPL-compatible licenses, because doing otherwise makes it difficult for others to participate and contribute code. [103] As a specific example of license incompatibility, Sun Microsystems ' ZFS cannot be included in the GPL-licensed Linux kernel, because it is licensed under the GPL-incompatible CDDL . Furthermore, ZFS is protected by patents, so distributing an independently developed GPL-ed implementation would still require Oracle's permission. [104]
A number of businesses use multi-licensing to distribute a GPL version and sell a proprietary license to companies wishing to combine the package with proprietary code, using dynamic linking or not. Examples of such companies include MySQL AB , Digia PLC ( Qt framework , before 2011 from Nokia ), Red Hat ( Cygwin ) and Riverbank Computing ( PyQt ). Other companies, like the Mozilla Foundation (products include Mozilla Application Suite , Mozilla Thunderbird and Mozilla Firefox ), used multi-licensing to distribute versions under the GPL and some other open-source licenses.

Text and other media
It is possible to use the GPL for text documents instead of computer programs, or more generally for all kinds of media, if it is clear what constitutes the source code (defined as "the preferred form of the work for making changes in it"). [105] For manuals and textbooks, though, the FSF recommends the GNU Free Documentation License (GFDL) instead, which it created for this purpose. [106] Nevertheless, the Debian developers recommended (in a resolution adopted in 2006) to license documentation for their project under the GPL, because of the incompatibility of the GFDL with the GPL (text licensed under the GFDL cannot be incorporated into GPL software). [107] [108] Also, the FLOSS Manuals foundation, an organization devoted to creating manuals for free software, decided to eschew the GFDL in favor of the GPL for its texts in 2007. [109]
If the GPL is used for fonts, any documents or images made with such fonts might also have to be distributed under the terms of the GPL. This is not the case in countries like the US and Canada where copyright law is inapplicable to the appearance of fonts, though program code inside a font file may still be covered—which can complicate font embedding (since the document could be considered 'linked' to the font). FSF provides an exception for cases where this is not desired. [110] [111]

Adoption
Historically, the GPL license family has been one of the most popular software licenses in the FOSS domain. [6] [8] [9] [10] [11] [13]
A 1997 survey of MetaLab , then the largest free software archive, showed that the GPL accounted for about half of the software licensed therein. [8] Similarly, a 2000 survey of Red Hat Linux 7.1 found that 53% of the source code was licensed under the GPL. [9] As of 2003 [update] , about 68% of all projects and 82.1% of the OSI certified licensed projects listed on SourceForge.net were from the GPL license family. [112] As of August 2008 [update] , the GPL family accounted for 70.9% of the 44,927 free software projects listed on Freecode . [10]
After the release of the GPLv3 in June 2007, adoption of this new GPL version was much discussed [113] and some projects decided against upgrading. For instance the linux kernel , [15] [44] MySQL , [114] BusyBox , [115] [116] AdvFS , [117] Blender , [118] [119] and VLC media player [120] decided against adopting the GPLv3. On the other hand, in 2009, two years after the release of the GPLv3, Google open-source programs office manager Chris DiBona reported that the number of open-source projects licensed software that had moved to GPLv3 from GPLv2 was 50%, counting the projects hosted at Google Code . [11]
In 2011, four years after the release of the GPLv3, 6.5% of all open-source license projects are GPLv3 while 42.5% are GPLv2 according to Black Duck Software data. [121] [122] Following in 2011 451 Group analyst Matthew Aslett argued in a blog post that copyleft licenses went into decline and permissive licenses increased, based on statistics from Black Duck Software. [123] [124] Similarly, in February 2012 Jon Buys reported that among the top 50 projects on GitHub five projects were under a GPL license, including dual licensed and AGPL projects. [125]
GPL usage statistic from 2009 to 2013 was extracted from Freecode data by Walter van Holst while analyzing license proliferation . [12]
In August 2013, according to Black Duck Software, the website's data show that the GPL license family is used by 54% of open-source projects, with a breakdown of the individual licenses shown in the following table. [13] However, a later study in 2013 showed that software licensed under the GPL license family has increased, and that even the data from Black Duck Software have shown a total increase of software projects licensed under GPL. The study used public information gathered from repositories of the Debian Project , and the study criticized Black Duck Software for not publishing their methodology used in collecting statistics. [128] Daniel German, Professor in the Department of Computer Science at the University of Victoria in Canada, presented a talk in 2013 about the methodological challenges in determining which are the most widely used free software licenses, and showed how he could not replicate the result from Black Duck Software. [129]
In 2015 according to BlackDuck the GPLv2 lost its first position on the MIT license and is now second, the GPLv3 dropped to fourth place while the Apache license kept its third position. [6]
A March 2015 analysis of the GitHub repositories revealed for the GPL license family an usage percentage of approx. 25% among licensed projects. [134] In June 2016 an analysis of Fedora Project 's packages revealed the GNU GPL version 2 or later as the most popular license, and the GNU GPL family as the most popular license family (followed by the MIT, BSD, and GNU LGPL families). [135]

Reception

Legal barrier to app stores
The GPL License is incompatible with many application digital distribution systems, like the Mac App Store , and certain other software distribution platforms (on smartphones as well as PCs). The problem lies in the right "To make a copy for your neighbour", as this right is violated by the integrated DRM-Systems made to prevent copying of paid software. Even if the application is free-as-in-beer in the App Store in question, it might result in a violation of that app store's terms. [136]
Note that there is a distinction between an app store , which sells DRM -restricted software under proprietary licenses, and the more general concept of digital distribution via some form of online software repository. Various UNIX-like distributions provide app repositories, including Fedora , RHEL , CentOS , Ubuntu , Debian , FreeBSD , OpenBSD and so on. These specific app repos all contain GPL-licensed software apps, in some cases even when the core project does not permit GPL-licensed code in the base system (for instance OpenBSD [137] ). In other cases, such as the Ubuntu App Store , proprietary commercial software applications and GPL-licensed applications are both available via the same system; the reason that the Mac App Store (and similar projects) is incompatible with GPL-licensed apps is not inherent in the concept of an app store, but is rather specifically due to Apple's terms-of-use requirement [136] that all apps in the store utilize Apple DRM-restrictions. Ubuntu's app store does not demand any such requirement: "These terms do not limit or restrict your rights under any applicable open source software licenses." [138]

Microsoft
In 2001, Microsoft CEO Steve Ballmer referred to Linux as "a cancer that attaches itself in an intellectual property sense to everything it touches". [139] [140] In response to Microsoft's attacks on the GPL, several prominent Free Software developers and advocates released a joint statement supporting the license. [141] Microsoft has released Microsoft Windows Services for UNIX , which contains GPL-licensed code. In July 2009, Microsoft itself released a body of around 20,000 lines of Linux driver code under the GPL. [142] The Hyper-V code that is part of the submitted code used open-source components licensed under the GPL and was originally statically linked to proprietary binary parts, the latter being inadmissible in GPL-licensed software. [143]

"Viral" nature
The description of the GPL as "viral" , when called 'General Public Virus' or 'GNU Public Virus' (GPV), dates back to a year after the GPLv1 was released. [144] [145] [146] [147] [148] [149]
In 2001 the term received broader public attention when Craig Mundie , Microsoft Senior Vice President, described the GPL as being "viral". [150] Mundie argues that the GPL has a "viral" effect in that it only allows the conveyance of whole programs, which means programs that link to GPL libraries must themselves be under a GPL-compatible license, else they cannot be combined and distributed.
In 2006 Richard Stallman responded in an interview that Mundie's metaphor of a "virus" is wrong as software under the GPL does not "attack" or "infect" other software. Stallman believes that comparing the GPL to a virus is an extremely unfriendly thing to say, and that a better metaphor for software under the GPL would be a spider plant : If one takes a piece of it and puts it somewhere else, it grows there too. [151] [152] [153]
On the other hand, the concept of a viral nature of the GPL was taken up by others later too. [154] [155] [156] [157] [158] For instance in 2008 the California Western School of Law characterized the GPL as: "The GPL license is ‘viral,’ meaning any derivative work you create containing even the smallest portion of the previously GPL licensed software must also be licensed under the GPL license" . [159]

Barrier to commercialization
The FreeBSD project has stated that "a less publicized and unintended use of the GPL is that it is very favorable to large companies that want to undercut software companies. In other words, the GPL is well suited for use as a marketing weapon, potentially reducing overall economic benefit and contributing to monopolistic behavior" and that the GPL can "present a real problem for those wishing to commercialize and profit from software" . [160]
Richard Stallman wrote about the practice of selling license exceptions to free software licenses as an example of ethically acceptable commercialization practice. Selling exceptions here means that the copyright holder of a given software releases it (along with the corresponding source code) to the public under a free software license, "then lets customers pay for permission to use the same code under different terms, for instance allowing its inclusion in proprietary applications". Stallman considered selling exceptions "acceptable since the 1990s, and on occasion I've suggested it to companies. Sometimes this approach has made it possible for important programs to become free software". Despite that the FSF doesn't practice selling exceptions, a comparison with the X11 license (which is a non-copyleft free software license) is proposed for suggesting that this commercialization technique should be regarded as ethically acceptable. Releasing a given program under a noncopyleft free software license would permit embedding the code in proprietary software. Stallman comments that "either we have to conclude that it's wrong to release anything under the X11 license—a conclusion I find unacceptably extreme—or reject this implication. Using a noncopyleft license is weak, and usually an inferior choice, but it's not wrong. In other words, selling exceptions permits some embedding in proprietary software, and the X11 license permits even more embedding. If this doesn't make the X11 license unacceptable, it doesn't make selling exceptions unacceptable". [161]

Open-source criticism
In 2000 developer and author Nikolai Bezroukov published an analysis and comprehensive critique of GPL's foundations and Stallman's software development model, called "Labyrinth of Software Freedom". [162] [163]
In 2005, open source software advocate Eric S. Raymond questioned the relevance of GPL at that point in time for the FOSS ecosystem, stating: "We don't need the GPL anymore. It's based on the belief that open source software is weak and needs to be protected. Open source would be succeeding faster if the GPL didn't make lots of people nervous about adopting it.". [164] Richard Stallman replied that: "GPL ensure that every user of a program gets the essential freedoms—to run it, to study and change the source code, to redistribute copies, and to publish modified versions ... [Raymond] addresses the issue in terms of different goals and values—those of "open source," which do not include defending software users' freedom to share and change software." [165]
In 2007 Allison Randal , who took part in the GPL draft committee, criticized the GPLv3 for being incompatible with the GPLv2 [166] and for missing clarity in the formulation. [167] Similarly, Whurley prophesised in 2007 the downfall of the GPL due to the lack of focus for the developers with GPLv3 which would drive them towards permissive licenses. [168]
In 2009 David Chisnall described in an InformIT article, "The Failure of the GPL" , the problems with the GPL, among them incompatibility and complexity of the license text. [169]
In 2014 dtrace developer and Joyent CTO Bryan Cantrill called the copyleft GPL a "Corporate Open Source Anti-pattern " by being "anti-collaborative" and recommended instead permissive software licenses. [170]

GPLv3 criticism
Already in September 2006, in the draft process of the GPLv3, several high-profile developers of the Linux kernel , for instance Linus Torvalds , Greg Kroah-Hartman and Andrew Morton , warned on a splitting of the FOSS community: "the release of GPLv3 portends the Balkanisation of the entire Open Source Universe upon which we rely." . [41] Similarly Benjamin Mako Hill argued in 2006 on the GPLv3 draft, noting that a united, collaborating community is more important than a single license. [171]
Following the GPLv3 release in 2007, some journalists [44] [121] [172] and Toybox developer Rob Landley [47] [48] criticized that with the introduction of the GPLv3 the split between the open source and free software community became wider than ever. As the significantly extended GPLv3 is essentially incompatible with the GPLv2, [94] compatibility between both is only given under the optional "or later" clause of the GPL, which was not taken for instance by the Linux kernel . [15] Bruce Byfield noted that before the release of the GPLv3, the GPLv2 was a unifying element between the open-source and the free software community. [121]
For the LGPLv3, GNU TLS maintainer Nikos Mavrogiannopoulos similarly argued, "If we assume that its [the LGPLv3] primary goal is to be used by free software, then it blatantly fails that.", [173] after he re-licensed GNU TLS from LGPLv3 back to LGPLv2.1 due to license compatibility issues. [174] [175]
Lawrence Rosen , attorney and computer specialist, praised in 2007 how the community using the Apache license were now able to work together with the GPL community in a compatible manner, as the problems of GPLv2 compatibility with Apache licensed software were resolved with the GPLv3. He said, "I predict that one of the biggest success stories of GPLv3 will be the realization that the entire universe of free and open source software can thus be combined into comprehensive open source solutions for customers worldwide." [176]
In July 2013 Flask developer Armin Ronacher draw a less optimistic resume on the GPL compatibility in the FOSS ecosystem when he concluded: "When the GPL is involved the complexities of licensing becomes a non fun version of a riddle." , also noting that the ASL 2.0 GPLv2 conflict still has impact on the ecosystem. [177] [178]

See also
WebPage index: 00073
Magnus Manske
Heinrich Magnus Manske (born 24 May 1974) is a senior staff scientist at the Wellcome Trust Sanger Institute in Cambridge , UK [1] [3] [4] [5] [6] and a software developer of one of the first versions of the MediaWiki software. [7]

Early life and education
Manske was born in Cologne , Germany.
Manske studied biochemistry at the University of Cologne and graduated in 2006 with a PhD; his dissertation was an open source tool for molecular biology called GENtle . [2] [8]

Career
As a student Manske was one of the first contributors to the Internet encyclopedia Nupedia , the precursor to Wikipedia , and later wrote one of the first versions of the MediaWiki software that Wikipedia runs on. [9] Manske has worked in Cambridge with the Wellcome Trust Sanger Institute since April 2007, [10] [11] [12] [13] [14] but remains active in the development of tools for Wikipedia [15] and its sister projects Wikidata and Wikimedia Commons . [16]
In 2012, Manske was co-author of a paper published in Nature that demonstrated new ways to identify areas where malaria parasites are evolving, and described techniques for mapping malarial drug resistance. The researchers developed a technique to extract the malaria parasite DNA directly from the blood, which minimizes errors in sequencing. [10] [17]

Development of MediaWiki
As a student, Manske was one of the most active contributors to the Nupedia project, [18] submitting content on biology topics [9] and developing tools and extensions for Nupedia. [19] Later, unhappy with the existing software's limitations, [18] Manske developed one of the first versions of what later became MediaWiki . [19] [20] His new version of the software was installed in 2002. [18]
The wiki software that was initially used for Wikipedia was called UseModWiki and was written in Perl . With issues of scale starting to present themselves as Wikipedia grew, in the summer of 2001, Manske started work on a replacement for UseModWiki that would be database-backed and would contain "wikipedia-specific features." [21] On 25 January 2002, Manske released the first version of a MySQL -based PHP wiki engine, called Phase II . [22] One innovation implemented by Manske in Phase II was the use of namespaces in order to separate different types of pages, such as the "Talk" or "User" namespace, which distinguished it from older Wiki software which didn't have different namespaces. [21] Phase II also introduced a number of other features which are still present, including file upload, watchlists, automatic signatures, and user contributions list. [23] [24] Manske's re-write also made it easier to integrate photographs in Wikipedia articles, and created a new user group: administrators empowered to delete pages and block vandals. [18] [25]
Manske was an adherent of open source and specifically the GPL license, and his work on the early versions of MediaWiki was released under the GPL license. [26]
Manske's Phase II software encountered load issues as Wikipedia continued to grow, so Lee Daniel Crocker did another re-write, which led to phase III, which was used from June 2002 and from 2003 onwards was called "MediaWiki". [22] The resultant MediaWiki software is now used as the core platform for both Wikipedia and many Wikimedia sister projects, as well as in many organizations and institutions.
Manske continued to develop tools and extensions for MediaWiki, including tools to map category membership, compute category intersections, and import images from Flickr to Commons . [27] [28] Manske also developed the Cite extension which brought an XML-like syntax to citation management. [29]

Recognition
Manske is recognized as the creator of the first article in the German Wikipedia, which was on the polymerase chain reaction , first written by him in 2001. [30] [31] [32]
Jimmy Wales in 2002 named 25 January as Magnus Manske day in honor of his contributions to Wikipedia, proclaiming that "Tonight at dinner, every Wikipedian should say a toast to Magnus and his many inventions." [33] Larry Sanger , in his memoir on the early history of Wikipedia, highlighted the contributions of Manske to the project and attributes the eventual success of Wikipedia to a core group of actors, with Manske playing an important role:
Manske, along with others, was recognized as a major contributor to MediaWiki by the USENIX Advanced Computing Technical Association in 2010, when MediaWiki and the Wikimedia Foundation were honoured with a STUG award (Software Tools User Group). [35]
WebPage index: 00074
VisualEditor
VisualEditor ( VE ) is a project to provide a "visual" or " WYSIWYG -like" online rich-text editor as a MediaWiki extension to Wikipedia . It was developed by the Wikimedia Foundation in partnership with Wikia . [2] In July 2013 the beta was enabled by default, with the ability to opt-out , for Mediawiki.org and several of the largest Wikipedias. [3] [4]
The Wikimedia Foundation considered it the most challenging technical project to date, while The Economist has called it Wikipedia's most significant change. [5] According to the The Daily Dot , Wikimedia Foundation's pursuit of wider participation may risk alienating existing editors. [6] In September 2013, English Wikipedia's VisualEditor was changed from opt-out to opt-in , following user complaints, [7] [8] but was returned to being available by default in October 2015 after further development. [9] A 2015 study by the Wikimedia Foundation found that VisualEditor failed to provide the anticipated benefits for new editors. [10]

Development
The original web-based Wikipedia editor provided by MediaWiki is a plain browser based [note 1] text editor where authors had to learn the wiki markup language to edit. [11] A WYSIWYG editor for Wikipedia has been planned for years in order to remove the need to learn the wiki markup language, thus reducing the technical hurdle for would-be Wikipedians , enabling wider participation in editing, and subsequently reversing the decline in editor numbers of 50,000 in 2006 to 35,000 in 2011, having peaked in 2007. [5] [6] It was part of a $1m project developing new features and making improvements. [5] An aim of the project is to allow both the former wiki markup editing and editing with the WYSIWYG VisualEditor. [12] According to Wikimedia Foundation's Jay Walsh, the hope is to redress under-represented contributions from Arabic , Portuguese , and Indic-language versions of the site. [6] [note 2]
According to Wikimedia Foundation "There are various reasons that lead existing and prospective contributors not to edit; among them, the complexity of wiki markup is a major issue. One of VisualEditor's goals is to empower knowledgeable and good-faith users to edit and become valuable members of the community, even if they’re not wiki markup experts. We also hope that, with time, experienced editors will find VisualEditor useful for some of their editing tasks." [4] In 2012, Sue Gardner , the executive director of the Wikimedia Foundation, said "we don't think that the visual editor, in and of itself, is going to solve the challenge", [13] and Wikipedia co-founder Jimmy Wales remarked "This is epically important". [14]

Rollout
MediaWiki is used by a large number of wikis , with smaller sites originally conceived as being rolled out first. [15] VisualEditor was planned to be rolled out on the English-language Wikipedia for editors with registered accounts, and then for anonymous editors. [16] The alpha version was made available to select users in December 2012, widened to all registered users in April. [17] It was default editor for users logged-into the English-language Wikipedia in July 2013. [4] [6] It was subsequently made opt-in on the English-language Wikipedia in September 2013 due to community complaints over its stability and implementation was buggy and had limitations [7] [8] (though it remained active for most non-English Wikipedias). [18] In 2015 it was readopted by English Wikipedia as active by default after it completed its beta development phase. [9]

Technical
The Wikimedia Foundation joined forces with Wikia to work on the project. [19] The implementation encountered challenges with the wiki markup language (the basis for Wikipedia articles), due to it being continuously extended over 12 years to include seldom-used rich and complex features making reproduction of the final article appearance dependent on many factors that were not easy to reproduce. [20] The technical implementation required improvements to MediaWiki in parsing , wiki markup language, the DOM and final HTML conversion. [21] A necessary component is a parser server called Parsoid [note 3] which is written in Node.js and was created to convert in both directions between wikitext and a format suitable for VisualEditor. [20] The Wikimedia Foundation considered VisualEditor its most challenging technical project to date. [5]
As of April 2015 [update] supported web browsers include modern versions of Chrome , Firefox , Midori , Opera , Safari and Internet Explorer (10+).
The VisualEditor MediaWiki extension is available for download by server operators and typically requires the latest version of MediaWiki.

Online rich-text editor
According to the VisualEditor team, the aim is "to create a reliable rich-text editor for MediaWiki", [22] a "visual editor" which is "WYSIWYG-like". [23] The implementation is split into a "core" online rich-text editor which can run independently of MediaWiki, [24] and a MediaWiki extension. [25] The MediaWiki extension is in the category "WYSIWYG extensions".

Assessments
The Daily Dot suggested that the Wikimedia Foundation's pursuit of more users may be at the risk of alienating the existing editors. [6] Some experienced editors have expressed concerns about the rollout and bugs, with the German Wikipedia community voting overwhelmingly against making the VisualEditor the new default, and expressing a preference for making it an "opt-in" feature instead. [6] [26] Despite these complaints, the Wikimedia Foundation continued with the rollout to other languages. [6] The Register said, "Our brief exploration suggests it certainly removes any need to so much as remember what kind of parenthesis belongs where." [17] The Economist ' s L.M., said it is "the most significant change in Wikipedia's short history." [5] Softpedia ran an article titled "Wikipedia's New VisualEditor Is the Best Update in Years and You Can Make It Better". [27] Some opponents have said that users may feel belittled by the implication that "certain people" are confused by wiki markup and therefore need the VisualEditor. [28]
The Daily Dot reported on 24 September 2013 that the Wikimedia Foundation had experienced a mounting backlash from the English Wikipedia community, which criticised the VisualEditor as slow, poorly implemented and prone to break articles' existing text formatting. In the resulting "test of wills" between the community and the Foundation, a single volunteer administrator overrode the Wikimedia Foundation's settings to change the availability of VisualEditor from opt-out to opt-in. The Foundation acquiesced, but vowed to continue developing and improving the VisualEditor. [7] [8]

Research results
The Wikimedia Foundation ran a controlled study on the effects of VisualEditor in May 2015. The study found that VisualEditor does not increase the number of newcomers who successfully begin editing, does not increase their productivity, and does not increase new editor retention . Editing was found to take significantly longer with VisualEditor, and new editors were less likely to save their work. [10] A previous June 2013 controlled test — when VisualEditor was less mature — showed similar neutral and negative results. [29]

See also

Notes
WebPage index: 00075
Women's history
Women's history is the study of the role that women have played in history and the methods required to do so. It includes the study of the history of the growth of woman's rights throughout recorded history , the examination of individual and groups of women of historical significance, and the effect that historical events have had on women. Inherent in the study of women's history is the belief that more traditional recordings of history have minimized or ignored the contributions of women and the effect that historical events had on women as a whole; in this respect, women's history is often a form of historical revisionism , seeking to challenge or expand the traditional historical consensus.
The main centers of scholarship have been the United States and Britain, where second-wave feminist historians, influenced by the new approaches promoted by social history , led the way. As activists in women's liberation , discussing and analyzing the oppression and inequalities they experienced as women, they believed it imperative to learn about the lives of their foremothers—and found very little scholarship in print. History was written mainly by men and about men's activities in the public sphere—war, politics, diplomacy and administration. Women are usually excluded and, when mentioned, are usually portrayed in sex-stereotypical roles such as wives, mothers, daughters and mistresses. The study of history is value-laden in regard to what is considered historically "worthy." [1] Other aspects of this area of study is the differences in women's lives caused by race, economic status, social status, and various other aspects of society. [2]

Regions

Europe
Changes came in the 19th and 20th centuries; for example, for women the right to equal pay is now enshrined in law. Women traditionally ran the household, bore and reared the children, were nurses, mothers, wives, neighbors, friends, and teachers. During periods of war, women were drafted into the labor market to undertake work that had been traditionally restricted to men. Following the wars, they invariably lost their jobs in industry and had to return to domestic and service roles. [3] [4] [5]

Great Britain
The history of Scottish women in the late 19th century and early 20th century was not fully developed as a field of study until the 1980s. In addition, most work on women before 1700 has been published since 1980. Several recent studies have taken a biographical approach, but other work has drawn on the insights from research elsewhere to examine such issues as work, family, religion, crime, and images of women. Scholars are also uncovering women's voices in their letters, memoirs, poetry, and court records. Because of the late development of the field, much recent work has been recuperative, but increasingly the insights of gender history, both in other countries and in Scottish history after 1700, are being used to frame the questions that are asked. Future work should contribute both to a reinterpretation of the current narratives of Scottish history and also to a deepening of the complexity of the history of women in late medieval and early modern Britain and Europe.
In Ireland studies of women, and gender relationships more generally, had been rare before 1990; they now are commonplace with some 3000 books and articles in print. [6]

France
French historians have taken a unique approach: there has been extensive scholarship in women's and gender history despite the lack of women's and gender study programs or departments at the university level. But approaches used by other academics in the research of broadly based social histories has been applied to the field of women's history as well. The high level of research and publication in women's and gender history is due to the high interest within French society. The structural discrimination in academia against the subject of gender history in France is changing due to the increase in international studies following the formation of the European Union, and more French scholars seeking appointments outside Europe. [7]

Pre-revolution
In the Ancien Régime in France , few women held any formal power; some queens did, as did the heads of Catholic convents. In the Enlightenment , the writings of philosopher Jean Jacques Rousseau provided a political program for reform of the ancien régime, founded on a reform of domestic mores. Rousseau's conception of the relations between private and public spheres is more unified than that found in modern sociology. Rousseau argued that the domestic role of women is a structural precondition for a "modern" society. [8]
Salic law prohibited women from rule; however, the laws for the case of a regency , when the king was too young to govern by himself, brought the queen into the centre of power. The queen could ensure the passage of power from one king to another—from her late husband to her young son—while simultaneously assuring the continuity of the dynasty.

Education for girls
Educational aspirations were on the rise and were becoming increasingly institutionalised in order to supply the church and state with the functionaries to serve as their future administrators. Girls were schooled too, but not to assume political responsibility. Girls were ineligible for leadership positions and were generally considered to have an inferior intellect to their brothers. France had many small local schools where working-class children - both boys and girls - learned to read, the better "to know, love, and serve God." The sons and daughters of the noble and bourgeois elites were given gender-specific educations: boys were sent to upper school, perhaps a university, while their sisters - if they were lucky enough to leave the house - would be sent to board at a convent with a vague curriculum. The Enlightenment challenged this model, but no real alternative was presented for female education. Only through education at home were knowledgeable women formed, usually to the sole end of dazzling their salons. [9] [10]

Germany
Before the 19th century, young women lived under the economic and disciplinary authority of their fathers until they married and passed under the control of their husbands. In order to secure a satisfactory marriage, a woman needed to bring a substantial dowry. In the wealthier families, daughters received their dowry from their families, whereas the poorer women needed to work in order to save their wages so as to improve their chances to wed. Under the German laws, women had property rights over their dowries and inheritances, a valuable benefit as high mortality rates resulted in successive marriages. Before 1789, the majority of women lived confined to society’s private sphere, the home. [11]
The Age of Reason did not bring much more for women: men, including Enlightenment aficionados, believed that women were naturally destined to be principally wives and mothers. Within the educated classes, there was the belief that women needed to be sufficiently educated to be intelligent and agreeable interlocutors to their husbands. However, the lower-class women were expected to be economically productive in order to help their husbands make ends meet. [12]
In the newly founded German State (1871), women of all social classes were politically and socially disenfranchised. The code of social respectability confined upper class and bourgeois women to their homes. They were considered socially and economically inferior to their husbands. The unmarried women were ridiculed, and the ones who wanted to avoid social descent could work as unpaid housekeepers living with relatives; the most able could work as governesses or they could become nuns. [13]
A significant number of middle-class families became impoverished between 1871 and 1890 as the pace of industrial growth was uncertain, and women had to earn money in secret by sewing or embroidery to contribute to the family income. [12] In 1865, the Allgemeiner Deutscher Frauenverein (ADF) was founded as an umbrella organisation for women's associations, demanding rights to education, employment and political participation. Three decades later, the Bund Deutscher Frauenverbände (BDF) replaced ADF and excluded from membership the proletarian movement that was part of the earlier group. The two movements had differing views concerning women's place in society, and accordingly, they also had different agendas. The bourgeois movement made important contributions to the access of women to education and employment (mainly office-based and teaching). The proletarian movement, on the other hand, developed as a branch of the Social Democratic Party. As factory jobs became available for women, they campaigned for equal pay and equal treatment. In 1908 German women won the right to join political parties, and in 1918 they were finally granted the right to vote. The emancipation of women in Germany was to be challenged in following years. [14]
Historians have paid special attention to the efforts by Nazi Germany to reverse the political and social gains that women made before 1933, especially in the relatively liberal Weimar Republic . [15] The role of women in Nazi Germany changed according to circumstances. Theoretically the Nazis believed that women must be subservient to men, avoid careers, devote themselves to childbearing and child-rearing, and be helpmates to the traditional dominant fathers in the traditional family. [16] But, before 1933, women played important roles in the Nazi organisation and were allowed some autonomy to mobilise other women. After Hitler came to power in 1933, the activist women were replaced by bureaucratic women, who emphasised feminine virtues, marriage, and childbirth.
As Germany prepared for war, large numbers of women were incorporated into the public sector and, with the need for full mobilisation of factories by 1943, all women were required to register with the employment office. Hundreds of thousands of women served in the military as nurses and support personnel, and another hundred thousand served in the Luftwaffe , especially helping to operate the anti—aircraft systems. [17] Women's wages remained unequal and women were denied positions of leadership or control. [18]
More than two million women were murdered in the Holocaust. The Nazi ideology viewed women generally as agents of fertility. Accordingly, it identified the Jewish woman as an element to be exterminated to prevent the rise of future generations. For these reasons, the Nazis treated women as prime targets for annihilation in the Holocaust. [19]

Eastern Europe
Interest in the study of women's history in Eastern Europe has been delayed. [20] [21] Representative is Hungary, where the historiography has been explored by Petö and Szapor (2007). Academia resisted incorporating this specialised field of history, primarily because of the political atmosphere and a lack of institutional support. Before 1945, historiography dealt chiefly with nationalist themes that supported the antidemocratic political agenda of the state. After 1945, academia reflected a Soviet model. Instead of providing an atmosphere in which women could be the subjects of history, this era ignored the role of the women's rights movement in the early 20th century. The collapse of Communism in 1989 was followed by a decade of promising developments in which biographies of prominent Hungarian women were published, and important moments of women's political and cultural history were the subjects of research. However, the quality of this scholarship was uneven and failed to take advantage of the methodological advances in research in the West. In addition, institutional resistance continued, as evidenced by the lack of undergraduate or graduate programs dedicated to women's and gender history at Hungarian universities. [22]

Russia
Women's history in Russia started to become important in the Czarist era, and concern was shown in the consciousness and writing of Alexander Pushkin . During the Soviet Era, feminism was developed along with ideals of equality, but in practice and in domestic arrangements, men often dominate. [23] [24]
By the 1990s new periodicals, especially Casus and Odysseus: Dialogue with Time, Adam and Eve stimulated women's history and, more recently, gender history. Using the concept of gender has shifted the focus from women to socially and culturally constructed notions of sexual difference. It has led to deeper debates on historiography and hold a promise of stimulating the development of a new "general" history able to integrate personal, local, social, and cultural history. [25] [26]

Asia and Pacific
General overviews of women in Asian history are scarce, since most specialists focus on China, Japan, India, Korea or another traditionally defined region. [27] [28]

China
Published work generally deals with women as visible participants in revolution, employment as vehicles for women's liberation, Confucianism and the cultural concept of family as sources of women's oppression. While rural marriage rituals, such as bride price and dowry, have remained the same in form, their function has changed. This reflects the decline of the extended family and the growth in women's agency in the marriage transaction. [29] In recent scholarship in China, the concept of gender has yielded a bounty of new knowledge in English- and Chinese-language writings. [30] [31]
Zhongguo fu nü sheng huo shi ( simplified Chinese : 中国妇女生活史 ; traditional Chinese : 中國婦女生活史 ; pinyin : Zhōngguó Fùnǚ Shēnghuó Shǐ ; literally: "Chinese Women's Life History") is a historical book written by Chen Dongyuan in 1928 and published by The Commercial Press in 1937. The book, the first to give a systematic introduction to women's history in China, has strongly influenced further research in this field. [32]
The book sheds a light on Chinese women's life ranging from ancient times (prior to Zhou Dynasty ) to the Republic of China . In the book, sections are separated based on dynasties in China. Sections are divided into segments to introduce different themes, such as marriage, feudal ethical codes, education for women, virtues, positions, concept of chastity, foot-binding and women’s rights movement in modern China. Inspired by the anti-traditional thoughts in New Culture Movement , the author devoted much effort to disclosing and denouncing the unfairness and suppression in culture, institutions and life that victimize women in China. According to the book, women’s conditions are slightly improved until modern China. in the Preface of the book, the author writes: since women in China are always subject to abuse, the history of women is, naturally the history of abuse of women in China. The author revealed the motivation: the book intends to explain how the principle of women being inferior to men evolves; how the abuse to women is intensified over time; and how the misery on women’s back experience the history change. The author wants to promote women’s liberation by revealing the political and social suppression of women.
Mann (2009) explores how Chinese biographers have depicted women over two millennia (221 BCE to 1911), especially during the Han dynasty . Zhang Xuecheng, Sima Qian, and Zhang Huiyan and other writers often study women of the governing class, and their representation in domestic scenes of death in the narratives and in the role of martyrs . [33]

Tibet
The historiography of women in the history of Tibet confronts the suppression of women's histories in the social narratives of an exiled community. McGranahan (2010) examines the role of women in the 20th century, especially during the Chinese invasion and occupation of Tibet . She studies women in the Tibetan resistance army, the subordination of women in a Buddhist society, and the persistent concept of menstrual blood as a contaminating agent. [34] 1998

Japan
Japanese women's history was marginal to historical scholarship until the late 20th century. The subject hardly existed before 1945, and, even after that date, many academic historians were reluctant to accept women's history as a part of Japanese history. The social and political climate of the 1980s in particular, favorable in many ways to women, gave opportunities for Japanese women's historiography and also brought the subject fuller academic recognition. Exciting and innovative research on Japanese women's history began in the 1980s. Much of this has been conducted not only by academic women's historians, but also by freelance writers, journalists, and amateur historians; that is, by people who have been less restricted by traditional historical methods and expectations. The study of Japanese women's history has become accepted as part of the traditional topics. [35]

Australia and New Zealand
With a handful of exceptions, there was little serious history of women in Australia or New Zealand before the 1970s. [36] [37] [38]
A pioneering study was Patricia Grimshaw , Women's Suffrage in New Zealand (1972), explaining how that remote colony became the first country in the world to give women the vote. Women's history as an academic discipline emerged in the mid-1970s, typified by Miriam Dixson, The Real Matilda: Woman and Identity in Australia, 1788 to the Present (1976). The first studies were compensatory, filling in the vacuum where women had been left out. In common with developments in the United States and Britain, there was a movement toward gender studies, with a field dominated by feminists. [39]
Other important topics include demography and family history. [40] [41] Of recent importance are studies of the role of women on the homefront, and in military service, during world wars. [42] See Australian women in World War I and Australian women in World War II .

Middle East
In the 1980s scholarship began to appear on topics regarding the Middle East. [43] [44] [45] [46]

Africa
Numerous short studies have appeared for women's history in African nations. [47] [48] [49] [50] [51] [52] Several surveys have appeared that put the sub-Sahara Africa in the context of women's history. [53] [54]
There are numerous studies for specific countries and regions, such as Nigeria. [55] and Lesotho. [56]
Scholars have turned their imagination to innovative sources for the history of African women, such as songs from Malawi, weaving techniques in Sokoto, and historical linguistics. [57]

Americas

United States
Apart from individual women, working largely on their own, the first organized systematic efforts to develop women's history came from the United Daughters of the Confederacy (UDC) in the early 20th century. It coordinated efforts across the South to tell the story of the women on the Confederate homefront, while the male historians spent their time with battles and generals. The women emphasized female activism, initiative, and leadership. They reported that when all the men left for war, the women took command, found ersatz and substitute foods, rediscovered their old traditional skills with the spinning wheel when factory cloth became unavailable, and ran all the farm or plantation operations. They faced danger without having menfolk in the traditional role of their protectors. [58] Historian Jacquelyn Dowd Hall argues that the UDC was a powerful promoter of women's history:
The work of women scholars was ignored by the male-dominated history profession until the 1960s, when the first breakthroughs came. [60] Gerda Lerner in 1963 offered the first regular college course in women's history. [61] The field of women's history exploded dramatically after 1970, along with the growth of the new social history and the acceptance of women into graduate programs in history departments. In 1972, Sarah Lawrence College began offering a Master of Arts Program in Women’s History, founded by Gerda Lerner, that was the first American graduate degree in the field. [62] Another important development was to integrate women into the history of race and slavery. A pioneer effort was Deborah Gray White 's ' Ar'n't I a Woman? Female Slaves in the Plantation South (1985), which helped to open up analysis of race, slavery, abolitionism and feminism, as well as resistance, power, and activism, and themes of violence, sexualities, and the body. [63] A major trend in recent years has been to emphasize a global perspective. [64] Although the word "women" is the eighth most commonly used word in abstracts of all historical articles in North America, it is only the twenty-third most used word in abstracts of historical articles in other regions. [65] Furthermore, "gender" appears about twice as frequently in American history abstracts compared to abstracts covering the rest of the world. [65]
In recent years, historians of women have reached out to web-oriented students. A pioneering effort and one example of these outreach efforts is the website "Women and Social Movements in the United States," maintained by Kathryn Kish Sklar and Thomas Dublin. [1] Another example is Click! The Ongoing Feminist Revolution , [2] a digital women's history exhibit produced by Clio Visualizing History. [3]

Canada

Themes

Rights and equality
Women's rights refers to the social and human rights of women. In the United States , the abolition movements sparked an increased wave of attention to the status of women, but the history of feminism reaches to before the 18th century. (See protofeminism .) The advent of the reformist age during the 19th century meant that those invisible minorities or marginalized majorities were to find a catalyst and a microcosm in such new tendencies of reform. The earliest works on the so-called "woman question" criticized the restrictive role of women, without necessarily claiming that women were disadvantaged or that men were to blame. In Britain, the Feminism movement began in the 19th century and continues in the present day. Simone de Beauvoir wrote a detailed analysis of women's oppression in her 1949 treatise The Second Sex. It became a foundational tract of contemporary feminism. [66] In the late 1960s and early 1970s, feminist movements , such as the one in the United States substantially changed the condition of women in the Western world. One trigger for the revolution was the development of the birth control pill in 1960, which gave women access to easy and reliable contraception in order to conduct family planning.

Capitalism
Women's historians have debated the impact of capitalism on the status of women. [67] [68] Taking a pessimistic side, Alice Clark argued that when capitalism arrived in 17th century England, it made a negative impact on the status of women as they lost much of their economic importance. Clark argues that in 16th century England, women were engaged in many aspects of industry and agriculture. The home was a central unit of production and women played a vital role in running farms, and in some trades and landed estates. Their useful economic roles gave them a sort of equality with their husbands. However, Clark argues, as capitalism expanded in the 17th century, there was more and more division of labor with the husband taking paid labor jobs outside the home, and the wife reduced to unpaid household work. Middle-class and women were confined to an idle domestic existence, supervising servants; lower-class women were forced to take poorly paid jobs. Capitalism, therefore, had a negative effect on many women. [69] In a more positive interpretation, Ivy Pinchbeck argues that capitalism created the conditions for women's emancipation. [70] Tilly and Scott have Emphasize the continuity and the status of women, finding three stages in European history. In the preindustrial era, production was mostly for home use and women produce much of the needs of the households. The second stage was the "family wage economy" of early industrialization, the entire family depended on the collective wages of its members, including husband, wife and older children. The third or modern stage is the "family consumer economy," in which the family is the site of consumption, and women are employed in large numbers in retail and clerical jobs to support rising standards of consumption. [71]

Employment
The 1870 US Census was the first to count "Females engaged in each and every occupation" and provides a snapshot of women's history. It reveals that, contrary to popular myth, not all American women of the Victorian period were "safe" in their middle-class homes or working in sweatshops. Women composed 15% of the total work force (1.8 million out of 12.5). They made up one-third of factory "operatives," and were concentrated in teaching, as the nation emphasized expanding education; dressmaking, millinery, and tailoring. Two-thirds of teachers were women. They also worked in iron and steel works (495), mines (46), sawmills (35), oil wells and refineries (40), gas works (4), and charcoal kilns (5), and held such surprising jobs as ship rigger (16), teamster (196), turpentine laborer (185), brass founder/worker (102), shingle and lathe maker (84), stock-herder (45), gun and locksmith (33), hunter and trapper (2). There were five lawyers, 24 dentists, and 2,000 doctors.

Marriage ages
Marriage ages of women can be used as an indicator of the position of women in society. Women´s age at marriage could influence economic development, partly because women marrying at higher ages had more opportunities to acquire human capital . On average, across the world, marriage ages of women have been rising. However countries such as Mexico, China, Egypt and Russia have shown a smaller increase in this measure of female empowerment than, for example, Japan. [72]

Sex and reproduction
In the history of sex , the social construction of sexual behavior—its taboos, regulation and social and political effects—has had a profound effect on women in the world since prehistoric times. Absent assured ways of controlling reproduction, women have practised abortion since ancient times; many societies have also practise infanticide to ensure the survival of older children. Historically, it is unclear how often the ethics of abortion (induced abortion) was discussed in societies. In the later half of the 20th century, some nations began to legalize abortion. This controversial subject has sparked heated debate and in some cases violence, as different parts of society have differing social and religious ideas about its meaning.
Women have been exposed to various tortuous sexual conditions and have been discriminated against in various fashions in history. In addition to women being sexual victims of troops in warfare, an institutionalized example was the Japanese military enslaving native women and girls as comfort women in military brothels in Japanese-occupied countries during World War II .

Clothing
The social aspects of clothing have revolved around traditions regarding certain items of clothing intrinsically suited different gender roles . In different periods, both women's and men's fashions have highlighted one area or another of the body for attention. In particular, the wearing of skirts and trousers has given rise to common phrases expressing implied restrictions in use and disapproval of offending behaviour. For example, ancient Greeks often considered the wearing of trousers by Persian men as a sign of an effeminate attitude. Women's clothing in Victorian fashion was used as a means of control and admiration. Reactions to the elaborate confections of French fashion led to various calls for reform on the grounds of both beauty (Artistic and Aesthetic dress) and health (dress reform; especially for undergarments and lingerie). Although trousers for women did not become fashionable until the later 20th century, women began wearing men's trousers (suitably altered) for outdoor work a hundred years earlier. In the 1960s, André Courrèges introduced long trousers for women as a fashion item, leading to the era of the pantsuit and designer jeans, and the gradual eroding of the prohibitions against girls and women wearing trousers in schools, the workplace, and fine restaurants. Corsets have long been used for fashion, and body modification, such as waistline reduction. There were, and are, many different styles and types of corsets, varying depending on the intended use, corset maker's style, and the fashions of the era.

Status
The social status of women in the Victoria Era is often seen as an illustration of the striking discrepancy between the nation's power and richness and what many consider its appalling social conditions. Victorian morality was full of contradictions. A plethora of social movements concerned with improving public morals co-existed with a class system that permitted and imposed harsh living conditions for many, such as women. In this period, an outward appearance of dignity and restraint was valued, but the usual "vices" continued, such as prostitution. In the Victorian era, the bathing machine was developed and flourished. It was a device to allow people to wade in the ocean at beaches without violating Victorian notions of modesty about having "limbs" revealed. The bathing machine was part of sea-bathing etiquette that was more rigorously enforced upon women than men.

Roaring Twenties
The Roaring Twenties is a term for society and culture in the 1920s in the Western world. It was a period of sustained economic prosperity with a distinctive cultural edge in the United States, Canada, and Western Europe, particularly in major cities.
Women's suffrage came about in many major countries in the 1920s, including United States, Canada, Great Britain. [73] many countries expanded women's voting rights in representative and direct democracies across the world such as, the US, Canada, Great Britain and most major European countries in 1917–21, as well as India. This influenced many governments and elections by increasing the number of voters available. Politicians responded by spending more attention on issues of concern to women, especially pacifism, public health, education, and the status of children. On the whole, women voted much like their menfolk, except they were more pacifistic. [74]
The 1920s marked a revolution in fashion. The new woman danced, drank, smoked and voted. She cut her hair short, wore make-up and partied. Sometimes she smoked a cigarette. She was known for being giddy and taking risks; she was a flapper. [75] More women took jobs making them more independent and free. With their desire for freedom and independence came as well change in fashion, welcoming a more comfortable style, where the waistline was just above the hips and loosen, and staying away from the Victorian style with a corset and tight waistline.

Great Depression
With widespread unemployment among men, poverty, and the need to help family members who are in even worse condition, The pressures were heavy on women during the Great Depression across the modern world. A primary role was as housewife. Without a steady flow of family income, their work became much harder in dealing with food and clothing and medical care. The birthrates fell everywhere, as children were postponed until families could financially support them. The average birthrate for 14 major countries fell 12% from 19.3 births per thousand population in 1930, to 17.0 in 1935. [76] In Canada, half of Roman Catholic women defied Church teachings and used contraception to postpone births. [77]
Among the few women in the labor force, layoffs were less common in the white-collar jobs and they were typically found in light manufacturing work. However, there was a widespread demand to limit families to one paid job, so that wives might lose employment if their husband was employed. [78] [79] [80] Across Britain, there was a tendency for married women to join the labor force, competing for part-time jobs especially. [81]
In rural and small-town areas, women expanded their operation of vegetable gardens to include as much food production as possible. In the United States, agricultural organizations sponsored programs to teach housewives how to optimize their gardens and to raise poultry for meat and eggs. [82] In American cities, African American women quiltmakers enlarged their activities, promoted collaboration, and trained neophytes. Quilts were created for practical use from various inexpensive materials and increased social interaction for women and promoted camaraderie and personal fulfillment. [83]
Oral history provides evidence for how housewives in a modern industrial city handled shortages of money and resources. Often they updated strategies their mothers used when they were growing up in poor families. Cheap foods were used, such as soups, beans and noodles. They purchased the cheapest cuts of meat—sometimes even horse meat—and recycled the Sunday roast into sandwiches and soups. They sewed and patched clothing, traded with their neighbors for outgrown items, and made do with colder homes. New furniture and appliances were postponed until better days. Many women also worked outside the home, or took boarders, did laundry for trade or cash, and did sewing for neighbors in exchange for something they could offer. Extended families used mutual aid—extra food, spare rooms, repair-work, cash loans—to help cousins and in-laws. [84]
In Japan, official government policy was deflationary and the opposite of Keynesian spending. Consequently, the government launched a nationwide campaign to induce households to reduce their consumption, focusing attention on spending by housewives. [85]
In Germany, the government tried to reshape private household consumption under the Four-Year Plan of 1936 to achieve German economic self-sufficiency. The Nazi women's organizations, other propaganda agencies and the authorities all attempted to shape such consumption as economic self-sufficiency was needed to prepare for and to sustain the coming war. Using traditional values of thrift and healthy living, the organizations, propaganda agencies and authorities employed slogans that called up traditional values of thrift and healthy living. However, these efforts were only partly successful in changing the behaviour of housewives. [86]

Religion
The Hindu , Jewish , Sikh , Islamic and Christian views about women have varied throughout the last two millennia, evolving along with or counter to the societies in which people have lived. For much of history, the role of women in the life of the church, both local and universal, has been downplayed, overlooked, or simply denied. [87] [88] [89]

Warfare
Warfare always engaged women as victims and objects of protection. [90] [91] During the twentieth century civilian women on the home front became increasingly important in supporting total warfare, as housewives, munitions workers, replacements for men in service, nurses, and combat soldiers. [92]

See also
The following is a list of articles in Wikipedia (and outside links where Wikipedia has no relevant articles) which are either about women's history, or containing relevant information, often in a "History" section.
WebPage index: 00076
Linux Virtual Server
Linux Virtual Server ( LVS ) is load balancing software for Linux kernel –based operating systems.
LVS is a free and open-source project started by Wensong Zhang in May 1998, subject to the requirements of the GNU General Public License (GPL), version 2. The mission of the project is to build a high-performance and highly available server for Linux using clustering technology, which provides good scalability, reliability and serviceability.

Overview
The major work of the LVS project is now to develop advanced IP load balancing software (IPVS), application-level load balancing software (KTCPVS), and cluster management components.
LVS can be used for building highly scalable and highly available network services, such as web, email, media and VoIP services, and integrating scalable network services into large-scale reliable e-commerce or e-government applications. LVS-based solutions already have been deployed in many real applications throughout the world, including Wikipedia .
The LVS components depend upon the Linux Netfilter framework, and its source code is available in the net/netfilter/ipvs subdirectory within the Linux kernel source. LVS is able to handle UDP, TCP layer-4 protocols as well as FTP passive connection by inspecting layer-7 packets. It provides a hierarchy of counters in the /proc directory.
The userland utility program used to configure LVS is called ipvsadm(8) , which requires superuser privileges to run.

Schedulers
LVS implements several balancing schedulers, listed below with the relevant source files: [3]

Glossary
Commonly used terms include the following: [4]

Examples
Setting up a virtual HTTP server with two real servers:
The first command assigns TCP port 80 on IP address 192.168.0.1 to the virtual server. The chosen scheduling algorithm for load balancing is round-robin ( -s rr ). The second and third commands are adding IP addresses of real servers to the LVS setup. The forwarded network packets shall be masked ( -m ).
Querying the status of the above configured LVS setup:

See also
WebPage index: 00077
Equinix
Equinix, Inc. is an American multinational company headquartered in Redwood City, California , that specializes in enabling global interconnection between organizations and their employees, customers, partners, data and clouds. The company is the leading global colocation data center provider by market share, [2] and it operates 175+ data centers in 44 major metropolitan areas in 22 countries on five continents.
Equinix was founded in 1998 to provide a neutral place where the networks forming the early internet could exchange data traffic. It expanded to Asia-Pacific in 2002 [3] and Europe in 2007. [4] The company later began operating facilities in Latin America in 2011 [5] and in the Middle East in 2012. [6] Its purchase of TelecityGroup in early 2016 established the company as the largest colocation provider in Europe. [7] In May 2017, Equinix completed the purchase of 29 Verizon data centers in a move to expand its presence across 15 markets in the U.S. and Latin America. [8]
The company offers colocation , interconnection solutions and related services to enterprises , content companies, systems integrators and 1,500+ network service providers worldwide. Equinix data centers host more than 2,750 cloud and IT service providers. Equinix offers several interconnection services, including Equinix Cross Connects, Equinix Performance Hub and Equinix Data Hub. The company operates the Equinix Cloud Exchange and an Internet Exchange. Its Professional Services group offers various consulting and technical support services.
Equinix says its broad geographic reach is a key differentiator that allows its customers to place equipment in proximity to their end users worldwide, which the company claims results in superior connectivity. Its global data center platform, which it calls Platform Equinix, is one of three components of a triple-ringed “ moat ” [9] the company says it must maintain to continue to outpace its competitors. It says the other two components are the interconnected industry ecosystems that populate its data center platform and its commitment to service excellence. [10]
The name Equinix combines words that describe what the company’s founders saw as its core attributes : Equ ality, n eutrality and i nternet e x change. The name is pronounced with a soft “e” on the first syllable so that it mimics the pronunciation of “equity.” [11]
Equinix reports 6,200+ employees globally and is listed on the NASDAQ stock exchange under the ticker symbol EQIX. [12]
Equinix reported 2016 revenues of $3.61 billion. [13]

History
Equinix was founded in 1998 by Al Avery and Jay Adelson , two facilities managers at Digital Equipment Corporation . The founders believed that existing data centers would not be sufficient to support the rapid growth of the internet and saw the opportunity to deploy data centers on a much larger scale to support this growth. In an interview, Adelson recalled that the early internet was "run by cowboys. … It just wasn't hardened, it wasn't commercialized. And for those of us that cared, we knew that a company like Equinix would need to exist. Somebody would have to be the steward of all this infrastructure." [14]
Equinix promoted its data center platform as a neutral place where competing networks could connect and share data traffic . [15] This peering model extended each carrier’s individual reach, regionally and globally, and its wider adoption enabled the massive peering that led to the explosive growth of the internet, the “network of networks.” [16]
Equinix focused on expanding interconnection from its inception, as it aimed to capitalize on the so-called “ network effect ,” through which each new customer would broaden the appeal of its platform. [17] Each new carrier, for example, would expand network route options for content providers, who were then increasingly drawn to Equinix and, in turn, increased Equinix’s appeal to advertisers who wanted to do business with those content providers. In an early interview, an Equinix marketing executive said the company’s strategy was to find “people who bring people who have relationships with people.” [15] The company’s first data centers opened in New York City , Silicon Valley , and the Washington D.C. area. [11] Soon after, the company began a continuous and steady expansion that aligned with its internal philosophy to always attempt to increase interconnection opportunities ahead of market demand. [18]

Expansion
Equinix’s overseas expansion began in 2002 in Asia-Pacific, after it acquired data centers in Hong Kong , Singapore , Sydney and Tokyo in mergers with i-STT and Pihana Pacific. [3]
Steve Smith took over as CEO in 2007, [1] the same year Equinix announced a $2 billion international expansion plan and entered the European market by acquiring data center operator IXEurope and its facilities in France , Germany , the Netherlands , Switzerland and the UK . That plan was completed with the 2010 opening of Equinix’s 50th global data center, in London, [19] but a more intense period of growth followed. Over the next seven years, the company nearly tripled its data center portfolio, growth the company attributes to increased demand for interconnection services caused by the emergence of cloud computing, along with the expansion of related trends such as the Internet of Things and big data . [20] [21] [22]
Equinix’s next major acquisition came in 2010, when the company purchased Switch and Data Facilities Company, Inc., a U.S. internet exchange and colocation services provider with locations in 23 North American markets. The transaction was valued at approximately $683.4 million. [23]
The company extended its operations to the Middle East and in Southeast Asia in 2012, announcing partnerships to enter the Dubai , UAE , [6] and Jakarta , Indonesia , markets. [24] Also in 2012, Equinix increased its commitment to Asia-Pacific, its fastest-growing region, by completing a $230.5 million all-cash transaction for Hong Kong-based data center provider Asia-Tone. In the transaction, Equinix gained a total of six data centers and one disaster recovery center spread across the Hong Kong, Shanghai and Singapore markets. At the time of the deal, Smith said the acquisition put Equinix “in a strong position to establish market leadership in Asia-Pacific." [25]
In 2014, Equinix strengthened its Latin American presence when it paid $225 million to complete the acquisition of ALOG Datacenters of Brazil S.A., the country’s leading provider of carrier-neutral data centers. Equinix had acquired a 53% stake in ALOG in 2011. [5]
In 2015, Equinix converted to a real estate investment trust (REIT) in a move to gain tax advantages and enhance shareholder value by offering a regular dividend. [26] In addition, Equinix acquired the professional services company Nimbo as part of a broader effort to develop its professional services business to assist customers executing data center migrations or advancing their network and hybrid cloud strategies. [27]
The year 2015 was an intense period of acquisition activity and global growth for Equinix. Early in the year, Equinix opened five new data centers on four continents , increasing the company's data center footprint to more than 10 million square feet. [28]
The largest acquisition in company history was announced at the end of May 2015, when Equinix said it would purchase the British company TelecityGroup . [7] The offer was cleared by the European Commission in November, but only after Equinix agreed to sell eight of its data centers around Europe to Digital Realty Trust for $874 million. [29] [30] In January 2016, Equinix announced that it had completed the Telecity acquisition in a transaction valued at approximately $3.8 billion.
The addition of TelecityGroup’s 34+ data centers more than doubled Equinix’s capacity in Europe, making the company the region’s largest retail colocation provider. The deal also added network and cloud density to better serve enterprise customers who viewed interconnection as a core IT design principle, and who were increasingly moving to highly interconnected, global data centers to accelerate business performance and innovation. [31]
The size of the deal raised questions about Equinix’s continued appetite for further expansion in the near term. Smith responded to the questions by noting Equinix ‘s strong balance sheet .
"This deal doesn't preclude us from taking action if there were other opportunities," he said. [7]
Four months after Smith’s comment, in September 2015, Equinix said it would buy Japanese provider Bit-Isle for $280 million. The deal, finalized in December 2015, doubled the number of Equinix data centers in Japan to 12 by adding five in Tokyo and one in Osaka . It also established Equinix as Japan’s fourth-largest data center provider. [32]
In 2016, Equinix opened new data centers in Dallas , Sydney and Tokyo [33] and announced a deal to acquire 29 data centers in 15 markets from Verizon for $3.6 billion. The purchase, which closed in May 2017, extended Equinix’s North American footprint into two new markets, Houston and Culpeper , Va., and expanded its Latin American presence by adding a facility in Sao Paulo and bringing the company into Bogota, Colombia , for the first time. [34] The company said all three new markets added an important element to its global platform, as Houston is an energy industry center, Culpeper is a highly secure hub for government business and Colombia has the highest number of subsea cable landings in Latin America. The deal also included more than 1,000 Verizon customers, more than 600 of whom were new to Equinix. [35] Many were from the enterprise market, which Equinix has aggressively targeted. [36]
In 2017, Equinix also opened a new data center in Sao Paulo. [37]
Smith frequently speaks of his goal to build Equinix into an “historically significant” company. He points to Equinix’s leading market position and commitment to continued expansion, saying it aligns with cloud-driven surges in global data traffic and the subsequent need for the colocation and interconnection services Equinix provides. “Equinix will be remembered in the history books for … how we interconnected the world,” he said. [38]

Data centers
Equinix has invested more than $17 billion since its founding to build its data center platform. [39]
Equinix calls its data centers International Business Exchanges (IBXs) to emphasize the collaboration and data exchange it says they enable. Chairman Peter Van Camp has compared them to " international airports where passengers from many different airlines make connections to get to their final destinations." [40] Smith said the interconnection available within his company's broad data center footprint allows organizations to distribute IT closer to employees, customers and markets without having to build and manage the needed connectivity themselves. "We spend a lot of capital to build these facilities and companies take advantage of it." [41] Equinix IBXs peer IP traffic to over 90% of the world's internet routes. Their Internet Exchanges route internet traffic between more than 1,400 telecommunications networks worldwide.
The company cites its global average uptime record to demonstrate the reliability of its facilities and services. Equinix received positive press coverage in the aftermath of Superstorm Sandy in 2012 after its customers in the Northeast U.S. experienced relatively limited outages, compared to widespread problems at other vendors. [42]
Equinix has received various International Standards Compliance (ISO) certifications related to physical and data security at its data centers. They include ISO / IEC 27001:2005 and 27001:2013 Information Security Management System Standard, related to information and physical security and business continuity, and the PCI-DSS Payment Card Industry Data Security Standard, related to physical security access to customer equipment. [43]

Cloud
Equinix says the cloud computing space is a primary focus. In a 2010 interview, Smith said the company’s top priority was to be “the home to as much cloud infrastructure as we possibly can.” [44] The Equinix Cloud Exchange is the focal point of company efforts to draw a heavy concentration of cloud providers inside its IBX data centers. It believes these providers can act as magnets to the businesses and other entities seeking services in the burgeoning cloud computing market. [45] As of the end of 2016, Equinix hosted more than 500 cloud providers. [46]
Some analysts have warned that long-term consolidation in the telecommunications industry caused by cloud will decrease demand for data center space and hurt the growth prospects of companies such as Equinix. [47] Equinix argues that accelerated cloud proliferation and adoption, along with the emergence of cloud-dependent trends and technologies such as big data, mobile and the Internet of Things, dramatically increase the value of its core interconnection offering. [20] [21] [22]
Equinix was described by an industry analyst as a “crucial middle man ,” connecting companies that build private clouds on the Equinix platform to the underlying networks and public cloud providers they depend on. [9]

Enterprise
Equinix has emphasized expanding its enterprise business as more companies move aggressively to compete as digital businesses. In a 2016 conference, Smith announced a company goal to exponentially increase its enterprise customer base in the next decade. Smith estimated the total addressable enterprise market at 350,000 globally and projected Equinix could land as many as 60,000 enterprise customers by 2026. [48] Equinix’s total customer base at the end of 2016 was about 8,500, including about 2,250 enterprise customers. [49]
Industry observers said a challenge for Equinix would be appealing to customers who didn’t know they needed what Equinix offers: “Courting enterprises … is more difficult than serving Equinix’s traditional customer base , consisting of financial, network, media, and cloud companies. These customers basically knew what they wanted from Equinix, and all Equinix had to do was deliver.” [50]
Equinix says its appeal to the enterprise market is based on its network density, cloud expertise and what it says is a growing enterprise need to obtain services from multiple cloud providers just to execute routine business. It argues the only way the enterprise can meet customer expectations is with what it called an “interconnection first” mindset [48] that prioritizes moving closer to end users and enabling faster data processing and analysis at the “edge of the network.” The company has devised what it calls an “edge strategy” for the enterprise, named the Interconnection Oriented Architecture. This strategy is designed to allow the enterprise to move closer to end users by capitalizing on Equinix’s global footprint and the access it provides to multiple networks and cloud providers. [51]
Equinix also said its interest in cultivating enterprise growth was to ward off the threat presented by the largest public cloud providers, all of whom it hosts in its data centers. [52]
Some analysts have argued that the continued growth of the public cloud giants – such as Amazon Web Services and Microsoft Azure – would eventually lead them to abandon third-party colocation providers like Equinix to build their own data centers and take advantage of economies of scale. [53]
In response, Equinix has positioned its facilities as meeting places where enterprises could directly access as many public cloud services as possible and build customized and business-critical applications. The company believes that attracting more enterprises as they begin relying on multi-cloud strategies will further establish Equinix as an essential interconnection point between the enterprise and the public cloud, including its largest providers. [51]

Ecosystems
Equinix has cultivated several digital ecosystems on its data center platform, and says they are an important differentiator because business models in the digital age depend on collaboration across industries and regions to innovate and speed up production cycles. [54]
Company managers said they began noticing in the early 2010s that customers in different ecosystems were collaborating inside their data centers. Equinix today positions itself in the broader market as a place to find and interconnect with partners, citing the number of mature ecosystems it hosts on its global data center platform. It has also pushed its channel partners to sell based on access to a variety of ecosystems on the Equinix platform. [55]
The ecosystems inside Equinix include: [54]

Sustainability
Equinix has implemented various renewable energy technologies and sustainability practices on its data center platform. In 2015, the company made a long-term pledge to power its entire data center platform with clean and renewable energy. [56]
The pledge followed criticism in 2014 from the environmental group Greenpeace , which said in an annual report on the environmental practices of Internet companies that Equinix had an insufficient commitment to renewable energy and carbon emissions mitigation. [57]
The following year, Equinix pledged to use 100% clean and renewable energy globally and signed deals with wind farms in Texas and Oklahoma to buy enough renewable energy to offset the energy consumption of its entire North American data center portfolio. [58]
Equinix said that, as a leader in an energy-intensive industry, it recognized that “delaying action on climate change will be costly in economic and human terms.” [56]
In a 2015 update to its 2014 report, Greenpeace said Equinix had made a “significant shift” in its renewable energy policies and that its 100% renewable power pledge “sets an important new bar that other colocation providers will need to meet.” [59]
Also in 2015, Equinix was also among the signatories of the White House ’s American Business Act on Climate Pledge. [60] As part of the pledge, signed in the run-up to the 2016 Paris Agreement on slowing climate change, dozens of U.S. corporations made commitments to a range of clean energy goals.

Data center (IBX) locations

See also
WebPage index: 00078
GNU General Public License
The GNU General Public License ( GNU GPL or GPL ) is a widely used free software license , which guarantees end users the freedom to run, study, share and modify the software. [6] The license was originally written by Richard Stallman of the Free Software Foundation (FSF) for the GNU Project , and grants the recipients of a computer program the rights of the Free Software Definition . [7] The GPL is a copyleft license, which means that derivative work can only be distributed under the same license terms. This is in distinction to permissive free software licenses , of which the BSD licenses and the MIT License are widely used examples. GPL was the first copyleft license for general use.
Historically, the GPL license family has been one of the most popular software licenses in the free and open-source software domain. [6] [8] [9] [10] [11] [12] [13] Prominent free software programs licensed under the GPL include the Linux kernel and the GNU Compiler Collection (GCC). David A. Wheeler argues that the copyleft provided by the GPL was crucial to the success of Linux -based systems, giving the programmers who contributed to the kernel the assurance that their work would benefit the whole world and remain free, rather than being exploited by software companies that would not have to give anything back to the community. [14]
In 2007, the third version of the license (GNU GPLv3) was released to address some perceived problems with the second version (GNU GPLv2) that were discovered during its long-time usage. To keep the license up to date, the GPL license includes an optional "any later version" clause, allowing users to choose between the original terms or the terms in new versions as updated by the FSF. Developers can omit it when licensing their software; for instance the Linux kernel is licensed under GPLv2 without the "any later version" clause. [15] [16]

History
The GPL was written by Richard Stallman in 1989, for use with programs released as part of the GNU project. The original GPL was based on a unification of similar licenses used for early versions of GNU Emacs (1985), [17] the GNU Debugger and the GNU C Compiler . [18] These licenses contained similar provisions to the modern GPL, but were specific to each program, rendering them incompatible, despite being the same license. [19] Stallman's goal was to produce one license that could be used for any project, thus making it possible for many projects to share code.
The second version of the license, version 2, was released in 1991. Over the following 15 years, members of the free software community became concerned over problems in the GPLv2 license that could let someone exploit GPL-licensed software in ways contrary to the license's intent. [20] These problems included tivoization (the inclusion of GPL-licensed software in hardware that refuses to run modified versions of its software), compatibility issues similar to those of the Affero General Public License —and patent deals between Microsoft and distributors of free and open source software, which some viewed as an attempt to use patents as a weapon against the free software community.
Version 3 was developed to attempt to address these concerns and was officially released on 29 June 2007. [21]

Version 1
Version 1 of the GNU GPL, [22] released on 25 February 1989, [23] prevented what were then the two main ways that software distributors restricted the freedoms that define free software. The first problem was that distributors may publish binary files only—executable, but not readable or modifiable by humans. To prevent this, GPLv1 stated that any vendor distributing binaries must also make the human-readable source code available under the same licensing terms (Sections 3a and 3b of the license).
The second problem was that distributors might add restrictions, either to the license, or by combining the software with other software that had other restrictions on distribution. The union of two sets of restrictions would apply to the combined work, thus adding unacceptable restrictions. To prevent this, GPLv1 stated that modified versions, as a whole, had to be distributed under the terms in GPLv1 (Sections 2b and 4 of the license). Therefore, software distributed under the terms of GPLv1 could be combined with software under more permissive terms, as this would not change the terms under which the whole could be distributed. However, software distributed under GPLv1 could not be combined with software distributed under a more restrictive license, as this would conflict with the requirement that the whole be distributable under the terms of GPLv1.

Version 2
According to Richard Stallman, the major change in GPLv2 was the "Liberty or Death" clause, as he calls it [19] – Section 7. The section says that licensees may distribute a GPL-covered work only if they can satisfy all of the license's obligations, despite any other legal obligations they might have. In other words, the obligations of the license may not be severed due to conflicting obligations. This provision is intended to discourage any party from using a patent infringement claim or other litigation to impair users' freedom under the license.
By 1990, it was becoming apparent that a less restrictive license would be strategically useful for the C library and for software libraries that essentially did the job of existing proprietary ones; [24] when version 2 of the GPL (GPLv2) was released in June 1991, therefore, a second license – the Library General Public License – was introduced at the same time and numbered with version 2 to show that both were complementary. The version numbers diverged in 1999 when version 2.1 of the LGPL was released, which renamed it the GNU Lesser General Public License to reflect its place in the philosophy.
Most commonly "GPLv2 or any later version" is stated by users of the license, to allow upgrading to GPLv3. See next section for details.

Version 3
In late 2005, the Free Software Foundation (FSF) announced work on version 3 of the GPL (GPLv3). On 16 January 2006, the first "discussion draft" of GPLv3 was published, and the public consultation began. The public consultation was originally planned for nine to fifteen months but finally stretched to eighteen months with four drafts being published. The official GPLv3 was released by FSF on 29 June 2007. GPLv3 was written by Richard Stallman, with legal counsel from Eben Moglen and the Software Freedom Law Center . [25]
According to Stallman, the most important changes are in relation to software patents , free software license compatibility, the definition of "source code", and hardware restrictions on software modification (" tivoization "). [25] [26] Other changes relate to internationalization, how license violations are handled, and how additional permissions can be granted by the copyright holder.
It also adds a provision that "strips" Digital Rights Management (DRM) of its legal value, so people can break anything a court might recognize as DRM on GPL software without breaking laws like the DMCA . [27]
The public consultation process was coordinated by the Free Software Foundation with assistance from Software Freedom Law Center, Free Software Foundation Europe , [28] and other free software groups. Comments were collected from the public via the gplv3.fsf.org web portal. [29] That portal runs purpose-written software called stet .
During the public consultation process, 962 comments were submitted for the first draft. [30] By the end, a total of 2,636 comments had been submitted. [31] [32] [33]
The third draft was released on 28 March 2007. [34] This draft included language intended to prevent patent-related agreements like the controversial Microsoft-Novell patent agreement and restricts the anti-tivoization clauses to a legal definition of a "User" or "consumer product". It also explicitly removed the section on "Geographical Limitations", whose probable removal had been announced at the launch of the public consultation.
The fourth discussion draft, [35] which was the last, was released on 31 May 2007. It introduced Apache License version 2.0 compatibility (prior versions are incompatible), clarified the role of outside contractors, and made an exception to avoid the perceived problems of a Microsoft–Novell style agreement, saying in Section 11 paragraph 6 that:
This aims to make future such deals ineffective. The license is also meant to cause Microsoft to extend the patent licenses it grants to Novell customers for the use of GPLv3 software to all users of that GPLv3 software; this is possible only if Microsoft is legally a "conveyor" of the GPLv3 software. [36] [37]
Also, early drafts of GPLv3 let licensors add an Affero -like requirement that would have plugged the ASP loophole in the GPL . [38] [39] As there were concerns expressed about the administrative costs of checking code for this additional requirement, it was decided to keep the GPL and the Affero license separated. [40]
Others, notably some high-profile developers of the Linux kernel , for instance Linus Torvalds , Greg Kroah-Hartman and Andrew Morton , commented to the mass media and made public statements about their objections to parts of discussion drafts 1 and 2. [41] The kernel developers referred to GPLv3 draft clauses regarding DRM / Tivoization , patents and "additional restrictions" and warned a Balkanisation of the "Open Source Universe". [41] [42] Linus Torvalds, who decided to not adopt the GPLv3 for the Linux kernel, [15] reiterated his criticism even years later. [43] [44]
GPLv3 improves compatibility with several open source software licenses such as Apache License, version 2.0, and the GNU Affero General Public License, which GPLv2 could not be combined with. [45] But on the downside, GPLv3 software can only be combined and share code with GPLv2 software if the used GPLv2 license has the optional "or later" clause and the software is upgraded to GPLv3. While the "GPLv2 or any later version" clause is considered by FSF as the most common form of licensing GPLv2 software, [46] for example Toybox developer Rob Landley described it as a lifeboat clause . [47] [48] Software projects licensed with the optional "or later" clause include the GNU Project , while a prominent example without the clause is the Linux kernel. [15]
The final version of the license text was published on 29 June 2007. [49]

Terms and conditions
The terms and conditions of the GPL must be made available to anybody receiving a copy of the work that has a GPL applied to it ("the licensee"). Any licensee who adheres to the terms and conditions is given permission to modify the work, as well as to copy and redistribute the work or any derivative version. The licensee is allowed to charge a fee for this service, or do this free of charge. This latter point distinguishes the GPL from software licenses that prohibit commercial redistribution. The FSF argues that free software should not place restrictions on commercial use, [50] and the GPL explicitly states that GPL works may be sold at any price.
The GPL additionally states that a distributor may not impose "further restrictions on the rights granted by the GPL". This forbids activities such as distributing of the software under a non-disclosure agreement or contract.
The fourth section for version 2 of the license and the seventh section of version 3 require that programs distributed as pre-compiled binaries be accompanied by a copy of the source code, a written offer to distribute the source code via the same mechanism as the pre-compiled binary, or the written offer to obtain the source code that the user got when they received the pre-compiled binary under the GPL. The second section of version 2 and the fifth section of version 3 also require giving "all recipients a copy of this License along with the Program". Version 3 of the license allows making the source code available in additional ways in fulfillment of the seventh section. These include downloading source code from an adjacent network server or by peer-to-peer transmission, provided that is how the compiled code was available and there are "clear directions" on where to find the source code.
The FSF does not hold the copyright for a work released under the GPL, unless an author explicitly assigns copyrights to the FSF (which seldom happens except for programs that are part of the GNU project). Only the individual copyright holders have the authority to sue when a license violation takes place.

Use of licensed software
Software under the GPL may be run for all purposes, including commercial purposes and even as a tool for creating proprietary software , for example when using GPL-licensed compilers . [51] Users or companies who distribute GPL-licensed works (e.g. software), may charge a fee for copies or give them free of charge. This distinguishes the GPL from shareware software licenses that allow copying for personal use but prohibit commercial distribution, or proprietary licenses where copying is prohibited by copyright law . The FSF argues that freedom-respecting free software should also not restrict commercial use and distribution (including redistribution): [50] the GPL explicitly states that GPL works may be sold at any price.
In purely private (or internal) use—with no sales and no distribution—the software code may be modified and parts reused without requiring the source code to be released. For sales or distribution, the entire source code need to be made available to end users, including any code changes and additions—in that case, copyleft is applied to ensure that end users retain the freedoms defined above. [52]
However, software running as an application program under a GPL-licensed operating system such as Linux is not required to be licensed under GPL or to be distributed with source-code availability—the licensing depends only on the used libraries and software components and not on the underlying platform. [53] For example, if a program consists only of own original custom software, or is combined with source code from other software components , [54] then the own custom software components need not be licensed under GPL and need not make their code available; even if the underlying operating system used is licensed under the GPL, applications running on it are not considered derivative works. [53] Only if GPLed parts are used in a program (and the program is distributed), then all other source code of the program needs to be made available under the same license terms. The GNU Lesser General Public license (LGPL) was created to have a weaker copyleft than the GPL, in that it does not require own custom-developed source code (distinct from the LGPLed parts) to be made available under the same license terms.

Copyleft
The distribution rights granted by the GPL for modified versions of the work are not unconditional. When someone distributes a GPL'd work plus his/her own modifications, the requirements for distributing the whole work cannot be any greater than the requirements that are in the GPL.
This requirement is known as copyleft. It earns its legal power from the use of copyright on software programs. Because a GPL work is copyrighted, a licensee has no right to redistribute it, not even in modified form (barring fair use ), except under the terms of the license. One is only required to adhere to the terms of the GPL if one wishes to exercise rights normally restricted by copyright law, such as redistribution. Conversely, if one distributes copies of the work without abiding by the terms of the GPL (for instance, by keeping the source code secret), he or she can be sued by the original author under copyright law.
Copyleft thus uses copyright law to accomplish the opposite of its usual purpose: instead of imposing restrictions, it grants rights to other people, in a way that ensures the rights cannot subsequently be taken away. It also ensures that unlimited redistribution rights are not granted, should any legal flaw be found in the copyleft statement. [ citation needed ]
Many distributors of GPL'ed programs bundle the source code with the executables . An alternative method of satisfying the copyleft is to provide a written offer to provide the source code on a physical medium (such as a CD) upon request. In practice, many GPL'ed programs are distributed over the Internet, and the source code is made available over FTP or HTTP . For Internet distribution, this complies with the license.
Copyleft applies only when a person seeks to redistribute the program. Developers may make private modified versions with no obligation to divulge the modifications, as long as they don't distribute the modified software to anyone else. Note that copyleft applies only to the software, and not to its output (unless that output is itself a derivative work of the program [55] ). For example, a public web portal running a modified derivative of a GPL'ed content management system is not required to distribute its changes to the underlying software, because its output is not a derivative.
There has been debate on whether it is a violation of the GPL to release the source code in obfuscated form, such as in cases in which the author is less willing to make the source code available. The consensus was that while unethical, it was not considered a violation. The issue was clarified when the license was altered with v2 to require that the "preferred" version of the source code be made available. [56]

License versus contract
The GPL was designed as a license , rather than a contract. [57] [58] In some Common Law jurisdictions, the legal distinction between a license and a contract is an important one: contracts are enforceable by contract law , whereas licenses are enforced under copyright law . However, this distinction is not useful in the many jurisdictions where there are no differences between contracts and licenses, such as Civil Law systems. [59]
Those who do not accept the GPL's terms and conditions do not have permission, under copyright law, to copy or distribute GPL licensed software or derivative works. However, if they do not redistribute the GPL'd program, they may still use the software within their organization however they like, and works (including programs) constructed by the use of the program are not required to be covered by this license.
Allison Randal argued that the GPLv3 as a license is unnecessarily confusing for lay readers, and could be simplified while retaining the same conditions and legal force. [60]
In April 2017 a US federal court ruled that an open-source license is an enforceable contract. [61]

Derivations
The text of the GPL is itself copyrighted , and the copyright is held by the Free Software Foundation.
The FSF permits people to create new licenses based on the GPL, as long as the derived licenses do not use the GPL preamble without permission. This is discouraged, however, since such a license might be incompatible with the GPL [62] and causes a perceived license proliferation .
Other licenses created by the GNU project include the GNU Lesser General Public License , the GNU Free Documentation License and Affero General Public License .
The text of the GPL is not itself under the GPL. The license's copyright disallows modification of the license. Copying and distributing the license is allowed since the GPL requires recipients to get "a copy of this License along with the Program". [63] According to the GPL FAQ, anyone can make a new license using a modified version of the GPL as long as he or she uses a different name for the license, does not mention "GNU", and removes the preamble, though the preamble can be used in a modified license if permission to use it is obtained from the Free Software Foundation (FSF) .

Linking and derived works

Libraries
According to the FSF , "The GPL does not require you to release your modified version, or any part of it. You are free to make modifications and use them privately, without ever releasing them." [64] However, if one releases a GPL-licensed entity to the public, there is an issue regarding linking: namely, whether a proprietary program that uses a GPL library is in violation of the GPL.
This key dispute is whether non-GPL software can legally statically link or dynamically link to GPL libraries. Different opinions exist on this issue. The GPL is clear in requiring that all derivative works of code under the GPL must themselves be under the GPL. Ambiguity arises with regards to using GPL libraries, and bundling GPL software into a larger package (perhaps mixed into a binary via static linking). This is ultimately a question not of the GPL per se , but of how copyright law defines derivative works. The following points of view exist:

Point of view: dynamic and static linking violate GPL
The Free Software Foundation (which holds the copyright of several notable GPL-licensed software products and of the license text itself) asserts that an executable which uses a dynamically linked library is indeed a derivative work. This does not however apply to separate programs communicating with one another. [65]
The Free Software Foundation also created the LGPL , which is nearly identical to the GPL, but with additional permissions to allow linking for the purposes of "using the library".
Richard Stallman and the FSF specifically encourage library-writers to license under the GPL so that proprietary programs cannot use the libraries, in an effort to protect the free-software world by giving it more tools than the proprietary world. [66]

Point of view: static linking violates GPL but unclear as of dynamic linking
Some people believe that while static linking produces derivative works, it is not clear whether an executable that dynamically links to a GPL code should be considered a derivative work (see Weak Copyleft ). Linux author Linus Torvalds agrees that dynamic linking can create derived works but disagrees over the circumstances. [67]
A Novell lawyer has written that dynamic linking not being derivative "makes sense" but is not "clear-cut", and that evidence for good-intentioned dynamic linking can be seen by the existence of proprietary Linux kernel drivers. [68]
In Galoob v. Nintendo the United States Ninth Circuit Court of Appeals defined a derivative work as having "'form' or permanence" and noted that "the infringing work must incorporate a portion of the copyrighted work in some form", [69] but there have been no clear court decisions to resolve this particular conflict.

Point of view: linking is irrelevant
According to an article in the Linux Journal , Lawrence Rosen (a one-time Open Source Initiative general counsel) argues that the method of linking is mostly irrelevant to the question about whether a piece of software is a derivative work ; more important is the question about whether the software was intended to interface with client software and/or libraries. [70] He states, "The primary indication of whether a new program is a derivative work is whether the source code of the original program was used [in a copy-paste sense], modified, translated or otherwise changed in any way to create the new program. If not, then I would argue that it is not a derivative work," [70] and lists numerous other points regarding intent, bundling, and linkage mechanism. He further argues on his firm's website [71] that such "market-based" factors are more important than the linking technique.
There is also the specific issue of whether a plugin or module (such as the NVidia or ATI graphics card kernel modules ) must also be GPL, if it could reasonably be considered its own work. This point of view suggests that reasonably separate plugins, or plugins for software designed to use plugins, could be licensed under an arbitrary license if the work is GPLv2. Of particular interest is the GPLv2 paragraph:
The GPLv3 has a different clause:
As a case study, some supposedly proprietary plugins and themes / skins for GPLv2 CMS software such as Drupal and WordPress have come under fire, with both sides of the argument taken. [72] [73]
The FSF differentiates on how the plugin is being invoked. If the plugin is invoked through dynamic linkage and it performs function calls to the GPL program then it is most likely a derivative work. [74]

Communicating and bundling with non-GPL programs
The mere act of communicating with other programs does not, by itself, require all software to be GPL; nor does distributing GPL software with non-GPL software. However, minor conditions must be followed that ensures the rights of GPL software is not restricted. The following is a quote from the gnu.org GPL FAQ , which describes to what extent software is allowed to communicate with and be-bundled-with GPL programs:
[75]
The FSF thus draws the line between "library" and "other program" via 1) "complexity" and "intimacy" of information exchange, and 2) mechanism (rather than semantics), but resigns that the question is not clear-cut and that in complex situations, case law will decide.

Legal status
The first known violation of the GPL was in 1989, when NeXT extended the GCC compiler to support Objective-C , but did not publicly release the changes. [76] After an inquiry they created a public patch . There was no lawsuit filed for this violation. [77]
In 2002, MySQL AB sued Progress NuSphere for copyright and trademark infringement in United States district court . NuSphere had allegedly violated MySQL's copyright by linking MySQL's GPL'ed code with NuSphere Gemini table without being in compliance with the license. After a preliminary hearing before Judge Patti Saris on 27 February 2002, the parties entered settlement talks and eventually settled. [78] After the hearing, FSF commented that "Judge Saris made clear that she sees the GNU GPL to be an enforceable and binding license." [79]
In August 2003, the SCO Group stated that they believed the GPL to have no legal validity, and that they intended to pursue lawsuits over sections of code supposedly copied from SCO Unix into the Linux kernel . This was a problematic stand for them, as they had distributed Linux and other GPL'ed code in their Caldera OpenLinux distribution, and there is little evidence that they had any legal right to do so except under the terms of the GPL. For more information, see SCO-Linux controversies and SCO v. IBM .
In April 2004, the netfilter / iptables project was granted a preliminary injunction against Sitecom Germany by Munich District Court after Sitecom refused to desist from distributing Netfilter's GPL'ed software in violation of the terms of the GPL. Harald Welte , of Netfilter, was represented by ifrOSS co-founder Till Jaeger. In July 2004, the German court confirmed this injunction as a final ruling against Sitecom. [80] The court's justification was that:
This exactly mirrored the predictions given previously by the FSF's Eben Moglen . This ruling was important because it was the first time that a court had confirmed that violating terms of the GPL could be a copyright violation and established jurisprudence as to the enforceability of the GPL version 2 under German law. [81]
In May 2005, Daniel Wallace filed suit against the Free Software Foundation in the Southern District of Indiana , contending that the GPL is an illegal attempt to fix prices (at zero). The suit was dismissed in March 2006, on the grounds that Wallace had failed to state a valid anti-trust claim; the court noted that "the GPL encourages, rather than discourages, free competition and the distribution of computer operating systems, the benefits of which directly pass to consumers". [82] Wallace was denied the possibility of further amending his complaint, and was ordered to pay the FSF's legal expenses.
On 8 September 2005, the Seoul Central District Court ruled that the GPL was not material to a case dealing with trade secrets derived from GPL-licensed work. [83] Defendants argued that since it is impossible to maintain trade secrets while being compliant with GPL and distributing the work, they are not in breach of trade secrets. This argument was considered without ground.
On 6 September 2006, the gpl-violations.org project prevailed in court litigation against D-Link Germany GmbH regarding D-Link's copyright-infringing use of parts of the Linux Kernel in storage devices they distributed. [84] The judgment stated that the GPL is valid, legally binding, and stands in German court. [85]
In late 2007, the BusyBox developers and the Software Freedom Law Center embarked upon a program to gain GPL compliance from distributors of BusyBox in embedded systems , suing those who would not comply. These were claimed to be the first US uses of courts for enforcement of GPL obligations. See BusyBox GPL lawsuits .
On 11 December 2008, the Free Software Foundation sued Cisco Systems, Inc. for copyright violations by its Linksys division, of the FSF's GPL-licensed coreutils , readline , Parted , Wget , GNU Compiler Collection , binutils , and GNU Debugger software packages, which Linksys distributes in the Linux firmware [86] of its WRT54G wireless routers , as well as numerous other devices including DSL and Cable modems, Network Attached Storage devices, Voice-Over-IP gateways, virtual private network devices and a home theater/media player device. [87]
After six years of repeated complaints to Cisco by the FSF, claims by Cisco that they would correct, or were correcting, their compliance problems (not providing complete copies of all source code and their modifications), of repeated new violations being discovered and reported with more products, and lack of action by Linksys (a process described on the FSF blog as a "five-years-running game of Whack-a-Mole" [87] ) the FSF took them to court.
Cisco settled the case six months later by agreeing "to appoint a Free Software Director for Linksys" to ensure compliance, "to notify previous recipients of Linksys products containing FSF programs of their rights under the GPL," to make source code of FSF programs freely available on its website, and to make a monetary contribution to the FSF. [88]
In 2011 it was noticed that GNU Emacs had violated its GPL license for the previous two years, by distributing binaries, but not the source code. [89] [90] [91] Richard Stallman described this incident as "a very bad mistake" , [92] no lawsuit was filed and the mistake was promptly resolved.

Compatibility and multi-licensing
Code licensed under several other licenses can be combined with a program under the GPL without conflict, as long as the combination of restrictions on the work as a whole does not put any additional restrictions beyond what GPL allows. [93] In addition to the regular terms of the GPL, there are additional restrictions and permissions one can apply:
FSF maintains a list [99] of GPL- compatible free software licenses [100] [101] with many of the most common free software licenses, such as the original MIT/X license , the BSD license (in its current 3-clause form) and the Artistic License 2.0. [102]
David A. Wheeler has advocated that free/open source software developers use only GPL-compatible licenses, because doing otherwise makes it difficult for others to participate and contribute code. [103] As a specific example of license incompatibility, Sun Microsystems ' ZFS cannot be included in the GPL-licensed Linux kernel, because it is licensed under the GPL-incompatible CDDL . Furthermore, ZFS is protected by patents, so distributing an independently developed GPL-ed implementation would still require Oracle's permission. [104]
A number of businesses use multi-licensing to distribute a GPL version and sell a proprietary license to companies wishing to combine the package with proprietary code, using dynamic linking or not. Examples of such companies include MySQL AB , Digia PLC ( Qt framework , before 2011 from Nokia ), Red Hat ( Cygwin ) and Riverbank Computing ( PyQt ). Other companies, like the Mozilla Foundation (products include Mozilla Application Suite , Mozilla Thunderbird and Mozilla Firefox ), used multi-licensing to distribute versions under the GPL and some other open-source licenses.

Text and other media
It is possible to use the GPL for text documents instead of computer programs, or more generally for all kinds of media, if it is clear what constitutes the source code (defined as "the preferred form of the work for making changes in it"). [105] For manuals and textbooks, though, the FSF recommends the GNU Free Documentation License (GFDL) instead, which it created for this purpose. [106] Nevertheless, the Debian developers recommended (in a resolution adopted in 2006) to license documentation for their project under the GPL, because of the incompatibility of the GFDL with the GPL (text licensed under the GFDL cannot be incorporated into GPL software). [107] [108] Also, the FLOSS Manuals foundation, an organization devoted to creating manuals for free software, decided to eschew the GFDL in favor of the GPL for its texts in 2007. [109]
If the GPL is used for fonts, any documents or images made with such fonts might also have to be distributed under the terms of the GPL. This is not the case in countries like the US and Canada where copyright law is inapplicable to the appearance of fonts, though program code inside a font file may still be covered—which can complicate font embedding (since the document could be considered 'linked' to the font). FSF provides an exception for cases where this is not desired. [110] [111]

Adoption
Historically, the GPL license family has been one of the most popular software licenses in the FOSS domain. [6] [8] [9] [10] [11] [13]
A 1997 survey of MetaLab , then the largest free software archive, showed that the GPL accounted for about half of the software licensed therein. [8] Similarly, a 2000 survey of Red Hat Linux 7.1 found that 53% of the source code was licensed under the GPL. [9] As of 2003 [update] , about 68% of all projects and 82.1% of the OSI certified licensed projects listed on SourceForge.net were from the GPL license family. [112] As of August 2008 [update] , the GPL family accounted for 70.9% of the 44,927 free software projects listed on Freecode . [10]
After the release of the GPLv3 in June 2007, adoption of this new GPL version was much discussed [113] and some projects decided against upgrading. For instance the linux kernel , [15] [44] MySQL , [114] BusyBox , [115] [116] AdvFS , [117] Blender , [118] [119] and VLC media player [120] decided against adopting the GPLv3. On the other hand, in 2009, two years after the release of the GPLv3, Google open-source programs office manager Chris DiBona reported that the number of open-source projects licensed software that had moved to GPLv3 from GPLv2 was 50%, counting the projects hosted at Google Code . [11]
In 2011, four years after the release of the GPLv3, 6.5% of all open-source license projects are GPLv3 while 42.5% are GPLv2 according to Black Duck Software data. [121] [122] Following in 2011 451 Group analyst Matthew Aslett argued in a blog post that copyleft licenses went into decline and permissive licenses increased, based on statistics from Black Duck Software. [123] [124] Similarly, in February 2012 Jon Buys reported that among the top 50 projects on GitHub five projects were under a GPL license, including dual licensed and AGPL projects. [125]
GPL usage statistic from 2009 to 2013 was extracted from Freecode data by Walter van Holst while analyzing license proliferation . [12]
In August 2013, according to Black Duck Software, the website's data show that the GPL license family is used by 54% of open-source projects, with a breakdown of the individual licenses shown in the following table. [13] However, a later study in 2013 showed that software licensed under the GPL license family has increased, and that even the data from Black Duck Software have shown a total increase of software projects licensed under GPL. The study used public information gathered from repositories of the Debian Project , and the study criticized Black Duck Software for not publishing their methodology used in collecting statistics. [128] Daniel German, Professor in the Department of Computer Science at the University of Victoria in Canada, presented a talk in 2013 about the methodological challenges in determining which are the most widely used free software licenses, and showed how he could not replicate the result from Black Duck Software. [129]
In 2015 according to BlackDuck the GPLv2 lost its first position on the MIT license and is now second, the GPLv3 dropped to fourth place while the Apache license kept its third position. [6]
A March 2015 analysis of the GitHub repositories revealed for the GPL license family an usage percentage of approx. 25% among licensed projects. [134] In June 2016 an analysis of Fedora Project 's packages revealed the GNU GPL version 2 or later as the most popular license, and the GNU GPL family as the most popular license family (followed by the MIT, BSD, and GNU LGPL families). [135]

Reception

Legal barrier to app stores
The GPL License is incompatible with many application digital distribution systems, like the Mac App Store , and certain other software distribution platforms (on smartphones as well as PCs). The problem lies in the right "To make a copy for your neighbour", as this right is violated by the integrated DRM-Systems made to prevent copying of paid software. Even if the application is free-as-in-beer in the App Store in question, it might result in a violation of that app store's terms. [136]
Note that there is a distinction between an app store , which sells DRM -restricted software under proprietary licenses, and the more general concept of digital distribution via some form of online software repository. Various UNIX-like distributions provide app repositories, including Fedora , RHEL , CentOS , Ubuntu , Debian , FreeBSD , OpenBSD and so on. These specific app repos all contain GPL-licensed software apps, in some cases even when the core project does not permit GPL-licensed code in the base system (for instance OpenBSD [137] ). In other cases, such as the Ubuntu App Store , proprietary commercial software applications and GPL-licensed applications are both available via the same system; the reason that the Mac App Store (and similar projects) is incompatible with GPL-licensed apps is not inherent in the concept of an app store, but is rather specifically due to Apple's terms-of-use requirement [136] that all apps in the store utilize Apple DRM-restrictions. Ubuntu's app store does not demand any such requirement: "These terms do not limit or restrict your rights under any applicable open source software licenses." [138]

Microsoft
In 2001, Microsoft CEO Steve Ballmer referred to Linux as "a cancer that attaches itself in an intellectual property sense to everything it touches". [139] [140] In response to Microsoft's attacks on the GPL, several prominent Free Software developers and advocates released a joint statement supporting the license. [141] Microsoft has released Microsoft Windows Services for UNIX , which contains GPL-licensed code. In July 2009, Microsoft itself released a body of around 20,000 lines of Linux driver code under the GPL. [142] The Hyper-V code that is part of the submitted code used open-source components licensed under the GPL and was originally statically linked to proprietary binary parts, the latter being inadmissible in GPL-licensed software. [143]

"Viral" nature
The description of the GPL as "viral" , when called 'General Public Virus' or 'GNU Public Virus' (GPV), dates back to a year after the GPLv1 was released. [144] [145] [146] [147] [148] [149]
In 2001 the term received broader public attention when Craig Mundie , Microsoft Senior Vice President, described the GPL as being "viral". [150] Mundie argues that the GPL has a "viral" effect in that it only allows the conveyance of whole programs, which means programs that link to GPL libraries must themselves be under a GPL-compatible license, else they cannot be combined and distributed.
In 2006 Richard Stallman responded in an interview that Mundie's metaphor of a "virus" is wrong as software under the GPL does not "attack" or "infect" other software. Stallman believes that comparing the GPL to a virus is an extremely unfriendly thing to say, and that a better metaphor for software under the GPL would be a spider plant : If one takes a piece of it and puts it somewhere else, it grows there too. [151] [152] [153]
On the other hand, the concept of a viral nature of the GPL was taken up by others later too. [154] [155] [156] [157] [158] For instance in 2008 the California Western School of Law characterized the GPL as: "The GPL license is ‘viral,’ meaning any derivative work you create containing even the smallest portion of the previously GPL licensed software must also be licensed under the GPL license" . [159]

Barrier to commercialization
The FreeBSD project has stated that "a less publicized and unintended use of the GPL is that it is very favorable to large companies that want to undercut software companies. In other words, the GPL is well suited for use as a marketing weapon, potentially reducing overall economic benefit and contributing to monopolistic behavior" and that the GPL can "present a real problem for those wishing to commercialize and profit from software" . [160]
Richard Stallman wrote about the practice of selling license exceptions to free software licenses as an example of ethically acceptable commercialization practice. Selling exceptions here means that the copyright holder of a given software releases it (along with the corresponding source code) to the public under a free software license, "then lets customers pay for permission to use the same code under different terms, for instance allowing its inclusion in proprietary applications". Stallman considered selling exceptions "acceptable since the 1990s, and on occasion I've suggested it to companies. Sometimes this approach has made it possible for important programs to become free software". Despite that the FSF doesn't practice selling exceptions, a comparison with the X11 license (which is a non-copyleft free software license) is proposed for suggesting that this commercialization technique should be regarded as ethically acceptable. Releasing a given program under a noncopyleft free software license would permit embedding the code in proprietary software. Stallman comments that "either we have to conclude that it's wrong to release anything under the X11 license—a conclusion I find unacceptably extreme—or reject this implication. Using a noncopyleft license is weak, and usually an inferior choice, but it's not wrong. In other words, selling exceptions permits some embedding in proprietary software, and the X11 license permits even more embedding. If this doesn't make the X11 license unacceptable, it doesn't make selling exceptions unacceptable". [161]

Open-source criticism
In 2000 developer and author Nikolai Bezroukov published an analysis and comprehensive critique of GPL's foundations and Stallman's software development model, called "Labyrinth of Software Freedom". [162] [163]
In 2005, open source software advocate Eric S. Raymond questioned the relevance of GPL at that point in time for the FOSS ecosystem, stating: "We don't need the GPL anymore. It's based on the belief that open source software is weak and needs to be protected. Open source would be succeeding faster if the GPL didn't make lots of people nervous about adopting it.". [164] Richard Stallman replied that: "GPL ensure that every user of a program gets the essential freedoms—to run it, to study and change the source code, to redistribute copies, and to publish modified versions ... [Raymond] addresses the issue in terms of different goals and values—those of "open source," which do not include defending software users' freedom to share and change software." [165]
In 2007 Allison Randal , who took part in the GPL draft committee, criticized the GPLv3 for being incompatible with the GPLv2 [166] and for missing clarity in the formulation. [167] Similarly, Whurley prophesised in 2007 the downfall of the GPL due to the lack of focus for the developers with GPLv3 which would drive them towards permissive licenses. [168]
In 2009 David Chisnall described in an InformIT article, "The Failure of the GPL" , the problems with the GPL, among them incompatibility and complexity of the license text. [169]
In 2014 dtrace developer and Joyent CTO Bryan Cantrill called the copyleft GPL a "Corporate Open Source Anti-pattern " by being "anti-collaborative" and recommended instead permissive software licenses. [170]

GPLv3 criticism
Already in September 2006, in the draft process of the GPLv3, several high-profile developers of the Linux kernel , for instance Linus Torvalds , Greg Kroah-Hartman and Andrew Morton , warned on a splitting of the FOSS community: "the release of GPLv3 portends the Balkanisation of the entire Open Source Universe upon which we rely." . [41] Similarly Benjamin Mako Hill argued in 2006 on the GPLv3 draft, noting that a united, collaborating community is more important than a single license. [171]
Following the GPLv3 release in 2007, some journalists [44] [121] [172] and Toybox developer Rob Landley [47] [48] criticized that with the introduction of the GPLv3 the split between the open source and free software community became wider than ever. As the significantly extended GPLv3 is essentially incompatible with the GPLv2, [94] compatibility between both is only given under the optional "or later" clause of the GPL, which was not taken for instance by the Linux kernel . [15] Bruce Byfield noted that before the release of the GPLv3, the GPLv2 was a unifying element between the open-source and the free software community. [121]
For the LGPLv3, GNU TLS maintainer Nikos Mavrogiannopoulos similarly argued, "If we assume that its [the LGPLv3] primary goal is to be used by free software, then it blatantly fails that.", [173] after he re-licensed GNU TLS from LGPLv3 back to LGPLv2.1 due to license compatibility issues. [174] [175]
Lawrence Rosen , attorney and computer specialist, praised in 2007 how the community using the Apache license were now able to work together with the GPL community in a compatible manner, as the problems of GPLv2 compatibility with Apache licensed software were resolved with the GPLv3. He said, "I predict that one of the biggest success stories of GPLv3 will be the realization that the entire universe of free and open source software can thus be combined into comprehensive open source solutions for customers worldwide." [176]
In July 2013 Flask developer Armin Ronacher draw a less optimistic resume on the GPL compatibility in the FOSS ecosystem when he concluded: "When the GPL is involved the complexities of licensing becomes a non fun version of a riddle." , also noting that the ASL 2.0 GPLv2 conflict still has impact on the ecosystem. [177] [178]

See also
WebPage index: 00079
Mirror website
Mirror websites or mirrors are replicas of other websites . The main purpose of mirrors is often reduced network traffic , improved access speed , or improved availability of the original site. [1] [2] Such websites have different URLs than the original, but host identical content to it [3] Mirrors can also serve as real-time backups. [4]

Examples
Examples of websites with notable mirrors are KickassTorrents , [5] [6] [7] [8] The Pirate Bay [9] [10] [11] [12] WikiLeaks , [13] [14] the website of the Environmental Protection Agency [15] [16] and Wikipedia . [17] [18] [19]

Malicious mirrors
There are known cases of mirror websites which attempt to gain sensitive information of or distribute malware to its users. [20] Other types of malicous mirrors might attempt to make profit from the content of other websites, identify users or manipulate website contents.

See also
WebPage index: 00080
Android (operating system)
Android is a mobile operating system developed by Google , based on the Linux kernel and designed primarily for touchscreen mobile devices such as smartphones and tablets . Android's user interface is mainly based on direct manipulation , using touch gestures that loosely correspond to real-world actions, such as swiping, tapping and pinching, to manipulate on-screen objects, along with a virtual keyboard for text input. In addition to touchscreen devices, Google has further developed Android TV for televisions, Android Auto for cars, and Android Wear for wrist watches, each with a specialized user interface. Variants of Android are also used on notebooks , game consoles , digital cameras , and other electronics.
Initially developed by Android Inc., which Google bought in 2005, Android was unveiled in 2007, along with the founding of the Open Handset Alliance – a consortium of hardware , software , and telecommunication companies devoted to advancing open standards for mobile devices. Beginning with the first commercial Android device in September 2008, the operating system has gone through multiple major releases, with the current version being 7.0 "Nougat" , released in August 2016. Android applications (" apps ") can be downloaded from the Google Play store, which features over 2.7 million apps as of February 2017. Android has been the best-selling OS on tablets since 2013, and runs on the vast majority [a] of smartphones. As of May 2017 [update] , Android has two billion monthly active users, and it has the largest installed base of any operating system.
Android's source code is released by Google under an open source license , although most Android devices ultimately ship with a combination of free and open source and proprietary software, including proprietary software required for accessing Google services. Android is popular with technology companies that require a ready-made, low-cost and customizable operating system for high-tech devices. Its open nature has encouraged a large community of developers and enthusiasts to use the open-source code as a foundation for community-driven projects, which deliver updates to older devices, add new features for advanced users or bring Android to devices originally shipped with other operating systems. The extensive variation of hardware in Android devices causes significant delays for software upgrades, with new versions of the operating system and security patches typically taking months before reaching consumers, or sometimes not at all. The success of Android has made it a target for patent and copyright litigation as part of the so-called " smartphone wars " between technology companies.

History
Android Inc. was founded in Palo Alto, California in October 2003 by Andy Rubin , Rich Miner , Nick Sears, and Chris White. [12] [13] Rubin described the Android project as "tremendous potential in developing smarter mobile devices that are more aware of its owner's location and preferences". [13] The early intentions of the company were to develop an advanced operating system for digital cameras , and this was the basis of its pitch to investors in April 2004. [14] The company then decided that the market for cameras was not large enough for its goals, and by five months later it had diverted its efforts and was pitching Android as a handset operating system that would rival Symbian and Microsoft Windows Mobile . [14] [15]
Rubin had difficulty attracting investors early on, and Android was facing eviction from its office space. Steve Perlman , a close friend of Rubin, brought him $10,000 in cash in an envelope, and shortly thereafter wired an undisclosed amount as seed funding. Perlman refused a stake in the company, and has stated "I did it because I believed in the thing, and I wanted to help Andy." [16] [17]
In July 2005, [13] Google acquired Android Inc. for at least $50 million. [18] Its key employees, including Rubin, Miner and White, joined Google as part of the acquisition. [13] Not much was known about the secretive Android at the time, with the company having provided few details other than that it was making software for mobile phones. [13] At Google, the team led by Rubin developed a mobile device platform powered by the Linux kernel . Google marketed the platform to handset makers and carriers on the promise of providing a flexible, upgradeable system. [19] Google had "lined up a series of hardware components and software partners and signaled to carriers that it was open to various degrees of cooperation". [20]
Speculation about Google's intention to enter the mobile communications market continued to build through December 2006. [21] An early prototype had a close resemblance to a BlackBerry phone, with no touchscreen and a physical QWERTY keyboard , but the arrival of 2007's Apple iPhone meant that Android "had to go back to the drawing board". [22] [23] Google later changed its Android specification documents to state that "Touchscreens will be supported", although "the Product was designed with the presence of discrete physical buttons as an assumption, therefore a touchscreen cannot completely replace physical buttons". [24] In September 2007, InformationWeek covered an Evalueserve study reporting that Google had filed several patent applications in the area of mobile telephony. [25] [26]
On November 5, 2007, the Open Handset Alliance , a consortium of technology companies including Google, device manufacturers such as HTC , Motorola and Samsung , wireless carriers such as Sprint and T-Mobile , and chipset makers such as Qualcomm and Texas Instruments , unveiled itself, with a goal to develop "the first truly open and comprehensive platform for mobile devices". [27] [28] [29] The first commercially available smartphone running Android was the HTC Dream , also known as T-Mobile G1, announced on September 23, 2008. [30] [31]
Since 2008, Android has seen numerous updates which have incrementally improved the operating system, adding new features and fixing bugs in previous releases. Each major release is named in alphabetical order after a dessert or sugary treat, with the first few Android versions being called " Cupcake ", " Donut ", " Eclair ", and " Froyo ", respectively. During its announcement of Android KitKat in 2013, Google explained that "Since these devices make our lives so sweet, each Android version is named after a dessert", although a Google spokesperson told CNN in an interview that "It’s kind of like an internal team thing, and we prefer to be a little bit — how should I say — a bit inscrutable in the matter, I’ll say". [32]
In 2010, Google launched its Nexus series of devices, a lineup in which Google partnered with different device manufacturers to produce new devices and introduce new Android versions. The series was described as having "played a pivotal role in Android's history by introducing new software iterations and hardware standards across the board", and became known for its " bloat-free " software with "timely [...] updates". [33] At its developer conference in May 2013, Google announced a special version of the Samsung Galaxy S4 , where, instead of using Samsung's own Android customization, the phone ran "stock Android" and was promised to receive new system updates fast. [34] The device would become the start of the Google Play edition program, and was followed by other devices, including the HTC One Google Play edition, [35] and Moto G Google Play edition. [36] In 2015, Ars Technica wrote that "Earlier this week, the last of the Google Play edition Android phones in Google's online storefront were listed as "no longer available for sale"" and that "Now they're all gone, and it looks a whole lot like the program has wrapped up". [37] [38]
From 2008 to 2013, Hugo Barra served as product spokesperson, representing Android at press conferences and Google I/O , Google’s annual developer-focused conference. He left Google in August 2013 to join Chinese phone maker Xiaomi . [39] [40] Less than six months earlier, Google's then- CEO Larry Page announced in a blog post that Andy Rubin had moved from the Android division to take on new projects at Google, and that Sundar Pichai would become the new Android lead. [41] [42] Pichai himself would eventually switch positions, becoming the new CEO of Google in August 2015 following the company's restructure into the Alphabet conglomerate, [43] [44] making Hiroshi Lockheimer the new head of Android. [45] [46]
In June 2014, Google announced Android One , a set of "hardware reference models" that would "allow [device makers] to easily create high-quality phones at low costs", designed for consumers in developing countries. [47] [48] [49] In September, Google announced the first set of Android One phones for release in India. [50] [51] However, Recode reported in June 2015 that the project was "a disappointment", citing "reluctant consumers and manufacturing partners" and "misfires from the search company that has never quite cracked hardware". [52] Plans to relaunch Android One surfaced in August 2015, [53] with Africa announced as the next location for the program a week later. [54] [55] A report from The Information in January 2017 stated that Google was "expanding its “Android One” program for low-cost smartphones to the U.S. in coming months". [56] [57]
Google introduced the Pixel and Pixel XL smartphones in October 2016, marketed as being the first phones made by Google, [58] [59] and exclusively featured certain software features, such as the Google Assistant , before wider rollout. [60] [61] The Pixel phones replaced the Nexus series, [62] and Rick Osterloh, Google's senior vice president of hardware, confirmed in March 2017 that a successor to the Pixel is coming later in 2017. [63]

Features

Interface
Android's default user interface is mainly based on direct manipulation , using touch inputs that loosely correspond to real-world actions, like swiping, tapping, pinching, and reverse pinching to manipulate on-screen objects, along with a virtual keyboard . [64] Game controllers and full-size physical keyboards are supported via Bluetooth or USB . [65] [66] The response to user input is designed to be immediate and provides a fluid touch interface, often using the vibration capabilities of the device to provide haptic feedback to the user. Internal hardware, such as accelerometers , gyroscopes and proximity sensors are used by some applications to respond to additional user actions, for example adjusting the screen from portrait to landscape depending on how the device is oriented, [67] or allowing the user to steer a vehicle in a racing game by rotating the device, simulating control of a steering wheel . [68]
Android devices boot to the homescreen, the primary navigation and information "hub" on Android devices, analogous to the desktop found on personal computers. Android homescreens are typically made up of app icons and widgets ; app icons launch the associated app, whereas widgets display live, auto-updating content, such as a weather forecast , the user's email inbox, or a news ticker directly on the homescreen. [69] A homescreen may be made up of several pages, between which the user can swipe back and forth. [70] Third-party apps available on Google Play and other app stores can extensively re- theme the homescreen, [71] and even mimic the look of other operating systems, such as Windows Phone . [72] Most manufacturers customize the look and features of their Android devices to differentiate themselves from their competitors. [73]
Along the top of the screen is a status bar, showing information about the device and its connectivity. This status bar can be "pulled" down to reveal a notification screen where apps display important information or updates. [70] Notifications are "short, timely, and relevant information about your app when it’s not in use", and when tapped, users are directed to a screen inside the app relating to the notification. [74] Beginning with Android 4.1 "Jelly Bean" , "expandable notifications" allow the user to tap an icon on the notification in order for it to expand and display more information and possible app actions right from the notification. [75]
An All Apps screen lists all installed applications, with the ability for users to drag an app from the list onto the home screen. A Recents screen lets users switch between recently used apps. [70]

Applications
Applications (" apps "), which extend the functionality of devices, are written using the Android software development kit (SDK) [76] and, often, the Java programming language. [77] Java may be combined with C / C++ , [78] together with a choice of non-default runtimes that allow better C++ support. [79] The Go programming language is also supported, although with a limited set of application programming interfaces (API). [80] In May 2017, Google announced support for Android app development in the Kotlin programming language . [81] [82]
The SDK includes a comprehensive set of development tools, [83] including a debugger , software libraries , a handset emulator based on QEMU , documentation, sample code, and tutorials. Initially, Google's supported integrated development environment (IDE) was Eclipse using the Android Development Tools (ADT) plugin; in December 2014, Google released Android Studio , based on IntelliJ IDEA , as its primary IDE for Android application development. Other development tools are available, including a native development kit (NDK) for applications or extensions in C or C++, Google App Inventor , a visual environment for novice programmers, and various cross platform mobile web applications frameworks . In January 2014, Google unveiled an framework based on Apache Cordova for porting Chrome HTML 5 web applications to Android, wrapped in a native application shell. [84]
Android has a growing selection of third-party applications, which can be acquired by users by downloading and installing the application's APK (Android application package) file, or by downloading them using an application store program that allows users to install, update, and remove applications from their devices. Google Play Store is the primary application store installed on Android devices that comply with Google's compatibility requirements and license the Google Mobile Services software. [85] [86] Google Play Store allows users to browse, download and update applications published by Google and third-party developers; as of July 2013 [update] , there are more than one million applications available for Android in Play Store. [87] As of July 2013 [update] , 50 billion applications have been installed. [88] [89] Some carriers offer direct carrier billing for Google Play application purchases, where the cost of the application is added to the user's monthly bill. [90] As of May 2017, there are over one billion active users a month for Gmail, Android, Chrome, Google Play and Maps.
Due to the open nature of Android, a number of third-party application marketplaces also exist for Android, either to provide a substitute for devices that are not allowed to ship with Google Play Store, provide applications that cannot be offered on Google Play Store due to policy violations, or for other reasons. Examples of these third-party stores have included the Amazon Appstore , GetJar , and SlideMe. F-Droid , another alternative marketplace, seeks to only provide applications that are distributed under free and open source licenses . [85] [91] [92] [93]

Memory management
Since Android devices are usually battery-powered, Android is designed to manage processes to keep power consumption at a minimum. When an application is not in use the system suspends its operation so that, while available for immediate use rather than closed, it does not use battery power or CPU resources. [94] [95] Android manages the applications stored in memory automatically: when memory is low, the system will begin invisibly and automatically closing inactive processes, starting with those that have been inactive for longest. [96] [97] Lifehacker reported in 2011 that third-party task killers were doing more harm than good. [98]

Virtual reality
At Google I/O on May 2016, Google announced Daydream , a virtual reality platform that relies on a smartphone and provides VR capabilities through a virtual reality headset and controller designed by Google itself. [99] The platform is built into Android starting with Android Nougat , differentiating from standalone support for VR capabilities. The software is available for developers, and was released in 2016.

Hardware
The main hardware platform for Android is the ARM ( ARMv7 and ARMv8-A architectures), with x86 , MIPS and MIPS64 , and x86-64 architectures also officially supported in later versions of Android. [100] [101] [102] The unofficial Android-x86 project provided support for the x86 architectures ahead of the official support. [103] [104] MIPS architecture was also supported before Google did. Since 2012, Android devices with Intel processors began to appear, including phones [105] and tablets. While gaining support for 64-bit platforms, Android was first made to run on 64-bit x86 and then on ARM64 . Since Android 5.0 "Lollipop", 64-bit variants of all platforms are supported in addition to the 32-bit variants. [100]
Requirements for the minimum amount of RAM for devices running Android 5.1 range from 512 MB of RAM for normal-density screens, to about 1.8 GB for high-density screens. [106] The recommendation for Android 4.4 is to have at least 512 MB of RAM, [107] while for "low RAM" devices 340 MB is the required minimum amount that does not include memory dedicated to various hardware components such as the baseband processor . [108] Android 4.4 requires a 32-bit ARMv7 , MIPS or x86 architecture processor (latter two through unofficial ports), [103] [109] together with an OpenGL ES 2.0 compatible graphics processing unit (GPU). [110] Android supports OpenGL ES 1.1, 2.0, 3.0, 3.1 and as of latest major version, 3.2 and Vulkan . Some applications may explicitly require a certain version of the OpenGL ES, and suitable GPU hardware is required to run such applications. [110]
Android devices incorporate many optional hardware components, including still or video cameras, GPS , orientation sensors , dedicated gaming controls, accelerometers , gyroscopes , barometers, magnetometers , proximity sensors , pressure sensors , thermometers, and touchscreens . Some hardware components are not required, but became standard in certain classes of devices, such as smartphones, and additional requirements apply if they are present. Some other hardware was initially required, but those requirements have been relaxed or eliminated altogether. For example, as Android was developed initially as a phone OS, hardware such as microphones were required, while over time the phone function became optional. [89] Android used to require an autofocus camera, which was relaxed to a fixed-focus camera [89] if present at all, since the camera was dropped as a requirement entirely when Android started to be used on set-top boxes .
In addition to running on smartphones and tablets, several vendors run Android natively on regular PC hardware with a keyboard and mouse. [111] [112] [113] [114] In addition to their availability on commercially available hardware, similar PC hardware-friendly versions of Android are freely available from the Android-x86 project, including customized Android 4.4. [115] Using the Android emulator that is part of the Android SDK , or third-party emulators, Android can also run non-natively on x86 architectures. [116] [117] Chinese companies are building a PC and mobile operating system, based on Android, to "compete directly with Microsoft Windows and Google Android". [118] The Chinese Academy of Engineering noted that "more than a dozen" companies were customising Android following a Chinese ban on the use of Windows 8 on government PCs. [119] [120] [121]

Development
Android is developed by Google until the latest changes and updates are ready to be released, at which point the source code is made available to the Android Open Source Project. [122] This source code can be found without modification on select devices, mainly the Nexus series of devices. [123] The source code is, in turn, adapted by original equipment manufacturers (OEMs) to run on their hardware. [124] [125] Android's source code does not contain the often proprietary device drivers that are needed for certain hardware components. [126]
In 2007, the green Android logo was designed for Google by graphic designer Irina Blok . The design team was tasked with a project to create a universally identifiable icon with the specific inclusion of a robot in the final design. After numerous design developments based on science fiction and space movies, the team eventually sought inspiration from the human symbol on restroom doors and modified the figure into a robot shape. As Android is open-source, it was agreed that the logo should be likewise, and since its launch the green logo has been reinterpreted into countless variations on the original design. [127]

Update schedule
Google announces major incremental upgrades to Android on a yearly basis. [128] The updates can be installed on devices over-the-air . [129] The latest major release is 7.0 "Nougat" , announced in March 2016, [130] and released the following August. [131] [132]
Compared to its primary rival mobile operating system, Apple 's iOS , Android updates typically reach various devices with significant delays. Except for devices with the Google Nexus brand, updates often arrive months after the release of the new version, or not at all. [133] This is partly due to the extensive variation in hardware in Android devices, [134] to which each upgrade must be specifically tailored, a time- and resource-consuming process. [135] Manufacturers often prioritize their newest devices and leave old ones behind. [136] Additional delays can be introduced by wireless carriers that, after receiving updates from manufacturers, further customize and brand Android to their needs and conduct extensive testing on their networks before sending the upgrade out to users. [136] [137] There are also situations in which upgrades are not possible due to one manufacturing partner not providing necessary updates to drivers . [138]
The lack of after-sale support from manufacturers and carriers has been widely criticized by consumer groups and the technology media. [139] [140] [141] Some commentators have noted that the industry has a financial incentive not to upgrade their devices, as the lack of updates for existing devices fuels the purchase of newer ones, [142] an attitude described as "insulting". [141] The Guardian complained that the method of distribution for updates is complicated only because manufacturers and carriers have designed it that way. [141] In 2011, Google partnered with a number of industry players to announce an "Android Update Alliance", pledging to deliver timely updates for every device for 18 months after its release; however, there has not been another official word about that alliance since its announcement. [136] [143]
In 2012, Google began decoupling certain aspects of the operating system (particularly its core applications) so they could be updated through the Google Play store independently of the OS. One of those components, Google Play Services , is a closed-source system-level process providing APIs for Google services, installed automatically on nearly all devices running Android 2.2 "Froyo" and higher. With these changes, Google can add new system functionality through Play Services and update apps without having to distribute an upgrade to the operating system itself. [144] As a result, Android 4.2 and 4.3 "Jelly Bean" contained relatively fewer user-facing changes, focusing more on minor changes and platform improvements. [145]
In May 2016, Bloomberg reported that Google was making efforts to keep Android more up-to-date, including accelerated rates of security updates, rolling out technological workarounds, reducing requirements for phone testing, and ranking phone makers in an attempt to "shame" them into better behavior. As stated by Bloomberg : "As smartphones get more capable, complex and hackable, having the latest software work closely with the hardware is increasingly important". Hiroshi Lockheimer, the Android lead, admitted that "It’s not an ideal situation", further commenting that the lack of updates is "the weakest link on security on Android". Wireless carriers were described in the report as the "most challenging discussions", due to carriers' slow approval time due to testing on their networks, despite some carriers, including Verizon and Sprint , having already shortened their respective approval times. HTC 's then-executive Jason Mackenzie called monthly security updates "unrealistic" in 2015, and Google was trying to persuade carriers to exclude security patches from the full testing procedures. In a further effort for persuasion, Google shared a list of top phone makers measured by updated devices with its Android partners, and is considering making the list public. Mike Chan, co-founder of phone maker Nextbit and former Android developer, said that "The best way to solve this problem is a massive re-architecture of the operating system", "or Google could invest in training manufacturers and carriers "to be good Android citizens"". [146] [147] [148]

Linux kernel
Android's kernel is based on one of the Linux kernel 's long-term support (LTS) branches. Since April 2014, Android devices mainly use versions 3.4, 3.10 or 3.18 of the Linux kernel. [149] [150] The specific kernel version depends on the actual Android device and its hardware platform; [151] [152] [153] Android has used various kernel versions since the version 2.6.25 that was used in Android 1.0. [154]
Android's variant of the Linux kernel has further architectural changes that are implemented by Google outside the typical Linux kernel development cycle, such as the inclusion of components like Binder , ashmem, pmem, logger, wakelocks, and different out-of-memory (OOM) handling. [155] [156] [157] Certain features that Google contributed back to the Linux kernel, notably a power management feature called "wakelocks", were rejected by mainline kernel developers partly because they felt that Google did not show any intent to maintain its own code. [158] [159] [160] Google announced in April 2010 that they would hire two employees to work with the Linux kernel community, [161] but Greg Kroah-Hartman , the current Linux kernel maintainer for the stable branch, said in December 2010 that he was concerned that Google was no longer trying to get their code changes included in mainstream Linux. [159] Some Google Android developers hinted that "the Android team was getting fed up with the process," because they were a small team and had more urgent work to do on Android. [162]
In August 2011, Linus Torvalds said that "eventually Android and Linux would come back to a common kernel, but it will probably not be for four to five years". [163] In December 2011, Greg Kroah-Hartman announced the start of Android Mainlining Project, which aims to put some Android drivers , patches and features back into the Linux kernel, starting in Linux 3.3. [164] Linux included the autosleep and wakelocks capabilities in the 3.5 kernel, after many previous attempts at merger. The interfaces are the same but the upstream Linux implementation allows for two different suspend modes: to memory (the traditional suspend that Android uses), and to disk (hibernate, as it is known on the desktop). [165] Google maintains a public code repository that contains their experimental work to re-base Android off the latest stable Linux versions. [166] [167]
The flash storage on Android devices is split into several partitions, such as /system for the operating system itself, and /data for user data and application installations. [168] In contrast to desktop Linux distributions, Android device owners are not given root access to the operating system and sensitive partitions such as /system are read-only . However, root access can be obtained by exploiting security flaws in Android, which is used frequently by the open-source community to enhance the capabilities of their devices, [169] but also by malicious parties to install viruses and malware . [170]
Android is a Linux distribution according to the Linux Foundation , [171] Google's open-source chief Chris DiBona , [172] and several journalists. [173] [174] Others, such as Google engineer Patrick Brady, say that Android is not Linux in the traditional Unix-like Linux distribution sense; Android does not include the GNU C Library (it uses Bionic as an alternative C library) and some of other components typically found in Linux distributions. [175]

Software stack
On top of the Linux kernel, there are the middleware , libraries and APIs written in C , and application software running on an application framework which includes Java -compatible libraries. Development of the Linux kernel continues independently of other Android's source code bases.
Until version 5.0, Android used Dalvik as a process virtual machine with trace-based just-in-time (JIT) compilation to run Dalvik "dex-code" (Dalvik Executable), which is usually translated from the Java bytecode . Following the trace-based JIT principle, in addition to interpreting the majority of application code, Dalvik performs the compilation and native execution of select frequently executed code segments ("traces") each time an application is launched. [176] [177] [178] Android 4.4 introduced Android Runtime (ART) as a new runtime environment, which uses ahead-of-time (AOT) compilation to entirely compile the application bytecode into machine code upon the installation of an application. In Android 4.4, ART was an experimental feature and not enabled by default; it became the only runtime option in the next major version of Android, 5.0. [179]
For its Java library, the Android platform uses a subset of the now discontinued Apache Harmony project. [180] In December 2015, Google announced that the next version of Android would switch to a Java implementation based on OpenJDK . [181]
Android's standard C library , Bionic , was developed by Google specifically for Android, as a derivation of the BSD 's standard C library code. Bionic itself has been designed with several major features specific to the Linux kernel. The main benefits of using Bionic instead of the GNU C Library (glibc) or uClibc are its smaller runtime footprint, and optimization for low-frequency CPUs. At the same time, Bionic is licensed under the terms of the BSD licence , which Google finds more suitable for the Android's overall licensing model. [178]
Aiming for a different licensing model, toward the end of 2012, Google switched the Bluetooth stack in Android from the GPL-licensed BlueZ to the Apache-licensed BlueDroid. [182]
Android does not have a native X Window System by default, nor does it support the full set of standard GNU libraries. This made it difficult to port existing Linux applications or libraries to Android, [175] until version r5 of the Android Native Development Kit brought support for applications written completely in C or C++ . [183] Libraries written in C may also be used in applications by injection of a small shim and usage of the JNI . [184]
Since Marshmallow, " Toybox ", a collection of command line utilities (mostly for use by apps, as Android doesn't provide a command line interface by default), replaced similar "Toolbox" collection. [185]
Android has another operating system, Trusty OS, within it, as a part of "Trusty" "software components supporting a Trusted Execution Environment (TEE) on mobile devices." "Trusty and the Trusty API are subject to change. [..] Applications for the Trusty OS can be written in C/C++ (C++ support is limited), and they have access to a small C library. [..] All Trusty applications are single-threaded; multithreading in Trusty userspace currently is unsupported. [..] Third-party application development is not supported in" the current version, and software running on the OS and processor for it, run the " DRM framework for protected content. [..] There are many other uses for a TEE such as mobile payments, secure banking, full-disk encryption, multi-factor authentication, device reset protection, replay-protected persistent storage, wireless display ("cast") of protected content, secure PIN and fingerprint processing, and even malware detection." [186]

Open-source community
Android has an active community of developers and enthusiasts who use the Android Open Source Project (AOSP) source code to develop and distribute their own modified versions of the operating system. [187] These community-developed releases often bring new features and updates to devices faster than through the official manufacturer/carrier channels, with a comparable level of quality; [188] provide continued support for older devices that no longer receive official updates; or bring Android to devices that were officially released running other operating systems, such as the HP TouchPad . Community releases often come pre- rooted and contain modifications not provided by the original vendor, such as the ability to overclock or over/undervolt the device's processor. [189] CyanogenMod was the most widely used community firmware, [190] and CyanogenMod has been discontinued and LineageOS is the successor of CyanogenMod. [191] Android-x86 is a version of Android for IBM PC compatibles . There have also been attempts with varying degrees of success to port Android to iPhones, notably the iDroid Project. [192]
Historically, device manufacturers and mobile carriers have typically been unsupportive of third-party firmware development. Manufacturers express concern about improper functioning of devices running unofficial software and the support costs resulting from this. [193] Moreover, modified firmwares such as CyanogenMod sometimes offer features, such as tethering , for which carriers would otherwise charge a premium. As a result, technical obstacles including locked bootloaders and restricted access to root permissions are common in many devices. However, as community-developed software has grown more popular, and following a statement by the Librarian of Congress in the United States that permits the " jailbreaking " of mobile devices, [194] manufacturers and carriers have softened their position regarding third party development, with some, including HTC , [193] Motorola , [195] Samsung [196] [197] and Sony , [198] providing support and encouraging development. As a result of this, over time the need to circumvent hardware restrictions to install unofficial firmware has lessened as an increasing number of devices are shipped with unlocked or unlockable bootloaders , similar to Nexus series of phones, although usually requiring that users waive their devices' warranties to do so. [193] However, despite manufacturer acceptance, some carriers in the US still require that phones are locked down, frustrating developers and customers. [199]

Security and privacy

Scope of surveillance by public institutions
As part of the broader 2013 mass surveillance disclosures it was revealed in September 2013 that the American and British intelligence agencies, the National Security Agency (NSA) and Government Communications Headquarters (GCHQ), respectively, have access to the user data on iPhone, BlackBerry, and Android devices. They are reportedly able to read almost all smartphone information, including SMS, location, emails, and notes. [200] In January 2014, further reports revealed the intelligence agencies' capabilities to intercept the personal information transmitted across the Internet by social networks and other popular applications such as Angry Birds , which collect personal information of their users for advertising and other commercial reasons. GCHQ has, according to The Guardian , a wiki -style guide of different apps and advertising networks, and the different data that can be siphoned from each. [201] Later that week, the Finnish Angry Birds developer Rovio announced that it was reconsidering its relationships with its advertising platforms in the light of these revelations, and called upon the wider industry to do the same. [202]
The documents revealed a further effort by the intelligence agencies to intercept Google Maps searches and queries submitted from Android and other smartphones to collect location information in bulk. [201] The NSA and GCHQ insist their activities are in compliance with all relevant domestic and international laws, although the Guardian stated "the latest disclosures could also add to mounting public concern about how the technology sector collects and uses information, especially for those outside the US, who enjoy fewer privacy protections than Americans." [201]

Common security threats
Research from security company Trend Micro lists premium service abuse as the most common type of Android malware, where text messages are sent from infected phones to premium-rate telephone numbers without the consent or even knowledge of the user. Other malware displays unwanted and intrusive advertisements on the device, or sends personal information to unauthorised third parties. [203] Security threats on Android are reportedly growing exponentially; however, Google engineers have argued that the malware and virus threat on Android is being exaggerated by security companies for commercial reasons, [204] [205] and have accused the security industry of playing on fears to sell virus protection software to users. [204] Google maintains that dangerous malware is actually extremely rare, [205] and a survey conducted by F-Secure showed that only 0.5% of Android malware reported had come from the Google Play store. [206]
In August 2015, Google announced that devices in the Google Nexus series would begin to receive monthly security patches . Google also wrote that "Nexus devices will continue to receive major updates for at least two years and security patches for the longer of three years from initial availability or 18 months from last sale of the device via the Google Store ." [207] [208] [209] The following October, researchers at the University of Cambridge concluded that 87.7% of Android phones in use had known but unpatched security vulnerabilities due to lack of updates and support. [210] [211] [212] Ron Amadeo of Ars Technica wrote also in August 2015 that "Android was originally designed, above all else, to be widely adopted. Google was starting from scratch with zero percent market share, so it was happy to give up control and give everyone a seat at the table in exchange for adoption. [...] Now, though, Android has around 75-80 percent of the worldwide smartphone market—making it not just the world's most popular mobile operating system but arguably the most popular operating system, period. As such, security has become a big issue. Android still uses a software update chain-of-command designed back when the Android ecosystem had zero devices to update, and it just doesn't work". [213] Following news of Google's monthly schedule, some manufacturers, including Samsung and LG, promised to issue monthly security updates, [214] but, as noted by Jerry Hildenbrand in Android Central in February 2016, "instead we got a few updates on specific versions of a small handful of models. And a bunch of broken promises". [215]
In a March 2017 post on Google's Security Blog, Android security leads Adrian Ludwig and Mel Miller wrote that "More than 735 million devices from 200+ manufacturers received a platform security update in 2016" and that "Our carrier and hardware partners helped expand deployment of these updates, releasing updates for over half of the top 50 devices worldwide in the last quarter of 2016". They also wrote that "About half of devices in use at the end of 2016 had not received a platform security update in the previous year", stating that their work would continue to focus on streamlining the security updates program for easier deployment by manufacturers. [216] Furthermore, in a comment to TechCrunch , Ludwig stated that the wait time for security updates had been reduced from "six to nine weeks down to just a few days", with 78% of flagship devices in North America being up-to-date on security at the end of 2016. [217]
Patches to bugs found in the core operating system often do not reach users of older and lower-priced devices. [218] [219] However, the open-source nature of Android allows security contractors to take existing devices and adapt them for highly secure uses. For example, Samsung has worked with General Dynamics through their Open Kernel Labs acquisition to rebuild Jelly Bean on top of their hardened microvisor for the "Knox" project. [220] [221]
Android smartphones have the ability to report the location of Wi-Fi access points, encountered as phone users move around, to build databases containing the physical locations of hundreds of millions of such access points. These databases form electronic maps to locate smartphones, allowing them to run apps like Foursquare , Google Latitude , Facebook Places , and to deliver location-based ads. [222] Third party monitoring software such as TaintDroid, [223] an academic research-funded project, can, in some cases, detect when personal information is being sent from applications to remote servers. [224]

Technical security features
Android applications run in a sandbox , an isolated area of the system that does not have access to the rest of the system's resources, unless access permissions are explicitly granted by the user when the application is installed. [225]
Since February 2012, Google has used its Google Bouncer malware scanner to watch over and scan apps available in the Google Play store. [226] [227] A "Verify Apps" feature was introduced in November 2012, as part of the Android 4.2 "Jelly Bean" operating system version, to scan all apps, both from Google Play and from third-party sources, for malicious behavior. [228] Originally only doing so during installation, Verify Apps received an update in 2014 to "constantly" scan apps, and in 2017 the feature was made visible to users through a menu in Settings. [229] [230]
Before installing an application, the Google Play store displays a list of the requirements an app needs to function. After reviewing these permissions, the user can choose to accept or refuse them, installing the application only if they accept. [231] In Android 6.0 "Marshmallow" , the permissions system was changed; apps are no longer automatically granted all of their specified permissions at installation time. An opt-in system is used instead, in which users are prompted to grant or deny individual permissions to an app when they are needed for the first time. Applications remember the grants, which can be revoked by the user at any time. [232] [233] The new permissions model is used only by applications developed for Marshmallow using its software development kit (SDK), and older apps will continue to use the previous all-or-nothing approach. Permissions can still be revoked for those apps, though this might prevent them from working properly, and a warning is displayed to that effect. [234] [235]
In September 2014, Jason Nova of Android Authority reported on a study by the German security company Fraunhofer AISEC in antivirus software and malware threats on Android. Nova wrote that "The Android operating system deals with software packages by sandboxing them; this does not allow applications to list the directory contents of other apps to keep the system safe. By not allowing the antivirus to list the directories of other apps after installation, applications that show no inherent suspicious behavior when downloaded are cleared as safe. If then later on parts of the app are activated that turn out to be malicious, the antivirus will have no way to know since it is inside the app and out of the antivirus’ jurisdiction". The study by Fraunhofer AISEC, examining antivirus software from Avast , AVG , Bitdefender , ESET , F-Secure , Kaspersky , Lookout , McAfee (formerly Intel Security) , Norton , Sophos , and Trend Micro , revealed that "the tested antivirus apps do not provide protection against customized malware or targeted attacks", and that "the tested antivirus apps were also not able to detect malware which is completely unknown to date but does not make any efforts to hide its malignity". [236]
In August 2013, Google announced Android Device Manager (renamed Find My Device in May 2017), [237] [238] a service that allows users to remotely track, locate, and wipe their Android device, [239] [240] with an Android app for the service released in December. [241] [242] In December 2016, Google introduced a Trusted Contacts app, letting users request location-tracking of loved ones during emergencies. [243] [244]

Licensing
The source code for Android is open-source : it is developed in private by Google, with the source code released publicly when a new version of Android is released. Google publishes most of the code (including network and telephony stacks ) under the non-copyleft Apache License version 2.0. which allows modification and redistribution. [245] [246] The license does not grant rights to the "Android" trademark, so device manufacturers and wireless carriers have to license it from Google under individual contracts. Associated Linux kernel changes are released under the copyleft GNU General Public License version 2, developed by the Open Handset Alliance , with the source code publicly available at all times. Typically, Google collaborates with a hardware manufacturer to produce a flagship device (part of the Nexus series) featuring the new version of Android, then makes the source code available after that device has been released. [247] The only Android release which was not immediately made available as source code was the tablet-only 3.0 Honeycomb release. The reason, according to Andy Rubin in an official Android blog post, was because Honeycomb was rushed for production of the Motorola Xoom , [248] and they did not want third parties creating a "really bad user experience" by attempting to put onto smartphones a version of Android intended for tablets. [249]
Only the base Android operating system (including some applications) is open-source software, whereas most Android devices ship with a substantial amount of proprietary software, such as Google Mobile Services , which includes applications such as Google Play Store , Google Search, and Google Play Services –  a software layer that provides APIs for the integration with Google-provided services, among others. These applications must be licensed from Google by device makers, and can only be shipped on devices which meet its compatibility guidelines and other requirements. [86] Custom, certified distributions of Android produced by manufacturers (such as TouchWiz and HTC Sense ) may also replace certain stock Android apps with their own proprietary variants and add additional software not included in the stock Android operating system. [85] There may also be " binary blob " drivers required for certain hardware components in the device. [85] [126]
Some stock applications in AOSP code that were formerly used by earlier versions of Android, such as Search, Music, and Calendar, have been abandoned by Google in favor of non-free replacements distributed through Play Store (Google Search, Google Play Music, and Google Calendar) that are no longer open-source. Moreover, open-source variants of some applications also exclude functions that are present in their non-free versions, such as Photosphere panoramas in Camera, and a Google Now page on the default home screen (exclusive to the proprietary version "Google Now Launcher", whose code is embedded within that of the main Google application). [85] [250] [251] [252]
Richard Stallman and the Free Software Foundation have been critical of Android and have recommended the usage of alternatives such as Replicant , because drivers and firmware vital for the proper functioning of Android devices are usually proprietary, and because the Google Play Store application can forcibly install or deinstall applications and, as a result, invite non-free software. although the Free Software Foundation has not found Google to use it for malicious reasons [253] [254]

Leverage over manufacturers
Google licenses their Google Mobile Services software, along with Android trademarks, only to hardware manufacturers for devices that meet Google's compatibility standards specified in the Android Compatibility Program document. [255] Thus, forks of Android that make major changes to the operating system itself do not include any of Google's non-free components, stay incompatible with applications that require them, and must ship with an alternative software marketplace in lieu of Google Play Store. [85] Examples of such Android forks are Amazon 's Fire OS (which is used on the Kindle Fire line of tablets, and oriented toward Amazon services), the Nokia X Software Platform (a fork used by the Nokia X family , oriented primarily toward Nokia and Microsoft services), and other forks that exclude Google apps due to the general unavailability of Google services in certain regions (such as China ). [256] [257] In 2014, Google also began to require that all Android devices which license the Google Mobile Services software display a prominent "Powered by Android" logo on their boot screens. [86]
Members of the Open Handset Alliance, which include the majority of Android OEMs, are also contractually forbidden from producing Android devices based on forks of the OS; [85] [258] in 2012, Acer Inc. was forced by Google to halt production on a device powered by Alibaba Group 's Aliyun OS with threats of removal from the OHA, as Google deemed the platform to be an incompatible version of Android. Alibaba Group defended the allegations, arguing that the OS was a distinct platform from Android (primarily using HTML5 apps), but incorporated portions of Android's platform to allow backwards compatibility with third-party Android software. Indeed, the devices did ship with an application store which offered Android apps; however, the majority of them were pirated . [259] [260] [261]

Reception
Android received a lukewarm reaction when it was unveiled in 2007. Although analysts were impressed with the respected technology companies that had partnered with Google to form the Open Handset Alliance, it was unclear whether mobile phone manufacturers would be willing to replace their existing operating systems with Android. [262] The idea of an open-source, Linux-based development platform sparked interest, [263] but there were additional worries about Android facing strong competition from established players in the smartphone market, such as Nokia and Microsoft, and rival Linux mobile operating systems that were in development. [264] These established players were skeptical: Nokia was quoted as saying "we don't see this as a threat," and a member of Microsoft's Windows Mobile team stated "I don't understand the impact that they are going to have." [265]
Since then Android has grown to become the most widely used smartphone operating system [266] [267] and "one of the fastest mobile experiences available." [268] Reviewers have highlighted the open-source nature of the operating system as one of its defining strengths, allowing companies such as Nokia ( Nokia X family ), [269] Amazon ( Kindle Fire ), Barnes & Noble ( Nook ), Ouya , Baidu and others to fork the software and release hardware running their own customised version of Android. As a result, it has been described by technology website Ars Technica as "practically the default operating system for launching new hardware" for companies without their own mobile platforms. [266] This openness and flexibility is also present at the level of the end user: Android allows extensive customisation of devices by their owners and apps are freely available from non-Google app stores and third party websites. These have been cited as among the main advantages of Android phones over others. [266] [270]
Despite Android's popularity, including an activation rate three times that of iOS, there have been reports that Google has not been able to leverage their other products and web services successfully to turn Android into the money maker that analysts had expected. [271] The Verge suggested that Google is losing control of Android due to the extensive customization and proliferation of non-Google apps and services –  Amazon's Kindle Fire line uses Fire OS , a heavily modified fork of Android which does not include or support any of Google's proprietary components, and requires that users obtain software from its competing Amazon Appstore instead of Play Store. [85] In 2014, in an effort to improve prominence of the Android brand, Google began to require that devices featuring its proprietary components display an Android logo on the boot screen. [86]
Android has suffered from "fragmentation", [272] a situation where the variety of Android devices, in terms of both hardware variations and differences in the software running on them, makes the task of developing applications that work consistently across the ecosystem harder than rival platforms such as iOS where hardware and software varies less. For example, according to data from OpenSignal in July 2013, there were 11,868 models of Android device, numerous different screen sizes and eight Android OS versions simultaneously in use, while the large majority of iOS users have upgraded to the latest iteration of that OS. [273] Critics such as Apple Insider have asserted that fragmentation via hardware and software pushed Android's growth through large volumes of low end, budget-priced devices running older versions of Android. They maintain this forces Android developers to write for the "lowest common denominator" to reach as many users as possible, who have too little incentive to make use of the latest hardware or software features only available on a smaller percentage of devices. [274] However, OpenSignal, who develops both Android and iOS apps, concluded that although fragmentation can make development trickier, Android's wider global reach also increases the potential reward. [273]

Market share
Research company Canalys estimated in the second quarter of 2009, that Android had a 2.8% share of worldwide smartphone shipments. [275] By the fourth quarter of 2010, this had grown to 33% of the market becoming the top-selling smartphone platform, [276] overtaking Symbian . [277] By the third quarter of 2011, Gartner estimated that more than half (52.5%) of the smartphone sales belonged to Android. [278] By the third quarter of 2012 Android had a 75% share of the global smartphone market according to the research firm IDC. [279]
In July 2011, Google said that 550,000 Android devices were being activated every day, [280] up from 400,000 per day in May, [281] and more than 100 million devices had been activated [282] with 4.4% growth per week. [280] In September 2012, 500 million devices had been activated with 1.3 million activations per day. [283] [284] In May 2013, at Google I/O , Sundar Pichai announced that 900 million Android devices had been activated. [285]
Android market share varies by location. In July 2012, "mobile subscribers aged 13+" in the United States using Android were up to 52%, [286] and rose to 90% in China. [287] During the third quarter of 2012, Android's worldwide smartphone shipment market share was 75%, [279] with 750 million devices activated in total. In April 2013 Android had 1.5 million activations per day. [284] As of May 2013 [update] , 48 billion applications ("apps") have been installed from the Google Play store, [288] and by September 2013, one billion Android devices have been activated. [289]
As of February 2017 [update] , the Google Play store has over 2.7 million Android applications published, [290] and As of May 2016 [update] , apps have been downloaded more than 65 billion times. [291] The operating system's success has made it a target for patent litigation as part of the so-called " smartphone wars " between technology companies. [292] [293]
Android devices account for more than half of smartphone sales in most markets, including the US, while "only in Japan was Apple on top" (September–November 2013 numbers). [294] At the end of 2013, over 1.5 billion Android smartphones have been sold in the four years since 2010, [295] [296] making Android the most sold phone and tablet OS. Three billion Android smartphones are estimated to be sold by the end of 2014 (including previous years). According to Gartner research company, Android-based devices outsold all contenders, every year since 2012. [297] In 2013, it outsold Windows 2.8:1 or by 573 million. [298] [299] [300] As of 2015 [update] , Android has the largest installed base of all operating systems; [18] Since 2013, devices running it also sell more than Windows, iOS and Mac OS X devices combined. [301]
According to StatCounter , which tracks only the use for browsing the web, Android is the most popular mobile operating system since August 2013. [302] Android is the most popular operating system for web browsing in India and several other countries (e.g. virtually all of Asia, with Japan and North Korea exceptions). According to StatCounter, Android is most used on mobile in all African countries, and it stated "mobile usage has already overtaken desktop in several countries including India, South Africa and Saudi Arabia", [303] with virtually all countries in Africa having done so already (except for seven countries, including Egypt), such as Ethiopia and Kenya in which mobile (including tablets) usage is at 90.46% (Android only, accounts for 75.81% of all use there). [304] [305]
While Android phones in the Western world commonly include Google's proprietary add-ons (such as Google Play) to the otherwise open-source operating system, this is increasingly not the case in emerging markets; "ABI Research claims that 65 million devices shipped globally with open-source Android in the second quarter of [2014], up from 54 million in the first quarter"; depending on country, percent of phones estimated to be based only on Android's source code (AOSP), forgoing the Android trademark: Thailand (44%), Philippines (38%), Indonesia (31%), India (21%), Malaysia (24%), Mexico (18%), Brazil (9%). [306]
According to a January 2015 Gartner report, "Android surpassed a billion shipments of devices in 2014, and will continue to grow at a double-digit pace in 2015, with a 26 percent increase year over year." This made it the first time that any general-purpose operating system has reached more than one billion end users within a year: by reaching close to 1.16 billion end users in 2014, Android shipped over four times more than iOS and OS X combined, and over three times more than Microsoft Windows . Gartner expected the whole mobile phone market to "reach two billion units in 2016", including Android. [307] Describing the statistics, Farhad Manjoo wrote in The New York Times that "About one of every two computers sold today is running Android. [It] has become Earth’s dominant computing platform." [18]
According to a Statistica 's estimate, Android smartphones had an installed base of 1.8 billion units in 2015, which was 76% of the estimated total number of smartphones worldwide. [308] [309] [b] Android has the largest installed base of any mobile operating system and, since 2013, the highest-selling operating system overall [298] [301] [311] [312] [313] with sales in 2012, 2013 and 2014 [314] close to the installed base of all PCs. [315] In the third quarter of 2015, Android's share of the global smartphone shipment market was 84.7%, the highest ever. [316] As of September 28, 2016, with 52.5% market share, Samsung remains the leading OEM for shipping Android running smartphones and tablets, followed by followed by LG, Huawei, Motorola, Lenovo, Sony, HTC, Asus, Alcatel and Xiaomi. [317]
By August 2016, the two biggest continents have gone mobile-majority, judged by web use ("desktop" has 46.92%–55.16% use worldwide, depending on day of the week, making some weeks desktop-minority; [318] lowest full month was at 50.05% [319] ); because of Android (see usage share of operating systems ), that has majority use on smartphones in virtually all countries (all continents have gone Android-majority, including North America [320] [321] except for Oceania, because of Australia), [322] with few exceptions (all of which have iOS -majority); in the US, Android is close to iOS, having exchanged majority position a few times, [323] Canada and the following are also exceptions: Japan, Philippines, Australia and the only exceptions in Europe are the UK, Switzerland, Belgium and the Nordic countries Denmark, Iceland, Sweden and Norway. [324]
By 2016, Android was on the majority of smartphones in virtually all countries in the world, [325] [326] excluding United States and Canada (while including North America continent as a whole [327] ), Australia and Japan. A few countries, such as the UK, lose Android-majority if tablets are included.
In September 2015, Google announced that Android had 1.4 billion monthly active users. [328] [329] This changed to 2 billion monthly active users in May 2017. [330] [331]

Adoption on tablets
Despite its success on smartphones, initially Android tablet adoption was slow. [332] One of the main causes was the chicken or the egg situation where consumers were hesitant to buy an Android tablet due to a lack of high quality tablet applications, but developers were hesitant to spend time and resources developing tablet applications until there was a significant market for them. [333] [334] The content and app "ecosystem" proved more important than hardware specs as the selling point for tablets. Due to the lack of Android tablet-specific applications in 2011, early Android tablets had to make do with existing smartphone applications that were ill-suited to larger screen sizes, whereas the dominance of Apple's iPad was reinforced by the large number of tablet-specific iOS applications. [334] [335]
Despite app support in its infancy, a considerable number of Android tablets (alongside those using other operating systems, such as the HP TouchPad and BlackBerry PlayBook ) were rushed out to market in an attempt to capitalize on the success of the iPad. [334] InfoWorld has suggested that some Android manufacturers initially treated their first tablets as a "Frankenphone business", a short-term low-investment opportunity by placing a smartphone-optimized Android OS (before Android 3.0 Honeycomb for tablets was available) on a device while neglecting user interface. This approach, such as with the Dell Streak , failed to gain market traction with consumers as well as damaging the early reputation of Android tablets. [336] [337] Furthermore, several Android tablets such as the Motorola Xoom were priced the same or higher than the iPad , which hurt sales. An exception was the Amazon Kindle Fire , which relied upon lower pricing as well as access to Amazon's ecosystem of applications and content. [334] [338]
This began to change in 2012, with the release of the affordable Nexus 7 and a push by Google for developers to write better tablet applications. [339] According to International Data Corporation, shipments of Android-powered tablets surpassed iPads in Q3 2012. [340]
As of the end of 2013, over 191.6 million Android tablets had sold in three years since 2011. [341] [342] This made Android tablets the most-sold type of tablet in 2013, surpassing iPads in the second quarter of 2013. [343]
According to the StatCounter's June 2015 web use statistics, Android tablets represent the majority of tablet devices used on the South American [344] (then lost majority) and African continents (60.23%), [345] while they have equaled with the iPad's market share in major countries on all continents (with the North America as an exception, though in El Salvador Android has the majority [346] ), and getting close to representing the majority on the whole Asian continent [347] having done so already in India (65.9%), [348] Indonesia (62.22%), [349] and most Middle-Eastern countries. [350] In about half of the European countries, Android tablets have a majority market share. [351] China is an exception for the major developing countries, in which Android phablets (classified as smartphones while similar in size to tablets) are more popular than Android tablets or iPads.
In March 2016, Galen Gruman of InfoWorld stated that Android devices could be a "real part of your business [..] there's no longer a reason to keep Android at arm's length. It can now be as integral to your mobile portfolio as Apple 's iOS devices are". [352] A year earlier, Gruman had stated that Microsoft 's own mobile Office apps were "better on iOS and Android" than on Microsoft's own Windows 10 devices. [353]

Platform usage
Charts in this section provide breakdowns of Android versions, based on devices accessing the Google Play Store in a seven-day period ending on May 2, 2017. [354] [c] Therefore, these statistics exclude devices running various Android forks that do not access the Google Play Store, such as Amazon's Fire tablets .
As of 2017 [update] , more than 60% of devices have OpenGL ES 3.0 or higher.

Application piracy
In general, paid Android applications can easily be pirated . [355] In a May 2012 interview with Eurogamer , the developers of Football Manager stated that the ratio of pirated players vs legitimate players was 9:1 for their game Football Manager Handheld . [356] However, not every developer agreed that piracy rates were an issue; for example, in July 2012 the developers of the game Wind-up Knight said that piracy levels of their game were only 12%, and most of the piracy came from China, where people cannot purchase apps from Google Play. [357]
In 2010, Google released a tool for validating authorized purchases for use within apps, but developers complained that this was insufficient and trivial to crack . Google responded that the tool, especially its initial release, was intended as a sample framework for developers to modify and build upon depending on their needs, not as a finished piracy solution. [358] Android "Jelly Bean" introduced the ability for paid applications to be encrypted, so that they may work only on the device for which they were purchased. [359] [360]

Legal issues
Both Android and Android phone manufacturers have been involved in numerous patent lawsuits. On August 12, 2010, Oracle sued Google over claimed infringement of copyrights and patents related to the Java programming language. [361] Oracle originally sought damages up to $6.1 billion, [362] but this valuation was rejected by a United States federal judge who asked Oracle to revise the estimate. [363] In response, Google submitted multiple lines of defense, counterclaiming that Android did not infringe on Oracle's patents or copyright, that Oracle's patents were invalid, and several other defenses. They said that Android's Java runtime environment is based on Apache Harmony , a clean room implementation of the Java class libraries, and an independently developed virtual machine called Dalvik . [364] In May 2012, the jury in this case found that Google did not infringe on Oracle's patents, and the trial judge ruled that the structure of the Java APIs used by Google was not copyrightable. [365] [366] The parties agreed to zero dollars in statutory damages for a small amount of copied code. [367] On May 9, 2014, the Federal Circuit partially reversed the district court ruling, ruling in Oracle's favor on the copyrightability issue, and remanding the issue of fair use to the district court. [368] [369]
In December 2015, Google announced that the next major release of Android ( Android Nougat ) would switch to OpenJDK , which is the official open-source implementation of the Java platform, instead of using the now-discontinued Apache Harmony project as its runtime. Code reflecting this change was also posted to the AOSP source repository. [180] In its announcement, Google claimed this was part of an effort to create a "common code base" between Java on Android and other platforms. [181] Google later admitted in a court filing that this was part of an effort to address the disputes with Oracle, as its use of OpenJDK code is governed under the GNU General Public License (GPL) with a linking exception , and that "any damages claim associated with the new versions expressly licensed by Oracle under OpenJDK would require a separate analysis of damages from earlier releases". [180] In June 2016, a United States federal court ruled in favor of Google, stating that its use of the APIs was fair use. [370]
In addition to lawsuits against Google directly, various proxy wars have been waged against Android indirectly by targeting manufacturers of Android devices, with the effect of discouraging manufacturers from adopting the platform by increasing the costs of bringing an Android device to market. [371] Both Apple and Microsoft have sued several manufacturers for patent infringement, with Apple's ongoing legal action against Samsung being a particularly high-profile case. In October 2011, Microsoft said they had signed patent license agreements with ten Android device manufacturers, whose products account for "70% in the U.S." . and 55% of the worldwide revenue for Android devices. [372] These include Samsung and HTC . [373] Samsung's patent settlement with Microsoft included an agreement to allocate more resources to developing and marketing phones running Microsoft's Windows Phone operating system. [371] Microsoft has also tied its own Android software to patent licenses, requiring the bundling of Microsoft Office Mobile and Skype applications on Android devices to subsidize the licensing fees, while at the same time helping to promote its software lines. [374] [375]
Google has publicly expressed its frustration for the current patent landscape in the United States, accusing Apple, Oracle and Microsoft of trying to take down Android through patent litigation, rather than innovating and competing with better products and services. [376] In September 2011, Google purchased Motorola Mobility for US$12.5 billion, which was viewed in part as a defensive measure to protect Android, since Motorola Mobility held more than 17,000 patents. [377] In December 2011, Google bought over a thousand patents from IBM . [378]
In 2013, FairSearch , a lobbying organization supported by Microsoft, Oracle and others, filed a complaint regarding Android with the European Commission , alleging that its free-of-charge distribution model constituted anti-competitive predatory pricing . The Free Software Foundation Europe , whose donors include Google, disputed the Fairsearch allegations. [379] On April 20, 2016, the EU filed a formal antitrust complaint against Google based upon the FairSearch allegations, arguing that its leverage over Android vendors, including the mandatory bundling of the entire suite of proprietary Google software, hindering the ability for competing search providers to be integrated into Android, and barring vendors from producing devices running forks of Android, constituted anti-competitive practices. [380] In August 2016, Google was fined US$6.75 million by the Russian Federal Antimonopoly Service (FAS) under similar allegations by Yandex . [381]

Other uses
Google has developed several variations of Android for specific use cases, including Android Wear for wearable devices such as wrist watches, [382] [383] Android TV for televisions, [384] [385] Android Auto for cars, [386] [387] and Brillo, [388] later renamed Android Things , for smart devices and Internet of things . [389] [390]
The open and customizable nature of Android allows device makers to use it on other electronics as well, including laptops and netbooks , [391] [392] smartbooks , [393] cameras, [394] headphones, [395] home automation systems, game consoles, mirrors, [396] media players, [397] and landline telephones. [398]
Ouya , a video game console running Android, became one of the most successful Kickstarter campaigns, crowdfunding US$8.5m for its development, [399] [400] and was later followed by other Android-based consoles, such as Nvidia 's Shield Portable –  an Android device in a video game controller form factor. [401]
In 2011, Google demonstrated "Android@Home", a home automation technology which uses Android to control a range of household devices including light switches, power sockets and thermostats. [402] Prototype light bulbs were announced that could be controlled from an Android phone or tablet, but Android head Andy Rubin was cautious to note that "turning a lightbulb on and off is nothing new", pointing to numerous failed home automation services. Google, he said, was thinking more ambitiously and the intention was to use their position as a cloud services provider to bring Google products into customers' homes. [403] [404]
Parrot unveiled an Android-based car stereo system known as Asteroid in 2011, [405] followed by a successor, the touchscreen-based Asteroid Smart, in 2012. [406] In 2013, Clarion released its own Android-based car stereo, the AX1. [407] In January 2014, at the Consumer Electronics Show (CES), Google announced the formation of the Open Automotive Alliance , a group including several major automobile makers ( Audi , General Motors , Hyundai , and Honda ) and Nvidia , which aims to produce Android-based in car entertainment systems for automobiles, "[bringing] the best of Android into the automobile in a safe and seamless way." [408]
Android comes preinstalled on a few laptops (a similar functionality of running Android applications is also available in Google's Chrome OS ) and can also be installed on personal computers by end users. [409] On those platforms Android provides additional functionality for physical keyboards [410] and mice , together with the " Alt-Tab " key combination for switching applications quickly with a keyboard. In December 2014, one reviewer commented that Android's notification system is "vastly more complete and robust than in most environments" and that Android is "absolutely usable" as one's primary desktop operating system. [411]
In October 2015, The Wall Street Journal reported that Android will serve as Google's future main laptop operating system, with the plan to fold Chrome OS into it by 2017. [412] [413] Google's Sundar Pichai, who led the development of Android, explained that "mobile as a computing paradigm is eventually going to blend with what we think of as desktop today." [412] and back in 2009, Google co-founder Sergey Brin himself said that Chrome OS and Android would "likely converge over time." [414] Lockheimer, who replaced Pichai as head of Android and Chrome OS, responded to this claim with an official Google blog post stating that "While we've been working on ways to bring together the best of both operating systems, there's no plan to phase out Chrome OS [which has] guaranteed auto-updates for five years". [415] That is unlike Android where support is shorter with " EOL dates [being..] at least 3 years [into the future] for Android tablets for education". [416]

See also

Notes
WebPage index: 00081
Optical disc
In computing and optical disc recording technologies , an optical disc ( OD ) is a flat, usually circular disc which encodes binary data ( bits ) in the form of pits (binary value of 0 or off, due to lack of reflection when read) and lands (binary value of 1 or on, due to a reflection when read) on a special material (often aluminium [1] ) on one of its flat surfaces. The encoding material sits atop a thicker substrate (usually polycarbonate ) which makes up the bulk of the disc and forms a dust defocusing layer. The encoding pattern follows a continuous, spiral path covering the entire disc surface and extending from the innermost track to the outermost track. The data is stored on the disc with a laser or stamping machine, and can be accessed when the data path is illuminated with a laser diode in an optical disc drive which spins the disc at speeds of about 200 to 4,000 RPM or more, depending on the drive type, disc format, and the distance of the read head from the center of the disc (inner tracks are read at a higher disc speed). Most optical discs exhibit a characteristic iridescence as a result of the diffraction grating formed by its grooves. [2] [3] This side of the disc contains the actual data and is typically coated with a transparent material, usually lacquer . The reverse side of an optical disc usually has a printed label, sometimes made of paper but often printed or stamped onto the disc itself. Unlike the 3½-inch floppy disk , most optical discs do not have an integrated protective casing and are therefore susceptible to data transfer problems due to scratches, fingerprints, and other environmental problems.
Optical discs are usually between 7.6 and 30 cm (3 to 12 in) in diameter, with 12 cm (4.75 in) being the most common size. A typical disc is about 1.2 mm (0.05 in) thick, while the track pitch (distance from the center of one track to the center of the next) ranges from 1.6 µm (for CDs ) to 320 nm (for Blu-ray discs ).
An optical disc is designed to support one of three recording types: read-only (e.g.: CD and CD-ROM ), recordable (write-once, e.g. CD-R ), or re-recordable (rewritable, e.g. CD-RW ). Write-once optical discs commonly have an organic dye recording layer between the substrate and the reflective layer. Rewritable discs typically contain an alloy recording layer composed of a phase change material , most often AgInSbTe , an alloy of silver , indium , antimony , and tellurium . [4]
Optical discs are most commonly used for storing music (e.g. for use in a CD player ), video (e.g. for use in a Blu-ray player), or data and programs for personal computers (PC). The Optical Storage Technology Association (OSTA) promotes standardized optical storage formats. Although optical discs are more durable than earlier audio-visual and data storage formats, they are susceptible to environmental and daily-use damage. Libraries and archives enact optical media preservation procedures to ensure continued usability in the computer's optical disc drive or corresponding disc player.
For computer data backup and physical data transfer, optical discs such as CDs and DVDs are gradually being replaced with faster, smaller solid-state devices, especially the USB flash drive . [ citation needed ] This trend is expected to continue as USB flash drives continue to increase in capacity and drop in price. [ citation needed ] Additionally, music purchased or shared over the Internet has significantly reduced the number of audio CDs sold annually.

History
The first recorded historical use of an optical disc was in 1884 when Alexander Graham Bell , Chichester Bell and Charles Sumner Tainter recorded sound on a glass disc using a beam of light. [5]
An early optical disc system existed in 1935, named Lichttonorgel . [ citation needed ]
An early analog optical disc used for video recording was invented by David Paul Gregg in 1958 [6] and patented in the US in 1961 and 1969. This form of optical disc was a very early form of the DVD ( U.S. Patent 3,430,966 ). It is of special interest that U.S. Patent 4,893,297 , filed 1989, issued 1990, generated royalty income for Pioneer Corporation's DVA until 2007 —then encompassing the CD, DVD , and Blu-ray systems. In the early 1960s, the Music Corporation of America bought Gregg's patents and his company, Gauss Electrophysics .
American inventor James T. Russell has been credited with inventing the first system to record a digital signal on an optical transparent foil which is lit from behind by a high-power halogen lamp. Russell's patent application was first filed in 1966 and he was granted a patent in 1970. Following litigation, Sony and Philips licensed Russell's patents (then held by a Canadian company, Optical Recording Corp.) in the 1980s. [7] [8] [9]
Both Gregg's and Russell's disc are floppy media read in transparent mode, which impose serious drawbacks. In the Netherlands in 1969, Philips Research physicist , Pieter Kramer invented an optical videodisc in reflective mode with a protective layer read by a focused laser beam U.S. Patent 5,068,846 , filed 1972, issued 1991. Kramer's physical format is used in all optical discs. In 1975, Philips and MCA began to work together, and in 1978, commercially much too late, they presented their long-awaited Laserdisc in Atlanta . MCA delivered the discs and Philips the players. However, the presentation was a commercial failure, and the cooperation ended.
In Japan and the U.S., Pioneer succeeded with the videodisc until the advent of the DVD. In 1979, Philips and Sony , in consortium, successfully developed the audio compact disc .
In the mid-1990s, a consortium of manufacturers developed the second generation of the optical disc, the DVD. [10]
Magnetic disks found limited applications in storing the data in large amount. So, there was the need of finding some more data storing techniques. As a result, it was found that by using optical means large data storing devices can be made which in turn gave rise to the optical discs.The very first application of this kind was the Compact Disc (CD) which was used in audio systems.
Sony and Philips developed the first generation of the CDs in the mid 1980s with the complete specifications for these devices. With the help of this kind of technology the possibility of representing the analog signal into digital signal was exploited to great level. For this purpose the 16 bit samples of the analog signal were taken at the rate of 44,100 samples per second . This sample rate was based on the Nyquist rate of 40,000 samples per second required to capture the audible frequency range to 20 kHz without aliasing, with an additional tolerance to allow the use of less-than-perfect analog audio pre-filters to remove any higher frequencies. [11] The first version of the standard allowed up to 75 minutes of music which required 650MB of storage.
The DVD disc appeared after the CD-ROM had become widespread in society.
The third generation optical disc was developed in 2000–2006, and was introduced as Blu-ray Disc. First movies on Blu-ray Discs were released in June 2006. [12] Blu-ray eventually prevailed in a high definition optical disc format war over a competing format, the HD DVD . A standard Blu-ray disc can hold about 25 GB of data, a DVD about 4.7 GB, and a CD about 700 MB.

First-generation
Initially, optical discs were used to store music and computer software. The Laserdisc format stored analog video signals for the distribution of home video , but commercially lost to the VHS videocassette format, due mainly to its high cost and non-re-recordability; other first-generation disc formats were designed only to store digital data and were not initially capable of use as a digital video medium.
Most first-generation disc devices had an infrared laser reading head. The minimum size of the laser spot is proportional to the wavelength of the laser, so wavelength is a limiting factor upon the amount of information that can be stored in a given physical area on the disc. The infrared range is beyond the long-wavelength end of the visible light spectrum, so it supports less density than shorter-wavelength visible light. One example of high-density data storage capacity, achieved with an infrared laser, is 700 MB of net user data for a 12 cm compact disc.
Other factors that affect data storage density include: the existence of multiple layers of data on the disc, the method of rotation ( Constant linear velocity (CLV), Constant angular velocity (CAV), or zoned-CAV), the composition of lands and pits, and how much margin is unused is at the center and the edge of the disc.

Second-generation
Second-generation optical discs were for storing great amounts of data, including broadcast-quality digital video. Such discs usually are read with a visible-light laser (usually red); the shorter wavelength and greater numerical aperture [13] allow a narrower light beam, permitting smaller pits and lands in the disc. In the DVD format, this allows 4.7 GB storage on a standard 12 cm, single-sided, single-layer disc; alternatively, smaller media, such as the DataPlay format, can have capacity comparable to that of the larger, standard compact 12 cm disc. [14]

Third-generation
Third-generation optical discs are in development, meant for distributing high-definition video and support greater data storage capacities, accomplished with short-wavelength visible-light lasers and greater numerical apertures. Blu-ray Disc and HD DVD uses blue-violet lasers and focusing optics of greater aperture, for use with discs with smaller pits and lands, thereby greater data storage capacity per layer. [13] In practice, the effective multimedia presentation capacity is improved with enhanced video data compression codecs such as H.264/MPEG-4 AVC and VC-1 .

Fourth-generation
The following formats go beyond the current third-generation discs and have the potential to hold more than one terabyte (1 TB ) of data:

Overview of optical types

Recordable and writable optical discs
There are numerous formats of optical direct to disk recording devices on the market, all of which are based on using a laser to change the reflectivity of the digital recording medium in order to duplicate the effects of the pits and lands created when a commercial optical disc is pressed. Formats such as CD-R and DVD-R are " Write once read many ", while CD-RW and DVD-RW are rewritable, more like a magnetic recording hard disk drive (HDD). Media technologies vary, M-DISC uses a different recording technique & media versus DVD-R and BD-R.

Specifications
WebPage index: 00082
Books LLC
Books LLC is an American publisher and book sales club based in Memphis , Tennessee . Its primary work is collecting Wikipedia and Wikia articles and selling them as printed and downloadable books. [1]

Print-on-demand and electronic products
Books LLC publishes print-on-demand paperback and downloadable compilations of English texts and documents from open knowledge sources such as Wikipedia . [2] [3] [4] [5] Books LLC's copies of the English Wikipedia are republished by Google Books . Titles are also published in French and German respectively under the names " Livres Groupe " and " Bücher Gruppe ". [6] [7] [8] Books' publications do not include the images from the original Web documents but, in their place, URLs pointing to the Web images. According to the FAQ page: “We understand how annoying that can be for the reader. But our first priority has to be respecting copyright laws. In addition, the resolution of the online photos are not high enough to print in a book.” [9]
In 2009, Books LLC and its sister imprint General Books LLC produced 224,460 and 11,887 titles respectively. [10] [11]

Imprint and book club names
In brackets: kind of material published and sold.

See also
WebPage index: 00083
Semantic Web
The Semantic Web is an extension of the Web through standards by the World Wide Web Consortium (W3C). [1] The standards promote common data formats and exchange protocols on the Web , most fundamentally the Resource Description Framework (RDF).
According to the W3C, "The Semantic Web provides a common framework that allows data to be shared and reused across application, enterprise, and community boundaries". [2] The term was coined by Tim Berners-Lee for a web of data that can be processed by machines. [3] While its critics have questioned its feasibility, proponents argue that applications in industry, biology and human sciences research have already proven the validity of the original concept. [4]
The 2001 Scientific American article by Berners-Lee, Hendler , and Lassila described an expected evolution of the existing Web to a Semantic Web. [5] In 2006, Berners-Lee and colleagues stated that: "This simple idea…remains largely unrealized". [6] In 2013, more than four million Web domains contained Semantic Web markup. [7]

Example
In the following example, the text 'Paul Schuster was born in Dresden' on a Website will be annotated, connecting a person with their place of birth. The following HTML -fragment shows how a small graph is being described, in RDFa -syntax using a schema.org vocabulary and a Wikidata ID:
The example defines the following five triples (shown in Turtle Syntax). Each triple represents one edge in the resulting graph: the first element of the triple (the subject ) is the name of the node where the edge starts, the second element (the predicate ) the type of the edge, and the last and third element (the object ) either the name of the node where the edge ends or a literal value (e.g. a text, a number, etc.).
The triples result in the graph shown in the given figure .
One of the advantages of using Uniform Resource Identifier (URIs) is that they can be dereferenced using the HTTP protocol. According to the so-called Linked Open Data principles, such a dereferenced URI should result in a document that offers further data about the given URI. In this example, all URIs, both for edges and nodes (e.g. http://schema.org/Person , http://schema.org/birthPlace , http://www.wikidata.org/entity/Q1731 ) can be dereferenced and will result in further RDF graphs, describing the URI, e.g. that Dresden is a city in Germany, or that a person, in the sense of that URI, can be fictional.
The second graph shows the previous example, but now enriched with a few of the triples from the documents that result from dereferencing http://schema.org/Person (green edge) and http://www.wikidata.org/entity/Q1731 (blue edges).
Additionally to the edges given in the involved documents explicitly, edges can be automatically inferred : the triple
from the original RDFa fragment and the triple
from the document at http://schema.org/Person (green edge in the Figure) allow to infer the following triple, given OWL semantics (red dashed line in the second Figure):

Background
The concept of the Semantic Network Model was formed in the early 1960s by the cognitive scientist Allan M. Collins , linguist M. Ross Quillian and psychologist Elizabeth F. Loftus as a form to represent semantically structured knowledge. When applied in the context of the modern internet, it extends the network of hyperlinked human-readable web pages by inserting machine-readable metadata about pages and how they are related to each other. This enables automated agents to access the Web more intelligently and perform more tasks on behalf of users. The term "Semantic Web" was coined by Tim Berners-Lee , [3] the inventor of the World Wide Web and director of the World Wide Web Consortium (" W3C "), which oversees the development of proposed Semantic Web standards. He defines the Semantic Web as "a web of data that can be processed directly and indirectly by machines".
Many of the technologies proposed by the W3C already existed before they were positioned under the W3C umbrella. These are used in various contexts, particularly those dealing with information that encompasses a limited and defined domain, and where sharing data is a common necessity, such as scientific research or data exchange among businesses. In addition, other technologies with similar goals have emerged, such as microformats .
Tim Berners-Lee originally expressed the vision of the Semantic Web as follows:
The Semantic Web is regarded as an integrator across different content, information applications and systems. It has applications in publishing, blogging, and many other areas.

Limitations of HTML
Many files on a typical computer can also be loosely divided into human readable documents and machine readable data . Documents like mail messages, reports, and brochures are read by humans. Data, such as calendars, addressbooks, playlists, and spreadsheets are presented using an application program that lets them be viewed, searched and combined.
Currently, the World Wide Web is based mainly on documents written in Hypertext Markup Language ( HTML ), a markup convention that is used for coding a body of text interspersed with multimedia objects such as images and interactive forms. Metadata tags provide a method by which computers can categorise the content of web pages, for example:
With HTML and a tool to render it (perhaps web browser software, perhaps another user agent ), one can create and present a page that lists items for sale. The HTML of this catalog page can make simple, document-level assertions such as "this document's title is 'Widget Superstore ' ", but there is no capability within the HTML itself to assert unambiguously that, for example, item number X586172 is an Acme Gizmo with a retail price of €199, or that it is a consumer product. Rather, HTML can only say that the span of text "X586172" is something that should be positioned near "Acme Gizmo" and "€199", etc. There is no way to say "this is a catalog" or even to establish that "Acme Gizmo" is a kind of title or that "€199" is a price. There is also no way to express that these pieces of information are bound together in describing a discrete item, distinct from other items perhaps listed on the page.
Semantic HTML refers to the traditional HTML practice of markup following intention, rather than specifying layout details directly. For example, the use of <em> denoting "emphasis" rather than <i> , which specifies italics . Layout details are left up to the browser, in combination with Cascading Style Sheets . But this practice falls short of specifying the semantics of objects such as items for sale or prices.
Microformats extend HTML syntax to create machine-readable semantic markup about objects including people, organisations, events and products. [9] Similar initiatives include RDFa , Microdata and Schema.org .

Semantic Web solutions
The Semantic Web takes the solution further. It involves publishing in languages specifically designed for data: Resource Description Framework (RDF), Web Ontology Language (OWL), and Extensible Markup Language ( XML ). HTML describes documents and the links between them. RDF, OWL, and XML, by contrast, can describe arbitrary things such as people, meetings, or airplane parts.
These technologies are combined in order to provide descriptions that supplement or replace the content of Web documents. Thus, content may manifest itself as descriptive data stored in Web-accessible databases , [10] or as markup within documents (particularly, in Extensible HTML ( XHTML ) interspersed with XML, or, more often, purely in XML, with layout or rendering cues stored separately). The machine-readable descriptions enable content managers to add meaning to the content, i.e., to describe the structure of the knowledge we have about that content. In this way, a machine can process knowledge itself, instead of text, using processes similar to human deductive reasoning and inference , thereby obtaining more meaningful results and helping computers to perform automated information gathering and research .
An example of a tag that would be used in a non-semantic web page:
Encoding similar information in a semantic web page might look like this:
Tim Berners-Lee calls the resulting network of Linked Data the Giant Global Graph , in contrast to the HTML-based World Wide Web . Berners-Lee posits that if the past was document sharing, the future is data sharing. His answer to the question of "how" provides three points of instruction. One, a URL should point to the data. Two, anyone accessing the URL should get data back. Three, relationships in the data should point to additional URLs with data.

Web 3.0
Tim Berners-Lee has described the semantic web as a component of "Web 3.0". [11]
"Semantic Web" is sometimes used as a synonym for "Web 3.0", [12] though the definition of each term varies. Web 3.0 has started to emerge as a movement away from the centralisation of services like search, social media and chat applications that are dependent on a single organisation to function. [13]

Challenges
Some of the challenges for the Semantic Web include vastness, vagueness, uncertainty, inconsistency, and deceit. Automated reasoning systems will have to deal with all of these issues in order to deliver on the promise of the Semantic Web.
This list of challenges is illustrative rather than exhaustive, and it focuses on the challenges to the "unifying logic" and "proof" layers of the Semantic Web. The World Wide Web Consortium (W3C) Incubator Group for Uncertainty Reasoning for the World Wide Web (URW3-XG) final report lumps these problems together under the single heading of "uncertainty". Many of the techniques mentioned here will require extensions to the Web Ontology Language (OWL) for example to annotate conditional probabilities. This is an area of active research. [14]

Standards
Standardization for Semantic Web in the context of Web 3.0 is under the care of W3C. [15]

Components
The term "Semantic Web" is often used more specifically to refer to the formats and technologies that enable it. [2] The collection, structuring and recovery of linked data are enabled by technologies that provide a formal description of concepts, terms, and relationships within a given knowledge domain . These technologies are specified as W3C standards and include:
The Semantic Web Stack illustrates the architecture of the Semantic Web. The functions and relationships of the components can be summarized as follows: [16]

Current state of standardization
Well-established standards:
Not yet fully realized:

Applications
The intent is to enhance the usability and usefulness of the Web and its interconnected resources by creating Semantic Web Services , such as:
Such services could be useful to public search engines, or could be used for knowledge management within an organization. Business applications include:
In a corporation, there is a closed group of users and the management is able to enforce company guidelines like the adoption of specific ontologies and use of semantic annotation . Compared to the public Semantic Web there are lesser requirements on scalability and the information circulating within a company can be more trusted in general; privacy is less of an issue outside of handling of customer data.

Skeptical reactions

Practical feasibility
Critics question the basic feasibility of a complete or even partial fulfillment of the Semantic Web, pointing out both difficulties in setting it up and a lack of general-purpose usefulness that prevents the required effort from being invested. In a 2003 paper, Marshall and Shipman point out the cognitive overhead inherent in formalizing knowledge, compared to the authoring of traditional web hypertext : [21]
According to Marshall and Shipman, the tacit and changing nature of much knowledge adds to the knowledge engineering problem, and limits the Semantic Web's applicability to specific domains. A further issue that they point out are domain- or organisation-specific ways to express knowledge, which must be solved through community agreement rather than only technical means. [21] As it turns out, specialized communities and organizations for intra-company projects have tended to adopt semantic web technologies greater than peripheral and less-specialized communities. [22] The practical constraints toward adoption have appeared less challenging where domain and scope is more limited than that of the general public and the World-Wide Web. [22]
Finally, Marshall and Shipman see pragmatic problems in the idea of ( Knowledge Navigator -style) intelligent agents working in the largely manually curated Semantic Web: [21]
Cory Doctorow 's critique (" metacrap ") is from the perspective of human behavior and personal preferences. For example, people may include spurious metadata into Web pages in an attempt to mislead Semantic Web engines that naively assume the metadata's veracity. This phenomenon was well-known with metatags that fooled the Altavista ranking algorithm into elevating the ranking of certain Web pages: the Google indexing engine specifically looks for such attempts at manipulation. Peter Gärdenfors and Timo Honkela point out that logic-based semantic web technologies cover only a fraction of the relevant phenomena related to semantics. [23] [24]

Censorship and privacy
Enthusiasm about the semantic web could be tempered by concerns regarding censorship and privacy . For instance, text-analyzing techniques can now be easily bypassed by using other words, metaphors for instance, or by using images in place of words. An advanced implementation of the semantic web would make it much easier for governments to control the viewing and creation of online information, as this information would be much easier for an automated content-blocking machine to understand. In addition, the issue has also been raised that, with the use of FOAF files and geolocation meta-data , there would be very little anonymity associated with the authorship of articles on things such as a personal blog . Some of these concerns were addressed in the "Policy Aware Web" project [25] and is an active research and development topic.

Doubling output formats
Another criticism of the semantic web is that it would be much more time-consuming to create and publish content because there would need to be two formats for one piece of data: one for human viewing and one for machines. However, many web applications in development are addressing this issue by creating a machine-readable format upon the publishing of data or the request of a machine for such data. The development of microformats has been one reaction to this kind of criticism. Another argument in defense of the feasibility of semantic web is the likely falling price of human intelligence tasks in digital labor markets, such as Amazon 's Mechanical Turk . [ citation needed ]
Specifications such as eRDF and RDFa allow arbitrary RDF data to be embedded in HTML pages. The GRDDL (Gleaning Resource Descriptions from Dialects of Language) mechanism allows existing material (including microformats) to be automatically interpreted as RDF, so publishers only need to use a single format, such as HTML.

Research activities on corporate applications
The first research group explicitly focusing on the Corporate Semantic Web was the ACACIA team at INRIA-Sophia-Antipolis , founded in 2002. Results of their work include the RDF(S) based Corese search engine , and the application of semantic web technology in the realm of E-learning . [26]
Since 2008, the Corporate Semantic Web research group, located at the Free University of Berlin , focuses on building blocks: Corporate Semantic Search, Corporate Semantic Collaboration, and Corporate Ontology Engineering. [27]
Ontology engineering research includes the question of how to involve non-expert users in creating ontologies and semantically annotated content [28] and for extracting explicit knowledge from the interaction of users within enterprises.

See also
WebPage index: 00084
Web browser
A web browser (commonly referred to as a browser ) is a software application for retrieving, presenting and traversing information resources on the World Wide Web . An information resource is identified by a Uniform Resource Identifier (URI/URL) that may be a web page , image, video or other piece of content. [1] Hyperlinks present in resources enable users easily to navigate their browsers to related resources.
Although browsers are primarily intended to use the World Wide Web, they can also be used to access information provided by web servers in private networks or files in file systems .
The most popular web browsers are Google Chrome , Microsoft Edge (preceded by Internet Explorer ), [2] [3] [4] Safari , Opera and Firefox .

History
The first web browser was invented in 1990 by Sir Tim Berners-Lee . Berners-Lee is the director of the World Wide Web Consortium (W3C), which oversees the Web's continued development, and is also the founder of the World Wide Web Foundation. His browser was called WorldWideWeb and later renamed Nexus. [5]
The first commonly available web browser with a graphical user interface was Erwise . The development of Erwise was initiated by Robert Cailliau .
In 1993, browser software was further innovated by Marc Andreessen with the release of Mosaic , "the world's first popular browser", [6] which made the World Wide Web system easy to use and more accessible to the average person. Andreesen's browser sparked the internet boom of the 1990s. [6] The introduction of Mosaic in 1993 – one of the first graphical web browsers – led to an explosion in web use. Andreessen, the leader of the Mosaic team at National Center for Supercomputing Applications (NCSA), soon started his own company, named Netscape , and released the Mosaic-influenced Netscape Navigator in 1994, which quickly became the world's most popular browser, accounting for 90% of all web use at its peak (see usage share of web browsers ).
Microsoft responded with its Internet Explorer in 1995, also heavily influenced by Mosaic, initiating the industry's first browser war . Bundled with Windows , Internet Explorer gained dominance in the web browser market; Internet Explorer usage share peaked at over 95% by 2002. [7]
Opera debuted in 1996; it has never achieved widespread use, having less than 2% browser usage share as of February 2012 according to Net Applications. [9] Its Opera-mini version has an additive share, in April 2011 amounting to 1.1% of overall browser use, but focused on the fast-growing mobile phone web browser market, being preinstalled on over 40 million phones. It is also available on several other embedded systems , including Nintendo 's Wii video game console.
In 1998, Netscape launched what was to become the Mozilla Foundation in an attempt to produce a competitive browser using the open source software model. That browser would eventually evolve into Firefox , which developed a respectable following while still in the beta stage of development; shortly after the release of Firefox 1.0 in late 2004, Firefox (all versions) accounted for 7% of browser use. [7] As of August 2011, Firefox has a 28% usage share. [9]
Apple 's Safari had its first beta release in January 2003; as of April 2011, it had a dominant share of Apple-based web browsing, accounting for just over 7% of the entire browser market. [9]
The most recent major entrant to the browser market is Chrome , first released in September 2008. Chrome's take-up has increased significantly year by year, by doubling its usage share from 8% to 16% by August 2011. This increase seems largely to be at the expense of Internet Explorer, whose share has tended to decrease from month to month. [10] In December 2011, Chrome overtook Internet Explorer 8 as the most widely used web browser but still had lower usage than all versions of Internet Explorer combined. [11] Chrome's user-base continued to grow and in May 2012, Chrome's usage passed the usage of all versions of Internet Explorer combined. [12] By April 2014, Chrome's usage had hit 45%. [13]
Internet Explorer was deprecated in Windows 10 , with Microsoft Edge replacing it as the default web browser. [14]

Business models
The ways that web browser makers fund their development costs has changed over time. The first web browser, WorldWideWeb, was a research project.
In addition to being freeware , Netscape Navigator and Opera were also sold commercially.
Internet Explorer, on the other hand, was bundled free with the Windows operating system (and was also downloadable free), and therefore it was funded partly by the sales of Windows to computer manufacturers and direct to users. Internet Explorer also used to be available for the Mac. It is likely that releasing IE for the Mac was part of Microsoft's overall strategy to fight threats to its quasi-monopoly platform dominance – threats such as web standards and Java – by making some web developers, or at least their managers, assume that there was "no need" to develop for anything other than Internet Explorer. In this respect, IE may have contributed to Windows and Microsoft applications sales in another way, through " lock-in " to Microsoft's browser.
In January 2009, the European Commission announced it would investigate the bundling of Internet Explorer with Windows operating systems from Microsoft, saying "Microsoft's tying of Internet Explorer to the Windows operating system harms competition between web browsers, undermines product innovation and ultimately reduces consumer choice." Microsoft Corp v Commission [15] [16]
Safari and Mobile Safari were likewise always included with macOS and iOS respectively, so, similarly, they were originally funded by sales of Apple computers and mobile devices, and formed part of the overall Apple experience to customers.
Some commercial web browsers are paid by search engine companies to make their engine default, or to include them as another option. For example, Yahoo! pays Mozilla , the maker of Firefox, to make Yahoo! Search the default search engine in Firefox. Mozilla makes enough money from this deal that it does not need to charge users for Firefox. By virtue of common ownership, Microsoft Edge , Internet Explorer, and Google Chrome default to their respective vendors' own search engines, Bing and Google Search , and may integrate with other platforms offered by the vendor. This encourages the use of their first-party services, which in turn, exposes users to advertising that can be used as a source of revenue.
Many less-well-known free software browsers, such as Konqueror , were hardly funded at all and were developed mostly by volunteers free of charge.

Function
The primary purpose of a web browser is to bring information resources to the user ("retrieval" or "fetching"), allowing them to view the information ("display", "rendering"), and then access other information ("navigation", "following links").
This process begins when the user inputs a Uniform Resource Locator (URL), for example http://en.wikipedia.org/ , into the browser. The prefix of the URL, the Uniform Resource Identifier or URI , determines how the URL will be interpreted. The most commonly used kind of URI starts with http: and identifies a resource to be retrieved over the Hypertext Transfer Protocol (HTTP). [17] Many browsers also support a variety of other prefixes, such as https: for HTTPS , ftp: for the File Transfer Protocol , and file: for local files . Prefixes that the web browser cannot directly handle are often handed off to another application entirely. For example, mailto: URIs are usually passed to the user's default e-mail application, and news: URIs are passed to the user's default newsgroup reader.
In the case of http , https , file , and others, once the resource has been retrieved the web browser will display it. HTML and associated content (image files, formatting information such as CSS , etc.) is passed to the browser's layout engine to be transformed from markup to an interactive document, a process known as "rendering". Aside from HTML, web browsers can generally display any kind of content that can be part of a web page. Most browsers can display images, audio, video, and XML files, and often have plug-ins to support Flash applications and Java applets . Upon encountering a file of an unsupported type or a file that is set up to be downloaded rather than displayed, the browser prompts the user to save the file to disk.
Information resources may contain hyperlinks to other information resources. Each link contains the URI of a resource to go to. When a link is clicked, the browser navigates to the resource indicated by the link's target URI, and the process of bringing content to the user begins again.

Market share

Features
Available web browsers range in features from minimal, text-based user interfaces with bare-bones support for HTML to rich user interfaces supporting a wide variety of file formats and protocols. Browsers which include additional components to support e-mail, Usenet news, and Internet Relay Chat (IRC), are sometimes referred to as " Internet suites " rather than merely "web browsers". [19] [20] [21]
All major web browsers allow the user to open multiple information resources at the same time, either in different browser windows or in different tabs of the same window. Major browsers also include pop-up blockers to prevent unwanted windows from "popping up" without the user's consent. [22] [23] [24] [25]
Most web browsers can display a list of web pages that the user has bookmarked so that the user can quickly return to them. Bookmarks are also called "Favorites" in Internet Explorer . In addition, all major web browsers have some form of built-in web feed aggregator . In Firefox , web feeds are formatted as "live bookmarks" and behave like a folder of bookmarks corresponding to recent entries in the feed. [26] In Opera , a more traditional feed reader is included which stores and displays the contents of the feed. [27]
Furthermore, most browsers can be extended via plug-ins , downloadable components that provide additional features.

User interface
Most major web browsers have these user interface elements in common: [28]
Major browsers also possess incremental find features to search within a web page.

Privacy and security
Most browsers support HTTP Secure and offer quick and easy ways to delete personally identifiable information such as the web cache, download history, form and search history, cookies , and browsing history. For a comparison of the current security vulnerabilities of browsers, see comparison of web browsers .

Standards support
Early web browsers supported only a very simple version of HTML. The rapid development of proprietary web browsers led to the development of non-standard dialects of HTML, leading to problems with interoperability. Modern web browsers support a combination of standards -based and de facto HTML and XHTML , which should be rendered in the same way by all browsers.

Extensibility
A browser extension is a computer program that extends the functionality of a web browser. Every major web browser supports the development of browser extensions.

Components
Web browsers consist of a user interface, layout engine , rendering engine, JavaScript interpreter, UI backend, networking component and data persistence component. These components achieve different functionalities of a web browser and together provide all capabilities of a web browser. [29]

See also
WebPage index: 00085
iPhone
iPhone ( / ˈ aɪ f oʊ n / EYE -fohn ) is a line of smartphones designed and marketed by Apple Inc. They run Apple's iOS mobile operating system. [15] The first generation iPhone was released on June 29, 2007; the most recent iPhone model is the iPhone 7 , which was unveiled at a special event on September 7, 2016. [16] [17]
The user interface is built around the device's multi-touch screen, including a virtual keyboard . The iPhone has Wi-Fi and can connect to cellular networks. An iPhone can shoot video (though this was not a standard feature until the iPhone 3GS ), take photos , play music , send and receive email, browse the web , send and receive text messages , follow GPS navigation , record notes, perform mathematical calculations, and receive visual voicemail . [18] Other functionality, such as video games, reference works, and social networking, can be enabled by downloading mobile apps . As of January 2017, Apple's App Store contained more than 2.2 million applications available for the iPhone and other iOS devices. [19]
Apple has released ten generations of iPhone models, each accompanied by one of the ten major releases of the iOS operating system. The original 1st-generation iPhone was a GSM phone and established design precedents, such as a button placement that has persisted throughout all releases and a screen size maintained for the next four iterations. The iPhone 3G added 3G network support, and was followed by the 3GS with improved hardware, the 4 with a metal chassis, higher display resolution and front-facing camera, and the 4S with improved hardware and the voice assistant Siri . The iPhone 5 featured a taller, 4-inch display and Apple's newly introduced Lightning connector . In 2013, Apple released the 5S with improved hardware and a fingerprint reader , and the lower-cost 5C , a version of the 5 with colored plastic casings instead of metal. They were followed by the larger iPhone 6 , with models featuring 4.7 and 5.5-inch displays. The iPhone 6S was introduced the following year, which featured hardware upgrades and support for pressure-sensitive touch inputs , as well as the SE —which featured hardware from the 6S but the smaller form factor of the 5S. In 2016, Apple unveiled the iPhone 7 and 7 Plus , which add water resistance, improved system and graphics performance, a new rear dual-camera setup on the Plus model, and new color options, while removing the 3.5 mm headphone jack found on previous phones. [20]
The iPhone's commercial success has been credited with reshaping the smartphone industry and helping to make Apple one of the world's most valuable publicly traded companies by 2011. [21] The original iPhone was one of the first phones to use a design featuring a slate format with a touchscreen interface. [22] Almost all modern smartphones have replicated this style of design. [ citation needed ]
In the US, the iPhone holds the largest share of the smartphone market. As of late 2015, the iPhone had a 43.6% market share, followed by Samsung (27.6%), LG (9.4%), and Motorola (4.8%). [23]

History and availability
Development of what was to become the iPhone began in 2004, when Apple started to gather a team of 1,000 employees to work on the highly confidential "Project Purple", [24] including Jonathan Ive , the designer behind the iMac and iPod. [25] Apple CEO Steve Jobs steered the original focus away from a tablet (which Apple eventually revisited in the form of the iPad ) and towards a phone. [26] Apple created the device during a secretive collaboration with Cingular Wireless (now AT&T Mobility ) at the time—at an estimated development cost of US$150 million over thirty months. [27]
Apple rejected the " design by committee " approach that had yielded the Motorola ROKR E1 , a largely unsuccessful collaboration with Motorola . Among other deficiencies, the ROKR E1's firmware limited storage to only 100 iTunes songs to avoid competing with Apple's iPod nano . [28] [29]
Cingular gave Apple the liberty to develop the iPhone's hardware and software in-house [30] [31] and even paid Apple a fraction of its monthly service revenue (until the iPhone 3G), [32] in exchange for four years of exclusive US sales, until 2011.
Jobs unveiled the iPhone to the public on January 9, 2007, at the Macworld 2007 convention at the Moscone Center in San Francisco. [33] The two initial models, a 4 GB model priced at US$499 and an 8 GB model at US$599 (both requiring a 2-year contract), went on sale in the United States on June 29, 2007, at 6:00 pm local time, while hundreds of customers lined up outside the stores nationwide. [34] The passionate reaction to the launch of the iPhone resulted in sections of the media dubbing it the 'Jesus phone'. [35] [36] Following this successful release in the US, the first generation iPhone was made available in the UK, France, and Germany in November 2007, and Ireland and Austria in the spring of 2008.
On July 11, 2008, Apple released the iPhone 3G in twenty-two countries, including the original six. [37] Apple released the iPhone 3G in upwards of eighty countries and territories. [38] Apple announced the iPhone 3GS on June 8, 2009, along with plans to release it later in June, July, and August, starting with the US, Canada and major European countries on June 19. Many would-be users objected to the iPhone's cost, [39] and 40% of users had household incomes over US$100,000. [40]
The back of the original first generation iPhone was made of aluminum with a black plastic accent. The iPhone 3G and 3GS feature a full plastic back to increase the strength of the GSM signal. [41] The iPhone 3G was available in an 8 GB black model, or a black or white option for the 16 GB model. The iPhone 3GS was available in both colors, regardless of storage capacity.
The iPhone 4 has an aluminosilicate glass front and back with a stainless steel edge that serves as the antennas . It was at first available in black; the white version was announced, but not released until April 2011, 10 months later.
Users of the iPhone 4 reported dropped/disconnected telephone calls when holding their phones in a certain way. This became known as antennagate . [42]
On January 11, 2011, Verizon announced during a media event that it had reached an agreement with Apple and would begin selling a CDMA iPhone 4 . Verizon said it would be available for pre-order on February 3, with a release set for February 10. [43] [44] In February 2011, the Verizon iPhone accounted for 4.5% of all iPhone ad impressions in the US on Millennial Media's mobile ad network. [45]
From 2007 to 2011, Apple spent $647 million on advertising for the iPhone in the US. [24]
On Tuesday, September 27, Apple sent invitations for a press event to be held October 4, 2011, at 10:00 am at the Cupertino Headquarters to announce details of the next generation iPhone, which turned out to be iPhone 4S . Over 1 million 4S models were sold in the first 24 hours after its release in October 2011. [46] Due to large volumes of the iPhone being manufactured and its high selling price, Apple became the largest mobile handset vendor in the world by revenue, in 2011, surpassing long-time leader Nokia . [47] American carrier C Spire Wireless announced that it would be carrying the iPhone 4S on October 19, 2011. [48]
In January 2012, Apple reported its best quarterly earnings ever, with 53% of its revenue coming from the sale of 37 million iPhones, at an average selling price of nearly $660. The average selling price has remained fairly constant for most of the phone's lifespan, hovering between $622 and $660. [49] The production price of the iPhone 4S was estimated by IHS iSuppli , in October 2011, to be $188, $207 and $245, for the 16 GB, 32 GB and 64 GB models, respectively. [50] Labor costs are estimated at between $12.50 and $30 per unit, with workers on the iPhone assembly line making $1.78 an hour. [51]
In February 2012, ComScore reported that 12.4% of US mobile subscribers used an iPhone. [52] Approximately 6.4 million iPhones are active in the US alone. [40]
On September 12, 2012, Apple announced the iPhone 5. It has a 4-inch display, up from its predecessors' 3.5-inch screen. The device comes with the same 326 pixels per inch found in the iPhone 4 and 4S. The iPhone 5 has the SoC A6 processor, the chip is 22% smaller than the iPhone 4S' A5 and is twice as fast, doubling the graphics performance of its predecessor. The device is 18% thinner than the iPhone 4S, measuring 7.6 millimetres (0.3 in), and is 20% lighter at 112 grams (4 oz).
On July 6, 2013, it was reported that Apple was in talks with Korean mobile carrier SK Telecom to release the next generation iPhone with LTE Advanced technology. [53]
On July 22, 2013, the company's suppliers said that Apple is testing out larger screens for the iPhone and iPad. "Apple has asked for prototype smartphone screens larger than 4 inches and has also asked for screen designs for a new tablet device measuring slightly less than 13 inches diagonally, they said." [54]
On September 10, 2013, Apple unveiled two new iPhone models during a highly anticipated press event in Cupertino. The iPhone 5C, a mid-range-priced version of the handset that is designed to increase accessibility due to its price is available in five colors (green, blue, yellow, pink, and white) and is made of plastic. The iPhone 5S comes in three colors (black, white, and gold) and the home button is replaced with a fingerprint scanner (Touch ID). Both phones shipped on September 20, 2013. [55]
On September 9, 2014, Apple revealed the iPhone 6 and the iPhone 6 Plus at an event in Cupertino. Both devices had a larger screen than their predecessor, at 4.7 and 5.5 inches respectively. [56]
In January 2015, "Apple stands on second slot i.e only 31% in US market". [57] Competing devices with Android operating system have a market share approximately 62% of the US, 82.7% of the Chinese market, and 73.3% of the European market (countries such as the UK, France, Germany Spain and Italy).
In 2016, Apple unveiled the iPhone 7 and 7 Plus , which add water resistance, improved system and graphics performance, a new dual-camera setup on the Plus model, new color options, and remove the 3.5 mm headphone jack. [20]

Sales and profits
Apple sold 6.1 million first generation iPhone units over five quarters. [58] Sales in the fourth quarter of 2008, temporarily surpassed those of Research In Motion 's (RIM) BlackBerry sales of 5.2 million units, which briefly made Apple the third largest mobile phone manufacturer by revenue, after Nokia and Samsung [59] (However, some of this income is deferred [60] ). Recorded sales grew steadily thereafter, and by the end of fiscal year 2010, a total of 73.5 million iPhones were sold. [61]
By 2010, the iPhone had a market share of barely 4% of all cellphones, however Apple pulled in more than 50% of the total profit that global cellphone sales generate. [62] Apple sold 14.1 million iPhones in the third quarter of 2010, representing a 91% unit growth over the year-ago quarter, which was well ahead of IDC's latest published estimate of 64% growth for the global smartphone market in the September quarter. Apple's sales surpassed that of Research in Motion 's 12.1 million BlackBerry units sold in their most recent quarter ended August 2010. [63] In the United States market alone for the third quarter of 2010, while there were 9.1 million Android-powered smartphones shipped for 43.6% of the market, Apple iOS was the number two phone operating system with 26.2% but the 5.5 million iPhones sold made it the most popular single device. [64]
On March 2, 2011, at the iPad 2 launch event, Apple announced that they had sold 100 million iPhones worldwide. [65] As a result of the success of the iPhone sales volume and high selling price, headlined by the iPhone 4S , Apple became the largest mobile handset vendor in the world by revenue in 2011, surpassing long-time leader Nokia . [47] While the Samsung Galaxy S II proved more popular than the iPhone 4S in parts of Europe, the iPhone 4S was dominant in the United States. [66]
In January 2012, Apple reported its best quarterly earnings ever, with 53% of its revenue coming from the sale of 37 million iPhones, at an average selling price of nearly $660. The average selling price has remained fairly constant for most of the phone's lifespan, hovering between $622 and $660. [49]
For the eight largest phone manufacturers in Q1 2012, according to Horace Dediu at Asymco, Apple and Samsung combined to take 99% of industry profits (HTC took the remaining 1%, while RIM, LG, Sony Ericsson, Motorola, and Nokia all suffered losses), with Apple earning 73 cents out of every dollar earned by the phone makers. As the industry profits grew from $5.3 billion in the first quarter of 2010 to $14.4 billion in the first quarter of 2012 (quadruple the profits in 2007), [67] [68] Apple had managed to increase its share of these profits. This is due to increasing carrier subsidies and the high selling prices of the iPhone, which had a negative effect on the wireless carriers (AT&T Mobility, Verizon, and Sprint) who have seen their EBITDA service margins drop as they sold an increasing number of iPhones. [69] [70] [71] By the quarter ended March 31, 2012, Apple's sales from the iPhone alone (at $22.7 billion) exceeded the total of Microsoft from all of its businesses ($17.4 billion). [72]
In the fourth quarter of 2012, the iPhone 5 and iPhone 4S were the best-selling handsets with sales of 27.4 million (13% of smartphones worldwide) and 17.4 million units, respectively, with the Samsung Galaxy S III in third with 15.4 million. According to Strategy Analytics' data, this was "an impressive performance, given the iPhone portfolio’s premium pricing," adding that the Galaxy S III’s global popularity "appears to have peaked" (the Galaxy S III was touted as an iPhone-killer by some in the press when it was released [73] [74] ). While Samsung has led in worldwide sales of smartphones, Apple's iPhone line has still managed to top Samsung's smartphone offerings in the United States, [75] with 21.4% share and 37.8% in that market, respectively. iOS grew 3.5% to a 37.8%, while Android slid 1.3% to fall to a 52.3% share. [76]
The continued top popularity of the iPhone despite growing Android competition was also attributed to Apple being able to deliver iOS updates over the air, while Android updates are frequently impeded by carrier testing requirements and hardware tailoring, forcing consumers to purchase a new Android smartphone to get the latest version of that OS. [77] However, by 2013, Apple's market share had fallen to 13.1%, due to the surging popularity of the Android offerings. [78]
Apple announced on September 1, 2013, that its iPhone trade-in program would be implemented at all of its 250 specialty stores in the US. For the program to become available, customers must have a valid contract and must purchase a new phone, rather than simply receive credit to be used at a later date. A significant part of the program's goal is to increase the number of customers who purchase iPhones at Apple stores rather than carrier stores. [79]
On September 20, 2013, the sales date of the iPhone 5S and 5C models, the longest ever queue was observed at the New York City flagship Apple store, in addition to prominent queues in San Francisco, US and Canada; however, locations throughout the world were identified for the anticipation of corresponding consumers. [80] Apple also increased production of the gold-colored iPhone 5S by an additional one-third due to the particularly strong demand that emerged. [81] Apple had decided to introduce a gold model after finding that gold was seen as a popular sign of a luxury product among Chinese customers. [82]
Apple released its opening weekend sales results for the 5C and 5S models, showing an all-time high for the product's sales figures, with 9 million handsets sold—the previous record was set in 2012, when 5 million handsets were sold during the opening weekend of the 5 model. This was the first time that Apple has simultaneously launched two models and the inclusion of China in the list of markets contributed to the record sales result. [83] Apple also announced that, as of September 23, 2013, 200 million devices were running the iOS 7 update, making it the "fastest software upgrade in history." [84]
An Apple Store located at the Christiana Mall in Newark, Delaware , US claimed the highest iPhones sales figures in November 2013. The store's high sales results are due to the absence of a sales tax in the state of Delaware . [85]
The finalization of a deal between Apple and China Mobile, the world's largest mobile network, was announced in late December 2013. The multi-year agreement provides iPhone access to over 760 million China Mobile subscribers. [86]

iPhone Upgrade Program
The iPhone Upgrade Program is a 24-month program designed for consumers to be able to get the latest iPhone every year, without paying the whole price up-front. The program consists of "low monthly payments", where consumers will gradually pay for the iPhone they have over a 24-month period, with an opportunity to switch (upgrade) to the new iPhone after 12 months of payment have passed. Once 12 months have passed, consumers can trade their current iPhone with a new one, and the payments are transferred from the old device to the new device, and the program "restarts" with a new 24-month period. [87]
Additional features of the program include unlocked handsets, which means consumers are free to pick the network carrier they want, and 2-year AppleCare+ protection, which includes "hardware repairs, software support, and coverage for up to two incidents of accidental damage". [87]
Criticism of the program includes the potential endless cycle of payments, with The Huffington Post ' s Damon Beres writing, "Complete the full 24-month payment cycle, and you’re stuck with an outdated phone. Upgrade every 12 months, and you’ll never stop owing Apple money for iPhones". Additionally, the program is limited to just the iPhone hardware; cell phone service from a network operator is not included. [88]

Legacy
Before the release of the iPhone, handset manufacturers such as Nokia and Motorola were enjoying record sales of cell phones based more on fashion and brand rather than technological innovation. [89] The smartphone market, dominated at the time by BlackBerry OS and Windows Mobile devices, was a "staid, corporate-led smartphone paradigm" focused on enterprise needs. Phones at the time were designed around carrier and business limits which were conservative with regards to bandwidth usage and battery life. [90] [91] Phones were sold in a very large number of models, often segmented by marketing strategy, confusing customers and sapping engineering resources. [92] [93] For example, phones marketed at business were often deliberately stripped of cameras or the ability to play music and games. [94] Apple's approach was to deliberately simplify its product line by offering just one model a year for all customers, while making it an expensive, high-end product.
Apple's marketing, developing from the success of iPod campaigns, allowed the phone to become a mass-market product with many buyers on launch day. Some market research has found that, unusually for a technology product, iPhone users are disproportionately female. [95] Ars Technica noted in 2012 that Apple had avoided 'patronizing' marketing to female customers, a practice used (often to sell low-quality, high-priced products) by many of its competitors. [96]
When then-CEO of Research in Motion Mike Lazaridis pried open an iPhone, his impression was of a Mac stuffed into a cellphone, as it used much more memory and processing power than the smartphones on the market at the time. [90] [91] With its capacitive touchscreen and consumer-friendly design, the iPhone fundamentally changed the mobile industry, with Steve Jobs proclaiming in 2007, that the phone was not just a communication tool but a way of life. [97]
The dominant mobile operating systems at the time such as Symbian , BlackBerry OS , and Windows Mobile were not designed to handle additional tasks beyond communication and basic functions. These operating systems never focused on applications and developers, and due to infighting among manufacturers as well as the complexity of developing on their low-memory hardware, they never developed a thriving ecosystem like Apple's App Store or Android 's Google Play . [97] [98] IPhone OS (renamed iOS in 2010) was designed as a robust OS with capabilities such as multitasking and graphics in order to meet future consumer demands. [94] Many services were provided by mobile carriers, who often extensively customized devices. Meanwhile, Apple's decision to base its OS on OS X had the unexpected benefit of allowing OS X developers to rapidly expand into iOS development. [99] Rival manufacturers have been forced to spend more on software and development costs to catch up to the iPhone. The iPhone's success has led to a decline in sales of high-end fashion phones and business-oriented smartphones such as Vertu and BlackBerry and respectively, as well as Nokia. [97] [100] Nokia realised the limitations of its operating system Symbian and attempted to develop a more advanced system, Maemo, without success. It ultimately agreed to a technology-sharing deal and then a takeover from Microsoft. [101]

Production
Up to the iPhone 4 , all iPhone models, as well as other iOS devices were manufactured exclusively by Foxconn , based in Taiwan . In 2011, after Tim Cook became CEO of the company, Apple changed its outsourcing strategy, for the first time increasing its supply partners. The iPhone 4s in 2012 was the first model which was manufactured simultaneously by two stand-alone companies: Foxconn as well as Pegatron , also based in Taiwan. Although Foxconn is still responsible for the larger share of production, Pegatron's orders have been slowly increased, with the company being tasked with producing a part of the iPhone 5C line in 2013, and 30% of the iPhone 6 devices in 2014. The 6 Plus model is being produced solely by Foxconn. [102]

Hardware

Screen and input
The touchscreen on the first five generations is a 9 cm (3.5 in) liquid crystal display with scratch-resistant glass, while the one on the iPhone 5 is 4 inches. [8] The capacitive touchscreen is designed for a bare finger, or multiple fingers for multi-touch sensing. The screens on the first three generations have a resolution of 320×480 ( HVGA ) at 163 ppi ; those on the iPhone 4 and iPhone 4S have a resolution of 640×960 at 326 ppi, and the iPhone 5 , 640×1136 at 326 ppi. All iPhones were and still are equipped with LCDs. The initial models were using twisted-nematic (TN) LCDs . Starting with iPhone 4, the technology was changed to in-plane switching (IPS) LCDs . The iPhone 5 model's screen results in an aspect ratio of approximately 16:9.
The touch and gesture features of the iPhone are based on technology originally developed by FingerWorks . [103] Most gloves and styli prevent the necessary electrical conductivity; [104] [105] [106] [107] although capacitive styli can be used with iPhone's finger-touch screen. The iPhone 3GS and later also feature a fingerprint -resistant oleophobic coating. [108]
The iPhone has a minimal hardware user interface, featuring five buttons . The only physical menu button is situated directly below the display, and is called the "Home button" because it closes the active app and navigates to the home screen of the interface. The home button is denoted not by a house, as on many other similar devices, but a rounded square , reminiscent of the shape of icons on the home screen. However, the Home button on iPhones with Apple's fingerprint recognition feature Touch ID (which use the Home button as the fingerprint sensor) have no symbol.
A multi-function sleep/wake button is located on the top of the device. It serves as the unit's power button, and also controls phone calls . When a call is received, pressing the sleep/wake button once silences the ringtone, and when pressed twice transfers the call to voicemail. Situated on the left spine are the volume adjustment controls. The iPhone 4 has two separate circular buttons to increase and decrease the volume; all earlier models house two switches under a single plastic panel, known as a rocker switch, which could reasonably be counted as either one or two buttons.
Directly above the volume controls is a ring/silent switch that when engaged mutes telephone ringing, alert sounds from new & sent emails, text messages, and other push notifications, camera shutter sounds, Voice Memo sound effects, phone lock/unlock sounds, keyboard clicks, and spoken auto-corrections. This switch does not mute alarm sounds from the Clock application, and in some countries or regions it will not mute the camera shutter or Voice Memo sound effects. [109] All buttons except Home were made of plastic on the original first generation iPhone and metal on all later models. The touchscreen furnishes the remainder of the user interface .
A software update in January 2008 [110] allowed the first-generation iPhone to use cell tower and Wi-Fi network locations trilateration , [111] despite lacking GPS hardware. Since the iPhone 3G generation, the iPhone employs A-GPS operated by the United States. Since the iPhone 4S generation the device also supports the GLONASS global positioning system, which is operated by Russia.
The iPhone 6S and 6S Plus, introduced in 2015, feature "force-touch" displays which allows the screen to recognize how hard it is being pressed. An example of how this technology will be used is lightly pressing the screen to preview a photograph and pressing down to take it.

Sensors
Latest iPhone devices feature eight sensors, which are used to adjust the screen based on operating conditions, enable motion-controlled games, and location-based services .

Proximity sensor
A proximity sensor deactivates the display and touchscreen when the device is brought near the face during a call. This is done to save battery power and to prevent inadvertent inputs from the user's face and ears.

Ambient light sensor
An ambient light sensor adjusts the display brightness which saves battery power and prevents the screen from being too bright or too dark.

Accelerometer
A 3-axis accelerometer senses the orientation of the phone and changes the screen accordingly, allowing the user to easily switch between portrait and landscape mode. [112] Photo browsing, web browsing, and music playing support both upright and left or right widescreen orientations. [113] Unlike the iPad , the iPhone does not rotate the screen when turned upside-down, with the Home button above the screen, unless the running program has been specifically designed to do so. The 3.0 update added landscape support for still other applications, such as email, and introduced shaking the unit as a form of input. [114] [115] The accelerometer can also be used to control third-party apps , notably games. It is also used for fitness tracking purposes, primarily as a pedometer .

Magnetometer
A magnetometer is built-in since the iPhone 3GS generation, which is used to measure the strength and/or direction of the magnetic field in the vicinity of the device. Sometimes certain devices or radio signals can interfere with the magnetometer requiring users to either move away from the interference or re-calibrate by moving the device in a figure 8 motion. Since the iPhone 3GS, the iPhone also features a Compass app which was unique at time of release, showing a compass that points in the direction of the magnetic field.

Gyroscopic sensor
Beginning with the iPhone 4 generation, Apple's smartphones also include a gyroscopic sensor , enhancing its perception of how it is moved.

Radio
The iPhone contains a chip capable of receiving radio signals , [116] however Apple has the FM radio feature switched off, claiming a lack of demand due to the popularity of music-streaming apps, [117] however it has been noted that cellphone manufacturers make money from selling data. [118] A campaign called "Free Radio On My Phone" was started to encourage cellphone manufacturers such as Apple to enable the radio on the phones they manufacture, reasons cited were that radio drains less power and is useful in an emergency such as the 2016 Fort McMurray Wildfire . [119]

Fingerprint sensor
Most iPhone models starting from iPhone 5S (excluding the iPhone 5C ) all feature Apple's fingerprint recognition sensor known as Touch ID , and is located on the Home button of the iPhone.

Barometer
Included on the iPhone 6 and later (excluding the iPhone SE ), a barometer used to determine air pressure, and elevation from the device. [120]

Audio and output
On the bottom of the iPhone, there is a speaker to the left of the dock connector and a microphone to the right. There is an additional loudspeaker above the screen that serves as an earpiece during phone calls. The iPhone 4 includes an additional microphone at the top of the unit for noise cancellation , and switches the placement of the microphone and speaker on the base on the unit—the speaker is on the right. [121] Volume controls are located on the left side of all iPhone models and as a slider in the iPod application.
The 3.5mm TRRS connector for the headphones is located on the top left corner of the device for the first five generations (original through 4S), after which time it was moved to the bottom left corner. [122] The headphone socket on the 1st-generation iPhone is recessed into the casing, making it incompatible with most headsets without the use of an adapter. [123] Subsequent generations eliminated the problem by using a flush-mounted headphone socket. Cars equipped with an auxiliary jack allow handsfree use of the iPhone while driving as a substitute for Bluetooth . The iPhone 7 and 7 Plus have no 3.5mm headphone jack, [124] and instead headsets must connect to the iPhone by Bluetooth , use Apple's Lightning port (which has replaced the 3.5mm headphone jack), or (for traditional headsets) use the Lightning to 3.5mm headphone jack adapter, which is included with all iPhone and 7 Plus units, and plugs into the Lightning port.
Apple's own headset has a multipurpose button near the microphone that can play or pause music, skip tracks, and answer or end phone calls without touching the iPhone. Some third-party headsets designed for the iPhone also include the microphone and control button. [125] The current headsets also provide volume controls, which are only compatible with more recent models. [126] A fourth ring in the audio jack carries this extra information.
The built-in Bluetooth 2.x+EDR supports wireless earpieces and headphones, which requires the HSP profile . Stereo audio was added in the 3.0 update for hardware that supports A2DP . [114] [115] While non-sanctioned third-party solutions exist, the iPhone does not officially support the OBEX file transfer protocol . [127] The lack of these profiles prevents iPhone users from exchanging multimedia files, such as pictures, music and videos, with other Bluetooth-enabled cell phones.
Composite [128] or component [129] video at up to 576i and stereo audio can be output from the dock connector using an adapter sold by Apple. IPhone 4 also supports 1024×768 VGA output [130] without audio, and HDMI output, [131] with stereo audio, via dock adapters. The iPhone did not support voice recording until the 3.0 software update. [114] [115]

Battery
The iPhone features an internal rechargeable lithium-ion battery . Like an iPod, but unlike most other mobile phones at the time of its launch, the battery is not user-replaceable. [123] [132] The iPhone can be charged when connected to a computer for syncing across the included USB to dock connector cable, similar to charging an iPod . Alternatively, a USB to AC adapter (or "wall charger," also included) can be connected to the cable to charge directly from an AC outlet .
Apple runs tests on preproduction units to determine battery life. Apple's website says that the battery life "is designed to retain up to 80% of its original capacity after 400 full charge and discharge cycles", [133] which is comparable to iPod batteries.
The battery life of early models of the iPhone has been criticized by several technology journalists as insufficient and less than Apple's claims. [134] [135] [136] [137] This is also reflected by a J. D. Power and Associates customer satisfaction survey, which gave the "battery aspects" of the iPhone 3G its lowest rating of 2 out of 5 stars. [138] [139]
If the battery malfunctions or dies prematurely, the phone can be returned to Apple and replaced for free while still under warranty . [140] The warranty lasts one year from purchase and can be extended to two years with AppleCare . The battery replacement service and its pricing was not made known to buyers until the day the product was launched; [141] [142] it is similar to how Apple (and third parties) replace batteries for iPods. The Foundation for Taxpayer and Consumer Rights , a consumer advocate group, has sent a complaint to Apple and AT&T over the fee that consumers have to pay to have the battery replaced. [141]
Since July 2007, third-party battery replacement kits have been available [143] at a much lower price than Apple's own battery replacement program. These kits often include a small screwdriver and an instruction leaflet, but as with many newer iPod models the battery in the first generation iPhone has been soldered in. Therefore, a soldering iron is required to install the new battery. The iPhone 3G uses a different battery fitted with a connector that is easier to replace. [144]
A patent filed by the corporation, published in late July 2013, revealed the development of a new iPhone battery system that uses location data in combination with data on the user's habits to moderate the handsets power settings accordingly. Apple is working towards a power management system that will provide features such as the ability to estimate the length of time a user will be away from a power source to modify energy usage and a detection function that adjusts the charging rate to best suit the type of power source that is being used. [145]

Camera
The 1st-generation iPhone and iPhone 3G have a fixed-focus 2.0- megapixel camera on the back for digital photos. It has no optical zoom, flash or autofocus , and does not natively support video recording. (iPhone (original) & 3G can record video via a third-party app available on the App Store, and jailbreaking also allows users to do so.) IPhone OS 2.0 introduced geotagging for photos.
The iPhone 3GS has a 3.2-megapixel camera with autofocus, auto white balance, and auto macro (up to 10 cm). Manufactured by OmniVision , the camera can also capture 640×480 ( VGA resolution) video at 30 frames per second, [146] although unlike higher-end CCD -based video cameras, it exhibits the rolling shutter effect. [147] The video can be cropped on the iPhone and directly uploaded to YouTube or other services.
The iPhone 4 introduced a 5.0- megapixel camera (2592×1936 pixels) that can record video at 720p resolution, considered high-definition . It also has a backside-illuminated sensor that can capture pictures in low light and an LED flash that can stay lit while recording video. [148] It is the first iPhone that can natively do high dynamic range photography . [149] The iPhone 4 also has a second camera on the front that can take VGA photos and record SD video. Saved recordings may be synced to the host computer, attached to email, or (where supported) sent by MMS .
The iPhone 4S' camera can shoot 8-MP stills and 1080p video, can be accessed directly from the lock screen, and can be triggered using the volume-up button as a shutter trigger. The built-in gyroscope can stabilize the image while recording video.
The iPhone 5 and iPhone 4S , running iOS 6 or later, can take panoramas using the built-in camera app, [150] and the iPhone 5 can also take still photos while recording video. [151]
The camera on the iPhone 5 reportedly shows purple haze when the light source is just out of frame, [152] although Consumer Reports said it "is no more prone to purple hazing on photos shot into a bright light source than its predecessor or than several Android phones with fine cameras..." [153]
On all five model generations, the phone can be configured to bring up the camera app by quickly pressing the home key twice. [154] On all iPhones running iOS 5 , it can also be accessed from the lock screen directly.
The iPhone 6S and 6S Plus are outfitted with 12 megapixel camera, with 4K HD video capability. Just as well, the user may change the resolution between 4K and 1080p from settings.
The iPhone 7 features Optical Image Stabilization on its rear camera, and the 7 Plus is the first iPhone to feature dual-lens camera (both 12 MP), and they both have a 7 MP front-facing camera. The second camera on the iPhone 7 Plus is a telephoto lens , which enables 2x optical zoom and up to 10x digital zoom. The rear cameras on the 7 and 7 Plus are both f/1.8 aperture . [124]

Storage
The iPhone was initially released with two options for internal storage size: 4 GB or 8 GB. On September 5, 2007, Apple discontinued the 4 GB models. [155] On February 5, 2008, Apple added a 16 GB model. [156] The iPhone 3G was available in 16 GB and 8 GB. The iPhone 3GS came in 16 GB and 32 GB variants and remained available in 8 GB until September 2012, more than three years after its launch. The iPhone 4 was available in 16 GB and 32 GB variants, as well as an 8 GB variant to be sold alongside the iPhone 4S at a reduced price point. The iPhone 4S was available in three sizes: 16 GB, 32 GB and 64 GB. The iPhone 5 and 5s were available in the same three sizes previously available to the iPhone 4S: 16 GB, 32 GB, and 64 GB. The lower-cost iPhone 5C model was initially available in 16 GB and 32 GB models; an 8 GB model was added later. The iPhone 6 and 6S are available in three sizes: 16 GB, 64 GB, and 128 GB. The iPhone SE is available in 16 GB and 64 GB variants. By the time the iPhone 7 and 7 Plus was released, Apple ditched the based model from 16 GB to 32 GB as the base storage. Both 7 & 7 Plus has a configuration of 32, 128, & 256 GB storage option. And Apple also doubled the storage on the iPhone 6s & 6s Plus in 2 configurations (32 GB & 128 GB).

SIM card
GSM models of the iPhone use a SIM card to identify themselves to the GSM network. The SIM sits in a tray, which is inserted into a slot at the top of the device. The SIM tray can be ejected with a paper clip or the "SIM ejector tool" (a simple piece of die-cut sheet metal) included with the iPhone 3G and 3GS in the United States and with all models elsewhere in the world. [157] [158] Some iPhone models shipped with a SIM ejector tool which was fabricated from an alloy dubbed " Liquidmetal ". [159] In most countries, the iPhone is usually sold with a SIM lock , which prevents the iPhone from being used on a different mobile network. [160]
The GSM iPhone 4 features a MicroSIM card that is located in a slot on the right side of the device. [161]
The CDMA model of the iPhone 4, just the same as any other CDMA-only cell phone, does not use a SIM card or have a SIM card slot.
An iPhone 4S activated on a CDMA carrier, however, does have a SIM card slot but does not rely on a SIM card for activation on that CDMA network. A CDMA-activated iPhone 4S usually has a carrier-approved roaming SIM preloaded in its SIM slot at the time of purchase that is used for roaming on certain carrier-approved international GSM networks only. The SIM slot is locked to only use the roaming SIM card provided by the CDMA carrier. [162] In the case of Verizon, for example, one can request that the SIM slot be unlocked for international use by calling their support number and requesting an international unlock if their account has been in good standing for the past 60 days. [163] This method only unlocks the iPhone 4S for use on international carriers. An iPhone 4S that has been unlocked in this way will reject any non international SIM cards (AT&T Mobility or T-Mobile USA, for example).
The iPhone 5 and later iPhones use the nano-SIM , in order to save more space for internal components.

Liquid contact indicators
All iPhones (as well as many other devices by Apple) have a small disc at the bottom of the headphone jack that changes from white to red on contact with water; the iPhone 3G and later models also have a similar indicator at the bottom of the dock connector . [164] Because Apple warranties do not cover water damage, employees examine the indicators before approving warranty repair or replacement .
The iPhone's indicators are more exposed than those in some mobile phones from other manufacturers, which carry them in a more protected location, such as beneath the battery behind a battery cover. The iPhone's can be triggered during routine use, by an owner's sweat, [165] steam in a bathroom, and other light environmental moisture. [166] Criticism led Apple to change its water damage policy for iPhones and similar products, allowing customers to request further internal inspection of the phone to verify if internal liquid damage sensors were triggered. [167]

Included items
All iPhone models include written documentation, and a dock connector to USB cable. The first generation and 3G iPhones also came with a cleaning cloth. The first generation iPhone included a stereo headset ( earbuds and a microphone) and a plastic dock to hold the unit upright while charging and syncing. The iPhone 3G includes a similar headset plus a SIM eject tool (the first generation model requires a paperclip ). The iPhone 3GS includes the SIM eject tool and a revised headset, which adds volume buttons (not functional with previous iPhone versions). [126] [168]
The iPhone 3G and 3GS are compatible with the same dock, sold separately, but not the first generation model's dock. [169] All versions include a USB power adapter, or "wall charger," which allows the iPhone to charge from an AC outlet . The iPhone 3G and iPhone 3GS sold in North America, Japan, Colombia, Ecuador, and Peru [170] [171] include an ultracompact USB power adapter.

Software
The iPhone, iPod Touch and iPad run an operating system known as iOS (formerly iPhone OS). It is a variant of the same Darwin operating system core that is found in Mac OS X . Also included is the " Core Animation " software component from Mac OS X v10.5 Leopard. Together with the PowerVR hardware (and on the iPhone 3GS, OpenGL ES 2.0), it is responsible for the interface's motion graphics . The operating system takes up less than half a gigabyte . [172]
It is capable of supporting bundled and future applications from Apple, as well as from third-party developers. Software applications cannot be copied directly from Mac OS X but must be written and compiled specifically for iOS.
Like the iPod, the iPhone is managed from a computer using iTunes . The earliest versions of the OS required version 7.3 or later , which is compatible with Mac OS X version 10.3.9 Panther or later, and 32-bit Windows XP or Vista . [173] The release of iTunes 7.6 expanded this support to include 64-bit versions of XP and Vista, [174] and a workaround has been discovered for previous 64-bit Windows operating systems. [175]
Apple provides free updates to the OS for the iPhone through iTunes, [172] and major updates have historically accompanied new models. [176] Such updates often require a newer version of iTunes—for example, the 3.0 update requires iTunes 8.2—but the iTunes system requirements have stayed the same. Updates include bug fixes, security patches and new features. [177] For example, iPhone 3G users initially experienced dropped calls until an update was issued. [178] [179]
Version 3.1 required iTunes 9.0, and iOS 4 required iTunes 9.2. iTunes 10.5, which is required to sync and activate iOS 5 , requires Mac OS X 10.5.8 or Leopard on G4 or G5 computers on 800 MHz or higher; versions 10.3 and 10.4 and 10.5–10.5.7 are no longer supported.
From September 9, 2014, all new iPhone models released were expected to include a new mobile wallet feature developed in conjunction with major credit card issuers American Express , MasterCard , and Visa . [180] Support was later added for Discover [181] and UnionPay [182] cards.

Interface
The interface is based around the home screen, a graphical list of available applications. iPhone applications normally run one at a time. Starting with the iPhone 4, a primitive version of multitasking came into play. Users could double click the home button to select recently opened applications. [183] However, the apps never ran in the background. Starting with iOS 7, though, apps can truly multitask, and each open application runs in the background when not in use, although most functionality is still available when making a call or listening to music. The home screen can be accessed at any time by a hardware button below the screen, closing the open application in the process. [184]
By default, the Home screen contains the following icons: Messages ( SMS and MMS messaging), Calendar, Photos, Camera, YouTube, Stocks, Maps ( Google Maps ), Weather, Voice Memos, Notes, Clock, Calculator, Settings, iTunes (store) , App Store , (on the iPhone 3GS and iPhone 4) Compass , FaceTime and GameCenter were added in iOS 4.0 and 4.1 respectively. In iOS 5, Reminders and Newsstand were added, as well as the iPod application split into separate Music and Videos applications. iOS 6 added Passbook as well as an updated version of Maps that relies on data provided by TomTom as well as other sources. iOS 6 also added a Clock application onto the iPad's homescreen. YouTube no longer came as a pre-installed application. Docked at the base of the screen, four icons for Phone , Mail , Safari (Internet), and Music delineate the iPhone's main purposes. [185] On January 15, 2008, Apple released software update 1.1.3, allowing users to create "Web Clips", home screen icons that resemble apps that open a user-defined page in Safari. After the update, iPhone users can rearrange and place icons (by holding down on any icon and moving it to the desired location once they start shaking) on up to nine other adjacent home screens, accessed by a horizontal swipe. [110]
Users can also add and delete icons from the dock, which is the same on every home screen. Each home screen holds up to twenty icons for iPhone 2G , 3G , 4 and 4S , while each home screen for iPhone 5 holds up to twenty-four icons due to a larger screen display, and the dock holds up to four icons. Users can delete Web Clips and third-party applications at any time, and may select only certain applications for transfer from iTunes. Apple's default programs, could only be removed since the iOS 10 update. The 3.0 update adds a system-wide search, known as Spotlight , to the left of the first home screen. [114] [115]
Almost all input is given through the touch screen, which understands complex gestures using multi-touch . The iPhone's interaction techniques enable the user to move the content up or down by a touch-drag motion of the finger. For example, zooming in and out of web pages and photos is done by placing two fingers on the screen and spreading them farther apart or bringing them closer together, a gesture known as " pinching ".
Scrolling through a long list or menu is achieved by sliding a finger over the display from bottom to top, or vice versa to go back. In either case, the list moves as if it is pasted on the outer surface of a wheel, slowly decelerating as if affected by friction. In this way, the interface simulates the physics of a real object.
Other user-centered interactive effects include horizontally sliding sub-selection, the vertically sliding keyboard and bookmarks menu, and widgets that turn around to allow settings to be configured on the other side. Menu bars are found at the top and bottom of the screen when necessary. Their options vary by program, but always follow a consistent style motif. In menu hierarchies, a "back" button in the top-left corner of the screen displays the name of the parent folder.

Phone
The iPhone allows audio conferencing , call holding, call merging, caller ID , and integration with other cellular network features and iPhone functions. For example, if music is playing when a call is received, the music fades out, and fades back in when the call has ended.
The proximity sensor shuts off the screen and touch-sensitive circuitry when the iPhone is brought close to the face, both to save battery and prevent unintentional touches. The iPhone does not support video calling or videoconferencing on versions prior to the fourth generation, as there is only one camera on the opposite side of the screen. [186]
The iPhone 4 supports video calling using either the front or back camera over Wi-Fi, a feature Apple calls FaceTime . [187] Voice control, introduced in the iPhone 3GS, allows users to say a contact's name or number and the iPhone will dial it. [188] The first two models only support voice dialing through third-party applications. [189]
The iPhone includes a visual voicemail (in some countries) [190] feature allowing users to view a list of current voicemail messages on-screen without having to call into their voicemail. Unlike most other systems, messages can be listened to and deleted in a non-chronological order by choosing any message from an on-screen list.
A music ringtone feature was introduced in the United States on September 5, 2007. Users can create custom ringtones from songs purchased from the iTunes Store for a small additional fee. The ringtones can be 3 to 30 seconds long from any part of a song, can fade in and out, pause from half a second to five seconds when looped, or loop continuously . All customizing can be done in iTunes, [191] or with Apple's GarageBand software 4.1.1 or later (available only on Mac OS X ) [192] or third-party tools. [193]
With the release of iOS 6 , which was released on September 19, 2012, Apple added features that enable the user to have options to decline a phone call when a person is calling them. The user can reply with a message, or to set a reminder to call them back at a later time. [194]

Multimedia
The layout of the music library is similar to that of an iPod or current Symbian S60 phones. The iPhone can sort its media library by songs, artists, albums, videos, playlists , genres , composers, podcasts , audiobooks , and compilations . Options are always presented alphabetically, except in playlists, which retain their order from iTunes . The iPhone uses a large font that allows users plenty of room to touch their selection.
Users can rotate their device horizontally to landscape mode to access Cover Flow . Like on iTunes, this feature shows the different album covers in a scroll-through photo library. Scrolling is achieved by swiping a finger across the screen. Alternatively, headset controls can be used to pause, play, skip, and repeat tracks. On the iPhone 3GS, the volume can be changed with the included Apple Earphones, and the Voice Control feature can be used to identify a track, play songs in a playlist or by a specific artist, or create a Genius playlist . [188]
The iPhone supports gapless playback . [195] Like the fifth-generation iPods introduced in 2005, the iPhone can play digital video , allowing users to watch TV shows and movies in widescreen . Double-tapping switches between widescreen and fullscreen video playback.
The iPhone allows users to purchase and download songs from the iTunes Store directly to their iPhone. The feature originally required a Wi-Fi network, but now since 2012, can use the cellular data network if one is not available. [196]
The iPhone includes software that allows the user to upload, view, and email photos taken with the camera . The user zooms in and out of photos by sliding two fingers further apart or closer together, much like Safari. The camera application also lets users view the camera roll, the pictures that have been taken with the iPhone's camera. Those pictures are also available in the Photos application, along with any transferred from iPhoto or Aperture on a Mac, or Photoshop on a Windows PC.

Internet connectivity
Internet access is available when the iPhone is connected to a local area Wi-Fi or a wide area GSM or EDGE network, both second-generation ( 2G ) wireless data standards. The iPhone 3G introduced support for third-generation UMTS and HSDPA 3.6, [197] the iPhone 4S introduced support for HSUPA networks (14.4 Mbit/s), and support for HSDPA 7.2 was introduced in the iPhone 3GS . [198] Networks accessible from iPhone models include 1xRTT (represented by a 1x on the status bar) and GPRS (shown as GPRS on the status bar), EDGE (shown as a capital E on the status bar), UMTS and EV-DO (shown as 3G), a faster version of UMTS and 4G (shown as a 4G symbol on the status bar), and LTE (shown as LTE on the status bar). [199]
AT&T introduced 3G in July 2004, [200] but as late as 2007, Steve Jobs stated that it was still not widespread enough in the US, and the chipsets not energy efficient enough, to be included in the iPhone. [105] [201] Support for 802.1X , an authentication system commonly used by university and corporate Wi-Fi networks, was added in the 2.0 version update. [202]
By default, the iPhone will ask to join newly discovered Wi-Fi networks and prompt for the password when required. Alternatively, it can join closed Wi-Fi networks manually. [203] The iPhone will automatically choose the strongest network, connecting to Wi-Fi instead of EDGE when it is available. [204] Similarly, the iPhone 3G and onwards prefer 3G to 2G , and Wi-Fi to either. [205]
Wi-Fi, Bluetooth , and 3G (on the iPhone 3G onwards) can all be deactivated individually. Airplane mode disables all wireless connections at once, overriding other preferences. However, once in Airplane mode, one can explicitly enable Wi-Fi and/or Bluetooth modes to join and continue to operate over one or both of those networks while the cellular network transceivers remain off.
The iPhone 3GS has a maximum download rate of 7.2 Mbit/s . [206] Furthermore, email attachments as well as apps and media from Apple's various stores must be smaller than 20 MB to be downloaded over a cellular network. [207] Larger files, often email attachments or podcasts, must be downloaded over Wi-Fi (which has no file size limits). If Wi-Fi is unavailable, one workaround is to open the files directly in Safari . [208]
Safari is the iPhone's native web browser , and it displays pages similar to its Mac and Windows counterparts. Web pages may be viewed in portrait or landscape mode and the device supports automatic zooming by pinching together or spreading apart fingertips on the screen, or by double-tapping text or images. [209] [210] Safari does not allow file downloads except for predefined extensions. The iPhone does not support Flash . [211]
Consequently, the UK's Advertising Standards Authority adjudicated that an advertisement claiming the iPhone could access "all parts of the internet" should be withdrawn in its current form, on grounds of false advertising . In a rare public letter in April 2010, Apple CEO Steve Jobs outlined the reasoning behind the absence of Flash on the iPhone (and iPad ). [212] The iPhone supports SVG , CSS , HTML Canvas , and Bonjour . [213]
Google Chrome was introduced to the iOS on June 26, 2012. In a review by Chitika on July 18, 2012, they announced that the Google Chrome web browser has 1.5% of the iOS web browser market since its release. [214]
The Maps application can access Google Maps in map, satellite , or hybrid form. It can also generate directions between two locations, while providing optional real-time traffic information. During the iPhone's announcement, Jobs demonstrated this feature by searching for nearby Starbucks locations and then placing a prank call to one with a single tap. [215] [216] Support for walking directions, public transit, and street view was added in the version 2.2 software update, but no voice-guided navigation. [217]
The iPhone 3GS and iPhone 4 can orient the map with its digital compass. [218] Apple also developed a separate application to view YouTube videos on the iPhone, which streams videos after encoding them using the H.264 codec. Simple weather and stock quotes applications also tap into the Internet.
IPhone users can and do access the Internet frequently, and in a variety of places. According to Google , in 2008, the iPhone generated 50 times more search requests than any other mobile handset. [219] According to Deutsche Telekom CEO René Obermann, "The average Internet usage for an iPhone customer is more than 100 megabytes . This is 30 times the use for our average contract-based consumer customers." [220] Nielsen found that 98% of iPhone users use data services, and 88% use the internet. [40] In China, the iPhone 3G and iPhone 3GS were built and distributed without Wi-Fi. [221]
With the introduction of the Verizon iPhone in January 2011, the issue of using internet while on the phone was brought to the public's attention. Under the two US carriers, internet and phone could be used simultaneously on AT&T networks, whereas Verizon networks only support the use of each separately. [222]
However, in 2014, Verizon announced that the iPhone 6 and 6 Plus would allow simultaneous voice and data over its LTE Network. [ citation needed ] T-Mobile and Sprint have enabled calls over Wi-Fi, with Verizon and AT&T soon doing the same. [223]

Text input
For text input, the iPhone implements a virtual keyboard on the touchscreen. It has automatic spell checking and correction, predictive word capabilities, and a dynamic dictionary that learns new words. The keyboard can predict what word the user is typing and complete it, and correct for the accidental pressing of keys near the presumed desired key. [224]
The keys are somewhat larger and spaced farther apart when in landscape mode , which is supported by only a limited number of applications. Touching a section of text for a brief time brings up a magnifying glass , allowing users to place the cursor in the middle of existing text. The virtual keyboard can accommodate 21 languages, including character recognition for Chinese. [225]
Alternate characters with accents (for example, letters from the alphabets of other languages) and emoji can be typed from the keyboard by pressing the letter for 2 seconds and selecting the alternate character from the popup. [226] The 3.0 update brought support for cut, copy, or pasting text, as well as landscape keyboards in more applications. [114] [115] On iPhone 4S and above, Siri allows dictation.

Email and text messages
The iPhone also features an email program that supports HTML email , which enables the user to embed photos in an email message. PDF , Word , Excel , and PowerPoint attachments to mail messages can be viewed on the phone. [227] Yahoo! offers a free push-email service for the iPhone. IMAP (although not Push-IMAP ) and POP3 mail standards are also supported, including Microsoft Exchange [228] and Kerio Connect . [229]
In the first versions of the iPhone firmware, this was accomplished by opening up IMAP on the Exchange server. Apple has also licensed Microsoft ActiveSync and supports the platform (including push email) with the release of iPhone 2.0 firmware. [230] [231] The iPhone will sync email account settings over from Apple's own Mail application, Microsoft Outlook , and Microsoft Entourage , or it can be manually configured on the device itself. The email program can access almost any IMAP or POP3 account. [232]
Text messages are presented chronologically in a mailbox format similar to Mail, which places all text from recipients together with replies. Text messages are displayed in speech bubbles (similar to iChat ) under each recipient's name. The iPhone has built-in support for email message forwarding, drafts, and direct internal camera-to-email picture sending. Support for multi-recipient SMS was added in the 1.1.3 software update. [233] Support for MMS was added in the 3.0 update, but not for the original first generation iPhone [114] [115] and not in the US until September 25, 2009. [234] [235]

Third-party applications
At WWDC 2007 on June 11, 2007, Apple announced that the iPhone would support third-party web applications using Ajax that share the look and feel of the iPhone interface. [236] On October 17, 2007, Steve Jobs, in an open letter posted to Apple's "Hot News" weblog , announced that a software development kit (SDK) would be made available to third-party developers in February 2008. The iPhone SDK was officially announced and released on March 6, 2008, at the Apple Town Hall facility. [237]
It is a free download, with an Apple registration, that allows developers to develop native applications for the iPhone and iPod Touch, then test them in an "iPhone simulator". However, loading an application onto a real device is only possible after paying an Apple Developer Connection membership fee. Developers are free to set any price for their applications to be distributed through the App Store , of which they will receive a 70% share. [238]
Developers can also opt to release the application for free and will not pay any costs to release or distribute the application beyond the membership fee. The App Store was launched with the release of iOS 2.0, on July 11, 2008. [231] The update was free for iPhone users; owners of older iPod Touches were required to pay US$10 for it. [239]
Once a developer has submitted an application to the App Store, Apple holds firm control over its distribution. Apple can halt the distribution of applications it deems inappropriate, for example, I Am Rich , a US$1000 program that simply demonstrated the wealth of its user. [240] Apple has been criticized for banning third-party applications that enable a functionality that Apple does not want the iPhone to have: In 2008, Apple rejected Podcaster , which allowed iPhone users to download podcasts directly to the iPhone claiming it duplicated the functionality of iTunes. [241] Apple has since released a software update that grants this capability. [217]
NetShare, another rejected app, would have enabled users to tether their iPhone to a laptop or desktop, using its cellular network to load data for the computer. [242] Many carriers of the iPhone later globally allowed tethering before Apple officially supported it with the upgrade to the iOS 3.0, with AT&T Mobility being a relative latecomer in the United States. [243] In most cases, the carrier charges extra for tethering an iPhone.
Before the SDK was released, third parties were permitted to design "Web Apps" that would run through Safari. [244] Unsigned native applications are also available for "jailbroken" phones. [245] The ability to install native applications onto the iPhone outside of the App Store is not supported by Apple, the stated reason being that such native applications could be broken by any software update, but Apple has stated it will not design software updates specifically to break native applications other than those that perform SIM unlocking. [246]
As of October 2013 [update] , Apple has passed 60 billion app downloads. [247] As of September 2016 [update] , there have been over 140 billion app downloads from the App Store. [248]

Reception
The iPhone has received positive reviews. In its launch year, it was called "revolutionary", [249] while newer iterations have received praise, such as being called "the best phone". [250]
The iPhone attracts users of all ages, [40] and besides consumer use, the iPhone has also been adopted for business purposes . [251]

Accessibility features
Starting with the iPhone 4S, Apple added an accessibility feature to optimize the function of the iPhone with hearing aids . [252] Apple released a program of Made for iPhone Hearing Aids. [253] These hearing aids deliver a power-efficient, high-quality digital audio experience and allow the user to manage the hearing aid right from your iPhone. Made for iPhone hearing aids also feature Live Listen . With Live Listen the iPhone acts as a remote microphone that sends sound to a Made for iPhone hearing aid. Live Listen can help the user hear a conversation in a noisy room or hear someone speaking across the room. [254]
The Braille Displays for iOS program was announced by Apple coinciding with the release of the iPhone 3GS, iPad and iPod Touch (3rd Generation) . This program added support for more than 50 Bluetooth wireless braille displays that work with iOS out of the box. The user only needs to pair the keyboard to the device to start using it to navigate the iOS device with VoiceOver without any additional software. iOS supports braille tables for more than 25 languages. [255]
IPhone lets the user know when an alert is sent to the it, in a variety of notice methods. It delivers both visual and vibrating alerts for incoming phone and FaceTime calls, new text messages, new and sent mail, and calendar events. You can set an LED light flash for incoming calls and alerts. Or have incoming calls display a photo of the caller. Users can choose from different vibration patterns or even create their own. [256]
The iPhone can enlarge text to make it more accessible for vision-impaired users, [257] and can accommodate hearing-impaired users with closed captioning and external TTY devices. [258] The iPhone 3GS also features white on black mode, VoiceOver (a screen reader ), and zooming for impaired vision, and mono audio for limited hearing in one ear. [259] Apple regularly publishes Voluntary Product Accessibility Templates which explicitly state compliance with the US regulation " Section 508 ". [260]
With the release of the newer iOS 9 for all iPhones, users now have the ability to choose between 2 different screen view options. The user can choose to have a standard view or zoomed view. When the iPhone is placed in a standard view setting, the icons are normal size and the text remains the same. With a zoomed view option, the icons on the screen and the text become slightly larger. This enables the user to have a more customized appearance and it can potentially help some users read the screen easier.
AssistiveTouch helps to adapt the Multi-Touch screen of an iOS device to your unique physical needs. This can be of great assistance to those who have difficulty with some gestures, like pinch, one can make them accessible with just a tap of a finger. The user can create their own gestures and customize the layout of the AssistiveTouch menu. If the user has trouble pressing the Home button, it can be set so that it can be activated with an onscreen tap. Gestures like rotate and shake are available even when if the iOS device is mounted on a wheelchair . [256]
Guided Access helps people with autism or other attention and sensory challenges stay focused on the task (or app) at hand. With Guided Access, a parent, teacher, or therapist can limit an iOS device to stay on one app by disabling the Home button, and limit the amount of time spent in an app. The user can even restrict access to the keyboard or touch input on certain areas of the screen. So wandering taps and gestures won’t distract from learning. [256]

Models
As of September 2016 [update] , 15 different iPhone models have been produced. The models in bold are the current flagship devices of the series:

Intellectual property
Apple has filed more than 200 patent applications related to the technology behind the iPhone. [262] [263]
LG Electronics claimed the design of the iPhone was copied from the LG Prada . Woo-Young Kwak, head of LG Mobile Handset R&D Center, said at a press conference: "we consider that Apple copied Prada phone after the design was unveiled when it was presented in the iF Design Award and won the prize in September 2006." [264]
On September 3, 1993, Infogear filed for the US trademark "I PHONE" [265] and on March 20, 1996, applied for the trademark "IPhone". [266] "I Phone" was registered in March 1998, [265] and "IPhone" was registered in 1999. [266] Since then, the I PHONE mark had been abandoned. [265] Infogear trademarks cover "communications terminals comprising computer hardware and software providing integrated telephone, data communications and personal computer functions" (1993 filing), [265] and "computer hardware and software for providing integrated telephone communication with computerized global information networks" (1996 filing). [267]
Infogear released a telephone with an integrated web browser under the name iPhone in 1998. [268] In 2000, Infogear won an infringement claim against the owners of the iphones.com domain name. [269] In June 2000, Cisco Systems acquired Infogear, including the iPhone trademark. [270] On December 18, 2006, they released a range of re-branded Voice over IP (VoIP) sets under the name iPhone. [271]
In October 2002, Apple applied for the "iPhone" trademark in the United Kingdom, Australia, Singapore, and the European Union. A Canadian application followed in October 2004, and a New Zealand application in September 2006. As of October 2006, only the Singapore and Australian applications had been granted.
In September 2006, a company called Ocean Telecom Services applied for an "iPhone" trademark in the United States, United Kingdom and Hong Kong, following a filing in Trinidad and Tobago. [272] As the Ocean Telecom trademark applications use exactly the same wording as the New Zealand application of Apple, it is assumed that Ocean Telecom is applying on behalf of Apple. [273] The Canadian application was opposed in August 2005, by a Canadian company called Comwave who themselves applied for the trademark three months later. Comwave has been selling VoIP devices called iPhone since 2004. [270]
Shortly after Steve Jobs' January 9, 2007 announcement that Apple would be selling a product called iPhone in June 2007, Cisco issued a statement that it had been negotiating trademark licensing with Apple and expected Apple to agree to the final documents that had been submitted the night before. [274] On January 10, 2007, Cisco announced it had filed a lawsuit against Apple over the infringement of the trademark iPhone, seeking an injunction in federal court to prohibit Apple from using the name. [275] More recently, [ when? ] Cisco claimed that the trademark lawsuit was a "minor skirmish" that was not about money, but about interoperability. [276]
On February 2, 2007, Apple and Cisco announced that they had agreed to temporarily suspend litigation while they held settlement talks, [277] and subsequently announced on February 20, 2007, that they had reached an agreement. Both companies will be allowed to use the "iPhone" name [278] in exchange for "exploring interoperability" between their security, consumer, and business communications products. [279]
The iPhone has also inspired several leading high-tech clones, [280] driving both the popularity of Apple and consumer willingness to upgrade iPhones quickly. [281]
On October 22, 2009, Nokia filed a lawsuit against Apple for infringement of its GSM, UMTS and WLAN patents. Nokia alleges that Apple has been violating ten Nokia patents since the iPhone initial release. [282]
In December 2010, Reuters reported that some iPhone and iPad users were suing Apple Inc. because some applications were passing user information to third-party advertisers without permission. Some makers of the applications such as Textplus4, Paper Toss , The Weather Channel , Dictionary.com , Talking Tom Cat and Pumpkin Maker have also been named as co-defendants in the lawsuit. [283]
In August 2012, Apple won a smartphone patent lawsuit in the U.S. against Samsung , the world's largest maker of smartphones, [284] however on December 6, 2016, SCOTUS reversed the decision that awarded nearly $400 million to Apple and returned the case to Federal Circuit court to define the appropriate legal standard to define "article of manufacture" because it is not the smartphone itself but could be just the case and screen to which the design patents relate. [285]
In March 2013, an Apple patent for a wraparound display was revealed. [286]

Secret tracking
Since April 20, 2011, a hidden unencrypted file on the iPhone and other iOS devices has been widely discussed in the media. [287] [288] It was alleged that the file, labeled "consolidated.db", constantly stores the iPhone user's movement by approximating geographic locations calculated by triangulating nearby cell phone towers , a technology proven to be inaccurate at times. [289] The file was released with the June 2010 update of Apple iOS4 and may contain almost a year's worth of data. Previous versions of iOS stored similar information in a file called "h-cells.plist". [290]
F-Secure discovered that the data is transmitted to Apple twice a day and postulate that Apple is using the information to construct their global location database similar to the ones constructed by Google and Skyhook through wardriving . [291] Nevertheless, unlike the Google "Latitude" application, which performs a similar task on Android phones, the file is not dependent upon signing a specific EULA or even the user's knowledge, but it is stated in the 15,200 word-long terms and conditions of the iPhone that "Apple and [their] partners and licensees may collect, use, and share precise location data, including the real-time geographic location of [the user's] Apple computer or device". [292]
The file is also automatically copied onto the user's computer once synchronized with the iPhone. An open source application named "iPhoneTracker", which turns the data stored in the file into a visual map, was made available to the public in April 2011. [293] While the file cannot be erased without jailbreaking the phone, it can be encrypted. [294]
Apple gave an official response on their web site on April 27, 2011, [295] after questions were submitted by users, the Associated Press and others. Apple clarified that the data is a small portion of their crowd-sourced location database cache of Wi-Fi hotspots and cell towers which is downloaded from Apple into the iPhone for making location services faster than with only GPS, therefore the data does not represent the locations of the iPhone. The volume of data retained was an error. Apple issued an update for iOS (version 4.3.3 , or 4.2.8 for the CDMA iPhone 4) which reduced the size of the cache, stopped it being backed up to iTunes, and erased it entirely whenever location services were turned off. [295] The upload to Apple can also be selectively disabled from "System services", "Cell Network Search." Regardless, in July 2014, a report on state-owned China Central Television labeled the iPhone a "national security concern." [296]
A feature that can be found under "location services" in the settings of the iPhone has also been found to be secretly tracking the user's information. This feature is called "frequent locations" and it can either be kept on or turned off. This feature is said to help the accuracy of the GPS and Apple Maps since it can log information about the locations the user has frequently visited. However, this feature also keeps track of the number of times that he/she has been to that location, the dates, and the exact times. A lot of people have found this feature to be intrusive of their personal lives and have since then had an option to keep it on or shut it off. [297]

Encryption and intelligence agency access
It was revealed as a part of the 2013 mass surveillance disclosures that the American and British intelligence agencies, the National Security Agency (NSA) and the Government Communications Headquarters (GCHQ) have access to the user data in iPhones, BlackBerrys, and Android phones, respectively. They can read almost all smartphone information, including SMS, location, emails, and notes. [298]
According to an article in The New York Times titled "Signaling Post-Snowden Era, New iPhone Locks Out N.S.A.", Apple has developed a new encryption method for iOS 8, described as "so deep that Apple could no longer comply with government warrants asking for customer information to be extracted from devices." [299]
Throughout 2015, prosecutors in the United States argued for the U.S. government to be able to compel decryption of iPhone contents. [300] [301] [302] [303] After the 2015 San Bernardino attack , the FBI recovered an iPhone 5C that was issued to one of the shooters by his employer, and iCloud backups of that phone from a month and a half before the shooting. (The shooters had destroyed their personal phones.) The U.S. government attempted to use the arcane and outdated All Writs Act to obtain a court order ordering Apple to produce an IPSW file that would allow investigators to brute force the passcode of the iPhone, which would equate to modernized slavery. [304] [305] [306] Tim Cook responded on the company's website, outlining a need for encryption, arguing that if they produce a backdoor for one device, it would inevitably be used to compromise the privacy of other iPhone users. [307] On February 19, Apple communicated to journalists that the password for the Apple ID for the iPhone had been changed within a day of the government obtaining it, preventing Apple from producing a workaround that would only target older devices. [308] See FBI–Apple encryption dispute .
As of April 2016, Apple's Privacy Policy addresses requests from government agencies for access to customers' data: "Apple has never worked with any government agency from any country to create a “backdoor” in any of our products or services. We have also never allowed any government access to our servers. And we never will." [309] In 2015 the Electronic Frontier Foundation awarded Apple 5 out of 5 stars “commend[ing] Apple for its strong stance regarding user rights, transparency, and privacy.” [310]

Restrictions
Apple tightly controls certain aspects of the iPhone. According to Jonathan Zittrain , the emergence of closed devices like the iPhone have made computing more proprietary than early versions of Microsoft Windows . [311]
The hacker community has found many workarounds, most of which are disallowed by Apple and make it difficult or impossible to obtain warranty service. [312] " Jailbreaking " allows users to install apps not available on the App Store or modify basic functionality. SIM unlocking allows the iPhone to be used on a different carrier's network. [313] However, in the United States, Apple cannot void an iPhone's warranty unless it can show that a problem or component failure is linked to the installation or placement of an after-market item such as unauthorized applications, because of the Federal Trade Commission 's Magnuson-Moss Warranty Act of 1975 . [314]
The iPhone also has an area and settings where users can set restrictions or parental controls [315] on apps that can be downloaded or used within the iPhone. The restrictions area requires a password. [316]

Activation
The iPhone normally prevents access to its media player and web features unless it has also been activated as a phone with an authorized carrier. On July 3, 2007, Jon Lech Johansen reported on his blog that he had successfully bypassed this requirement and unlocked the iPhone's other features with a combination of custom software and modification of the iTunes binary. He published the software and offsets for others to use. [317]
Unlike the first generation iPhone, the iPhone 3G must be activated in the store in most countries. [318] This makes the iPhone 3G more difficult, but not impossible, to hack. The need for in-store activation, as well as the huge number of first-generation iPhone and iPod Touch users upgrading to iPhone OS 2.0, caused a worldwide overload of Apple's servers on July 11, 2008, the day on which both the iPhone 3G and iPhone OS 2.0 updates as well as MobileMe were released. After the update, devices were required to connect to Apple's servers to authenticate it, causing many devices to be temporarily unusable. [319]
Users on the O2 network in the United Kingdom, however, can buy the phone online and activate it via iTunes as with the previous model. [320] Even where not required, vendors usually offer activation for the buyer's convenience. In the US, Apple has begun to offer free shipping on both the iPhone 3G and the iPhone 3GS (when available), reversing the in-store activation requirement. Best Buy and Walmart will also sell the iPhone. [321]

Unapproved third-party software and jailbreaking
The iPhone's operating system is designed to only run software that has an Apple-approved cryptographic signature . This restriction can be overcome by "jailbreaking" the phone, [322] which involves replacing the iPhone's firmware with a slightly modified version that does not enforce the signature check. Doing so may be a circumvention of Apple's technical protection measures . [323] Apple, in a statement to the United States Copyright Office in response to Electronic Frontier Foundation (EFF) lobbying for a DMCA exception for this kind of hacking, claimed that jailbreaking the iPhone would be copyright infringement due to the necessary modification of system software. [324] However, in 2010, Jailbreaking was declared officially legal in the United States by the DMCA . [325] Jailbroken iPhones may be susceptible to computer viruses, but few such incidents have been reported. [326] [327]
iOS and Android 2.3.3 'Gingerbread' may be set up to dual boot on a jailbroken iPhone with the help of OpeniBoot or iDroid. [328] [329]
In 2007, 2010, and 2011, developers released a series of tools called JailbreakMe that used security vulnerabilities in Mobile Safari rendering to jailbreak the device (which allows users to install any compatible software on the device instead of only App Store apps). [330] [331] [332] Each of these exploits were quickly fixed by iOS updates from Apple. Theoretically these flaws could have also been used for malicious purposes. [333]
In July 2011, Apple released iOS 4.3.5 (4.2.10 for CDMA iPhone) to fix a security vulnerability with certificate validation. [334]
Following the release of the iPhone 5S model, a group of German hackers called the Chaos Computer Club announced on September 21, 2013, that they had bypassed Apple's new Touch ID fingerprint sensor by using "easy everyday means." The group explained that the security system had been defeated by photographing a fingerprint from a glass surface and using that captured image as verification. The spokesman for the group stated: "We hope that this finally puts to rest the illusions people have about fingerprint biometrics. It is plain stupid to use something that you can't change and that you leave everywhere every day as a security token." [335] [336]

SIM unlocking

United States
Most iPhones were and are still sold with a SIM lock , which restricts the use of the phone to one particular carrier, a common practice with subsidized GSM phones. Unlike most GSM phones however, the phone cannot be officially unlocked by entering a code. The locked/unlocked state is maintained on Apple's servers per IMEI and is set when the iPhone is activated. [337]
While the iPhone was initially sold in the US only on the AT&T network with a SIM lock in place, various hackers have found methods to " unlock " the phone from a specific network. [338] Although AT&T, Sprint, T-Mobile and Verizon are the only authorized iPhone carriers in the United States, unlocked iPhones can be used with other carriers. [339] For example, an unlocked iPhone may be used on the T-Mobile network in the US but, while an unlocked iPhone is compatible with T-Mobile's voice network, it may not be able to make use of 3G functionality (i.e. no mobile web or e-mail, etc.). [340] [ not in citation given ] More than a quarter of the original 1st generation iPhones sold in the US were not registered with AT&T. Apple speculates that they were likely shipped overseas and unlocked, a lucrative market before the iPhone 3G's worldwide release. [39] [341]
On March 26, 2009, AT&T in the United States began selling the iPhone without a contract, though still SIM-locked to their network. [342] The up-front purchase price of such iPhone units is often twice as expensive as those bundled with contracts. [343]
Outside of the United States, policies differ, especially in US territories and insular areas like Guam ; GTA Teleguam was the exclusive carrier for the iPhone since its introduction, as none of the four US carriers (AT&T, Sprint, T-Mobile, and Verizon) have a presence in the area. [344] Since 2013, Docomo Pacific ended GTA's exclusivity starting with the iPhone 5. [345]
Beginning April 8, 2012, AT&T began offering a factory SIM unlock option (which Apple calls a "whitelisting", allowing it to be used on any carrier the phone supports) for iPhone owners. [346]
It has been reported that all of the Verizon 4G LTE phones come factory unlocked. After such discovery, Verizon announced that all of their 4G LTE phones, including iPhones, would remain unlocked. This is due to the regulations that the FCC has placed on the 700 MHz C-Block spectrum, which is used by Verizon. [347]

United Kingdom
In the United Kingdom, O2 , EE , 3 , Vodafone , and Tesco Mobile sell the device under subsidised contracts, or for use on pay as you go. They are locked to the network initially, though they can usually be unlocked either after a certain period of contract length has passed, or for a small fee (with the exception of the 3 network, which will unlock the device at any time for no charge). [348] However, all current versions of iPhone are available for purchase SIM-free from the Apple Store or Apple's Online Store, consequently, they are unlocked for use on any GSM network too. [349]

Australia and other countries
Four major carriers in Australia ( Optus , Telstra , Virgin Mobile , and Vodafone ) [350] offer legitimate unlocking, now at no cost for all iPhone devices, both current and prior models.
Internationally, policies vary, but many carriers sell the iPhone unlocked for full retail price. [160]

Legal battles over brand name

Mexico
In 2003, four years before the iPhone was officially introduced, the trademark iFone was registered in Mexico by a communications systems and services company, iFone. [351] Apple tried to gain control over its brandname, but a Mexican court denied the request. The case began in 2009, when the Mexican firm sued Apple. The Supreme court of Mexico upheld that iFone is the rightful owner and held that Apple iPhone is a trademark violation. [352]

Brazil
In Brazil the brand IPHONE was registered in 2000 by the company then called Gradiente Eletrônica S.A., now IGB Eletrônica S.A. According to the filing, Gradiente foresaw the revolution in the convergence of voice and data over the Internet at the time. [353]
In Brazil, the final battle over the brandname concluded in 2008. On December 18, 2012, IGB launched its own line of Android smartphones under the tradename to which it has exclusive rights in the local market. [353] In February 2013, the Brazilian Patent and Trademark Office (known as "Instituto Nacional da Propriedade Industrial") issued a ruling that Gradiente Eletrônica, not Apple, owned the "iPhone" mark in Brazil. The "iPhone" term was registered by Gradiente in 2000, 7 years before Apple’s release of its first iPhone. This decision came 3 months after Gradiente Eletrônica launched a lower-cost smartphone using the iPhone brand. [354]
In June 2014, Apple won, for the second time, the right to use the brandname in Brazil. The court ruling determined that the Gradiente's registration does not own exclusive rights on the brand. Although Gradiente intended to appeal, with the decision Apple can use freely the brand without paying royalties to the Brazilian company. [355]

Philippines
In the Philippines , Solid Group launched the MyPhone brand in 2007. Stylized as "my|phone", Solid Broadband filed a trademark application of that brand. Apple later filed a trademark case at the Intellectual Property Office of the Philippines (IPOPHL) against Solid Broadband's MyPhone for "confusingly similar" to the iPhone and that it may likely "deceive" or "cause confusion" among consumers.
However, on May 19, 2015, Apple lost the trademark battle to Solid Group. The decision was signed by IPO director Nathaniel Arevalo, who also reportedly said that it was unlikely that consumers would be confused between the "iPhone" and the "MyPhone". "This is a case of a giant trying to claim more territory than what it is entitled to, to the great prejudice of a local 'Pinoy Phone' merchant who has managed to obtain a significant foothold in the mobile phone market through the marketing and sale of innovative products under a very distinctive trademark," Arevalo later added. [356] [357]
Solid Broadband noted that Apple can still appeal the IPO's decision within 30 days after receipt of a copy of the decision. The decision becomes final and executory if no appeal is filed on time. [358]

See also
WebPage index: 00086
Andrew Brown (writer)
Andrew Brown (born 1955 in London) is a British journalist, writer, and editor. [2] He was one of the founding staff members of The Independent , where he worked as religious correspondent, parliamentary sketch writer, and a feature writer. [3] He has written extensively on technology for Prospect and the New Statesman and been a feature writer on the Guardian. [4] He has worked as the editor for the Belief section of the The Guardian 's Comment is Free which won a Webby under his leadership [5] and is currently a leader writer and member of the paper's editorial board. He is also the press columnist of the Church Times . [6] In The Beginning was the Worm (2004) was shortlisted for the Aventis Prize . Fishing in Utopia (2008) won the Orwell Prize and was nominated for the Dolman Best Travel Book Award in 2009.

Views

English Wikipedia
Andrew Brown fears English Wikipedia has outcompeted rival encyclopedias and problems that lead to criticism of Wikipedia will continue. Brown fears "charlatans and liars" have most to gain from editing Wikipedia and potential idealistic contributors are discouraged due to difficulties editing the site especially through smartphones. [7]

New Atheism
Brown has been a fierce critic of the New Atheists . He has attacked Sam Harris for his advocacy of torture , [8] and Richard Dawkins for the cult of personality that has grown around him. [9] He is sceptical of the concept of memes. [10] [11]

Christianity
Brown has described himself as someone for whom "Christianity is only true backwards." [12] He has written that he is "constantly astonished by the way in which the Church of England contains such a large number of clever, learned and dedicated people giving their lives to an institution that is none of those things." [13]

Bibliography

Awards and nominations
WebPage index: 00087
Logistic function
A logistic function or logistic curve is a common "S" shape ( sigmoid curve ), with equation:
where
For values of x in the range of real numbers from −∞ to +∞, the S-curve shown on the right is obtained (with the graph of f approaching L as x approaches +∞ and approaching zero as x approaches −∞).
The function was named in 1844–1845 by Pierre François Verhulst , who studied it in relation to population growth. [2] The initial stage of growth is approximately exponential ; then, as saturation begins, the growth slows, and at maturity, growth stops.
The logistic function finds applications in a range of fields, including artificial neural networks , biology (especially ecology ), biomathematics , chemistry , demography , economics , geoscience , mathematical psychology , probability , sociology , political science , linguistics , and statistics .

Mathematical properties
The standard logistic function is the logistic function with parameters ( k = 1, x 0 = 0, L = 1) which yields
In practice, due to the nature of the exponential function e − x , it is often sufficient to compute the standard logistic function for x over a small range of real numbers such as a range contained in [−6, +6].

Derivative
The standard logistic function has an easily calculated derivative:
f ( x ) = 1 1 + e − x = e x 1 + e x {\displaystyle f(x)={\frac {1}{1+e^{-x}}}={\frac {e^{x}}{1+e^{x}}}}
d d x f ( x ) = e x ⋅ ( 1 + e x ) − e x ⋅ e x ( 1 + e x ) 2 {\displaystyle {\frac {d}{dx}}f(x)={\frac {e^{x}\cdot (1+e^{x})-e^{x}\cdot e^{x}}{(1+e^{x})^{2}}}}
d d x f ( x ) = e x ( 1 + e x ) 2 = f ( x ) ( 1 − f ( x ) ) {\displaystyle {\frac {d}{dx}}f(x)={\frac {e^{x}}{(1+e^{x})^{2}}}=f(x)(1-f(x))}
The logistic function also has the property that:
Thus, x ↦ f ( x ) − 1 / 2 {\displaystyle x\mapsto f(x)-1/2} is an odd function .
The derivative of the logistic function has the property that:

Logistic differential equation
The standard logistic function is the solution of the simple first-order non-linear ordinary differential equation
with boundary condition f (0) = 1/2. This equation is the continuous version of the logistic map .
The qualitative behavior is easily understood in terms of the phase line : the derivative is null when function is unit and the derivative is positive for f between 0 and 1, and negative for f above 1 or less than 0 (though negative populations do not generally accord with a physical model). This yields an unstable equilibrium at 0, and a stable equilibrium at 1, and thus for any function value greater than zero and less than unit, it grows to unit.
The above equation can be rewritten in the following steps:
Which is a special case of the Bernoulli differential equation and has the following solution:
Choosing the constant of integration C = 1 {\displaystyle C=1} gives the other well-known form of the definition of the logistic curve
More quantitatively, as can be seen from the analytical solution, the logistic curve shows early exponential growth for negative argument, which slows to linear growth of slope 1/4 for an argument near zero, then approaches one with an exponentially decaying gap.
The logistic function is the inverse of the natural logit function and so can be used to convert the logarithm of odds into a probability . In mathematical notation the logistic function is sometimes written as expit [3] in the same form as logit . The conversion from the log-likelihood ratio of two alternatives also takes the form of a logistic curve.
The logistic sigmoid function is related to the hyperbolic tangent , A.p. by
or
The latter relationship follows from
The hyperbolic tangent relationship leads to another form for the logistic function's derivative:
which ties the logistic function into the logistic distribution .

Rotational symmetry about (0, ½)
The sum of the logistic function and its reflection about the vertical axis, f (− x ) is
The logistic function is thus rotationally symmetrical about the point (0, 1 / 2 ). [4]

Applications

In ecology: modeling population growth
A typical application of the logistic equation is a common model of population growth (see also population dynamics ), originally due to Pierre-François Verhulst in 1838, where the rate of reproduction is proportional to both the existing population and the amount of available resources, all else being equal. The Verhulst equation was published after Verhulst had read Thomas Malthus ' An Essay on the Principle of Population . Verhulst derived his logistic equation to describe the self-limiting growth of a biological population. The equation was rediscovered in 1911 by A. G. McKendrick for the growth of bacteria in broth and experimentally tested using a technique for nonlinear parameter estimation. [5] The equation is also sometimes called the Verhulst-Pearl equation following its rediscovery in 1920 by Raymond Pearl (1879–1940) and Lowell Reed (1888–1966) of the Johns Hopkins University . [6] Another scientist, Alfred J. Lotka derived the equation again in 1925, calling it the law of population growth .
Letting P represent population size ( N is often used in ecology instead) and t represent time, this model is formalized by the differential equation :
where the constant r defines the growth rate and K is the carrying capacity .
In the equation, the early, unimpeded growth rate is modeled by the first term + rP . The value of the rate r represents the proportional increase of the population P in one unit of time. Later, as the population grows, the modulus of the second term (which multiplied out is − rP 2 /K ) becomes almost as large as the first, as some members of the population P interfere with each other by competing for some critical resource, such as food or living space. This antagonistic effect is called the bottleneck , and is modeled by the value of the parameter K . The competition diminishes the combined growth rate, until the value of P ceases to grow (this is called maturity of the population). The solution to the equation (with P 0 {\displaystyle P_{0}} being the initial population) is
where
Which is to say that K is the limiting value of P : the highest value that the population can reach given infinite time (or come close to reaching in finite time). It is important to stress that the carrying capacity is asymptotically reached independently of the initial value P (0) > 0, and also in the case that P (0) > K .
In ecology , species are sometimes referred to as r -strategist or K -strategist depending upon the selective processes that have shaped their life history strategies. Choosing the variable dimensions so that n measures the population in units of carrying capacity, and τ measures time in units of 1/r , gives the dimensionless differential equation

Time-varying carrying capacity
Since the environmental conditions influence the carrying capacity, as a consequence it can be time-varying: K ( t ) > 0, leading to the following mathematical model:
A particularly important case is that of carrying capacity that varies periodically with period T :
It can be shown that in such a case, independently from the initial value P (0) > 0, P ( t ) will tend to a unique periodic solution P * ( t ), whose period is T .
A typical value of T is one year: In such case K ( t ) may reflect periodical variations of weather conditions.
Another interesting generalization is to consider that the carrying capacity K ( t ) is a function of the population at an earlier time, capturing a delay in the way population modifies its environment. This leads to a logistic delay equation, [7] which has a very rich behavior, with bistability in some parameter range, as well as a monotonic decay to zero, smooth exponential growth, punctuated unlimited growth (i.e., multiple S-shapes), punctuated growth or alternation to a stationary level, oscillatory approach to a stationary level, sustainable oscillations, finite-time singularities as well as finite-time death.

In statistics and machine learning
Logistic functions are used in several roles in statistics . For example, they are the cumulative distribution function of the logistic family of distributions , and they are, a bit simplified, used to model the chance a chess player has to beat his opponent in the Elo rating system . More specific examples now follow.

Logistic regression
Logistic functions are used in logistic regression to model how the probability p of an event may be affected by one or more explanatory variables : an example would be to have the model
where x is the explanatory variable and a and b are model parameters to be fitted and f is the standard logistic function.
Logistic regression and other log-linear models are also commonly used in machine learning . A generalisation of the logistic function to multiple inputs is the softmax activation function , used in multinomial logistic regression .
Another application of the logistic function is in the Rasch model , used in item response theory . In particular, the Rasch model forms a basis for maximum likelihood estimation of the locations of objects or persons on a continuum , based on collections of categorical data, for example the abilities of persons on a continuum based on responses that have been categorized as correct and incorrect.

Neural networks
Logistic functions are often used in neural networks to introduce nonlinearity in the model and/or to clamp signals to within a specified range . A popular neural net element computes a linear combination of its input signals, and applies a bounded logistic function to the result; this model can be seen as a "smoothed" variant of the classical threshold neuron .
A common choice for the activation or "squashing" functions, used to clip for large magnitudes to keep the response of the neural network bounded [8] is
which is a logistic function. These relationships result in simplified implementations of artificial neural networks with artificial neurons . Practitioners caution that sigmoidal functions which are antisymmetric about the origin (e.g. the hyperbolic tangent ) lead to faster convergence when training networks with backpropagation . [9]
The logistic function is itself the derivative of another proposed activation function, the softplus .

In medicine: modeling of growth of tumors
Another application of logistic curve is in medicine, where the logistic differential equation is used to model the growth of tumors. This application can be considered an extension of the above-mentioned use in the framework of ecology (see also the Generalized logistic curve , allowing for more parameters). Denoting with X ( t ) the size of the tumor at time t , its dynamics are governed by:
which is of the type:
where F ( X ) is the proliferation rate of the tumor.
If a chemotherapy is started with a log-kill effect, the equation may be revised to be
where c ( t ) is the therapy-induced death rate. In the idealized case of very long therapy, c ( t ) can be modeled as a periodic function (of period T ) or (in case of continuous infusion therapy) as a constant function, and one has that
i.e. if the average therapy-induced death rate is greater than the baseline proliferation rate then there is the eradication of the disease. Of course, this is an oversimplified model of both the growth and the therapy (e.g. it does not take into account the phenomenon of clonal resistance).

In chemistry: reaction models
The concentration of reactants and products in autocatalytic reactions follow the logistic function.

In physics: Fermi distribution
The logistic function determines the statistical distribution of fermions over the energy states of a system in thermal equilibrium. In particular, it is the distribution of the probabilities that each possible energy level is occupied by a fermion, according to Fermi–Dirac statistics .

In linguistics: language change
In linguistics, the logistic function can be used to model language change : [10] an innovation that is at first marginal begins to spread more quickly with time, and then more slowly as it becomes more universally adopted.

In economics and sociology: diffusion of innovations
The logistic function can be used to illustrate the progress of the diffusion of an innovation through its life cycle.
In The Laws of Imitation (1890), Gabriel Tarde describes the rise and spread of new ideas through imitative chains. In particular, Tarde identifies three main stages through which innovations spread: the first one corresponds to the difficult beginnings, during which the idea has to struggle within a hostile environment full of opposing habits and beliefs; the second one corresponds to the properly exponential take-off of the idea, with f ( x ) = 2 x {\displaystyle f(x)=2^{x}} ; finally, the third stage is logarithmic, with f ( x ) = log ⁡ ( x ) {\displaystyle f(x)=\log(x)} , and corresponds to the time when the impulse of the idea gradually slows down while, simultaneously new opponent ideas appear. The ensuing situation halts or stabilizes the progress of the innovation, which approaches an asymptote.
In the history of economy, when new products are introduced there is an intense amount of research and development which leads to dramatic improvements in quality and reductions in cost. This leads to a period of rapid industry growth. Some of the more famous examples are: railroads, incandescent light bulbs, electrification , cars and air travel. Eventually, dramatic improvement and cost reduction opportunities are exhausted, the product or process are in widespread use with few remaining potential new customers, and markets become saturated.
Logistic analysis was used in papers by several researchers at the International Institute of Applied Systems Analysis ( IIASA ). These papers deal with the diffusion of various innovations, infrastructures and energy source substitutions and the role of work in the economy as well as with the long economic cycle. Long economic cycles were investigated by Robert Ayres (1989). [11] Cesare Marchetti published on long economic cycles and on diffusion of innovations. [12] [13] Arnulf Grübler’s book (1990) gives a detailed account of the diffusion of infrastructures including canals, railroads, highways and airlines, showing that their diffusion followed logistic shaped curves. [14]
Carlota Perez used a logistic curve to illustrate the long ( Kondratiev ) business cycle with the following labels: beginning of a technological era as irruption , the ascent as frenzy , the rapid build out as synergy and the completion as maturity . [15]

See also

Notes
WebPage index: 00088
Parliament of Canada
The Parliament of Canada ( French : Parlement du Canada ) is the federal legislative branch of Canada , seated at Parliament Hill in the national capital, Ottawa , Ontario . The body consists of the Canadian monarch , represented by a viceroy , the governor general ; an upper house : the Senate ; and a lower house : the House of Commons . Each element has its own officers and organization. The governor general summons and appoints each of the 105 senators on the advice of the Prime Minister of Canada , while the 338 members of the House of Commons—called members of parliament (MPs)—are directly elected by eligible Canadian voters, with each MP representing a single electoral district , commonly referred to as a riding .
By constitutional convention , the House of Commons is the dominant branch of parliament, the Senate and Crown rarely opposing its will. The Senate reviews legislation from a less partisan standpoint and the monarch or viceroy provides the necessary Royal Assent to make bills into law. The governor general also summons parliament, while either the viceroy or monarch can prorogue or dissolve parliament , the latter in order to call a general election . Either will read the Throne Speech . The most recent parliament , summoned by Governor General David Johnston in 2015, is the 42nd since Confederation in 1867.

Composition
The Parliament of Canada is composed of three parts: the monarch, the Senate, and the House of Commons. [1] Each has a distinct role, but work in conjunction within the legislative process . This format was inherited from the United Kingdom and thus is a near identical copy of the parliament at Westminster , the greatest differences stemming from situations unique to Canada, such as the impermanent nature of the monarch's residency in the country and the lack of a peerage to form the upper chamber.
Only those who sit in the House of Commons are called members of parliament (MPs); the term is never applied to senators, even though the Senate is a part of parliament. Though legislatively less powerful, senators take higher positions in the national order of precedence . No individual may serve in more than one chamber of parliament at the same time.

Monarch
The sovereign's place in the legislature, formally called the Queen-in-Parliament , [2] is defined by the Constitution Act, 1867 , and various conventions . [1] Neither she nor her viceroy, however, participates in the legislative process, save for signifying the Queen's approval to a bill passed by both houses of parliament, known as the granting of Royal Assent , which is necessary for a bill to be enacted as law. All federal bills thus begin with the phrase "Now, therefore, Her Majesty, by and with the advice and consent of the Senate and House of Commons of Canada, enacts as follows ..." [3] and, as such, the Crown is immune from acts of parliament unless expressed otherwise in the act itself. [4] The governor general will normally perform the task of granting Royal Assent, though the monarch may also do so, at the request of either the Cabinet or the viceroy, who may defer assent to the sovereign as per the constitution. [5]
As both the monarch and his or her representatives are traditionally barred from the House of Commons, any parliamentary ceremonies in which they are involved take place in the Senate chamber. The upper and lower houses do, however, each contain a mace , which indicate the authority of the Queen-in-Parliament and the privilege granted to that body by her, [6] [7] both bearing a crown at their apex. The original mace for the Senate was that used in the Legislative Council of the Province of Canada after 1849, while that of the House of Commons was inherited from the Legislative Assembly of the Province of Canada , first used in 1845. Following the burning of the Centre Block on 3 February 1916, the City of London , England , donated a replacement, which is still used today. The temporary mace, made of wood, and used until the new one arrived from the United Kingdom in 1917, is still carried into the Senate each 3 February. [8] The Senate's 1.6-metre-long mace comprises brass and gold. The Senate may not sit if its mace is not in the chamber; it typically sits on the table with the crown facing the throne, [9] though it may, during certain ceremonies, be held by the Mace Bearer, standing adjacent to the governor general or monarch in the Senate. [10]
Members of the two houses of parliament must also express their loyalty to the sovereign and defer to her authority, as the Oath of Allegiance must be sworn by all new parliamentarians before they may take their seats. Further, the official opposition is formally called as Her Majesty's Loyal Opposition , to signify that, though they may be opposed to the incumbent Cabinet's policies, these MPs remain dedicated to the apolitical Crown. [11] [12]

Senate
The upper house of the Parliament of Canada, the Senate ( French : Sénat ), is a group of 105 individuals appointed by the governor general on the advice of the prime minister; [13] all those summoned to the Senate by the viceroy must, per the constitution, be a minimum of 30 years old, be a subject of the monarch, and own property with a net worth of at least $4,000, in addition to owning land worth no less than $4,000 within the province he or she is to represent. [14] Senators served for life until 1965, when a constitutional amendment imposed a mandatory retirement age of 75. Senators may, however, resign their seats prior to that mark, and can lose their position should they fail to attend two consecutive sessions of parliament.
The Senate is divided equally amongst four geographic regions: 24 for Ontario , 24 for Quebec , 24 for the Maritimes (10 for Nova Scotia , 10 for New Brunswick , and four for Prince Edward Island ), and 24 for the Western provinces (six each for Manitoba , British Columbia , Saskatchewan , and Alberta ). [15] Newfoundland and Labrador , which became a Canadian province in 1949, is represented by six senators, though the province is not part of a senatorial division. Further, Canada's three territories—the Northwest Territories , Yukon , and Nunavut —are allocated one senator each. An additional four or eight senators may be temporarily appointed by the governor general, provided the approval of the Queen is secured, and the four divisions are equally represented, thus putting the maximum possible number of senators at 113. This power has been employed only once since 1867: to ensure the passage of the bill establishing the Goods and Services Tax , Prime Minister Brian Mulroney in 1990 advised Queen Elizabeth II to appoint extra senators.

House of Commons
The elected component of the Canadian parliament is the House of Commons ( French : Chambre des communes ), with each member chosen by a plurality of eligible voters in each of the country's federal electoral districts , or ridings. To run for one of the 338 seats in the lower house , an individual must be at least 18 years old, and each winner holds office until parliament is dissolved, after which they may seek re-election. The ridings are regularly reorganised according to the results of each decennial national census ; [16] however, the "senatorial clause" of the Constitution Act, 1867, guarantees each province at least as many MPs as it has senators, [17] and the "grandfather clause" permits each province as many MPs as it had in either 1976 or 1985. [16] The existence of this legislation has pushed the size of the House of Commons above the required minimum of 282 seats.

Jurisdiction
The powers of the Parliament of Canada are limited by the constitution, which divides legislative abilities between the federal and provincial legislatures; in general, provincial legislatures may only pass laws relating to topics explicitly reserved for them by the constitution (such as education, provincial officers, municipal government, charitable institutions, and "matters of a merely local or private nature" [18] ), while any matter not under the exclusive authority of the provincial legislatures is within the scope of the federal parliament's power. Thus, parliament alone can pass laws relating to, among other things, the postal service , census , military , navigation and shipping, fishing , currency , banking , weights and measures , bankruptcy , copyrights , patents , First Nations , and naturalization . [19] In some cases, however, the jurisdictions of the federal and provincial parliaments may be more vague. For instance, the parliament in Ottawa regulates marriage and divorce in general, but the solemnization of marriage is regulated only by the provincial legislatures. Other examples include the powers of both the federal and provincial parliaments to impose taxes, borrow money, punish crimes, and regulate agriculture .
The powers of the Canadian parliament are also limited by the Canadian Charter of Rights and Freedoms , though most provisions in that document can be overridden use of the notwithstanding clause . [20] Such a claim, however, has never been used by the federal parliament, though it has been employed by some provincial legislatures. Laws violating any part of the constitution are invalid and may be ruled unconstitutional by the courts .

Officers
Each of the parliament's two chambers is presided over by a speaker ; that for the Senate is a member of that house appointed by the governor general, as advised by the prime minister, while the equivalent for the House of Commons is a member of parliament elected by the other members of that body. In general, the powers of the latter are greater than those of the former; following the British model, the upper chamber is essentially self-regulating, whereas the lower chamber is controlled from the chair. In 1991, however, the powers of the Speaker of the Senate were expanded, moving the position closer to that in the Commons.
The Usher of the Black Rod of the Senate of Canada is the most senior protocol position in parliament, being the personal messenger to the legislature of the sovereign and governor general. He or she is also a floor officer of the Senate responsible for security in that chamber, as well as for protocol, administrative, and logistical details of important events taking place on Parliament Hill, [21] such as the Speech from the Throne , Royal Assent ceremonies, state funerals , or the investiture of a new governor general. [22]
Other officers of parliament include the Auditor General , Chief Electoral Officer , Official Languages Commissioner , Privacy Commissioner , Access to Information Commissioner , Conflict of Interest and Ethics Commissioner , Public Sector Integrity Commissioner , and Commissioner of Lobbying . These individuals are appointed by either one or both houses, to which they report through the speaker of that house. They are sometimes referred to as Agents of Parliament . [23] Another key official is the Parliamentary Librarian of Canada , a position established in 1871 under the Library of Parliament Act, charged with the running of the Library of Parliament .

Term
The Constitution Act, 1867, outlines that the governor general alone is responsible for summoning parliament, though it remains the monarch's prerogative to prorogue and dissolve the legislature, after which the writs for a general federal election are usually dropped by the governor general at Rideau Hall . Upon completion of the election, the viceroy, on the advice of his or her prime minister, then issues a royal proclamation summoning parliament to assemble. On the date given, new MPs are sworn-in and then are, along with returning MPs, called to the Senate, where they are instructed to elect their speaker and return to the House of Commons to do so before adjourning. [24]
The new parliamentary session is marked by the opening of parliament , during which either the monarch, the governor general, or a royal delegate, [n 1] reads the Speech From the Throne . MPs receive the Royal Summons to these events from the Usher of the Black Rod [25] after he knocks on the doors of the lower house that have been slammed shut, [26] to illustrate the Commons' right to deny entry to anyone, including the monarch, but excepting royal messengers. [27] Once MPs are gathered behind the Bar of the Senate—save for the prime minister, the only MP permitted into the Senate proper to sit near the throne dais—the speaker of the lower house presents him or herself to the monarch or governor general and formally claims the rights and privileges of the House of Commons, to which the Speaker of the Senate, on behalf of the Crown, replies in acknowledgement after the sovereign or viceroy takes their seat on the throne. [24] The speech is then read, outlining the programme of the Cabinet for the upcoming legislative session.
A parliamentary session lasts until a prorogation, after which, without ceremony, both chambers of the legislature cease all legislative business until the governor general issues another proclamation calling for a new session to begin; except for the election of a speaker for the House of Commons and his or her claiming of that house's privileges, the same procedures for the opening of parliament are again followed. After a number of such sessions—these have ranged from one to seven [28] —each parliament comes to an end via dissolution , which is effected by the governor general. As a general election typically follows. The timing of a dissolution is usually politically motivated, with the prime minister selecting a moment most advantageous to his or her political party. The end of a parliament may also be necessary, however, if the majority of MPs revoke their confidence in the prime minister's ability to govern, or the legally mandated four-year maximum is reached; [29] no parliament has yet been allowed to expire in such a fashion.

Procedure
Both houses determine motions by voice vote ; the presiding officer puts the question and, after listening to shouts of "yea" and "nay" from the members, announces which side is victorious. This decision by the Speaker is final, unless a recorded vote is demanded by members—at least two in the Senate and five in the House of Commons. Members of both houses vote by rising in their places to be counted; the Speaker of the Senate is permitted to vote on a motion or bill—though does so irregularly, in the interest of impartiality—and, if there is no majority, the motion is defeated. In the Commons, however, the Speaker cannot vote, unless to break a tie, at which time he or she will customarily vote in favour of the status quo . The constitution establishes the quorums to be 15 senators in the upper house and 20 members in the lower house, the Speaker of each body being counted within the tally.
Voting can thus take three possible forms: whenever possible, leaving the matter open for future consideration and allowing for further discussion by the house; when no further discussion is possible, taking into account that the matter could somehow be brought back in future and be decided by a majority in the house; or, leaving a bill in its existing form rather than having it amended. For example, during the vote on the 2005 budget , which was considered a vote of confidence , the Speaker of the House of Commons cast the tie-breaking vote during the second reading , moving in favour of the budget and allowing its passage. If the vote on the third reading had again been tied, the speaker would have been expected to vote against the bill, bringing down the government.
Simultaneous interpretation for both official languages, English and French , is provided at all times during sessions of both houses.

Legislative functions
Laws, known in their draft form as bills , may be introduced by any member of either house. However, most bills originate in the House of Commons, of which most are put forward by ministers of the Crown , making them government bills, as opposed to private members' bills or private senators' bills, which are launched by MPs and senators, respectively, who are not in cabinet. Draft legislation may also be categorised as public bills, if they apply to the general public, or private bills , if they concern a particular person or limited group of people. Each bill then goes through a series of stages in each chamber, beginning with the first reading . It is not, however, until the bill's second reading that the general principles of the proposed law are debated; though rejection is a possibility, such is not common for government bills.
Next, the bill is sent by the house where it is being debated to one of several different committees. The Standing Orders outline the general mandate for all committees, allowing them to review: bills as they pertain to relevant departments; the program and policy plans, as well as the projected expenditures, and the effectiveness of the implementation thereof, for the same departments; and the analysis of the performance of those departments. [30] Most often, bills end up before a standing committee , which is a body of members or senators who specialise in a particular subject (such as foreign affairs ), and who may hear testimony from ministers and experts, debate the bill, and recommend amendments. The bill may also be committed to the Committee of the Whole , a body consists of, as the name suggests, all the members of the chamber in question. Finally, the bill could be referred to an ad hoc committee established solely to review the piece of legislation in question. Each chamber has their own procedure for dealing with this, with the Senate establishing special committees that function like most other committees, and the House of Commons establishing legislative committees, the chair of the latter being appointed by the speaker of the House of Commons, and is normally one of his deputies. Whichever committee is used, any amendments proposed by the committee are considered by the whole house in the report stage. Furthermore, additional amendments not proposed by the committee may also be made.
After the report stage (or, if the committee made no amendments to the bill, immediately after the committee stage), the final phase of the bill—the third reading —occurs, at which time further amendments are not permitted in the House of Commons, but are allowed in the Senate. If one house passes amendments that the other will not agree to, and the two houses cannot resolve their disagreements, the bill fails. If, however, it passes the third reading, the bill is sent to the other house of parliament, where it passes through the same stages; amendments made by the second chamber require the assent of the original house in order to stand part of the final bill. Once the bill is passed in identical form by both houses, it is presented for Royal Assent ; in theory, the governor general has three options: he or she may grant Royal Assent, thereby making the bill into law; withhold Royal Assent, thereby vetoing the bill; or reserve the bill for the signification of the Queen's pleasure , which allows the sovereign to personally grant or withhold assent. If the governor general does grant Royal Assent, the monarch may, within two years, disallow the bill, thus annulling the law in question. In the federal sphere, no bill has ever been denied royal approval.
In conformity with the British model, only the House of Commons may originate bills for the imposition of taxes or for the appropriation of Crown funds. Otherwise, the theoretical power of both houses over bills is equal, with the assent of each being required for passage. In practice, however, the House of Commons is the dominant chamber of parliament, with the Senate rarely exercising its powers in a way that opposes the will of the democratically elected house.

Relationship with the executive
The Canadian government consists of the monarch, predominantly represented by his or her governor general, in council , which is a collection of ministers of the Crown appointed by the governor general to direct the use of the executive powers . Per the tenets of responsible government , these individuals are almost always drawn from the parliament, and then are predominantly from the House of Commons, the only body to which the ministers are held accountable, typically during Question Period , wherein the ministers are obliged to answer questions posed by members of the loyal opposition. Hence, the person who can command the confidence of the lower chamber—usually the leader of the party with the most seats therein—is the one who is typically appointed as prime minister. Should that person not actually hold a seat in the House of Commons, he or she will, by convention, seek election to one at the earliest possible opportunity; frequently, in such situations, a junior Member of Parliament who holds a safe seat will resign to allow the prime minister to run for that riding in a by-election . If no party holds a majority, it is customary for the governor general to summon a minority government or coalition government , depending on which the commons will support.
The lower house may attempt to bring down the government by either rejecting a motion of confidence —generally initiated by a minister to reinforce the Cabinet's support in the commons—or by passing a motion of no confidence—introduced by the opposition to display its distrust of the Cabinet. Important bills that form part of the government's agenda will usually be considered matters of confidence; the budget is always a matter of confidence. Where a government has lost the confidence of the House of Commons, the prime minister is obliged to either resign (allowing the governor general to appoint the Leader of the Opposition to the office) or seek the dissolution of parliament and the call of a general election. A precedent, however, was set in 1968, when the government of Lester B. Pearson unexpectedly lost a confidence vote but was allowed to remain in power with the mutual consent of the leaders of the other parties.
In practice, the House of Commons' scrutiny of the government is quite weak in comparison to the equivalent chamber in other countries using the Westminster system. With the plurality voting system used in parliamentary elections tending to provide the governing party with a large majority and a party system that gives leaders strict control over their caucus (to the point that MPs may be expelled from their parties for voting against the instructions of party leaders), there is often limited need to compromise with other parties. Additionally, Canada has fewer MPs, a higher turnover rate of MPs after each election, and an Americanised system for selecting political party leaders, leaving them accountable to the party membership rather than caucus, as is the case in the United Kingdom; [31] John Robson of the National Post opined that Canada's parliament had become a body akin to the American Electoral College , "its sole and ceremonial role to confirm the executive in power." [32] At the end of the 20th century and into the 21st, analysts—such as Jeffrey Simpson , Donald Savoie , and John Gomery —argued that both parliament and the Cabinet had become eclipsed by prime ministerial power. [33] Thus, defeats of majority governments on issues of confidence are very rare. In contrast, a minority government is more volatile, and is more likely to fall due to loss of confidence. The last prime minister to lose a confidence vote was Stephen Harper in 2011, prior to which was Paul Martin in 2005 and Joe Clark in 1979. All these occurrences involved minority governments.

Privileges
The institution of parliament possesses a number of privileges, collectively and accordingly known as parliamentary privilege , each house being the guardian and administrator of its own set of rights. Parliament itself determines the extent of parliamentary privilege, each house overseeing its own affairs, but the constitution bars it from conferring any "exceeding those at the passing of such an Act held, enjoyed, and exercised by the [British House of] Commons... and by the Members thereof." [34]
The foremost dispensation held by both houses of the legislature is that of freedom of speech in debate; nothing said within the chambers may be questioned by any court or other institution outside of Parliament. In particular, a member of either house cannot be sued for slander based on words uttered in the course of parliamentary proceedings, the only restraint on debate being set by the standing orders of each house. Further, MPs and senators are immune to arrest in civil cases (but not for allegedly criminal actions), and from jury service and attendance in courts as witnesses. They may, however, be disciplined by their own colleagues for breach of the rules, including contempt of parliament —disobedience of its authority; for example, giving false testimony before a parliamentary committee—and breaches of its own privileges.
The Canadian Heraldic Authority , on 15 April 2008, granted the Parliament of Canada, as an institution, a heraldic achievement composed of symbols of the three elements of parliament: the escutcheon of the Royal Arms of Canada (representing the Queen) with the maces of the House of Commons and Senate crossed behind (representing each of those chambers). [35]
The budget for the Parliament of Canada for the 2010 fiscal year was $ 583,567,000. [36]

History
Following the cession of New France to the United Kingdom in the 1763 Treaty of Paris , Canada was governed according to the Royal Proclamation issued by King George III in that same year. To this was added the Quebec Act , by which the power to make ordinances was granted to a governor-in-council , both the governor and council being appointed by the British monarch in Westminster , on the advice of his or her ministers there. In 1791, the Province of Quebec was divided into Upper and Lower Canada , each with an elected legislative assembly , an appointed legislative council , and a governor, mirroring the parliamentary structure in Britain.
During the War of 1812 , American troops set fire to the buildings of the Legislative Assembly of Upper Canada in York (now Toronto ). In 1841, the British government united the two Canadas into the Province of Canada , with a single legislature composed of, again, an assembly, council, and governor general; the 84 members of the lower chamber were equally divided among the two former provinces, though Lower Canada had a higher population. The governor still held significant personal influence over Canadian affairs until 1848, when responsible government was implemented in Canada.
The actual site of the parliament shifted on a regular basis: From 1841 to 1844, it sat in Kingston , where the present Kingston General Hospital now stands; from 1844 until the 1849 fire that destroyed the building , the legislature was in Montreal ; and, after a few years of alternating between Toronto and Quebec City , the legislature was finally moved to Ottawa in 1856, Queen Victoria having chosen that city as Canada's capital in 1857.
The modern-day Parliament of Canada came into existence in 1867, in which year the Parliament of the United Kingdom of Great Britain and Ireland passed the British North America Act, 1867 , uniting the provinces of New Brunswick , Nova Scotia , and Canada—with the Province of Canada split into Quebec and Ontario —into a single federation called the Dominion of Canada . Though the form of the new federal legislature was again nearly identical to the parliament of the United Kingdom, the decision to retain this model was made with heavy influence from the just-concluded American Civil War , which indicated to many Canadians the faults of the American federal system, with its relatively powerful states and a less powerful federal government. The British North America Act limited the powers of the provinces, providing that all subjects not explicitly delegated to them by that document remain within the authority of the federal parliament, while simultaneously giving the provinces unique powers in certain agreed-upon areas of funding.
Full legislative autonomy was granted by the Statute of Westminster, 1931 , passed by the United Kingdom and ratified by the Canadian parliament. Though the statute allowed the Parliament of Canada to repeal or amend previously British laws as they applied to Canada, it did not permit the abrogation of Canada's constitution, including the British North America Acts. Hence, whenever a constitutional amendment was sought by the Canadian parliament, the enactment of a British law became necessary, though Canada's consent was required. The Parliament of Canada was granted limited power to amend the constitution by a British Act of Parliament in 1949, but it was not permitted to affect the powers of provincial governments, the official positions of the English and French languages, or the maximum five-year term of the legislature.
The Canadian Cabinet last requested the Parliament of the United Kingdom to enact a constitutional amendment in 1982, in the form of the Canada Act . This legislation terminated the power of the British parliament's ability to legislate for Canada and the authority to amend the constitution was transferred to Canadian legislative authorities. Most amendments require the consent of the Senate, the House of Commons, and the legislative assemblies of two-thirds of the provinces representing a majority of the population; the unanimous consent of provincial legislative assemblies is required for certain amendments, including those affecting the sovereign, the governor general, the provincial lieutenant governors , the official status of the English and French languages, the Supreme Court of Canada , and the amending formulas themselves.

See also

Notes
WebPage index: 00089
RNA Biology
RNA Biology is a peer-reviewed scientific journal in the field of ribonucleic acid ( RNA ) research. It is indexed for MEDLINE . The editor-in-chief is Renee Schroeder ( University of Vienna ).

Wikipedia initiative
The journal launched a new section for descriptions of families of RNA molecules in December 2008 and requires contributing authors to also submit a draft article on the RNA family for publication in Wikipedia . The journal submits the draft article to peer review and then publish it in Wikipedia. This initiative is a collaboration between the journal and the consortium that produces the Rfam database of RNA families. [1]

Abstracting and indexing
The journal is abstracted and indexed in:
According to the Journal Citation Reports , the journal has a 2014 impact factor of 4.974. [2]
WebPage index: 00090
Myspace
Myspace is a social networking website offering an interactive, user-submitted network of friends, personal profiles, blogs, groups, photos, music, and videos. It is headquartered in Beverly Hills, California . [5] [6]
Myspace was acquired by News Corporation in July 2005 for $580 million. [7] From 2005 to 2008, Myspace was the largest social networking site in the world, and in June 2006 surpassed Google as the most visited website in the United States. [8] [9] In April 2008, Myspace was overtaken by Facebook in the number of unique worldwide visitors, and was surpassed in the number of unique U.S. visitors in May 2009, [10] though Myspace generated $800 million in revenue during the 2008 fiscal year. [11] Since then, the number of Myspace users has declined steadily in spite of several redesigns. [12] As of March 2017, Myspace was ranked 3,178 by total Web traffic, and 1,650 In the United States. [4]
Myspace had a significant influence on pop culture and music [13] and created a gaming platform that launched the successes of Zynga and RockYou , among others. [14] Despite an overall decline, in 2015 Myspace still had 50.6 million unique monthly visitors and has a pool of nearly 1 billion active and inactive registered users. [15]
In June 2009, Myspace employed approximately 1,600 employees. [3] [16] In June 2011, Specific Media Group and Justin Timberlake jointly purchased the company for approximately $35 million. [17] On February 11, 2016 it was announced that MySpace and its parent company had been bought by Time Inc. [18]

History

2003–2005: Beginnings
In August 2003, several eUniverse employees with Friendster accounts saw potential in its social networking features. The group decided to mimic the more popular features of the website. Within 10 days, the first version of Myspace was ready for launch, implemented using ColdFusion . [12] [19] A complete infrastructure of finance, human resources, technical expertise, bandwidth , and server capacity was available for the site. The project was overseen by Brad Greenspan (eUniverse's Founder, Chairman, CEO), who managed Chris DeWolfe (MySpace's starting CEO), Josh Berman, Tom Anderson (MySpace's starting president), and a team of programmers and resources provided by eUniverse.
The first Myspace users were eUniverse employees. The company held contests to see who could sign up the most users. [20] eUniverse used its 20 million users and e-mail subscribers to breathe life into Myspace, [21] and move it to the head of the pack of social networking websites. A key architect was tech expert Toan Nguyen who helped stabilize the Myspace platform when Brad Greenspan asked him to join the team. [22] Co-founder and CTO Aber Whitcomb played an integral role in software architecture, utilizing the then superior development speed of ColdFusion over other dynamic database driven server-side languages of the time. Despite over ten times the number of developers, Friendster , which was developed in JavaServer Pages (jsp), could not keep up with the speed of development of Myspace and cfm.
The MySpace.com domain was originally owned by YourZ.com, Inc., intended until 2002 for use as an online data storage and sharing site. By 2004, it was transitioned from a file storage service to a social networking site. A friend, who also worked in the data storage business, reminded Chris DeWolfe that he had earlier bought the domain MySpace.com. [23] DeWolfe suggested they charge a fee for the basic Myspace service. [24] Brad Greenspan nixed the idea, believing that keeping Myspace free was necessary to make it a successful community. [25]

2005–2008: Rise and purchase by News Corp.
Myspace quickly gained popularity among teenage and young adult social groups. In February 2005, DeWolfe held talks with Mark Zuckerberg over acquiring Facebook but DeWolfe rejected Zuckerberg's $75 million asking price. [26]
Some employees of Myspace, including DeWolfe and Berman, were able to purchase equity in the property before MySpace and its parent company eUniverse (now renamed Intermix Media ) was bought. In July 2005, in one of the company's first major Internet purchases, Rupert Murdoch 's News Corporation (the parent company of Fox Broadcasting and other media enterprises) purchased Myspace for US$580 million. [19] [27] News Corporation had beat out Viacom by offering a higher price for the website, [28] and the purchase was seen as a good investment at the time. [28] Of the $580 million purchase price, approximately $327 million has been attributed to the value of Myspace according to the financial adviser fairness opinion . [29] Within a year, Myspace had tripled in value from its purchase price. [28] News Corporation saw the purchase as a way to capitalize on Internet advertising, and drive traffic to other News Corporation properties. [27]
After losing the bidding war for Myspace, Viacom chairman Sumner Redstone stunned the entertainment industry in September 2006 when he fired Tom Freston from the position of CEO. Redstone believed that the failure to acquire MySpace contributed to the 20% drop in Viacom's stock price in 2006 up to the date of Freston's ouster. Freston's successor as CEO, Philippe Dauman, was quoted as saying "never, ever let another competitor beat us to the trophy". Redstone told interviewer Charlie Rose that losing MySpace had been "humiliating", adding, "MySpace was sitting there for the taking for $500 million" (Myspace was sold in 2012 by News Corp for $35 million.) [30]
In January 2006, Fox announced plans to launch a UK version of Myspace in a bid to "tap into the UK music scene", which they did. [31] They released a version in China and launched similar versions in other countries.
The 100 millionth account was created on August 9, 2006, in the Netherlands. [32]
On November 1, 2007, Myspace and Bebo joined the Google -led OpenSocial alliance , which already included Friendster, Hi5 , LinkedIn , Plaxo , Ning and Six Apart . OpenSocial was to promote a common set of standards for software developers to write programs for social networks. Facebook remained independent. Google had been unsuccessful in building its own social networking site Orkut in the U.S. market and was using the alliance to present a counterweight to Facebook. [33] [34] [35] [36]
By late 2007 and into 2008, Myspace was considered the leading social networking site, and consistently beat out main competitor Facebook in traffic. Initially, the emergence of Facebook did little to diminish Myspace's popularity; at the time, Facebook was targeted only at college students. At its peak, when News Corp attempted to merge it with Yahoo! in 2007, Myspace was valued at $12 billion. [37] [38]

2008–2012: Decline and sale by News Corp.
On April 19, 2008, Facebook overtook Myspace in the Alexa rankings. [39] [40] Since then, Myspace has seen a continuing loss of membership, and there are several suggestions for its demise, including the fact that it stuck to a "portal strategy" of building an audience around entertainment and music, whereas Facebook and Twitter continually added new features to improve the social-networking experience. [41] [42]
Marvin L. Gittelman suggested that the $900 million three-year advertisement deal with Google, while being a short-term cash windfall, was a handicap in the long run. That deal required Myspace to place even more ads on its already heavily advertised space, which made the site slow, more difficult to use, and less flexible. Myspace could not experiment with its own site without forfeiting revenue, while rival Facebook was rolling out a new clean site design. [43] [44] MySpace CEO Chris DeWolfe reported that he had to push back against Fox Interactive Media's sales team who monetized the site without regard to user experience. [12]
While Facebook focused on creating a platform that allowed outside developers to build new applications, Myspace built everything in-house. Shawn Gold, Myspace's former head of marketing and content, said "Myspace went too wide and not deep enough in its product development. We went with a lot of products that were shallow and not the best products in the world". The products division had introduced many features (communication tools such as instant messaging, a classifieds program, a video player, a music player, a virtual karaoke machine, a self-serve advertising platform, profile-editing tools, security systems, privacy filters, and Myspace book lists, among others). However, the features were often buggy and slow as there was insufficient testing, measuring, and iterating. [12]
Danah Boyd , a senior researcher at Microsoft Research , noted of social networking websites that Myspace and others were a very peculiar business—one in which companies might serially rise, fall, and disappear, as "Influential peers pull others in on the climb up—and signal to flee when it's time to get out". The volatility of social networks was exemplified in 2006 when Connecticut Attorney General Richard Blumenthal launched an investigation into children's exposure to pornography on Myspace; the resulting media frenzy and Myspace's inability to build an effective spam filter gave the site a reputation as a "vortex of perversion". Around that time, specialized social media companies such as Twitter formed and began targeting Myspace users, while Facebook rolled out communication tools which were seen as safe in comparison to Myspace. Boyd compared the shift of white, middle-class kids from the "seedy" Myspace to the "supposedly safer haven" of Facebook, to the " white flight " from American cities; the perception of Myspace eventually drove advertisers away as well. [12] In addition, Myspace had particular problems with vandalism, phishing, malware and spam which it failed to curtail, making the site seem inhospitable. [45]
These have been cited as factors why users, who as teenagers were Myspace's strongest audience in 2006 and 2016, [46] [47] had been migrating to Facebook. Facebook, which started strong with the 18-to-24 group (mostly college students), [48] has been much more successful than Myspace at attracting elderly men. [49] [50] [51]
Chairman and CEO Rupert Murdoch was said to be frustrated that Myspace never met expectations, as a distribution outlet for Fox studio content, and missing the US$1 billion mark in total revenues. [52] That resulted in DeWolfe and Anderson gradually losing their status within Murdoch's inner circle of executives, plus DeWolfe's mentor Peter Chernin , the President and COO of News Corp. who was based in Los Angeles, departed the company. Former AOL executive Jonathan Miller, who joined News Corp in charge of the digital media business, was in the job for three weeks when he shuffled Myspace's executive team in April 2009. Myspace President Tom Anderson stepped down while Chris DeWolfe was replaced as Myspace CEO by former Facebook COO Owen Van Natta . [53] [54] A News Corp. meeting in March 2009 over the direction of Myspace was reportedly the catalyst for that management shakeup, with the Google search deal about to expire, the departure of key personnel (Myspace's COO, SVP of engineering, and SVP of strategy) to form a startup. Furthermore, the opening of extravagant new offices around the world was questioned, as rival Facebook did not have similarly expensive expansion plans yet it still attracted international users at a rapid rate. [12] The changes to Myspace's executive ranks was followed in June 2009 by a layoff of 37.5% of its workforce (including 30% of its U.S. employees), reducing employees from 1,600 to 1,000. [12]
In 2009, around the time that Myspace underwent layoffs and a management shakeup, the site "relied on drastic redesigns as Hail Mary passes to get users back". However this may have backfired for Myspace, as it is noted that users generally disliked interface tweaks on rival Facebook (which avoided major site redesigns). [46] [55]
Myspace has attempted to redefine itself as a social entertainment website, with more of a focus on music, movies, celebrities, and TV, instead of a social networking website. Myspace also developed a linkup with Facebook that would allow musicians and bands to manage their Facebook profiles. CEO Mike Jones was quoted as saying that Myspace now is a "complementary offer" to Facebook Inc., which is "not a rival anymore". [38]
In March 2011, market research figures released by comScore suggested that Myspace had lost 10 million users between January and February 2011, and that it had fallen from 95 million to 63 million unique users during the previous twelve months. [56] Myspace registered its sharpest audience declines in the month of February 2011, as traffic fell 44% from a year earlier to 37.7 million unique U.S. visitors. Advertisers have been reported as unwilling to commit to long term deals with the site. [57]
In late February 2011, News Corp officially put the site up for sale, which was estimated to be worth $50–200 million. [58] Losses from last quarter of 2010 were $156 million, over double of the previous year, which dragged down the otherwise strong results of parent News Corp. [3] [59] The deadline for bids, May 31, 2011, passed without any above the reserve price of $100 million being submitted [60] It has been said that the rapid deterioration in Myspace's business during the most recent quarter deterred many potential suitors. [3]
On June 29, 2011, Myspace announced to label partners and press via email that it had been acquired by Specific Media for an undisclosed sum, rumoured to be a figure as low as $35m. [61] [62] CNN reported that Myspace sold for $35 million, and noted that it was "far less than the $580 million News Corp. paid for Myspace in 2005". [63] Rupert Murdoch went on to call the Myspace purchase a "huge mistake". [64] Time Magazine compared News Corporation's purchase of Myspace to Time Warner 's purchase of AOL – a conglomerate trying to stay ahead of the competition. [28] Many former executives have gone onto further success after departing Myspace. [65]

2012–2016: Relaunch
On September 24, 2012, Timberlake tweeted a link to a video that featured a redesigned Myspace, dubbed the "new Myspace". [66] Timberlake stated in an interview with the Hollywood Reporter that he believed he was "bridging the gap" between artists and their fan bases. [67]
On January 15, 2013, the new Myspace entered its publicly accessible open beta phase, featuring written editorial content, radio stations, music mixes and videos. [68] Music was streamed through a constant music player located at the bottom of the page, while musicians could track the location of their top fans, who were identified by the number of times they played the artist's music. Although the unveiling was purposefully scheduled on the same date as the release of Timberlake's new music single, the event was overshadowed by Facebook's announcement of its "graph search" function on the same day.
Christian Parkes, vice-president of global marketing, explained in a May 2013 interview that the redesign was undertaken with brands in mind:
The official launch of the new Myspace occurred on June 12, 2013, [69] and included the launch of a corresponding mobile app for the new Myspace, providing users access to streaming radio stations curated by artists and Myspace, as well as personal radio stations created by users themselves. The app's social features facilitate connections between users who possess similar interests, and users can also create animated GIF files, which can be shared on Myspace and other social platforms. The app was launched on Apple Inc. 's App Store, while a mobile website was also designed for those users without access to an iOS device. The newly designed platform also included new analytics tools for artists to manage their digital presence from a single location, and, at the time of the launch, the Myspace music catalog consisted of over 50 million songs. [69] [70]
As part of the discontinuation of the "Classic MySpace" and the launch of the new platform, the user content from the old MySpace was deleted. Myspace explained on its website that it would no longer feature "Blogs, Private Messages, Videos, Comments or Posts, Custom background design and Games," acknowledging that "this is upsetting to some." Myspace received a large amount of online complaints from users and eventually locked the primary discussion thread. The complaints described the loss of poems and personal notes, photos of dead friends, intimate messages, and games that cost significant amounts of time and money. [71]
In July 2013, Myspace revealed its new hires for editorial content: Joseph Patel, previously a producer at Vice , became the vice president of content and "creative," while editors Benjamin Meadows-Ingram ( Billboard ) and Monica Herrera ( Rolling Stone ) were subsequently recruited by Patel. [72] As of October 1, 2013, Myspace said it had 36 million users. [73]

2016–present: Time Inc. ownership
On February 11, 2016 it was announced that MySpace and its parent company had been bought by Time Inc. [18]
In May 2016, the data for almost 360 million MySpace accounts was offered on the "Real Deal" dark market website. The leaked data included email addresses, usernames and weakly encrypted passwords ( SHA1 hashes of the first 10 characters of the password converted to lowercase and stored without a cryptographic salt [74] ). [75] The exact data breach date is unknown, but analysis of the data suggests it was exposed eight years before being made public, in approximately 2008. [76]

Features
Since YouTube's founding in 2005, Myspace users have had the ability to embed YouTube videos in their Myspace profiles. Realizing the competitive threat to the new Myspace Videos service, Myspace banned embedded YouTube videos from its user profiles. Myspace users widely protested the ban, prompting Myspace to lift the ban shortly thereafter. [77]
There were a variety of environments in which users could access Myspace content on their mobile phone. American mobile phone provider Helio released a series of mobile phones in early 2006 that could utilize a service known as Myspace Mobile to access and edit one's profile and communicate with, and view the profiles of other members. [78] Additionally, UIEvolution and Myspace developed a mobile version of Myspace for a wider range of carriers, including AT&T , Vodafone [79] and Rogers Wireless . [80]
Full service classifieds listing offered beginning in August 2006. It has grown by 33 percent in one year since inception. Myspace Classifieds was launched right at the same time the site appeared on the internet. [81]
MySpace uses an implementation of Telligent Community for its forum system. [82]

Music
In late 2003, Fin Leavell encoded his personal music into a Myspace profile, becoming the first Myspace musician. [83]
Shortly after Myspace was sold to Rupert Murdoch , the owner of Fox News and 20th Century Fox, in 2005, they launched their own record label, MySpace Records , in an effort to discover unknown talent on Myspace Music. [23] Regardless of the artist already being famous or still looking for a break into the industry, artists can upload their songs onto Myspace and have access to millions of people on a daily basis. Some well known singers such as Lily Allen , Owl City , Hollywood Undead , Sean Kingston , Arctic Monkeys , Ice Nine Kills , and Drop Dead, Gorgeous gained fame through Myspace. The availability of music on this website continues to develop, largely driven by young talent. Over eight million artists have been discovered by Myspace and many more continue to be discovered daily. [84] In late 2007, the site launched The MySpace Transmissions , a series of live-in-studio recordings by well-known artists.

Redesigns

Past redesigns
On March 10, 2010, Myspace added some new features, like a recommendation engine for new users which suggests games, music and videos based on their previous search habits. The security on Myspace was also accounted to, with the criticism of Facebook , to make it a safer site. The security of Myspace enables users to choose if the content could be viewed for Friends Only, 18 and older, or Everyone. The website will also release several mobile micro applications for Myspace gamers besides sending them games alerts. The site may release 20 to 30 micro apps and go mobile in 2011. [85]
In October 2010, Myspace introduced a beta version of a new site design on a limited scale, with plans to switch all interested users to the new site in late November. Chief executive Mike Jones said the site is no longer competing with Facebook as a general social networking site. Instead, Myspace would be music-oriented and would target younger people. Jones believed most younger users would continue to use the site after the redesign, though older users might not. The goal of the redesign is to increase the number of Myspace users and how long they spend there. On October 26, BTIG analyst Richard Greenfield said, "Most investors have written off MySpace now", and he was unsure whether the changes would help the company recover. [86]
In November 2010, Myspace changed its logo to coincide with the new site design. The word "my" appears in the Helvetica font, followed by a symbol representing a space. The logo change was announced on October 8, 2010 and appeared on the site on November 11, 2010. [87] Also that month, MySpace integrated with Facebook Connect – calling it "Mash Up with Facebook" in an announcement widely seen as the final act of acknowledging Facebook's domination of the social networking industry. [88]
In January 2011, it was announced that the Myspace staff would be reduced by 47%. [89] Despite the new design, user adoption continued to decrease. [90]
In September 2012, a new redesign was announced (but no date given) making Myspace more visual and apparently optimized for tablets. [91]
By April 2013 (presumably before), users were able to transfer over to the new Myspace redesign.

Corporate information

International versions
Since early 2006, Myspace has offered the option to access the service in different regional versions. The alternative regional versions present automated content according to locality (e.g., UK users see other UK users as "Cool New People", and UK-oriented events and adverts, etc.), offer local languages other than English, or accommodate the regional differences in spelling and conventions in the English-speaking world (e.g., United States: "favorites", mm/dd/yyyy; the rest of the world: "favourites", dd/mm/yyyy).

MySpace Developer Platform (MDP)
On February 5, 2008, Myspace set up a developer platform which allows developers to share their ideas and write their own Myspace applications. The opening was inaugurated with a workshop at the MySpace offices in San Francisco two weeks before the official launch. The MDP is based on the OpenSocial API which was presented by Google in November 2007 to support social networks to develop social and interacting widgets and can be seen as an answer to Facebook's developer platform. The first public beta of the Myspace Apps was released on March 5, 2008, with around 1,000 applications available. [92] [93]

Myspace server infrastructure
At QCon London 2008, [94] Myspace Chief Systems Architect Dan Farino indicated that Myspace was sending 100 gigabits of data per second out to the Internet, of which 10 gigabits was HTML content and the remainder was media such as videos and pictures. The server infrastructure consists of over 4,500 web servers (running Windows Server 2003 , IIS 6.0, ASP.NET and .NET Framework 3.5), over 1,200 cache servers (running 64-bit Windows Server 2003), and over 500 database servers (running 64-bit Windows Server 2003 and SQL Server 2005) as well as a custom distributed file system which runs on Gentoo Linux .
As of 2009, Myspace has started migrating from HDD to SSD technology in some of their servers, resulting in space and power usage savings. [95]

Revenue model
Myspace operates solely on revenues generated by advertising as its revenue model possesses no user-paid features. [96] Through its Web site and affiliated ad networks, Myspace is second only to Yahoo! in its capacity to collect data about its users and thus in its ability to use behavioral targeting to select the ads each visitor sees. [97]
On August 8, 2006, search engine Google signed a $900 million deal to provide a Google search facility and advertising on Myspace. [98] [99] [100] Myspace has proven to be a windfall for many smaller companies that provide widgets or accessories to the social networking giant. Companies such as Slide.com , RockYou , and YouTube were all launched on Myspace as widgets providing additional functionality to the site. Other sites created layouts to personalize the site and made hundreds of thousands of dollars for its owners most of whom were in their late teens and early twenties. [101] [102]
In November 2008, Myspace announced that user-uploaded content that infringed on copyrights held by MTV and its subsidiary networks would be redistributed with advertisements that would generate revenue for the companies. [103]

Acquisition of Imeem
On November 18, 2009, Imeem was acquired by Myspace Music for an undisclosed amount. After the acquisition was completed on December 8, 2009, it was confirmed that Myspace Music bought Imeem for less than $1 million in cash. [104] Myspace has also stated that they will be transitioning Imeem's users, and migrating all their play lists over to Myspace Music. On January 15, 2010, Myspace began restoring Imeem playlists. [105]

Mobile application
Along with its website redesign, Myspace also completely redesigned their mobile application. The redesigned app in the Apple App Store was released in early June 2013. The program features a tool for users to create and edit gif images and post them to their Myspace stream. The app also allows users to stream available "live streams" of concerts. New users are able to join Myspace from the app by signing in with Facebook or Twitter or by signing up with email.

Availability
The Myspace mobile app is available in the Google Play store, but is no longer available in the Apple App Store . The mobile web app can be accessed by visiting Myspace.com from a mobile device.

Filters
The Myspace app offers ten filters that can be added when the user has just taken a photo or just created a GIF .

Radio
The app allows users to play Myspace radio channels from the device. Users can select from genre stations, featured stations, and user or artist stations. A user can build their own station by connecting and listening to songs on Myspace's desktop website. The user is given six skips per station.

See also
• Vine (service)
WebPage index: 00091
Freedom of panorama
Freedom of panorama ( FOP ) is a provision in the copyright laws of various jurisdictions that permits taking photographs and video footage and creating other images (such as paintings) of buildings and sometimes sculptures and other art works which are permanently located in a public place , without infringing on any copyright that may otherwise subsist in such works, and the publishing of such images. [1] [ better source needed ] Panorama freedom statutes or case law limit the right of the copyright owner to take action for breach of copyright against the creators and distributors of such images. It is an exception to the normal rule that the copyright owner has the exclusive right to authorize the creation and distribution of derivative works . The phrase is derived from the German term Panoramafreiheit ("panorama freedom").

Laws around the world
Many countries have similar provisions restricting the scope of copyright law in order to explicitly permit photographs involving scenes of public places or scenes photographed from public places. Other countries, though, differ widely in their interpretation of the principle. [1]

European Union
In the European Union , Directive 2001/29/EC provides for the possibility of member states having a freedom of panorama clause in their copyright laws, but does not require such a rule. [2] [3]
Panoramafreiheit is defined in article 59 of the German Urheberrechtsgesetz , [4] in section 62 of the United Kingdom Copyright, Designs and Patents Act 1988 , [5] and it exists in several other countries [6] or even "a large majority of Member States". [7]
There are also European countries such as Italy [8] and Iceland , [9] where there is no freedom of panorama at all. In Italy, despite many official protests [10] and a national initiative [11] led by the lawyer Guido Scorza and the journalist Luca Spinelli (who highlighted the issue), [8] the publishing of photographic reproductions of public places is still prohibited, in accordance with the old Italian copyright laws [12] [13] made more restrictive by a law called Codice Urbani which states, among other provisions, that to publish pictures of "cultural goods" (meaning in theory every cultural and artistic object and place) for commercial purposes it is mandatory to obtain an authorization from the local branch of the Ministry of Arts and Cultural Heritage, the Soprintendenza .

Litigation
An example of litigation due to the heterogeneous EU legislation is the Hundertwasserentscheidung (Hundertwasser decision), a case won by Friedensreich Hundertwasser in Germany against a German company for use of a photo of an Austrian building. [14]
While not related to copyright, but to Codice Urbani , a case which reached a national supreme court ( Corte di Cassazione ) is Ministero dei Beni e delle Attività Culturali vs. Stoneage S.r.l. (Cass. civ. Sez. VI - 1 Ordinanza, 23-04-2013, n. 9757, rv. 626365).
On 4 April 2016 the Swedish Supreme Court ruled that Wikimedia Sweden infringed on the copyright of artists of public artwork by creating a website and database of public artworks in Sweden, containing images of public artwork uploaded by the public. [15] [16] [17] Swedish copyright law contains an exception to the copyright holder's exclusive right to make their works available to the public that allows depictions of public artwork. [18] :2-5 The Swedish Supreme Court decided to take a restrictive view of this copyright exception. [18] :6 The Court determined that the database was not of insignificant commercial value, for both the database operator or those accessing the database, and that "this value should be reserved for the authors of the works of art. Whether the operator of the database actually has a commercial purpose is then irrelevant." [18] :6 The case was returned to a lower court to determine damages that Wikimedia Sweden owes to the collective rights management agency Bildkonst Upphovsrätt i Sverige (BUS), which initiated the lawsuit on behalf of artists they represent. [18] :2,7

France
Since 7 October 2016, article L122-5 of the French Code of Intellectual Property provides for a limited freedom of panorama for works of architecture and sculpture. The code authorizes "reproductions and representations of works of architecture and sculpture, placed permanently in public places ( voie publique ), and created by natural persons , with the exception of any usage of a commercial character". [19]
The limits to freedom of panorama in France have a drastic effect on Wikipedia articles about French architecture. Wikimedia Commons editors routinely delete any images of recent French architecture, despite the changes in the law. For example, on February 1, 2017 the image of the new Philharmonie de Paris concert hall by architect Jean Nouvel , opened in 2015, was deleted as a violation of freedom of panorama. There is no clear image of the Louis Vuitton Foundation building by Frank Gehry , opened in Paris in 2016; only a poor quality image where the building is unidentifiable, and another where it mostly hidden by trees. There is no image of the Le Corbusier Foundation building by French architect Le Corbusier , and no clear image of the Centre Pompidou Museum. [ relevant? – discuss ] [ citation needed ]

Australia
In Australia , freedom of panorama is dealt with in the federal Copyright Act 1968 , sections 65 to 68. Section 65 provides: "The copyright in a work ... that is situated, otherwise than temporarily, in a public place, or in premises open to the public, is not infringed by the making of a painting, drawing, engraving or photograph of the work or by the inclusion of the work in a cinematograph film or in a television broadcast". This applies to any "artistic work" as defined in paragraph (c) of section 10: a "work of artistic craftsmanship" (but not a circuit layout). [20]
However, " street art " may be protected by copyright. [21] [22] [23]
Section 66 of the Act provides exceptions to copyright infringement for photos and depictions of buildings and models of buildings. [20]

Canada
Section 32.2(1) of the Copyright Act (Canada) states the following:
The Copyright Act also provides specific protection for the incidental inclusion of another work seen in the background of a photo. Photos that "incidentally and not deliberately" include another work do not infringe copyright.

United States
United States copyright law contains the following provision:
The definition of "architectural work" is a building, [25] which is defined as "humanly habitable structures that are intended to be both permanent and stationary, such as houses and office buildings, and other permanent and stationary structures designed for human occupancy, including but not limited to churches, museums, gazebos, and garden pavilions". [26] This freedom of panorama for buildings does not apply to art, however. [27]

Former USSR
Almost all countries from the former Soviet Union lack freedom of panorama. Exceptions are three countries whose copyright laws were amended recently. The first was Republic of Moldova in July 2010, when the law in question was approximated to EU standards. [28] Armenia followed in April 2013 with an updated Armenian law on copyright . [29] Freedom of panorama was partially adopted in Russia on October 1, 2014; from this day, one is allowed to take photos of buildings and gardens visible from public places, but that does not include sculptures and other 3-dimensional works. [30]

Two-dimensional works
The precise extent of this permission to make pictures in public places without having to worry about copyrighted works being in the image differs amongst countries. [1] In most countries, it applies only to images of three-dimensional works [31] that are permanently installed in a public place, "permanent" typically meaning "for the natural lifetime of the work". [32] [33] In Switzerland, even taking and publishing images of two-dimensional works such as murals or graffiti is permitted, but such images cannot be used for the same purpose as the originals. [32]

Public space
Many laws have subtle differences in regard to public space and private property. Whereas the photographer's location is irrelevant in Austria, [1] in Germany the permission applies only if the image was taken from public ground, and without any further utilities such as ladders, lifting platforms, airplanes etc. [4] Under certain circumstances, the scope of the permission is also extended to actually private grounds, e.g. to publicly accessible private parks and castles without entrance control, however with the restriction that the owner may then demand a fee for commercial use of the images. [34]
In many Eastern European countries the copyright laws limit this permission to non-commercial uses of the images only. [35]
There are also international differences in the particular definition of a "public place". In most countries, this includes only outdoor spaces (for instance, in Germany), [4] while some other countries also include indoor spaces such as public museums (this is for instance the case in the UK [5] and in Russia ). [36]
There has been a controversy among Filipino photographers and establishment managements. On June 12, 2013, Philippine Independence Day , pro-photography group, Bawal Mag-Shoot dito, launched at the Freedom to Shoot Day protest at Luneta Park . The group is protesting for their right to take photos on historical and public places, especially in Luneta and Intramuros . The park management imposes a fee for D-SLR photographers to shoot images for commercial purposes but it was also reported that security guards also charge 500 pesos to shoot photos even for non-commercial purposes, an act which the advocacy group branded as "extortion". The group also claimed that there is discrimination against Filipino photographers and claimed that the management is lenient on foreign photographers. There is no official policy on taking photographs of historical places and the group has called legislators to create a law on the matter. [37]

Anti-terrorism laws
Tension has arisen in countries where freedom to take pictures in public places conflicts with more recent anti-terrorism legislation . In the United Kingdom, the powers granted to police under section 44 of the Terrorism Act 2000 have been used on numerous occasions [ citation needed ] to stop amateur and professional photographers from taking photographs of public areas. Under such circumstances, police are required to have "reasonable suspicion" that a person is a terrorist. [38] While the Act does not prohibit photography, critics have alleged that these powers have been misused to prevent lawful public photography. [39] Notable instances have included the investigation of a schoolboy, [40] a Member of Parliament [41] and a BBC photographer. [42] [43] The scope of these powers has since been reduced, and guidance around them issued to discourage their use in relation to photography, following litigation in the European Court of Human Rights . [44]

See also
WebPage index: 00092
Reuters
Reuters / ˈ r ɔɪ t ər z / is an international news agency headquartered in London , England . It is a division of Thomson Reuters .
Until 2008, the Reuters news agency formed part of an independent company, Reuters Group plc , which was also a provider of financial market data. Since the acquisition of Reuters Group by the Thomson Corporation in 2008, the Reuters news agency has been a part of Thomson Reuters, making up the media division. Reuters transmits news in English , French , Arabic , Spanish , German , Italian , Portuguese , Russian , Japanese , Korean , Urdu , and Chinese . It was established in 1851.

History

Nineteenth century
The Reuter agency was established in 1851 by Paul Julius Reuter in Britain at the London Royal Exchange . Paul Reuter worked at a book-publishing firm in Berlin and was involved in distributing radical pamphlets at the beginning of the Revolutions in 1848 . These publications brought much attention to Reuter, who in 1850 developed a prototype news service in Aachen using homing pigeons and electric telegraphy from 1851 on in order to transmit messages between Brussels and Aachen. [2]
Upon moving to England, he founded Reuter's Telegram Company in 1851. Headquartered in London, the company initially covered commercial news, serving banks, brokerage houses, and business firms. [2] The first newspaper client to subscribe was the London Morning Advertiser in 1858. [2] [3] Afterwards more newspapers signed up, with Britannica Encyclopedia writing that "the value of Reuters to newspapers lay not only in the financial news it provided but in its ability to be the first to report on stories of international importance." [2] Reuter's agency built a reputation in Europe and the rest of the world as the first to report news scoops from abroad. [4] Reuters was the first to report Abraham Lincoln 's assassination in Europe, for instance, in 1865. [2] [4] In 1872, Reuters expanded into the far east, followed by South America in 1874. Both expansions were made possible by advances in overland telegraphs and undersea cables. [4] In 1883, Reuters began transmitting messages electrically to London newspapers. [4]

1900s
In 1923, Reuters began using radio to transmit news internationally, a pioneering act. [4] In 1925, The Press Association (PA) of Great Britain acquired a majority interest in Reuters, and full owners some years later. [2] During the world wars, The Guardian reported that Reuters "came under pressure from the British government to serve national interests. In 1941 Reuters deflected the pressure by restructuring itself as a private company." The new owners formed the Reuters Trust. [4] In 1941, the PA sold half of Reuters to the Newspaper Proprieters' Association, and co-ownership was expanded in 1947 to associations that represented daily newspapers in New Zealand and Australia . [2] The Reuters Trust Principles were put in place to maintain the company's independence. [1] At "that point, Reuters had become "one of the world's major news agencies, supplying both text and images to newspapers, other news agencies, and radio and television broadcasters." [2] Also at that point, it directly or through national news agencies provided service "to most countries, reaching virtually all the world's leading newspapers and many thousands of smaller ones," according to Brittanica . [2]
In 1961, Reuters scooped news of the erection of the Berlin Wall . [5] Becoming one of the first news agencies to transmit financial data over oceans via computers in the 1960s, [2] in 1973 Reuters "began making computer-terminal displays of foreign-exchange rates available to clients." [2] In 1981, Reuters began making electronic transactions on its computer network, and afterwards developed a number of electronic brokerage and trading services. [2] Reuters was floated as a public company in 1984, [5] when Reuters Trust was listed on the stock exchanges [4] such as the London Stock Exchange (LSE) and NASDAQ . [2] Reuters published the first story of the Berlin Wall being breached in 1989. [5]

2000s
Share price grew during the dotcom boom , then fell after the banking troubles in 2001. [4] In 2002, Brittanica wrote that most news throughout the world came from three major agencies: the Associated Press , Reuters, and Agence France-Presse . [6] Reuters merged with Thomson Corporation in Canada in 2008, forming Thomson Reuters. [2] In 2009, Thomson Reuters withdrew from the LSE and the NASDAQ, instead listing its shares on the Toronto Stock Exchange and the New York Stock Exchange . [2] The last surviving member of the Reuters family founders, Marguerite, Baroness de Reuter , died at age 96 on 25 January 2009. [7] As of 2010, Reuters was headquartered in New York City, and provided financial information to clients while also maintaining its traditional news-agency business. [2]
In 2012, Thomson Reuters appointed Jim Smith as CEO. [1] Almost every major news outlet in the world subscribed to Reuters as of 2014. Reuters operated in more than 200 cities in 94 countries in about 20 languages as of 2014. [ citation needed ] In July 2016, Thomson Reuters agreed to sell its intellectual property and science operation for $3.55 billion to private equity firms. [8] In October 2016, Thomson Reuters announced expansions and relocations to Toronto . [8] As part of cuts and restructuring, in November 2016, Thomson Reuters Corp. eliminated 2,000 worldwide jobs out of its around 50,000 employees. [8]

Journalists
The Reuters News Agency employs some 2,500 journalists and 600 photojournalists in about 200 locations worldwide. Reuters journalists use the Reuters Handbook of Journalism [9] as a guide for fair presentation and disclosure of relevant interests, to maintain the values of integrity and freedom upon which their reputation for reliability, accuracy, speed and exclusivity relies. [9]
In May 2000, Kurt Schork , an American reporter , was killed in an ambush while on assignment in Sierra Leone . In April and August 2003, news cameramen Taras Protsyuk and Mazen Dana were killed in separate incidents by U.S. troops in Iraq . In July 2007, Namir Noor-Eldeen and Saeed Chmagh were killed when they were struck by fire from a U.S. military Apache helicopter in Baghdad. [10] [11] During 2004, cameramen Adlan Khasanov in Chechnya and Dhia Najim in Iraq were also killed. In April 2008, cameraman Fadel Shana was killed in the Gaza Strip after being hit by an Israeli tank . [12]
The first Reuters journalist to be taken hostage [ dubious – discuss ] in action was Anthony Grey . Detained by the Chinese government while covering China's Cultural Revolution in Peking in the late 1960s, it was said to be in response to the jailing of several Chinese journalists by the colonial British government of Hong Kong . [13] He was considered to be the first political hostage of the modern age and was released after being imprisoned for 27 months from 1967 to 1969. Awarded an OBE by the British Government after his release, he went on to become a best-selling historical novelist.
In May 2016 the Ukrainian website Myrotvorets published the names and personal data of 4,508 journalists, including Reuters reporters, and other media staff from all over the world, who were accredited by the self-proclaimed authorities in the separatist -controlled regions of eastern Ukraine . [14]

Fatalities

Criticism and controversy

Policy of objective language
Reuters has a policy of taking a "value-neutral approach," which extends to not using the word "terrorist" in its stories, a practice which has attracted criticism following the September 11 attacks . [16] Reuters' editorial policy states: "We are committed to reporting the facts and in all situations avoid the use of emotive terms. The only exception is when we are quoting someone directly or in indirect speech." [17] (The Associated Press , by contrast, does use the term "terrorist" in reference to non-governmental organizations who carry out attacks on civilian populations. [16] )
Following the September 11 attacks, Reuters global head of news Stephen Jukes reiterated the policy in an internal memo and later explained to media columnist Howard Kurtz (who criticized the policy): "We all know that one man's terrorist is another man's freedom fighter, and that Reuters upholds the principle that we do not use the word terrorist...We're trying to treat everyone on a level playing field, however tragic it's been and however awful and cataclysmic for the American people and people around the world. We're there to tell the story. We're not there to evaluate the moral case." [16]
In early October 2001, CEO Tom Glocer and editor-in-chief Geert Linnebank and Jukes later released a statement acknowledging that Jukes' memo "had caused deep offence among members of our staff, our readers, and the public at large" and wrote: "Our policy is to avoid the use of emotional terms and not make value judgments concerning the facts we attempt to report accurately and fairly. We apologize for the insensitive manner in which we characterized this policy and extend our sympathy to all those who have been affected by these tragic events." [18]
In September 2004, The New York Times reported that Reuters global managing editor, David A. Schlesinger objected to Canadian newspapers' editing of Reuters articles to insert the word terrorist . Schlesinger said: "my goal is to protect our reporters and protect our editorial integrity." [19]

Climate change reporting
In July 2013, David Fogarty, former Reuters climate change correspondent in Asia, resigned after a career of almost 20 years with the company and wrote about a "climate of fear" which resulted in "progressively, getting any climate change-themed story published got harder" following comments from then deputy editor-in-chief Paul Ingrassia that he was a " climate change sceptic ". In his comments, Fogarty stated that "Some desk editors happily subbed and pushed the button. Others agonised and asked a million questions. Debate on some story ideas generated endless bureaucracy by editors frightened to take a decision, reflecting a different type of climate within Reuters—the climate of fear," and that "by mid-October, I was informed that climate change just wasn't a big story for the present. …Very soon after that conversation I was told my climate change role was abolished." [20] [21] Ingrassia, currently Reuters' managing editor, formerly worked for The Wall Street Journal and Dow Jones for 31 years. [22] Reuters responded to Fogarty's piece by stating that "Reuters has a number of staff dedicated to covering this story, including a team of specialist reporters at Point Carbon and a columnist. There has been no change in our editorial policy." [23]
Subsequently, climate blogger Joe Romm cited a Reuters article on climate as employing " false balance ", and quoted Dr. Stefan Rahmstorf, Co-Chair of Earth System Analysis at the Potsdam Institute that "[s]imply, a lot of unrelated climate skeptics nonsense has been added to this Reuters piece. In the words of the late Steve Schneider, this is like adding some nonsense from the Flat Earth Society to a report about the latest generation of telecommunication satellites. It is absurd." Romm opined that "We can't know for certain who insisted on cramming this absurd and non-germane 'climate sceptics nonsense' into the piece, but we have a strong clue. If it had been part of the reporter's original reporting, you would have expected direct quotes from actual skeptics, because that is journalism 101. The fact that the blather was all inserted without attribution suggests it was added at the insistence of an editor." [24]

Photograph controversies
According to Ynetnews , Reuters was accused of bias against Israel in its coverage of the 2006 Israel–Lebanon conflict after the wire service used two doctored photos by a Lebanese freelance photographer, Adnan Hajj. [25] In August 2006, Reuters announced it had severed all ties with Hajj and said his photographs would be removed from its database. [26]
In 2010, Reuters was criticised again by Haaretz for "anti-Israeli" bias when it cropped the edges of photos, removing commandos' knives held by activists and a naval commando's blood from photographs taken aboard the Mavi Marmara during the Gaza flotilla raid , a raid that left nine Turkish activists dead. It has been alleged that in two separate photographs, knives held by the activists were cropped out of the versions of the pictures published by Reuters. [27] Reuters said it is standard operating procedure to crop photos at the margins, and replaced the cropped images with the original ones after it was brought to the agency's attention. [27]

Accusations of pro-Fernando Henrique Cardoso bias
In March 2015, the Brazilian affiliate of Reuters released a text containing an interview with Brazilian ex-president Fernando Henrique Cardoso about the ongoing Petrobrás scandal . One of the paragraphs mentioned a comment by a former Petrobrás manager, in which he suggests corruption in that company may date back to Cardoso's presidency. Attached to it, there was a comment between parenthesis: " Podemos tirar se achar melhor " ("we can take it out if [you] think it's better"), [28] which is now absent from the current version of the text. [29] The agency later issued a text in which they confirm the mistake, explaining it was a question by one of the Brazilian editors to the journalist who wrote the original text in English, and that it was not supposed to be published. [30]

See also
WebPage index: 00093
Prix Ars Electronica
The Prix Ars Electronica is one of the best known and longest running yearly prizes in the field of electronic and interactive art , computer animation , digital culture and music. It has been awarded since 1987 by Ars Electronica ( Linz , Austria ).
In 2005, the Golden Nica, the highest prize, was awarded in six categories: "Computer Animation/Visual Effects," "Digital Musics," "Interactive Art," "Net Vision," "Digital Communities" and the "u19" award for "freestyle computing." Each Golden Nica came with a prize of € 10,000, apart from the u19 category, where the prize was € 5,000. In each category, there are also Awards of Distinction and Honorary Mentions.
The Golden Nica is replica of the Greek Nike of Samothrace . It is a handmade wooden statuette, plated with gold, so each trophy is unique: approximately 35 cm high, with a wingspan of about 20 cm, all on a pedestal. "Prix Ars Electronica" is a phrase composed of French, Latin and Spanish words, loosely translated as "Electronic Arts Prize."

Golden Nica winners

Computer animation / film / vfx
The "Computer Graphics" category (1987–1994) was open to different kinds of computer images. The "Computer Animation" (1987–1997) was replaced by the current "Computer Animation/Visual Effects" category in 1998. New York artist and musician John Fekner received honorary awards for Concrete People and The Last Days of Good and Evil in 1987 and 1988.

Computer Graphics

Computer Animation

Computer Animation/Visual Effects

Digital Music
This category is for those making electronic music and sound art through digital means. From 1987 to 1998 the category was known as " Computer music ." Two Golden Nicas were awarded in 1987, and none in 1990. There was no Computer Music category in 1991.

Hybrid art

[the next idea] voestalpine Art and Technology Grant

Interactive Art
Prizes in the category of interactive art have been awarded since 1990. This category applies to many categories of works, including installations and performances, characterized by audience participation, virtual reality, multimedia and telecommunication.

Internet-related categories
In the categories "World Wide Web" (1995 – 96) and ".net" (1997 – 2000), interesting web-based projects were awarded, based on criteria like web-specificity, community-orientation, identity and interactivity . In 2001, the category became broader under the new name "Net Vision / Net Excellence", with rewards for innovation in the online medium.

World Wide Web

.net

Net Vision / Net Excellence

Digital Communities
A category begun in 2004 with support from SAP (and a separate ceremony in New York City two months before the main Ars Electronica ceremony) to celebrate the 25th birthday of Ars Electronica. Two Golden Nicas were awarded.
WebPage index: 00094
Peter Gabriel
Peter Brian Gabriel (born 13 February 1950) is an English singer-songwriter, record producer, and humanitarian who rose to fame as the original lead singer and flautist of the progressive rock band Genesis . [1] After leaving Genesis in 1975, [7] [8] Gabriel launched a solo career with " Solsbury Hill " as his first single. His 1986 album, So , is his best-selling release and is certified triple platinum in the UK and five times platinum in the U.S. [9] [10] The album's most successful single, " Sledgehammer ", won a record nine MTV Awards at the 1987 MTV Video Music Awards and remains the most played music video in the history of MTV. [11]
Gabriel has been a champion of world music for much of his career. He co-founded the WOMAD festival in 1982. [12] He has continued to focus on producing and promoting world music through his Real World Records label. He has also pioneered digital distribution methods for music, co-founding OD2 , one of the first online music download services. [13] Gabriel has also been involved in numerous humanitarian efforts. In 1980, he released the anti- apartheid single " Biko ". [12] He has participated in several human rights benefit concerts, including Amnesty International 's Human Rights Now! tour in 1988, and co-founded the Witness human rights organisation in 1992. [12] Gabriel developed The Elders with Richard Branson , which was launched by Nelson Mandela in 2007. [14]
Gabriel has won three Brit Awards —winning Best British Male in 1987, [15] six Grammy Awards , [16] thirteen MTV Video Music Awards , the first Pioneer Award at the BT Digital Music Awards , [17] the Q magazine Lifetime Achievement, [18] the Ivor Novello Award for Lifetime Achievement, [19] and the Polar Music Prize . [20] He was made a BMI Icon at the 57th annual BMI London Awards for his "influence on generations of music makers". [21]
In recognition of his many years of human rights activism, he received the Man of Peace award from the Nobel Peace Prize laureates, [22] and Time magazine named him one of the 100 most influential people in the world. [23] AllMusic has described Gabriel as "one of rock's most ambitious, innovative musicians, as well as one of its most political". [24] He was inducted into the Rock and Roll Hall of Fame as a member of Genesis in 2010, [25] followed by his induction as a solo artist in 2014. [26] In March 2015, he was awarded an honorary doctorate from the University of South Australia in recognition of his achievements in music.

Early life
Peter Brian Gabriel was born in Chobham , Surrey . [27] His father, Ralph Parton Gabriel (1912–2012), was an electrical engineer , and his mother, Edith Irene ( née Allen), who was from a musical family, taught him to play the piano at an early age. [28] His great-great-great-uncle, Sir Thomas Gabriel, 1st Baronet , served as Lord Mayor of London from 1866 to 1877. [28] Gabriel attended Cable House, a private primary school in Woking ; St. Andrews Prep School in Horsell ; and Charterhouse School in Godalming from 1963. He played drums in his first rock bands, and Mike Rutherford commented in 1985 that "Pete was—and still is, I think—a frustrated drummer". [29]

Genesis
Gabriel founded Genesis in 1967 with fellow Charterhouse School pupils Tony Banks , Anthony Phillips , Mike Rutherford , and drummer Chris Stewart . The name of the band was suggested by fellow Charterhouse alumnus, the pop music impresario Jonathan King , who produced their first album, From Genesis to Revelation .
Gabriel has said to be influenced by many different sources in his way of singing, such as Family lead singer Roger Chapman and theatrical singer Arthur Brown . In 1970, he played the flute on the Cat Stevens album, Mona Bone Jakon .
Genesis drew some attention in Britain and eventually also in Italy, Belgium, Germany, and other European countries, largely due to Gabriel's flamboyant stage presence, which involved numerous bizarre costume changes and comical, dreamlike stories told as the introduction to each song (originally Gabriel developed these stories solely to cover the time between songs that the rest of the band would take tuning their instruments [30] and fixing technical glitches). The concerts made extensive use of black light with the normal stage lighting subdued or off. A backdrop of fluorescent white sheets and a comparatively sparse stage made the band into a set of silhouettes, with Gabriel's fluorescent costume and make-up providing the only other sources of light.

Costumes
Early Genesis concerts were hampered by a bad public address system that made it difficult for audiences to understand what Gabriel was singing. According to Mike Rutherford, this drove Gabriel to find other ways to impress his personality on the audience, leading to his performing in various costumes. [29]
In an episode of the 2007 British documentary series Seven Ages of Rock , Steve Hackett recalled the first appearance of Gabriel "in costume". It was the dress-wearing, fox-headed entity immortalised on the cover of Foxtrot . Hackett and the rest of the band had no inkling that Gabriel was going to do this, and at the time Hackett worried that it would ruin the performance. It was a success, encouraging Gabriel to continue wearing stage clothes while singing.
Among Gabriel's many famous costumes, which he developed to visualise the musical ideas of the band as well as to gain press coverage, were "Batwings" for the band's usual opening number, " Watcher of the Skies ". Other costumes included "The Flower" and "Magog", which were both alternately worn for " Supper's Ready " from the album Foxtrot . "Britannia" was worn for " Dancing with the Moonlit Knight ", and "The Reverend" for " The Battle of Epping Forest " (both from Selling England by the Pound ). "The Old Man" was worn for " The Musical Box " from Nursery Cryme . "The Slipperman" and "Rael" were worn during "The Colony of Slippermen", in which "Rael" was the protagonist of the album The Lamb Lies Down on Broadway .

Departure
Gabriel's departure from Genesis on 15 August 1975 [7] [8] —which stunned fans of the group and left many commentators wondering if the band could survive—was the result of several factors. His stature as the lead singer of the band and the added attention garnered by his flamboyant stage persona led to tensions within the band. [30] Genesis had always operated more or less as a collective, and Gabriel's burgeoning public profile led to fears within the group that he was being unfairly singled out as the creative hub. The band also began to feel confined by the reputation (and fans' expectations) attached to their famously elaborate theatrical performances, believing that the visual element of their performances was receiving more attention than their music.
Tensions were heightened by the ambitious album and tour of the concept work The Lamb Lies Down on Broadway , a Gabriel-created concept piece that saw him taking on the lion's share of the lyric writing. During the writing and recording of The Lamb Lies Down on Broadway , Gabriel was approached by director William Friedkin , allegedly because Friedkin had found Gabriel's short story in the liner notes to Genesis Live interesting. Gabriel left Genesis to pursue a film project with Friedkin, only to rejoin a week later. [29] The decision to quit the band was made before the tour supporting The Lamb Lies Down on Broadway , with Gabriel explaining his decision to the band while keeping it from the press until the conclusion of that tour. Bassist Mike Rutherford recalled that they all "could see it coming". [29] Although tensions were high, both Gabriel and the remaining members of Genesis have stated publicly that Gabriel left the band on good terms.
The breaking point came with the difficult pregnancy of Gabriel's wife, Jill, and the subsequent birth of their first child, Anna-Marie. When he opted to stay with his sick daughter and wife, rather than record and tour, the resentment from the rest of the band led Gabriel to conclude that he had to leave the group. " Solsbury Hill ", Gabriel's debut single as a solo artist, recorded in 1976 and appearing on the "Car" album in 1977, was written specifically about his departure from Genesis. The song reached the Top 20 in the UK Singles Chart, and also charted on the Billboard Hot 100 in 1978, reaching No. 68. [31] In 1982, Gabriel reunited with his former Genesis colleagues for the one-off concert Six of the Best , to recoup debts that arose from his involvement in the staging of the first WOMAD concert.

Solo career
Gabriel did not title his first four solo albums, which were all labelled Peter Gabriel using the same typeface, but which featured different cover designs (by Hipgnosis ); in all of these designs, Gabriel's face is wholly or partially obscured in some way. The albums are usually differentiated by number in order of release ( I, II, III, IV ), or by sleeve design, with the first three solo albums often referred to as Car , Scratch , and Melt respectively, in reference to their cover artwork. His fourth solo album, also called Peter Gabriel , was titled Security in the U.S. at the behest of Geffen Records. For many years, Gabriel was managed by Gail Colson . [32]
After acquiescing to distinctive titles, Gabriel used a series of two-letter words to title his next three albums: So , Us , and Up . His most recent greatest hits compilation is titled Hit ; within the two-CD package, disc one is labelled "Hit" and disc two is labelled "Miss", an echo of the 1996 compilations by Joni Mitchell entitled Hits and Misses .

1976–1985: 
Gabriel recorded his first self-titled solo album in 1976 and 1977 with producer Bob Ezrin . His first solo success came with the single " Solsbury Hill ", an autobiographical piece about a personal spiritual experience on top of the Iron Age hill fort , Solsbury Hill , in Somerset, England. Gabriel has said of the song's meaning, "It's about being prepared to lose what you have for what you might get ... It's about letting go." [33] Although mainly happy with the music, Gabriel felt that the album, and especially the track "Here Comes the Flood" was over-produced. Sparser versions can be heard on Robert Fripp 's Exposure , and on Gabriel's greatest hits compilation Shaking the Tree: Sixteen Golden Greats (1990).
Gabriel worked with guitarist Fripp as producer of his second solo LP , in 1978. This album was leaner, darker and more experimental, and yielded decent reviews, but no major hits.
Gabriel developed a new interest in world music (especially percussion), and for bold production, which made extensive use of recording tricks and sound effects. His third album is often credited as the first LP to use the now-famous " gated drum " sound. [34] Phil Collins played drums on several tracks, including the opener, "Intruder", which featured the gated reverbed , cymbal-less drum kit sound which Collins would also use on his single " In the Air Tonight ", becoming his signature sound in the 1980s. Gabriel had requested that his drummers use no cymbals in the album's sessions, and when he heard the result he asked Collins to play a simple pattern for several minutes, then built "Intruder" around it. The album achieved some chart success with the songs " Games Without Frontiers " (No. 4 UK, No. 48 U.S.), and " Biko ".
Arduous and occasionally damp recording sessions at his rural English estate in 1981 and 1982 resulted in Gabriel's fourth LP release , on which Gabriel took more production responsibility. It was one of the first commercial albums recorded entirely to digital tape (using a Sony mobile truck) and featured the early, extremely expensive, Fairlight CMI sampling computer, which had already made its first brief appearances on the previous album. Gabriel combined a variety of sampled and deconstructed sounds with world-beat percussion and other unusual instrumentation to create a radically new, emotionally charged soundscape. The sleeve art consisted of inscrutable, video-based imagery. Despite the album's peculiar sound, odd appearance, and often disturbing themes, it sold very well. This album featured his first Top 40 hit in the U.S., " Shock the Monkey ", as well as the song " I Have the Touch ". The music video for "Shock the Monkey", which featured Gabriel in white face paint and a caged macaque , received heavy play on MTV . Geffen Records gave his fourth self-titled album a name in the U.S., Security , to mark his arrival on the label and to differentiate the album from the first three.
Alternate versions of Gabriel's third and fourth albums were also released with German lyrics. The third album consisted of the studio recording overdubbed with new vocals, while the fourth album was also remixed and several tracks were extended or altered in slight ways.
Gabriel toured extensively for each of his albums. Initially, he pointedly eschewed the theatrics that had defined his tenure with Genesis. For his second solo tour, his entire band shaved their heads. By the time of the fourth album, he began involving elaborate stage props and acrobatics which had him suspended from gantries, distorting his face with Fresnel lenses and mirrors, and wearing unusual make-up. Recordings of the 1982 tour supporting his fourth Peter Gabriel album were released as the double LP Plays Live . Some of the dates of his 1983 summer tour of the U.S. and Canada included a section opening for David Bowie .
The stage was set for Gabriel's critical and commercial break-out with his next studio release, which was in production for almost three years. During the recording and production of the album, he also developed the film soundtrack for Alan Parker 's 1984 feature Birdy , which consisted of new material as well as remixed instrumental tracks from his previous studio album.

1985–2000: 
In 1985, Gabriel recorded his fifth studio album So . Released in 1986, Gabriel achieved his greatest popularity with songs from So ; [2] the album charted at No. 1 in the UK Albums Chart and No. 2 on the Billboard 200 in the U.S. [35] [36] It is certified triple platinum in the UK, and five times platinum in the U.S. [9] [10] The album produced three UK Top 20 hits, " Sledgehammer ", " Big Time ", and " Don't Give Up " – a duet with Kate Bush . [35] The album also produced three Top 40 hits in the U.S., "Sledgehammer", " In Your Eyes " (featured in the John Cusack film Say Anything ), and "Big Time". [36] "Sledgehammer" peaked at No. 1 in the United States, knocking Genesis' " Invisible Touch " off the top spot, and No. 4 in the UK. [36] The ballad "Don't Give Up" was about the devastation of unemployment. Gabriel co-produced So with Daniel Lanois , also known for his work with U2 and Brian Eno . [37] In 1990, Rolling Stone ranked So number No. 14 on its list of "Top 100 Albums of the Eighties". [38]
"Sledgehammer," which dealt specifically with the themes of sex and sexual relations through lyrical innuendos, was accompanied by a much-lauded music video, which was a collaboration with director Stephen R. Johnson , Aardman Animations , [11] and the Brothers Quay . The video set a new standard for art in the music video industry and won nine MTV Video Music Awards in 1987, a record which still stands as of 2015. [11] "Sledgehammer" is the most played music video in the history of MTV , [11] and in 1998, it was named the station's number one animated video of all time. [39] A follow-up video for the song " Big Time " also broke new ground in music video animation and special effects. The song is a story of "what happens to you when you become a little too successful", in Gabriel's words. The success of the album earned Peter Gabriel two awards at the Brit Awards in 1987 : Best British Male Solo Artist and Best British Video for "Sledgehammer". [15] Gabriel was also nominated for four Grammy Awards : Best Male Rock Vocal Performance , Song of the Year , Record of the Year (all three for "Sledgehammer"), and Album of the Year for So . [40]
In 1989, Gabriel released Passion , the soundtrack for Martin Scorsese 's movie The Last Temptation of Christ . For this work, he received his first Grammy Award , in the category of Best New Age Performance. He also received a Golden Globe nomination for Best Original Score – Motion Picture. The video that accompanied the album, ZAAR, was done by Stefan Roloff in his pioneering Moving Painting technique.
Gabriel released Us in 1992 (also co-produced with Lanois), an album in which he explored the pain of recent personal problems; his failed first marriage, and the growing distance between him and his first daughter.
Gabriel's introspection within the context of the album Us can be seen in the first single release " Digging in the Dirt " directed by John Downer. Accompanied by a disturbing video featuring Gabriel covered in snails and various foliage, this song made reference to the psychotherapy which had taken up much of Gabriel's time since the previous album. Gabriel describes his struggle to get through to his daughter in "Come Talk To Me" directed by Matt Mahurin , which featured backing vocals by Sinéad O'Connor . O'Connor also lent vocals to "Blood of Eden", directed by Nichola Bruce and Michael Coulson, the third single to be released from the album, and once again dealing with relationship struggles, this time going right back to Adam 's rib for inspiration. The result was one of Gabriel's most personal albums. It met with less success than So , reaching No. 2 in the album chart on both sides of the Atlantic, and making modest chart impact with the singles "Digging in the Dirt" and the funkier " Steam ", which evoked memories of "Sledgehammer". Gabriel followed the release of the album with a world tour (with Paula Cole or Joy Askew filling O'Connor's vocal role) and accompanying double CD and DVD Secret World Live in 1994.
Gabriel employed an innovative approach in the marketing of the Us album. Not wishing to feature only images of himself, he asked artist filmmakers Nichola Bruce and Michael Coulson to co-ordinate a marketing campaign using contemporary artists. Artists such as Helen Chadwick , Rebecca Horn , Nils-Udo , Andy Goldsworthy , David Mach and Yayoi Kusama collaborated to create original artworks for each of the 11 songs on the multi-million-selling CD. Coulson and Bruce documented the process on Hi-8 video. Bruce left Real World and Coulson continued with the campaign, using the documentary background material as the basis for a promotional EPK, the long-form video All About Us and the interactive CD-ROM Xplora1 .
Gabriel won three more Grammy Awards , all in the Music Video category. He won the Grammy Award for Best Short Form Music Video in 1993 and 1994 for the videos to "Digging in the Dirt" and "Steam" respectively. Gabriel also won the 1996 Grammy Award for Best Long Form Music Video for his Secret World Live video.

2000–present: 
Following a five-year hiatus, Gabriel re-emerged with OVO , a soundtrack for the live Millennium Dome Show in London in 2000, and Long Walk Home , the music from the Australian movie Rabbit-Proof Fence , early in 2002. This soundtrack also received a Golden Globe Award nomination for Best Original Score – Motion Picture.
In September 2002, Gabriel released Up , his first full-length studio album in a decade. Entirely self-produced, Up returned to some of the themes of his work in the late 1970s and early 1980s. Only one of the three singles managed to crack the top 50—in part because almost every track exceeded six minutes in length, with multiple sections—but the album sold well globally, as Gabriel continued to draw from a loyal fan base from his almost four decades in the music business. Up was followed by a world tour featuring his daughter Melanie Gabriel on backing vocals, and two concert DVDs, Growing Up Live (2003) and Still Growing Up: Live & Unwrapped (2004).
In 2008, Gabriel contributed to the WALL-E soundtrack several new songs with Thomas Newman , including the film's closing song, " Down to Earth ", for which they received the Grammy Award for Best Song Written for a Motion Picture, Television or Other Visual Media . The song was also nominated for the Golden Globe for Best Original Song – Motion Picture and the Academy Award for Best Original Song .
In 2010, Gabriel released Scratch My Back . The album is composed entirely of cover songs including material written by such artists as David Bowie , Lou Reed , Arcade Fire , Radiohead , Regina Spektor , and Neil Young . The concept for the record was that Gabriel covered songs by various artists, and those artists, in turn, covered Gabriel songs released on a follow-up album called And I'll Scratch Yours . [41] Scratch My Back features only orchestral instrumentation; there were no guitars, drums, or electronic elements that are usual attributes of Gabriel records. A brief tour followed the album's release where Gabriel performed with a full orchestra and two female backup singers, his daughter Melanie Gabriel and Norwegian singer-songwriter Ane Brun .
On 11 October 2011, Gabriel released New Blood , a collection of his earlier songs recorded with an orchestra. A special edition of the album features solely instrumental versions of some of the songs.
In Autumn 2012, Gabriel embarked on the Back to Front Tour in which he performed the entire So album with a band composed of the musicians who originally played on the record, to mark its 25th anniversary. [42] Following this tour, Gabriel took a sabbatical to spend time with his family. Early 2014 saw another Back to Front tour in Europe. [43] [44]
On 16 June 2016, Peter Gabriel released the single "I'm Amazing". The song was written several years prior, in part as a tribute to Muhammad Ali . [45] As such, the single was released two weeks after Ali's death 3 June.
On 21 June 2016, Peter Gabriel embarked on a joint tour with Sting entitled Rock Paper Scissors . [46] Each of the two musicians sang not only his own songs but also the songs by the other.

Musicians and collaborators as a solo artist
Gabriel has worked with a relatively stable crew of musicians and recording engineers throughout his solo career. Bass and Stick player Tony Levin performed on every Gabriel studio album and every live tour except for Scratch My Back , the soundtracks Passion and Long Walk Home , and the New Blood Tour. Guitar player David Rhodes has been Gabriel's guitarist of choice since 1979. Prior to So , Jerry Marotta was Gabriel's preferred drummer, both in the studio and on the road. (For the So and Us albums and tours Marotta was replaced by Manu Katché , who was then replaced by Ged Lynch on parts of the Up album and all of the subsequent tour). Gabriel is known for choosing top-flight collaborators, from co-producers such as Ezrin, Fripp, Lillywhite, and Lanois to musicians such as Natalie Merchant , Elizabeth Fraser , L. Shankar , Trent Reznor , Youssou N'Dour , Larry Fast , Nusrat Fateh Ali Khan , Sinéad O'Connor , Kate Bush , Ane Brun , Paula Cole , John Giblin , Peter Hammill , Papa Wemba , Manu Katché , Bayete , Milton Nascimento , Phil Collins , and Stewart Copeland .
Over the years, Gabriel has collaborated with singer Kate Bush several times; Bush provided backing vocals for Gabriel's "Games Without Frontiers" and "No Self Control" in 1980, and female lead vocal for "Don't Give Up" (a Top 10 hit in the UK) in 1986, and Gabriel appeared on her television special. Their duet of Roy Harper 's " Another Day " was discussed for release as a single, but never appeared. [ citation needed ]
He also collaborated with Laurie Anderson on two versions of her composition "Excellent Birds" – one for her 1984 album Mister Heartbreak , and a slightly different version called "This is the Picture (Excellent Birds)", which appeared on cassette and CD versions of So . In 1987, when presenting Gabriel with an award for his music videos, Anderson related an occasion in which a recording session had gone late into the night and Gabriel's voice had begun to sound somewhat strange, almost dreamlike. It was discovered that he had fallen asleep in front of the microphone, but had continued to sing. [ citation needed ]
Gabriel sang (along with Jim Kerr of Simple Minds ) on "Everywhere I Go", from The Call 's 1986 release, Reconciled . On Toni Childs ' 1994 CD, The Woman's Boat , Gabriel sang on the track, "I Met a Man". [ citation needed ]
In 1998, Gabriel appeared on the soundtrack of Babe: Pig in the City as the singer of the song "That'll Do", written by Randy Newman . The song was nominated for an Academy Award , and Gabriel and Newman performed it at the following year's Oscar telecast. He performed a similar soundtrack appearance for the 2004 film Shall We Dance? , singing a cover version of "The Book of Love" by The Magnetic Fields .
Gabriel appeared on Robbie Robertson 's self-titled album, singing on "Fallen Angel"; co-wrote two Tom Robinson singles; and appeared on Joni Mitchell 's 1988 album Chalk Mark in a Rainstorm , on the track "My Secret Place".
In 2001, Gabriel contributed lead vocals to the song "When You're Falling" on Afro Celt Sound System 's Volume 3: Further in Time . [47] In the summer of 2003, Gabriel performed in Ohio with a guest performance by Uzbek singer Sevara Nazarkhan .
Gabriel collaborated on tracks with electronic musician BT , who also worked on the OVO soundtrack with him. The tracks were never released, as the computers, they were contained on were stolen from BT's home in California. He also sang the lyrics for Deep Forest on their theme song for the movie Strange Days . In addition, Gabriel has appeared on Angelique Kidjo 's 2007 album Djin Djin , singing on the song "Salala".
Gabriel has recorded a cover of the Vampire Weekend single " Cape Cod Kwassa Kwassa " with Hot Chip , where his name is mentioned several times in the chorus. He substitutes the original line "But this feels so unnatural / Peter Gabriel too / This feels so unnatural/ Peter Gabriel too" with "It feels so unnatural / Peter Gabriel too / and it feels so unnatural / to sing your own name." [ citation needed ]

WOMAD and other projects
Gabriel's interest in world music was first apparent on his third album. This influence has increased over time, and he is the driving force behind the World of Music, Arts and Dance (WOMAD) movement. He created the Real World Studios and record label to facilitate the creation and distribution of such music by various artists, and he has worked to educate Western culture about the work of such musicians as Yungchen Lhamo , Nusrat Fateh Ali Khan , and Youssou N'dour . He has a long-standing interest in human rights and launched Witness , [48] a nonprofit which trains human rights activists to use video and online technologies to expose human rights abuses. In 2006, his work with WITNESS and his long-standing support of peace and human rights causes was recognised by the Nobel Peace Prize Laureates with the Man of Peace award.
In the 1990s, with Steve Nelson of Brilliant Media and director Michael Coulson , he developed advanced multimedia CD-ROM-based entertainment projects, creating Xplora (the world's largest selling music CD-ROM), and subsequently the EVE CD-ROM. EVE was a music and art adventure game directed by Michael Coulson and co-produced by the Starwave Corporation in Seattle; it won the Milia d'Or award Grand Prize at the Cannes in 1996.
In 1994, Gabriel starred in the Breck Eisner short film "Recon" as a detective who enters the minds of murder victims to find their killer's identity.
Gabriel helped pioneer a new realm of musical interaction in 2001, visiting Georgia State University 's Language Research Center to participate in keyboard jam sessions with bonobo apes from the Democratic Republic of the Congo. (This experience inspired the song "Animal Nation", which was performed on Gabriel's 2002 "Growing Up" tour and was featured on the Growing Up Live DVD and The Wild Thornberrys Movie soundtrack.) Gabriel's desire to bring attention to the intelligence of primates also took the form of ApeNet , a project that aimed to link great apes through the internet, enabling the first interspecies internet communication. [49]
He was one of the founders of on Demand Distribution ( OD2 ), one of the first online music download services. Its technology is used by MSN Music UK and others, and has become the dominant music download technology platform for stores in Europe. [ citation needed ] OD2 was bought by US company Loudeye in June 2004 and subsequently by Finnish mobile giant Nokia in October 2006 for $60 million. [ citation needed ]
Gabriel is co-founder (with Brian Eno ) of a musicians union called Mudda, short for "magnificent union of digitally downloading artists." [50] [51]
In 2000, Peter Gabriel collaborated with Zucchero, Anggun and others in a charity for kids with AIDS. Erick Benzi wrote words and music and Patrick Bruel, Stephan Eicher, Faudel, Lokua Kanza, Laam, Nourith, Axelle Red have accepted to sing it. [ citation needed ]
In 2003, Gabriel contributed a song for the video game Uru: Ages Beyond Myst. [52] In 2004, Gabriel contributed another song ("Curtains") and contributed voice work on another game in the Myst franchise, Myst IV: Revelation . [53]
During the latter part of 2004, Gabriel spent time in a village in eastern Nepal with musician Ram Sharan Nepali, learning esoteric vocal techniques. Gabriel subsequently invited Nepali to attend and perform at the Womad festival in Adelaide , Australia. [ citation needed ]
In June 2005, Gabriel and broadcast industry entrepreneur David Engelke purchased Solid State Logic , a manufacturer of mixing consoles and digital audio workstations. [54]
In May 2008, Gabriel's Real World Studios , in partnership with Bowers & Wilkins , started the Bowers & Wilkins Music Club – now known as Society of Sound – a subscription-based music retail site. Albums are currently available in either Apple Lossless or FLAC format. [55]
He is one of the founding supporters of Asteroid Day . [56]

Activist for humanitarian causes
In 1986, he started what has become a longstanding association with Amnesty International , becoming a pioneering participant in all 28 of Amnesty's Human rights concerts – a series of music events and tours staged by the US Section of Amnesty International between 1986–1998. He performed during the six-concert A Conspiracy of Hope US tour in June 1986; the twenty-concert Human Rights Now! world tour in 1988; the Chile: Embrace of Hope Concert in 1990 and at The Paris Concert For Amnesty International in 1998. He also performed in Amnesty's Secret Policeman's Ball benefit shows in collaboration with other artists and friends such as Lou Reed, David Gilmour and Youssou N'Dour; Gabriel closed those concerts performing his anti- apartheid anthem "Biko". [57] He spoke of his support for Amnesty on NBC 's Today Show in 1986. [58]
Inspired by the social activism he encountered in his work with Amnesty, in 1992, Gabriel co-founded WITNESS , a non-profit organisation that equips, trains and supports locally based organisations worldwide to use video and the internet in human rights documentation and advocacy.
In 1995, Gabriel and Cape Verdean human rights activist Vera Duarte were awarded the North–South Prize in its inaugural year. [59] [60]
In the late 1990s, Gabriel and entrepreneur Richard Branson discussed with Nelson Mandela their idea of a small, dedicated group of leaders, working objectively and without any vested personal interest to solve difficult global conflicts.
On 18 July 2007, in Johannesburg , South Africa, Nelson Mandela announced the formation of a new group, The Elders , in a speech he delivered on the occasion of his 89th birthday. Kofi Annan serves as Chair of The Elders and Gro Harlem Brundtland as Deputy Chair. The other members of the group are Martti Ahtisaari , Ela Bhatt , Lakhdar Brahimi , Fernando Henrique Cardoso , Jimmy Carter , [61] Hina Jilani , Graça Machel , Mary Robinson , [61] and Ernesto Zedillo . Desmond Tutu is an Honorary Elder, as was Nelson Mandela. The Elders is independently funded by a group of donors, including Branson and Gabriel.
The Elders use their collective skills to catalyse peaceful resolutions to long-standing conflicts, articulate new approaches to global issues that are causing or may later cause immense human suffering, and share wisdom by helping to connect voices all over the world. They work together to consider carefully which specific issues to approach.
In November 2007, Gabriel's non-profit group WITNESS launched The Hub , a participatory media site for human rights.
In September 2008, Gabriel was named as the recipient of Amnesty International's 2008 Ambassador of Conscience Award . In the same month, he received Quadriga United We Care award of Werkstatt Deutschland along with Boris Tadić , Eckart Höfling and Wikipedia. The award was presented to him by Queen Silvia of Sweden . [62]
In 2010, Gabriel lent his support to the campaign to release Sakineh Mohammadi Ashtiani , an Iranian woman sentenced to death by stoning after being convicted of committing adultery. [63]
In December 2013, Gabriel posted a warm video message in tribute to the deceased former South African president and anti-apartheid leader Nelson Mandela . Gabriel was quoted:
Gabriel has criticized Air France for their continued transport of monkeys to laboratories. In a letter to the airline, Gabriel wrote that in laboratories, “primates are violently force-fed chemicals, inflicted with brain damage, crippled, addicted to cocaine or alcohol, deprived of food and water, or psychologically tormented and ultimately killed." [66]
In March 2014, Gabriel publicly supported #withsyria, a campaign to rally support for victims of the Syrian Civil War . [67]
In November 2014, Gabriel, along with Pussy Riot , and Iron & Wine supported Hong Kong protesters at Hong Kong's Lennon Wall in their efforts. [68]
In March 2015, Gabriel was awarded an Honorary Doctorate by the University of South Australia in recognition of his commitment to creativity and its transformational power in building power in building peace and understanding. [69]
He composed the song "The Veil" for Oliver Stone 's film Snowden . [70]

Politics
Gabriel has been described as one of rock's most political musicians by AllMusic . [24] In 1992, on the 20th anniversary of the Bloody Sunday tragedy, Gabriel joined several left-wing figures such as Peter Hain , Jeremy Corbyn , Tony Benn , Ken Loach , John Pilger , and Adrian Mitchell in voicing his support for a demonstration in London calling for British withdrawal from Northern Ireland . [71]
At the 1997 general election , he declared his support for the Labour Party , which won that election by a landslide after 18 years out of power, led by Tony Blair . [72] In 1998, he was named in a list of the biggest private financial donors to Labour. [73] However, he subsequently distanced himself from the Labour government following Tony Blair 's support for George W. Bush and Britain's involvement in the Iraq War , which he strongly opposed. [74] Gabriel later explained his decision for funding Labour, saying, "after all those years of Thatcher , that was the only time I've put money into a political party because I wanted to help get rid of the Tory government of that time". [75]
In 2005, Gabriel gave a Green Party of England and Wales general election candidate special permission to record a cover of his song " Don't Give Up " for his campaign. [76] In 2010, The Guardian described Gabriel as "a staunch advocate of proportional representation ". [77] In 2013, he stated that he had become more interested in online petitioning organisations to effect change than traditional party politics. [74]
In 2012, Gabriel condemned the use of his music by conservative American talk radio personality Rush Limbaugh during a controversial segment in which Limbaugh vilified Georgetown University law student Sandra Fluke . A statement on behalf of Gabriel read: "Peter was appalled to learn that his music was linked to Rush Limbaugh's extraordinary attack on Sandra Fluke. It is obvious from anyone that knows Peter's work that he would never approve such a use. He has asked his representatives to make sure his music is withdrawn and especially from these unfair, aggressive and ignorant comments." [78]
Gabriel has declared his support for the two-state solution to the Israeli–Palestinian conflict . In 2014, he contributed songs to a new compilation album to raise funds for humanitarian organisations aiding Palestinian Arabs in Gaza . Gabriel was quoted: "I am certain that Israelis and Palestinians will both benefit from a two-state solution based on the 1967 borders. We have watched Palestinians suffer for too long, especially in Gaza. I am not, and never was, anti-Israeli or anti-Semitic , but I oppose the policy of the Israeli government, oppose injustice and oppose the occupation... I am proud to be one of the voices asking the Israeli government: 'Where is the two-state solution that you wanted so much?' and clearly say that enough is enough." [79]

Appearances: 2005–present
Gabriel produced and performed at the Eden Project Live 8 concert in July 2005. In his earliest days, Gabriel played flute on Cat Stevens 's first album on the Island records label , Mona Bone Jakon as a "nervous session musician ". Stevens, now known as Yusuf Islam, joined him on stage 33 years after that experience, in Johannesburg during Nelson Mandela 's 46664 concert. The two performed the Stevens hit " Wild World ".
A double DVD set, Still Growing Up: Live & Unwrapped , was released in October 2005. FIFA asked Gabriel and Brian Eno to organise an opening ceremony for the 2006 FIFA World Cup finals in Germany, planned to take place a couple of days before the start of the tournament; however, the show was cancelled in January 2006 by FIFA.
Rumours of a possible reunion of the original Genesis line-up began circulating in 2004 after Phil Collins stated in an interview that he was open to the idea of sitting back behind the drums and "let Peter be the singer." The classic line-up has only reformed for a live performance once before, in 1982 . However, the group did work together to create a new version of the 1974 song " The Carpet Crawlers ", ultimately released on the Turn It On Again: The Hits album as "The Carpet Crawlers 1999". Gabriel later met with other Genesis band members, to discuss a possible reunion tour of The Lamb Lies Down on Broadway . He chose to opt out of a reunion tour, and his former bandmates, Collins, Banks, and Rutherford chose to tour as Genesis without him.
At the opening ceremonies of the Winter Olympics in Turin , Italy, Gabriel performed John Lennon 's " Imagine " during the opening of the festivities on 10 February 2006.
In November 2006, the Seventh World Summit of Nobel Peace Laureates in Rome presented Gabriel with the Man of Peace award. The award, presented by former President of the USSR and Nobel Peace Prize winner Mikhail Gorbachev and Walter Veltroni , Mayor of Rome, was an acknowledgement of Gabriel's extensive contribution and work on behalf of human rights and peace. The award was presented in the Giulio Cesare Hall of the Campidoglio in Rome. At the end of the year, he was awarded the Q magazine Lifetime Achievement Award, presented to him by American musician Moby . In an interview published in the magazine to accompany the award, Gabriel's contribution to music was described as "vast and enduring."
Gabriel took on a project with the BBC World Service 's competition "The Next Big Thing" to find the world's best young band. Gabriel judged the final six young artists with William Orbit , Geoff Travis, and Angélique Kidjo .
The Times reported on 21 January 2007, that Peter Gabriel had announced that he planned to release his next album in the US without the aid of a record company. Gabriel, an early pioneer of digital music distribution, had raised £2 million towards recording and 'shipping' his next album, Big Blue Ball in a venture with investment boutique Ingenious Media . Gabriel is expected to earn double the money that he would through a conventional record deal. Commercial director Duncan Reid of Ingenious explains the business savvy of the deal, saying, "If you're paying a small distribution fee and covering your own marketing costs, you enjoy the lion's share of the proceeds of the album. Gabriel is expected to outsource CD production for worldwide release through Warner Bros. Records . The new album deal covers the North America territory, where Gabriel is currently out of contract. [80]
The album Big Blue Ball was launched in America thanks to a venture capital trust initiative. Bosses at London-based firm Ingenious raised more than 2 million GBP to help promote the release in the United States. The venture capitalists, Gabriel and his Real World Limited partners, have created a new joint venture company, High Level Recordings Limited, to oversee the release of the album, which took place in 2008. Gabriel appeared on a nationwide tour for the album in 2009. [81]
Gabriel was a judge for the 6th and 8th annual Independent Music Awards to support independent artists. [82]
In February 2009, Gabriel announced that he would not be performing on the 2008 Academy Awards telecast because producers of the show were limiting his performance of " Down to Earth " from WALL-E to 65 seconds. John Legend and the Soweto Gospel Choir performed the song in his stead.
Gabriel's 2009 tour appearances included Mexico, Argentina, Chile, Peru, and Venezuela. His first ever performance in Peru was held in Lima on 20 March 2009, during his second visit to the country. His concert in Mexico City, on 27 March 2009, attracted more than 38,000 fans.
On 25 July 2009, he played at WOMAD Charlton Park, his only European performance of the year, to promote Witness. The show included two tracks from the then-forthcoming Scratch My Back : Paul Simon 's "The Boy in the Bubble" and The Magnetic Fields ' "The Book of Love". [83]
On 21 August 2010, Gabriel performed a live set for " Guitar Center Sessions" on DirecTV . The episode also included an interview with Gabriel by the host of the program, Nic Harcourt . [84]
On 5 October 2011, at the Royal Festival Hall, London Gabriel appeared at the end of an interview of US President Carter by Channel 4 News presenter, Jon Snow, to lead the 2,500-strong audience in a rendition of happy birthday to mark the President turning 87.
On 9 November 2011, he appeared and performed on CBS 's Late Show with David Letterman .
In 2012, Gabriel toured in celebration of the 25th anniversary of the So album. The tour was called "Back to Front", and included the members of the original "So" tour including Tony Levin , David Rhodes, and others. Each performance included the entire So album. A highlight of the tour was a cameo appearance by John Cusack at the Hollywood Bowl and Santa Barbara performances. At these performances, Cusack appeared from offstage during the intro to "In Your Eyes" and handed Gabriel a ghetto blaster, a homage to his scene serenading Ione Skye in the movie "Say Anything".
Gabriel's cover of "Heroes" by David Bowie features in the 2014 movie Lone Survivor , starring Mark Wahlberg . [85] The cover was also featured in the 2016 Netflix series, Stranger Things . [86] Gabriel subsequently performed the song with a full symphony orchestra at a concert at the Brandenburg Gate to mark the 25th anniversary of the tearing down of the Berlin Wall on 9th Nov 2014, as the song lyrics reference "standing by the Wall," Bowie's original version having been written and recorded in West Berlin.
In 2014, he appeared as himself in The Life of Rock with Brian Pern . The titular character, portrayed by Simon Day , is an affectionate parody of Gabriel. Pern claims to have "invented world music" and been "the first musician to use Plasticine in videos". [87] Also in 2014, Gabriel was inducted into the Rock and Roll Hall of Fame for his solo career by Coldplay frontman Chris Martin . They performed Gabriel's "Washing of the Water" together.
In 2016, he was featured on a song named "Artificial Intelligence" (referred as 'A.I.') by the American pop-rock band OneRepublic [88] [89]

Personal life
Gabriel has two daughters with his first wife, Jill Moore: [61] Anna-Marie (born 26 July 1974) and Melanie (born 23 August 1976). He was married to Moore from 17 March 1971 until their divorce in 1987. Moore's father was Lord Moore of Wolvercote . Anna-Marie is a filmmaker who filmed and directed Gabriel's Growing Up on Tour: A Family Portrait and Still Growing Up: Live & Unwrapped DVDs and Melanie is a musician who has been a backing vocalist in her father's band since 2002. Gabriel has two sons with his second wife, Meabh Flynn: [41] Isaac Ralph (born 27 September 2001) and Luc (born 5 July 2008). Gabriel and Flynn have been married since 9 June 2002.
In the late 1980s and early 1990s, Gabriel lived with actress Rosanna Arquette . He has resided in Wiltshire for many years; he runs Real World Studios from Box, Wiltshire . He previously lived in the Woolley Valley near Bath, Somerset . In 2010, he joined a campaign to stop an agricultural development at the valley, which had also inspired his first solo single " Solsbury Hill " in 1977. [90]

Discography

With Genesis

Solo albums

Awards

Grammy Awards

MTV Video Music Awards

See also
WebPage index: 00095
Cultural impact of The Colbert Report
The Colbert Report , which premiered in American cable television on October 17, 2005, has had a massive cultural impact since its inception, when the show introduced the word " truthiness ". Issues in and references to American and world culture are attributed to the character played by Stephen Colbert , who calls his followers the Colbert Nation. The Colbert Report is a late-night talk and news satire television program hosted by Stephen Colbert that aired on Comedy Central from October 17, 2005 to December 18, 2014 for 1,447 episodes. The show focused on a fictional anchorman character named Stephen Colbert , played by his real-life namesake. The character, described by Colbert as a "well-intentioned, poorly informed, high-status idiot ", is a caricature of televised political pundits . Furthermore, the show satirized conservative personality-driven political talk programs, particularly Fox News ' The O'Reilly Factor . The Colbert Report is a spin-off of Comedy Central's The Daily Show , where he acted as a correspondent for the program for several years while developing the character.
The program was created by Colbert, Jon Stewart , and Ben Karlin . The show's writing was grounded in improvisation , and often lampooned current events stories. The show's structure also included a guest interview, in which the Colbert character attempts to deconstruct his opponent's argument. The show was taped in New York City 's Hell's Kitchen neighborhood, and the program's set is "hyper-American," epitomizing the character's ego. The show was taped and broadcast Monday through Thursday, with weeks taken off at multiple points in a given year for breaks. The Colbert Report saw immediate critical and ratings successes, leading to various awards, including two prestigious Peabody Awards . The show's cultural influence—which occasionally would require a fair degree of participation from the show’s audience, dubbed the "Colbert Nation"—extended beyond the program a number of times. This impact included the character running for U.S. President twice, co-hosting a rally at the National Mall , presenting a controversial performance at the White House Correspondents' Dinner , and establishing a real Super PAC that raised a million dollars. In addition, the show inspired various forms of multimedia, including music and multiple best-selling books.

Presented as non-satirical journalism
In May 2006, the Tom DeLay Legal Defense Trust posted a video of The Colbert Report on its website and sent out a mass email urging DeLay supporters to watch how "Hollywood liberal" Robert Greenwald "crashed and burned . . . when promoting his new attack on Tom DeLay." [1] The video featured Colbert asking questions such as, "Who hates America more, you or Michael Moore ?" [2] The Trust's email describes its content as "the truth behind Liberal Hollywood's" film about DeLay , and characterizes the Colbert Report clip with the headline, "Colbert Cracks the Story on Real Motivations Behind the Movie." On June 8, 2006, Colbert responded by conducting an "Exclusive Fake Interview" on his show with DeLay. Three different interviews with DeLay on different networks were spliced for humorous effect, and Colbert ended the "interview" by saying "I do hope you enjoyed my manipulation of your words." DeLay has since appeared as a guest on the program.
On July 25, 2006, [3] Colbert responded to television networks—specifically Fox News , NBC's The Today Show and ABC's Good Morning America —which took comments made by Florida Congressman Robert Wexler on The Colbert Report out of context (e.g.: "I enjoy cocaine and the company of prostitutes because they are a fun thing to do."). Wexler, who ran unopposed in the then-upcoming election, made the comments in response to urging by Colbert that he "say some things that would really lose the election for [Wexler] if [Wexler] were contested." [4] Colbert criticized the major networks' morning news shows that featured the interview in a misleading and a negative light, by showing clips from many of the "fluff" pieces they favored instead of "real" news. Colbert subsequently told his viewers to "vote Wexler, the man's got a sense of humor, unlike, evidently, journalists."

Animals named for Stephen Colbert
Colbert announced on his March 28, 2006 show that he had been contacted by San Francisco Zoo officials seeking his permission to name an unhatched bald eagle after him. [5] The eagle, affectionately dubbed Stephen Jr. on The Report , was bred to be reintroduced into the wild, as a part of the zoo's California Bald Eagle Breeding Program. Colbert celebrated the chick's birth on the April 17, 2006, program, and has since given updates on the bird's development. He has criticized the bird for migrating to Canada, and has attempted to lure him back to the U.S. On December 24, 2008, Stephen Jr. (tag A-46) was photographed at the Lower Klamath National Wildlife Refuge on the California/Oregon border. [6]
On September 30, 2006, the Saginaw Spirit , an OHL hockey team in Saginaw, Michigan , named its co-mascot Steagle Colbeagle the Eagle in honor of Colbert, despite the fact that it was spotted holding a Canadian flag during the anthem. [7] Before the introduction of the mascot, the team record was 0–3–0–1, but once the Steagle was introduced, the team improved their record to 44–21–0–3 by the season's end, [8] before losing in the first round of the playoffs. [9] On January 27, 2007, Oshawa, Ontario declared March 20 of that year (John Gray's birthday) Stephen Colbert Day after mayor John Gray bet Colbert that the Oshawa Generals would beat the Spirit, and Saginaw won 5–4. [10]
In the latter part of March 2007, Drexel University named a leatherback turtle in honor of Colbert in their Great Turtle Race. [11] "Stephanie Colburtle the Leatherback Turtle " came in second place, losing to a turtle named Billie. [12]
On June 24, 2008, Dr. Jason Bond , an associate professor with the Department of Biology at East Carolina University , appeared on the show because he agreed to name a spider after Stephen Colbert. They negotiated over what kind of spider would be named after Stephen, and Colbert told the professor that they would "settle this in the next couple of weeks". During the interview, the visual approximation of Bond changed between different pictures depicting Spider-Man , including Tobey Maguire (the actor who played Spider-Man in the films) and costumed people/animals. The spider was officially announced on August 6 as the Aptostichus stephencolberti . [13]
Colbert announced on February 5, 2009 that the UC Santa Cruz Marine Lab named an elephant seal in honor of him: Stelephant Colbert the Elephant Seal.

Places and things named for Stephen Colbert
In 2006, the ice hockey team Saginaw Spirit named their mini mascot "Steagle Colbeagle" after they held a naming contest. As of 2014, the team still had Steagle Colbeagle as their mini mascot. [14]
In February 2007, Ben & Jerry's unveiled a new ice cream flavor in honor of Stephen Colbert, named Stephen Colbert's AmeriCone Dream (available only in the United States). Colbert waited until Easter to sample the ice cream because he "gave up sweets for Lent ." [15] Colbert will donate all proceeds to charity through the new Stephen Colbert AmeriCone Dream Fund, which will distribute the money to various causes. [16] The flavor is described as "a decadent melting pot of vanilla ice cream with fudge-covered waffle cone pieces and a caramel swirl." [17] The company's founders appeared on the show on March 5, 2007 to discuss the ice cream and to plug their "grassroots education and advocacy project", TrueMajority .
On August 22, 2007, Richard Branson , who was being interviewed as a guest, announced that one of his Virgin America aeroplanes would be named Air Colbert. Colbert announced on April 2, 2008, during a ThreatDown segment, that the plane had been grounded after one of its engines was damaged by a bird strike .
During the sweepstakes for naming the new wing on the International Space Station , Stephen Colbert announced on his show that there was a write-in section where you could write your own suggestion for a name in. He encouraged his fans to write in "Colbert". When the sweepstakes was over, NASA announced that "Colbert" had beaten the next-most-popular choice, "Serenity," by over 40,000 votes on March 11, 2009. "Colbert" received 230,539 votes out of nearly 1.2 million cast. [18]
On April 15, 2009, NASA announced that instead of the new module being named after him, a treadmill on board the space station would be called the Combined Operational Load Bearing External Resistance Treadmill (C.O.L.B.E.R.T.). [19]

Honors bestowed by media organizations
Time magazine's James Poniewozik named it one of the Top 10 Returning Series of 2007, ranking it at #7. [20]
Colbert has appeared on the covers of several major magazines, including Wired , Rolling Stone , Esquire , Sports Illustrated (as sponsor of the US Speedskating team) and Newsweek , in which he was the Guest Editor.
On March 12, 2007, the Editor-in-Chief of Marvel Comics , Joe Quesada , awarded Stephen Colbert the shield of the recently deceased superhero Captain America . [21] The letter to Colbert accompanying the shield stated that " the Star-Spangled Avenger has bequeathed... his indestructible shield to the only man he believed to have the red, white, and blue balls to carry the mantle." Colbert promised to use the shield "only to fight for justice...and to impress girls." It was, in fact, one of only two full-sized prop shields which had previously been kept in the Marvel offices. [22] On January 29, 2008, Quesada (now president of Marvel) returned to announce that Colbert's fictional campaign for the presidency was still active in the Marvel universe, references to which have appeared in Marvel comics since. Colbert appears on the cover of Amazing Spider-Man #573 .
At the end of 2008, The Colbert Report was named the number one television series of that year by Entertainment Weekly .
In 2010 Colbert won the Golden Tweet Award. [23]

Arts
On October 17, 2008, it was announced that the portrait of Stephen from his second year of The Colbert Report was accepted into the national portrait collection at the National Museum of American History for its November reopening. [ citation needed ]

Athletics
On November 2, 2009, Colbert, representing the Colbert Nation, signed an on-air sponsorship agreement with U.S. speedskating executive director Robert Crowley. [24] Fundraising via The Colbert Report ultimately raised $300,000 for the US Winter Olympics speedskating team. [25] Coverage of the show's efforts also led to Colbert personally being invited to be the official ombudsman at the oval for the Olympics, [26] appointed as the official assistant sports psychologist for the US olympics speed skating team, and as such is now an official member of the team, [27] [28] and invited by Dick Ebersol , to be part of NBC 's 2010 Winter Olympics coverage team. [29]

Congressional response
In response to the "Better Know a District" segment, Rahm Emanuel , then the Democratic Caucus chair, instructed incoming freshmen not to do appearances on the show. [30]

Neologisms
The Colbert Report has created new words. Besides " truthiness ", Colbert has coined other terms including "freem", which is "freedom without the do, because I do it all for you." [31] Other words include: "eneagled", a blend of "enabled" and "eagle", thus meaning "to be given the characteristics of an eagle" and "mantasy", meaning male fantasies, such as running away from the wife to become free, a word to which Colbert claims to hold a trademark. In 2009, he coined the word "engayify" meaning "to gay it up." [32]

Wikipedia references
Colbert has made repeated references on the show to Wikipedia , which he refers to as his favorite website, generally in " The Wørd " segment. Colbert's first reference to Wikipedia was on the July 31, 2006, broadcast, when "The Word" was Wikiality , defined as the concept that " together we can create a reality that we all agree on—the reality we just agreed on ." [33] The premise of wikiality is that reality is what the wiki says it is. [34] He explained that on Wikipedia "any user can change any entry, and if enough users agree with them, it becomes true." He also told his viewers to go onto Wikipedia, in the article elephants , and to edit it so that it would say: "Elephant population in Africa has tripled over the past six months." The suggestion resulted in numerous changes to Wikipedia articles related to elephants and Africa. Editing of the concerned articles was restricted to prevent further edits. The pages' protection expiration date has been postponed several times as the vandalism ensued again.
Other words invented relating to Wikipedia include "Wikilobbying", regarding which Colbert explained "when money determines Wikipedia entries, reality has become a commodity", alluding to a case in which Microsoft allegedly hired someone to tamper with Wikipedia, [35] and " Self-determination ", where corporations are allowed to act out their fantasies online by editing their own Wikipedia entries. Colbert described Wikipedia as " Second Life for corporations", saying if a corporation wants to pretend to be someone else online, then that is their business. [36]
In another episode, the guest Dan Zaccagnino was discussing his new wiki where readers can share their musical compositions. Colbert compared this to "wiki-wiki-wikipedia."
On May 24, 2007, the guest was Jimmy Wales , the co-founder of Wikipedia. Stephen Colbert called Wikipedia a "battlefield for information", a tool which "brings democracy to information" and moves away from the views of the "elite who study things and got to say what is or is not real". During the interview, Colbert showed a sentence on the screen: "Librarians are hiding something." Wales could not see it, with the implication that Wales could not personally stop a critical mass of individuals from editing a page when he did not know which page was the target. Wales responded that "the interesting thing about The Colbert Report is that Wikipedians watch it." [37]
On June 9, 2008, Colbert mentioned Warren G. Harding as being a "secret negro president", and said that for proof, "the G stands for Gangsta" which he changed on Wikipedia so that he could cite a source. [38]

Hungarian bridge campaign
In 2006, the Ministry of Transport of Hungary launched an online call for public suggestions to name a future motorway bridge over the Danube , just north of Budapest . Ministry officials said the Hungarian Geographical Name Committee would choose from among the three submitted candidates with the most votes, guided by suggestions submitted by "local governments, cartographers, linguists, and other experts". [39] Users offered hundreds of suggestions, among them the "'You Can Go To Bratislava But Not Over This Bridge' Bridge" and the "Chuck Norris Bridge", which led in votes for some time. [40] Colbert noted the effort in his "Tip of the hat, wag of the finger" segment on August 9, [41] and in the following weeks, he continued to ask viewers to vote for him. On August 22, Hungarian news sites reported Colbert had won the first round of voting with 17,231,724 votes, [42] which is 7 million more people than there are in all of Hungary. Hungary changed the voting rules after the members of the Colbert Nation Forums developed a bot to stuff the ballot box, requiring registration to vote in the second phase. That night, Colbert asked his viewers to cease their efforts, [43] and offered apologies, [43] spending a segment honoring Hungary, its history, and its contributions to the world. [44]
On September 14, 2006, Colbert introduced his guest András Simonyi , Ambassador of the Republic of Hungary to the United States. The ambassador presented Mr. Colbert with a declaration certifying him as the winner of the second and final round of voting. The document bore the signatures of Hungarian government officials and the country's official seal . [45] Included in the text, as read by the ambassador, were two important conditions required for the name of the bridge to be made official. First, Colbert must be fluent in Hungarian . Colbert responded by pronouncing the Hungarian name Nicholas Zrinyi (incorrectly referring to Miklós Zrínyi ) and híd (meaning 'bridge' in Hungarian); Simonyi quickly certified him as fluent. [46] The second requirement proved more onerous: To have the bridge named after him, Colbert would have to be deceased. Colbert protested, but the ambassador presented him with a Hungarian passport and 10,000 Hungarian Forint ( HUF ), noting that this would allow Colbert to enter Hungary at any time, without restriction. He also brought attention to the portrait of King St. Stephen , the first King of Hungary, on the 10,000 HUF bill. Finally Simonyi implied that the question of Colbert's ineligibility by virtue of being alive might be resolved if Colbert were to accept an invitation to visit the bridge site in Budapest ; Colbert responded by trying to bribe the ambassador with the 10,000 HUF bill. [46] On September 28, 2006, it was announced that the bridge will be named "Megyeri Bridge", although the name did not make it to the second round. According to the Geographical Name Committee, the name was selected because the bridge connects Káposztásmegyer with Békásmegyer . [47]

Remix competitions
Colbert sometimes implicitly suggests that his fans remix various aspects of his show simply by telling them not to remix it. In January 2009, he told viewers not to remix his interview with Lawrence Lessig , founder of Creative Commons , which lets authors of creative works share them with the world. [48] This type of fan involvement is similar to the Colbert Green Screen Challenge, which focuses on video content editing. [49]

Running for President in 2008
On October 16, 2007, Colbert announced on The Report that he would be running for president. He had chosen no vice-president , though he was considering choosing Vladimir Putin , Mike Huckabee , or himself as his running mate. [50] Colbert declared that he was only running in South Carolina , his home state, and was contemplating running for both the Republican and Democratic parties as a " favorite son ." [51] [52] Colbert covered his story in the segment "The Hail to the Cheese Stephen Colbert NachoCheese Doritos 2008 Presidential Campaign Coverage" and promoted his campaign on his special election website colbert08.org, as under law he cannot use colbertnation.com. [53]
On October 21, 2007, Colbert was interviewed by host Tim Russert on NBC 's Meet The Press . Colbert explained why he changed the pronunciation of his name (from "COLE-Bert" to "Cole-BEAR"), demanded to know whether Russert believes that God supports our enemies in Iraq and revealed that he had no interest in winning the Presidency (he just wanted to run).
On November 1, 2007, the executive council of the South Carolina Democratic Party voted 13-3 to keep his name off the ballot and refunded his US$2500 filing fee. [54] By November 5, 2007, Colbert had officially dropped his Presidential bid. [55]

Canton insults and apologies
On the July 21, 2008, episode of The Colbert Report , Colbert made a comment about John McCain making a campaign stop in Canton, Ohio , and "not the crappy Canton in Georgia ." [56] The comment resulted in a local uproar, with the Canton, Georgia mayor insisting Colbert had never visited the town along with an invitation for him to do so. [56]
On July 30, 2008, Colbert apologized for the story, insisting that he was incorrect and that the real crappy Canton was Canton, Kansas , after which he made several jokes at the Kansas town's expense including calling the town a "shit hole". [57] [58] [59] Reaction from Mayor Brad Smiley and local residents was negative, [60] while Kansas governor Kathleen Sebelius invited Colbert to spend a night in Canton's historic jail. [61]
On August 5, 2008, Colbert apologized to the citizens of Canton, Kansas, he then maintained that he had meant to direct his mock derision towards Canton, South Dakota , by calling it "North Dakota's dirty ashtray" and satirizing the town in song. [62] [63]
On August 12, 2008, Colbert once again apologized to his latest comedic target even though local reaction to the insult was fairly mild. Referring to a line from his satirical song, Colbert said that not all the dogs run away from Canton, South Dakota but "some stay and develop a drinking problem." [64] However, as with the previous apologies, he began a new tirade on another Canton. According to Colbert, Canton, Texas is nothing but an "incorporated outhouse" and "one steaming pile of longhorn dung." He then asked the audience if they'd seen the town's tourism video. A video then promptly followed showing a monkey humping another monkey with the words Canton, Texas placed in the corner and an arrow pointing from the name to the monkey on bottom. [65] This jab at the Texas town had been predicted by Governor Sebelius at the end of her July 31, 2008, remarks. [66] In response to Colbert's comments, a Canton, Texas city councilman joked that he wanted Colbert to come there so he could "mash his nose". [67]
On October 28, 2008, Colbert reacted to the news that Barack Obama was campaigning for president in Canton, Ohio — the original good Canton — by saying that he was forced to admit that Canton, Ohio in fact was the real crappy Canton all along. [68]

The Colbert Bump
The "Colbert Bump" is defined, connotatively by the Report , as an increase in popularity of a person (author, musician, politician, etc.) or thing (website, etc.) as a result of appearing as a guest on or (in the case of a thing) being mentioned on the show. For example, if a politician appears on The Colbert Report , they may become more popular with certain voters and thus are more likely to be elected. According to a study by James H. Fowler , contributions to Democratic politicians rose 40% for 30 days after an appearance on the show. [69] The Mozilla Foundation also experienced a noticeable spike in the download rates of the Firefox browser right after the launch of Firefox 3 was mentioned in the program. [70] Magazines such as GQ , Newsweek and Sports Illustrated have all had sales spikes when Colbert appeared on their covers. [71]

Colbert Bump (alcoholic beverage)
A cocktail called the Colbert Bump was designed during the August 4th, 2009, episode of The Colbert Report by author David Wondrich. The recipe for a Colbert Bump includes one ounce of Cherry Heering liqueur , one and a half ounces of gin , a quarter of an ounce of lemon juice, and a little soda water . [72]

NASA's Node 3
In March 2009, NASA ran an online contest to name the new node of the International Space Station . Colbert encouraged his viewers to write in his name. By the end, 230,539 "Colbert" votes were cast. This beat Serenity , the top NASA choice, by more than 40,000 votes. [73] On April 14, 2009, Astronaut "Suni" Sunita Williams appeared on The Colbert Report , and announced the name of the node to be Tranquility. However, the treadmill the astronauts use to work out on will be named "C.O.L.B.E.R.T." for "Combined Operational Load Bearing External Resistance Treadmill" and this will be located in Tranquility. [74]
The Mars Society temporarily renamed their Mars Desert Research Station “Colbert” for one week in April, citing the unlikelihood that Node 3 would not be given that name in a press release. [75]

2009 Gulf special
On March 17, 2009, Colbert announced that the show would be broadcasting from somewhere in the The Gulf at some future date. He did not reveal where and when the taping would occur, as that would have presented a security breach. As part of this he made a segment called "Where in the world and when in time is Stephen Colbert gonna be in the Gulf?" In it he offered vague clues to the location via a Press Your Luck board, and described the location being somewhere "sandy" and "packed with troops". [76] He repeatedly alluded to places he might go, but refused to divulge specific details. On the May 4 show, director and writer J. J. Abrams (" LOST ," " Alias ," Cloverfield ," and Star Trek ) appeared as a guest, and revealed that he had found out where Colbert was headed and had hidden clues to the location throughout the broadcast, leaving it to the viewers to decipher their meaning. [77] On June 5, 2009, Colbert officially announced that "Operation Iraqi Stephen: Going Commando" would be taping and performing shows in Iraq from June 8 to June 11, 2009, making The Colbert Report the first TV show in USO history to produce a week of shows in a combat zone. [78] The shows were filmed at Camp Victory in the Al-Faw Palace .
Colbert's head was shaved on stage by General Raymond T. Odierno , Commander of the Multinational Force - Iraq , at the videotaped order given by President Barack Obama during the first of four taped performances that were aired June 8, 2009 through June 11, 2009. [79] He wore a civilian two-piece suit , but with a twist: it was tailored from the United States Army 's UCP digital camouflage pattern and featured an American Flag and Private rank insignias on the sleeves. Vice President Joe Biden and Senators John McCain and Jim Webb made special appearances, as did former GOP Vice Presidential nominee Sarah Palin and former Presidents Bill Clinton , George W. Bush , and George H.W. Bush . Actor Tom Hanks was also featured heavily in a pre-recorded segment encouraging viewers to support care packages for the troops. At the start of all four episodes filmed in Iraq, Stephen Colbert carried a golf club on stage to honor Bob Hope 's decades-long service, 250 years according to Colbert, for the USO across the world.

U.S. Olympic speed skating sponsorship
On November 2, 2009, Colbert, representing the Colbert Nation , signed a sponsorship agreement on-air with U.S. speedskating executive director Robert Crowley after learning of the loss of the team's prominent sponsor: Dutch bank DSB . The specifics of this sponsorship are unclear. [24] The change of the logo of the U.S. speed skating team's cap, from DSB to Colbert Nation was later shown in the November 12 episode and further episodes. Colbert has also taken aim at Canada , mocking and insulting them, claiming they weren't giving the speed skaters adequate ice time at Olympic rink in Vancouver in preparation for the 2010 Winter Olympics . The city of Richmond , where the Olympic Oval is located, had responded with a letter inviting him to be the official ombudsman at the oval for the Olympics, under the condition that he wear the official ombudsman headgear (a pink tuque ) at all times. [26]
As a result, Stephen attempted to make the US Olympic team in various sports including speedskating in which the final spot would be his if he were to beat Shani Davis in a 500m race. He lost, but became the Assistant Sport Psychologist to the US Speedskating team (Colbert holds an honorary Doctorate in Fine Arts). This perk also ensured that he would be able to be a journalist covering the 2010 Winter Olympics on his own show, and NBC where he filed speedskating reports during their primetime telecasts with Bob Costas as host. Conversely, Colbert would also feature NBC hosts and reporters covering the Olympics in his coverage of the games, alongside Olympic athletes, many of whom were from the US Speedskating team.

Raj Patel's identification as the "Messiah"
Benjamin Creme , self-proclaimed esotericist , and owner of Share International magazine, identified the coming " Messiah / Maitreya (or future Buddha )" as having been: born in 1972; traveled to London from India in 1977; been dark-skinned, and having a stutter. Shortly after Raj Patel , the author of the recent book, "The Value of Nothing" appeared on the show, Creme stated the Messiah had appeared on a popular television program in the United States. Patel was identified as such messiah by Creme's followers, as he fulfilled all of Creme's previous predictions. In the March 15, 2010 show, Colbert claimed this was product of a Colbert Bump and called Patel on the telephone to confirm his status as the deity, which the author denied. However, Benjamin Creme had also predicted that the Messiah would deny his status, reinforcing his followers' belief in Patel's divine status. [80] [81]

Global Zero
Queen Noor of Jordan appeared on The Colbert Report to promote Global Zero , an international , non-partisan , initiative to achieve phased, binding, and verifiable elimination of nuclear weapons . Colbert signed the Global Zero declaration during the interview.

2006 White House Correspondents Association Dinner
Stephen Colbert gave a satirical speech at the 2006 White House Correspondents' Association Dinner in which he criticized the Bush Administration and the news media . The performance earned him praise and scorn in the media.

2010 Congressional testimony for migrant workers
On September 24, 2010, Colbert testified in character before the House Judiciary Subcommittee on Immigration, Citizenship, and Border Security. He was invited by committee chairwoman Zoe Lofgren to describe his experience participating in the United Farm Workers ' "Take Our Jobs" program, where he spent a day working alongside migrant workers in upstate New York . [82] At the end of his often-humorous testimony, Colbert broke character in responding to a question from Rep. Judy Chu , D-CA, and explained why he cares about the plight of migrant workers.
“I like talking about people who don't have any power, and this seems like one of the least powerful people in the United States are migrant workers who come and do our work, but don't have any rights as a result. And yet we still invite them to come here and at the same time ask them to leave. And that's an interesting contradiction to me. And, you know, 'Whatsoever you do for the least of my brothers,' and these seem like the least of our brothers right now... Migrant workers suffer and have no rights. [83] "
Democratic committee member John Conyers questioned whether it was appropriate for the comedian to appear before Congress and asked him to leave the hearing. [84] Though Colbert offered to depart at the direction of the committee chairwoman, Rep. Lofgren requested that he stay at least until all opening testimony had been completed, whereupon Conyers withdrew his request. [85]
Conservative pundits took aim at his Congress testimony not long after. [86]
"As John Conyers notes, the media and spectators turned out to see whether Colbert would address the panel seriously as an expert on immigration and make the panel a joke, or stay in character and make the panel a bigger joke," - Ed Morrissey , Hot Air . [87]

"Keep Fear Alive" rally
After Glenn Beck 's Restoring Honor rally in Washington, D.C. , a user on Reddit suggested that Colbert hold his own Restoring Truthiness rally, and a website and Facebook page for the event were set up overnight. [88] A big announcement was first hinted at by Jon Stewart on September 7, [89] who ended the show with a Moment of Zen supposedly holding a clue about the announcement. [90] On the same day, Colbert acknowledged the online movement. He played a clip of Beck saying the geese at his event were sent by God, and insisted that he would need his own God geese to tell him to hold a rally. After consulting "Geese Witherspoon" and a bottle of Grey Goose vodka, Colbert responded to Stewart's announcement with a promise to make "an even bigger counter-announcement." [91]
The next day, Stewart responded to Colbert's announcement by showing what was going on inside his brain, [92] and yet again ended the show with an announcement-related Moment of Zen, [93] as he did the next day. [94]
The following Monday, Colbert finally responded to Stewart, first mocking him for his facial hair, then responding to the "brain-cam" with his own "gut-cam." Colbert then brought up the online rally and how viewers were sending him toy geese and releasing doves outside his studio, finally asking people to stop sending him live animals. [95]
To get Colbert's attention, Reddit users also encouraged people to donate to DonorsChoose , a charity organization that benefits teachers across the country. In less than a day, members of the Colbert Nation raised over $100,000. As of October 28, 2010, Reddit raised more than $500,000.
September 15's Daily Show ended with a toss to Colbert, and both acted coy about their competing announcements. That night's Colbert Report ended with Colbert announcing he would make his big announcement the next day, dropping hints that it would have to do with D.C. and the date October 10, 2010. On September 16, Colbert announced plans to host a 'Keep Fear Alive' rally on the National Mall on October 30, 2010.
The combined Rally to Restore Sanity and/or Fear was duly held on October 30, attracting nearly a quarter of a million participants. As the result of the rally, Metrorail set a new Saturday ridership record of 825,437 trips, as compared to about 350,000 trips on a normal Saturday. [96]

Colbert Super PAC
On March 10, 2011, during a segment on 2012 presidential contender Tim Pawlenty 's political action committee (PAC), Colbert announced the formation of his own PAC [97] and hosted former FEC Chairman Trevor Potter on the program to help him fill out the paperwork. [98] [99] Troubled by the fact that large corporations were not donating to his SuperPAC, Potter explained that corporations prefer to support political causes when the corporations can remain anonymous. Therefore, he helped Colbert set up a 501(c)(4) Delaware Shell corporation in which donations can be given anonymously without limit and used for political purposes. Colbert asked what the difference was between that and money laundering. Potter answered, "It's hard to say." [100] On November 12, Colbert shut down his Super PAC, citing the death of his fictional advisor Ham Rove . Potter advised him that by making a check with the remaining funds out to his 501(c)(4), with an "Agency Letter" telling it to transfer that money to a second anonymous 501(c)(4), he would not have to tell anyone - even the IRS - what happened to that money. [101]

Washington Redskins incident
Following the creation of a non-profit organization by the owner of the Washington Redskins , Colbert said that he would create the "Ching-Chong Ding-Dong Foundation for Sensitivity to Orientals or Whatever" in order to bring attention to the hypocrisy of the name. The following day, Suey Park created the hashtag #CancelColbert in order to draw attention to a tweet made by the Comedy Central-operated account of The Colbert Report . Park interpreted the tweet, which repeated the line from Colbert's show, as a tasteless joke made at the expense of Asians and Asian-Americans. Park's hashtag became a trending topic on Twitter, and the tweet was later deleted. [102] While the hashtag was trending, Park's efforts were widely criticized by the media. Later stating that he never wants "this to happen again," Colbert deleted the Comedy Central-run account on his show and directed people to follow his personal account. [103]

Amazon.com and Hachette Book Group controversy
On the June 4, 2014 episode, Colbert briefly broke down the details of a dispute that Amazon was having with book publisher Hachette , who has printed every book he has authored (and is the issuer for thousands more authors). In an LA Times interview; fellow Hachette author James Patterson expressed fear in regard to the depths Amazon will go to maintain control over book pricing, publishing, and sales of any e-retail in general. [104] The main issue between the two companies is of e-book sales, and Amazon's lowering of prices to what certain publishers consider unsustainable for their business. Considering Amazon's domination when it comes to bookselling (between 40 and 50% of all book sales) and the myriad products it has since taken to selling due to its success with literature, it is slowing down or halting all sales of published or upcoming books produced by Hachette with no end in sight, notifying potential customers that it would be a better decision to shop elsewhere for any Hachette books. In response to his books being delayed or outright banned from being ordered, Colbert (as well as fellow Hachette author Sherman Alexie ) encouraged the Colbert Nation to order California from first-time author Edan Lepucki from Portland, Oregon bookseller Powell's Books linked to Colbert's website (as well as "I Didn't Buy It on Amazon" stickers) and urged fans to refuse to purchase anything from Amazon until the conflict is resolved in a fair way. [105] On the July 21 episode, Colbert repeated the experiment, asking Lepucki for a recommendation of another Hachette author. Lepucki gave the nod to Stephan Eirik Clark who authored Sweetness #9 . [106] [107]
WebPage index: 00096
Scrubs (season 7)
The seventh season of the American comedy television series Scrubs premiered on NBC on October 25, 2007 and concluded on May 8, 2008 and consists of 11 episodes. This was the final season to air on NBC after it was picked up by ABC .
Season 7 was confirmed to have a reduced number of 18 episodes and was likely to be the final season. [1] Due to the 2007–2008 Writers Guild of America strike , only 11 episodes were finished and 6 aired before the strike. During the strike, it was unknown if production on the final episodes would resume or that a possible series finale would air due to the actors' contracts expiring if the strike were to last a long time. [2] After the strike was over, the final five episodes aired starting April 10, 2008. Episode 12, titled "My Commitment" was partially completed before the strike, but was never completed or aired. Some material shot for "My Commitment" was later used in the season 8 episode, " My Nah Nah Nah ".
Season 7 continues to focus on the fact that J.D. has to grow up. It's mentioned in several episodes throughout the season. He also has to deal with his newborn son, Sam. Both Elliot and J.D. deal with the fact that they may be with the wrong person (Keith and Kim). Turk tries to grow closer with Carla. Dr. Cox gets a temporary promotion. Dr. Kelso has to deal with the fact that the hospital has a mandatory retirement policy. Plus, the Janitor starts dating Lady , who works at the hospital.

Cast and characters

Main cast

Production

Episodes
WebPage index: 00097
List of Battlestar Galactica (2004 TV series) episodes
Battlestar Galactica is an American military science fiction television series , and part of the Battlestar Galactica franchise . The show was developed by Ronald D. Moore as a re-imagining of the 1978 Battlestar Galactica television series created by Glen A. Larson . The series first aired as a three-hour miniseries (comprising four broadcast hours) in December 2003 on the Sci-Fi Channel . The television series debuted in the United Kingdom on Sky1 on October 18, 2004, and premiered in the United States on the Sci-Fi Channel on January 14, 2005.
The story arc of Battlestar Galactica is set in a distant star system, where a civilization of humans live on a series of planets known as the Twelve Colonies . In the past, the Colonies had been at war with a cybernetic race of their own creation, known as the Cylons . With the unwitting help of a human named Gaius Baltar , the Cylons, now in human form, launch a sudden sneak attack on the Colonies, laying waste to the planets and devastating their populations. Out of a population numbering in the billions, only approximately 50,000 humans survive, most of whom were aboard civilian spaceships that avoided destruction. Of all the Colonial Fleet, the eponymous Battlestar Galactica appears to be the only military capital ship that survived the attack. Under the leadership of Colonial Fleet officer Commander William "Bill" Adama and President Laura Roslin , the Galactica and its crew take up the task of leading the small fugitive fleet of survivors into space in search of a fabled refuge known as Earth .

Series overview

Miniseries (2003)

Television series

Season 1 (2004–05)

Season 2 (2005–06)

Season 3 (2006–07)

Razor

Season 4 (2008–09)

The Plan

Webisodes (2006–09)

Home media releases
Notes:

Broadcast ratings

Online availability
In January 2006, Apple began offering the miniseries, season 1 and season 2 episodes for purchase on the U.S. version of its iTunes Store . [31] In December 2007, the Battlestar Galactica episodes were removed from iTunes along with other NBC Universal content. [32] The episodes returned to iTunes when NBC Universal announced their return to iTunes in September 2008. [33] In February 2009, the series became available in high-definition format at the UK iTunes Store. [34]
The series has also been available via Amazon Video , [35] Hulu , [36] Netflix , [37] PlayStation Network [38] and Xbox Video . [39]
WebPage index: 00098
The New Yorker
The New Yorker is an American magazine of reportage, commentary, criticism, essays, fiction, satire, cartoons, and poetry. It is published by Condé Nast . Started as a weekly in 1925, the magazine is now published 47 times annually, with five of these issues covering two-week spans.
Although its reviews and events listings often focus on the cultural life of New York City , The New Yorker has a wide audience outside of New York. It is well known for its illustrated and often topical covers, its commentaries on popular culture and eccentric Americana , its attention to modern fiction by the inclusion of short stories and literary reviews , its rigorous fact checking and copyediting , its journalism on politics and social issues , and its single-panel cartoons sprinkled throughout each issue.

History
The New Yorker debuted on February 21, 1925. It was founded by Harold Ross and his wife, Jane Grant , a New York Times reporter. Ross wanted to create a sophisticated humor magazine that would be different from perceivably "corny" humor publications such as Judge , where he had worked, or Life . Ross partnered with entrepreneur Raoul H. Fleischmann (who founded the General Baking Company [3] ) to establish the F-R Publishing Company. The magazine's first offices were at 25 West 45th Street in Manhattan . Ross edited the magazine until his death in 1951. During the early, occasionally precarious years of its existence, the magazine prided itself on its cosmopolitan sophistication. Ross famously declared in a 1925 prospectus for the magazine: "It has announced that it is not edited for the old lady in Dubuque ." [4]
Although the magazine never lost its touches of humor, it soon established itself as a pre-eminent forum for serious fiction literature and journalism. Shortly after the end of World War II , John Hersey 's essay Hiroshima filled an entire issue. In subsequent decades the magazine published short stories by many of the most respected writers of the 20th and 21st centuries, including Ann Beattie , Truman Capote , John Cheever , Roald Dahl , Mavis Gallant , Geoffrey Hellman , John McNulty , Joseph Mitchell , Alice Munro , Haruki Murakami , Vladimir Nabokov , John O'Hara , Dorothy Parker , Philip Roth , J. D. Salinger , Irwin Shaw , James Thurber , John Updike , Eudora Welty , Stephen King , and E. B. White . Publication of Shirley Jackson 's " The Lottery " drew more mail than any other story in the magazine's history.
In its early decades, the magazine sometimes published two or even three short stories a week, but in recent years the pace has remained steady at one story per issue. While some styles and themes recur more often than others in its fiction, the stories are marked less by uniformity than by variety, and they have ranged from Updike's introspective domestic narratives to the surrealism of Donald Barthelme , and from parochial accounts of the lives of neurotic New Yorkers to stories set in a wide range of locations and eras and translated from many languages. [ citation needed ] Kurt Vonnegut said that The New Yorker has been an effective instrument for getting a large audience to appreciate modern literature. Vonnegut's 1974 interview with Joe David Bellamy and John Casey contained a discussion of The New Yorker ' s influence:
The non-fiction feature articles (which usually make up the bulk of the magazine's content) cover an eclectic array of topics. Recent subjects have included eccentric evangelist Creflo Dollar , the different ways in which humans perceive the passage of time, and Münchausen syndrome by proxy .
The magazine is notable for its editorial traditions. Under the rubric Profiles , it publishes articles about notable people such as Ernest Hemingway , Henry R. Luce and Marlon Brando , Hollywood restaurateur Michael Romanoff , magician Ricky Jay and mathematicians David and Gregory Chudnovsky . Other enduring features have been "Goings on About Town", a listing of cultural and entertainment events in New York, and "The Talk of the Town", a miscellany of brief pieces—frequently humorous, whimsical or eccentric vignettes of life in New York—written in a breezily light style, or feuilleton , although in recent years the section often begins with a serious commentary. For many years, newspaper snippets containing amusing errors, unintended meanings or badly mixed metaphors ("Block That Metaphor") have been used as filler items, accompanied by a witty retort. There is no masthead listing the editors and staff. And despite some changes, the magazine has kept much of its traditional appearance over the decades in typography, layout, covers and artwork. The magazine was acquired by Advance Publications , the media company owned by Samuel Irving Newhouse Jr , in 1985. [6]
Ross was succeeded as editor by William Shawn (1951–87), followed by Robert Gottlieb (1987–92) and Tina Brown (1992–98). Among the important nonfiction authors who began writing for the magazine during Shawn's editorship were Dwight Macdonald , Kenneth Tynan , and Hannah Arendt ; to a certain extent all three authors were controversial, Arendt the most obviously so (her Eichmann in Jerusalem reportage appeared in the magazine before it was published as a book), but in each case Shawn proved an active champion.
Brown's nearly six-year tenure attracted more controversy than Gottlieb's or even Shawn's, thanks to her high profile (Shawn, by contrast, had been an extremely shy, introverted figure) and the changes which she made to a magazine that had retained a similar look and feel for the previous half-century. She introduced color to the editorial pages (several years before The New York Times ) and photography, with less type on each page and a generally more modern layout. More substantively, she increased the coverage of current events and hot topics such as celebrities and business tycoons, and placed short pieces throughout "Goings on About Town", including a racy column about nightlife in Manhattan. A new letters-to-the-editor page and the addition of authors' bylines to their "Talk of the Town" pieces had the effect of making the magazine more personal. The current editor of The New Yorker is David Remnick , who succeeded Brown in July 1998. [7]
Tom Wolfe wrote about the magazine: "The New Yorker style was one of leisurely meandering understatement, droll when in the humorous mode, tautological and litotical when in the serious mode, constantly amplified, qualified, adumbrated upon, nuanced and renuanced, until the magazine's pale-gray pages became High Baroque triumphs of the relative clause and appository modifier". [8]
Joseph Rosenblum, reviewing Ben Yagoda 's About Town , a history of the magazine from 1925 to 1985, wrote, "... The New Yorker did create its own universe. As one longtime reader wrote to Yagoda, this was a place 'where Peter DeVries ... [ sic ] was forever lifting a glass of Piesporter , where Niccolò Tucci (in a plum velvet dinner jacket ) flirted in Italian with Muriel Spark , where Nabokov sipped tawny port from a prismatic goblet (while a Red Admirable perched on his pinky), and where John Updike tripped over the master's Swiss shoes, excusing himself charmingly". [9]
As far back as the 1940s the magazine's commitment to fact-checking was already well known. [10] Yet the magazine played a role in a literary scandal and defamation lawsuit over two 1990s articles by Janet Malcolm , who wrote about Sigmund Freud 's legacy. Questions were raised about the magazine's fact-checking process. [11] As of 2010, The New Yorker employs 16 fact checkers. [12] In July 2011, the magazine was sued for defamation in United States district court for a July 12, 2010 article written by David Grann, [13] [14] but the case was summarily dismissed. [15] [16]
Since the late 1990s, The New Yorker has used the Internet to publish current and archived material. It maintains a website with some content from the current issue (plus exclusive web-only content). Subscribers have access to the full current issue online, as well as a complete archive of back issues viewable as they were originally printed. In addition, The New Yorker' s cartoons are available for purchase online. A digital archive of back issues from 1925 to April 2008 (representing more than 4,000 issues and half a million pages) has also been issued on DVD-ROMs and on a small portable hard drive. More recently, an iPad version of the current issue of the magazine has been released.
In its November 1, 2004 issue, the magazine for the first time endorsed a presidential candidate, choosing to endorse Democrat John Kerry over incumbent Republican George W. Bush . [17] This was continued in 2008 when the magazine endorsed Barack Obama over John McCain , [18] in 2012 when it endorsed Obama over Mitt Romney , [19] and in 2016 when it endorsed Hillary Clinton over Donald Trump . [20]

Cartoons
The New Yorker has featured cartoons (usually gag cartoons ) since it began publication in 1925. The cartoon editor of The New Yorker for years was Lee Lorenz , who first began cartooning in 1956 and became a New Yorker contract contributor in 1958. [21] After serving as the magazine's art editor from 1973 to 1993 (when he was replaced by Françoise Mouly ), he continued in the position of cartoon editor until 1998. His book The Art of the New Yorker: 1925–1995 (Knopf, 1995) was the first comprehensive survey of all aspects of the magazine's graphics. In 1998, Robert Mankoff took over as cartoon editor, and since then Mankoff has edited at least 14 collections of New Yorker cartoons. In addition, Mankoff usually contributes a short article to each book, describing some aspect of the cartooning process or the methods used to select cartoons for the magazine.
The New Yorker' s stable of cartoonists has included many important talents in American humor, including Charles Addams , Peter Arno , Charles Barsotti , George Booth , Roz Chast , Tom Cheney , Sam Cobean , Leo Cullum , Richard Decker , J. B. Handelsman , Helen E. Hokinson , Ed Koren , Reginald Marsh , Mary Petty , George Price , Charles Saxon , David Snell , Otto Soglow , Saul Steinberg , William Steig , James Stevenson , Richard Taylor, James Thurber , Pete Holmes , Barney Tobey, and Gahan Wilson .
Many early New Yorker cartoonists did not caption their own cartoons. In his book The Years with Ross , Thurber describes the newspaper's weekly art meeting, where cartoons submitted over the previous week would be brought up from the mail room to be gone over by Ross, the editorial department and a number of staff writers. Cartoons would often be rejected or sent back to artists with requested amendments, while others would be accepted and captions written for them. Some artists hired their own writers; Helen Hokinson hired James Reid Parker in 1931. ( Brendan Gill relates in his book Here at The New Yorker that at one point in the early 1940s, the quality of the artwork submitted to the magazine seemed to improve. It was later found out that the office boy (a teenaged Truman Capote ) had been acting as a volunteer art editor, dropping pieces he didn't like down the far edge of his desk.) [22]
Several of the magazine's cartoons have climbed to a higher plateau of fame. One 1928 cartoon drawn by Carl Rose and captioned by E. B. White shows a mother telling her daughter, "It's broccoli, dear." The daughter responds, "I say it's spinach and I say the hell with it." The phrase " I say it's spinach " entered the vernacular (and three years later, the Broadway musical Face the Music included Irving Berlin 's musical number titled " I Say It's Spinach (And The Hell With It) "). [23] The catchphrase " back to the drawing board " originated with the 1941 Peter Arno cartoon showing an engineer walking away from a crashed plane, saying, "Well, back to the old drawing board." [24] [25]
The most reprinted is Peter Steiner 's 1993 drawing of two dogs at a computer, with one saying, " On the Internet, nobody knows you're a dog ". According to Mankoff, Steiner and the magazine have split more than $100,000 in fees paid for the licensing and reprinting of this single cartoon, with more than half going to Steiner. [26] [27]
Over seven decades, many hardcover compilations of cartoons from The New Yorker have been published, and in 2004, Mankoff edited The Complete Cartoons of The New Yorker , a 656-page collection with 2004 of the magazine's best cartoons published during 80 years, plus a double CD set with all 68,647 cartoons ever published in the magazine. This features a search function allowing readers to search for cartoons by a cartoonist's name or by year of publication. The newer group of cartoonists in recent years includes Pat Byrnes , Frank Cotham, Michael Crawford, Joe Dator, Drew Dernavich, J. C. Duffy , Carolita Johnson, Zachary Kanin, Farley Katz, Robert Leighton , Glen Le Lievre, Michael Maslin, Ariel Molvig, Paul Noth, Barbara Smaller, David Sipress, Mick Stevens, Julia Suits , Christopher Weyant, P. C. Vey , and Jack Ziegler. The notion that some New Yorker cartoons have punchlines so non sequitur that they are impossible to understand became a subplot in the Seinfeld episode " The Cartoon ", as well as a playful jab in an episode of The Simpsons , " The Sweetest Apu ".
In April 2005, the magazine began using the last page of each issue for "The New Yorker Cartoon Caption Contest ". Captionless cartoons by The New Yorker' s regular cartoonists are printed each week. Captions are submitted by readers, and three are chosen as finalists. Readers then vote on the winner, and any resident of the US, UK, Australia, Ireland, or Canada (except Quebec ) age 18 or older can vote. [ citation needed ] Each contest winner receives a print of the cartoon (with the winning caption), signed by the artist who drew the cartoon.

Films
The New Yorker has been the source of a number of movies. Both fiction and non-fiction pieces have been adapted for the big screen, including: Flash of Genius (2008), based on a true account of the invention of the intermittent windshield wiper by John Seabrook ; Away From Her , adapted from Alice Munro's short story "The Bear Came Over The Mountain", which debuted at the 2007 Sundance Film Festival ; The Namesake (2007), similarly based on Jhumpa Lahiri 's novel which originated as a short story in the magazine; The Bridge (2006), based on Tad Friend 's 2003 non-fiction piece "Jumpers"; Brokeback Mountain (2005), an adaptation of the short story by Annie Proulx which first appeared in the October 13, 1997, issue of The New Yorker; Jonathan Safran Foer 's 2001 debut in The New Yorker , which later came to theaters in Liev Schreiber's debut as both screenwriter and director, Everything is Illuminated (2005); Michael Cunningham 's The Hours , which appeared in the pages of The New Yorker before becoming the film that garnered the 2002 Best Actress Academy Award for Nicole Kidman ; Adaptation (2002), which Charlie Kaufman based on Susan Orlean 's The Orchid Thief , written for The New Yorker; Frank McCourt's Angela's Ashes , which also appeared, in part, in The New Yorker in 1996 before its film adaptation was released in 1999; The Addams Family (1991) and its sequel, Addams Family Values (1993), both inspired by the work of famed New Yorker cartoonist Charles Addams ; Brian De Palma 's Casualties of War (1989), which began as a New Yorker article by Daniel Lang; Boys Don't Cry (1999), starring Hilary Swank, began as an article in the magazine, and Iris (2001), about the life of Iris Murdoch and John Bayley, the article written by John Bayley for the New Yorker, before he completed his full memoir, the film starring Judi Dench and Jim Broadbent; The Swimmer (1968), starring Burt Lancaster , based on a John Cheever short story from The New Yorker; In Cold Blood (1967), the widely nominated adaptation of the 1965 non-fiction serial written for The New Yorker by Truman Capote ; Pal Joey (1957), based on a series of stories by John O'Hara; Mister 880 (1950), starring Edmund Gwenn , based on a story by longtime editor St. Clair McKelway ; The Secret Life of Walter Mitty (1947) which began as a story by longtime New Yorker contributor James Thurber; and Meet Me in St. Louis (1944), adapted from Sally Benson 's short stories.
The history of The New Yorker has also been portrayed in film: In Mrs. Parker and the Vicious Circle , a film about the celebrated Algonquin Round Table starring Jennifer Jason Leigh as Dorothy Parker , Sam Robards portrays founding editor Harold Ross trying to drum up support for his fledgling publication. The magazine's former editor, William Shawn , is portrayed in Capote (2005), Infamous (2006) and Hannah Arendt (2012).
The 2015 documentary, Very Semi-Serious, produced by Redora Films, presents a behind-the-scenes look at the cartoons of The New Yorker .

Style
The New Yorker' s signature display typeface, used for its nameplate and headlines and the masthead above The Talk of the Town section, is Irvin, named after its creator, the designer-illustrator Rea Irvin . [28] The body text of all articles in The New Yorker is set in Adobe Caslon . [29]
One uncommonly formal feature of the magazine's in-house style is the placement of diaeresis marks in words with repeating vowels —such as reëlected , preëminent and coöperate —in which the two vowel letters indicate separate vowel sounds. [30] The magazine also continues to use a few spellings that are otherwise little used, such as focussed , venders , teen-ager , [31] traveller , marvellous , carrousel , [32] and cannister . [33]
The magazine also spells out the names of numerical amounts, such as "two million three hundred thousand dollars" instead of "$2.3 million", even for very large figures. [34]

Readership
Notwithstanding its title, The New Yorker is read nationwide, with 53 percent of its circulation in the top ten U.S. metropolitan areas. According to Mediamark Research Inc., the average age of The New Yorker reader in 2009 was 47 (compared to 43 in 1980 and 46 in 1990). The average household income of The New Yorker readers in 2009 was $109,877 (the average income in 1980 was $62,788 and the average income in 1990 was $70,233). [35]

Eustace Tilley
The magazine's first cover illustration, a dandy peering at a butterfly through a monocle , was drawn by Rea Irvin , the magazine's first art editor, based on an 1834 caricature of the then Count d'Orsay which appeared as an illustration [36] in the 11th edition of the Encyclopædia Britannica . The gentleman on the original cover, now referred to as "Eustace Tilley", is a character created by Corey Ford for The New Yorker . The hero of a series entitled "The Making of a Magazine", which began on the inside front cover of the August 8 issue that first summer, Tilley was a younger man than the figure on the original cover. His top hat was of a newer style, without the curved brim. He wore a morning coat and striped trousers. Ford borrowed Eustace Tilley's last name from an aunt—he had always found it vaguely humorous. "Eustace" was selected by Ford for euphony . [37]
The character has become a kind of mascot for The New Yorker , frequently appearing in its pages and on promotional materials. Traditionally, Rea Irvin's original Tilley cover illustration is used every year on the issue closest to the anniversary date of February 21, though on several occasions a newly drawn variation has been substituted. [38]

Covers

"View of the World" cover
Saul Steinberg created 85 covers and 642 internal drawings and illustrations for the magazine. His most famous work is probably its March 29, 1976 cover, [39] an illustration most often referred to as "View of the World from 9th Avenue ", sometimes referred to as "A Parochial New Yorker's View of the World" or "A New Yorker's View of the World", which depicts a map of the world as seen by self-absorbed New Yorkers.
The illustration is split in two, with the bottom half of the image showing Manhattan 's 9th Avenue, 10th Avenue , and the Hudson River (appropriately labeled), and the top half depicting the rest of the world. The rest of the United States is the size of the three New York City blocks and is drawn as a square, with a thin brown strip along the Hudson representing "Jersey" , the names of five cities ( Los Angeles ; Washington, D.C. ; Las Vegas ; Kansas City ; and Chicago ) and three states ( Texas , Utah , and Nebraska ) scattered among a few rocks for the United States beyond New Jersey. The Pacific Ocean, perhaps half again as wide as the Hudson, separates the United States from three flattened land masses labeled China, Japan and Russia.
The illustration—humorously depicting New Yorkers' self-image of their place in the world, or perhaps outsiders' view of New Yorkers' self-image—inspired many similar works, including the poster for the 1984 film Moscow on the Hudson ; that movie poster led to a lawsuit, Steinberg v. Columbia Pictures Industries, Inc. , 663 F. Supp. 706 ( S.D.N.Y. 1987), which held that Columbia Pictures violated the copyright that Steinberg held on his work.
The cover was later satirized by Barry Blitt for the cover of The New Yorker on October 6, 2008. The cover featured Sarah Palin looking out of her window seeing only Alaska, with Russia in the far background. [40]
The March 21, 2009 cover of The Economist , "How China sees the World", is also an homage to the original image, but depicting the viewpoint from Beijing's Chang'an Avenue instead of Manhattan. [41]

9/11
Hired by Tina Brown in 1992, Art Spiegelman worked for The New Yorker for ten years but resigned a few months after the September 11 terrorist attacks . The cover created by Françoise Mouly and Spiegelman for the September 24, 2001 issue of The New Yorker received wide acclaim and was voted in the top ten of magazine covers of the past 40 years by the American Society of Magazine Editors, which commented:
At first glance, the cover appears to be totally black, but upon close examination it reveals the silhouettes of the World Trade Center towers in a slightly darker shade of black. In some situations, the ghost images only become visible when the magazine is tilted toward a light source. [42] In September 2004, Spiegelman reprised the image on the cover of his book In the Shadow of No Towers , in which he relates his experience of the Twin Towers attack and the psychological after-effects.

"New Yorkistan"
In the December 2001 issue the magazine printed a cover by Maira Kalman and Rick Meyerowitz showing a map of New York in which various neighborhoods were labeled with humorous names reminiscent of Middle Eastern and Central Asian place names and referencing the neighborhood's real name or characteristics (e.g., "Fuhgeddabouditstan", "Botoxia"). The cover had some cultural resonance in the wake of September 11, and became a popular print and poster. [43] [44]

Controversial covers

Crown Heights in 1993
For the 1993 Valentine's Day issue, the magazine cover by Art Spiegelman depicted a black woman and a Hasidic Jewish man kissing, referencing the Crown Heights riot of 1991. [45] [46] The cover was criticized by both black and Jewish observers. [47] Jack Salzman and Cornel West describe the reaction to the cover as the magazine's "first national controversy". [48]

2008 Obama cover satire and controversy
"The Politics of Fear", a cartoon by Barry Blitt featured on the cover of the July 21, 2008 issue, depicts then presumptive Democratic presidential nominee Barack Obama in the turban and salwar kameez typical of many Muslims , fist bumping with his wife, Michelle , portrayed with an Afro and wearing camouflage trousers with an assault rifle slung over her back. They are standing in the Oval Office , with a portrait of Osama Bin Laden hanging on the wall and an American flag burning in the fireplace in the background. [49]
Many New Yorker readers saw the image as a lampoon of "The Politics of Fear", as the image was titled. Some of Obama's supporters as well as his presumptive Republican opponent, Sen. John McCain , accused the magazine of publishing an incendiary cartoon whose irony could be lost on some readers. However, editor David Remnick felt the image's obvious excesses rebuffed the concern that it could be misunderstood, even by those unfamiliar with the magazine. [50] [51] "The intent of the cover," he said, "is to satirize the vicious and racist attacks and rumors and misconceptions about the Obamas that have been floating around in the blogosphere and are reflected in public opinion polls. What we set out to do was to throw all these images together, which are all over the top and to shine a kind of harsh light on them, to satirize them." [52]
In an interview on Larry King Live shortly after the magazine issue began circulating, Obama said, "Well, I know it was The New Yorker' s attempt at satire... I don't think they were entirely successful with it". But Obama also pointed to his own efforts to debunk the allegations portrayed in The New Yorker cover through a web site his campaign set up: "[They are] actually an insult against Muslim-Americans, something that we don't spend a lot of time talking about." [53] [54]
Later that week, The Daily Show ' s Jon Stewart continued The New Yorker cover's argument about Obama stereotypes with a piece showcasing a montage of clips containing such stereotypes culled from various legitimate news sources. [55] The New Yorker Obama cover was later parodied by Stewart and Stephen Colbert on the October 3, 2008, cover of Entertainment Weekly magazine, with Stewart as Obama and Colbert as Michelle, photographed for the magazine in New York City on September 18. [56]
New Yorker covers are not always related to the contents of the magazine or are only tangentially so. In this case, the article in the July 21, 2008, issue about Obama did not discuss the attacks and rumors but rather Obama's political career. The magazine later endorsed Obama for president.
This parody was most likely inspired by Fox News host E. D. Hill 's paraphrasing of an anonymous internet comment in asking whether a gesture made by Obama and his wife Michelle was a "terrorist fist jab". [57] [58] Later, Hill's contract was not renewed. [59]

2013 Bert and Ernie cover
The New Yorker chose an image of Bert and Ernie by artist Jack Hunter, titled 'Moment of Joy', as the cover of their July 8, 2013 publication which covers the Supreme Court decisions on the Defense of Marriage Act and California Proposition 8 . [60] The Sesame Street characters have long been rumored in popular culture and urban legend to be homosexual partners, though Sesame Workshop has repeatedly denied this, saying they are merely "puppets" and have no sexual orientation. [61] Reaction was mixed. Online magazine Slate criticized the cover, which shows Ernie leaning on Bert's shoulder as they watch a television with the Supreme Court justices on the screen, saying "it's a terrible way to commemorate a major civil-rights victory for gay and lesbian couples." The Huffington Post , meanwhile, said it was "one of [the magazine's] most awesome covers of all time." [62]

Books

Movies

See also
WebPage index: 00099
Wikimedia project
A Wikimedia project is a wiki -based community project run by the Wikimedia Foundation , [1] a nonprofit organization founded in San Francisco , California in 2003. The Foundation operates many free software and free content projects, listed below, which are developed by volunteers of the Wikimedia movement .

List
Content on most Wikimedia Foundation websites is licensed for redistribution under v3.0 of the Attribution and Share-alike Creative Commons licenses . This content is sourced from contributing volunteers and from resources with few or no copyright restrictions, such as copyleft material and works in the public domain .
The order in the table below is as listed in the Wikimedia SiteMatrix .
The only Wikimedia Foundation project which has been closed and deleted is the September 11 Memorial wiki, which was created shortly after the September 11, 2001 terrorist attacks .

Language codes
Wikimedia projects use language codes to identify language specific editions of a Wikimedia project. The codes are often used as a subdomain, e.g. for the Wikipedia editions it is a subdomain below wikipedia.org. Interlanguage links in the English Wikipedia are sorted by that code.
The codes mostly correspond to the language codes defined by ISO 639-1 and ISO 639-3 , and the decision of which language code to use is mostly in accordance with the IETF language tag policy.
One code is not a language code ('be-x-old') but refers to a specific orthography.

Codes that do not agree with the ISO 639 meaning or are deprecated
Several codes are used for project editions where the ISO 639 meaning is different from the content contained in the project. [2]
This makes the codes partially "not useful". [3]

Project codes

Internal code changes
The following codes have been changed for the mark-up: [11]

Redirects

Usage in Wikimedia projects

Project specific codes

Wikidata
In Wikidata a property named "Wikimedia language code" exists. [14]

Software projects and other backstage projects
The Foundation also hosts various private wikis – such as those for the Chapters Committee, Board of Trustees, OTRS , and Wikimedia Foundation Office – and various public wikis – for specialized groups such as Outreach , [24] the regional Chapters, and thematic organizations.

See also
WebPage index: 00100
Wikivoyage
Wikivoyage is a free web-based travel guide for travel destinations and travel topics written by volunteer authors. It is a sister project of Wikipedia and supported and hosted by the same non-profit Wikimedia Foundation . Wikivoyage has been called the "Wikipedia of travel guides". [2]
The project was started when editors at the German and then Italian versions of Wikitravel decided in September 2006 to move their editing activities and then current content to a new site, in accordance with the site license, a procedure known as " forking ". The resulting site went live as "Wikivoyage" on December 10, 2006 and was owned and operated by a German association set up for that purpose, Wikivoyage e.V. (which continues to be its representative association). Content was published under the copyleft license Creative Commons Attribution-ShareAlike .
In 2012, following a lengthy history of dissatisfaction with their existing host, [3] the English-language version community of Wikitravel also decided as a community to fork their project. In a two-way move, the English Wikitravel community re-merged with Wikivoyage under the Wikivoyage brand, and also all Wikivoyage language versions moved their operations to be hosted by the Wikimedia Foundation (WMF), a non-profit organization hosting several of the world's largest wiki-based communities such as Wikipedia. [4] [5] Following agreements by the various communities involved and the Wikimedia Foundation, the site was moved to the WMF servers in December 2012 and the whole of Wikivoyage was officially re-launched as a Wikimedia project on January 15, 2013, the day of the 12th anniversary of Wikipedia's launch. [6]

Description
Using a wiki model, Wikivoyage is built through collaboration of Wikivoyagers from around the globe. Articles can cover different levels of geographic specificity, from continents to districts of a city. These are logically connected in a hierarchy, by specifying that the location covered in one article "is within" the larger location described by another. The project also includes articles on travel-related topics, phrasebooks for travelers, and suggested itineraries.
Wikivoyage is a multilingual project available in nine languages, with each language-specific project developed independently. While now a Wikimedia project, it was begun independently. Wikivoyage content is broadly categorized as: destinations, itineraries, phrasebooks, and travel topics.

Destinations
Geographical units within the geographical hierarchy may be described in articles, based on the criterion, "can you sleep there?"
The hierarchy includes:
Attractions such as hotels, restaurants, bars, stores, nightclubs, tour operators, museums, statues or other works of art, city parks, town squares or streets, festivals or events, transport systems or stations, bodies of water, and uninhabited islands are listed in the article for the place within which they are located.

Itineraries
An itinerary describes a group of destinations according to a temporal division rather than a spatial one and will list destinations and attractions to visit during a given amount of time, with recommended durations of stay and routes to follow. Itineraries may cross geographical regions, but usually have a well-defined path.

Phrasebooks
A phrasebook includes:

Travel topics
Travel topics are articles that deal with a specific topic of interest to travelers that is too large or detailed to go in a specific travel guide destination page; travel tips that are so general that they apply to nearly all destinations and do not need to be in each specific travel guide; major events that occur in different places; and specialist travel information, such as regional guides to scuba-diving sites.

Organization and operation

Mode of operation
Wikivoyage uses the free MediaWiki software (developed for Wikipedia) to allow internet-based editing without requiring registration. Quality assurance occurs in the same way as on Wikipedia: through reciprocal control by editors. The use of the same software is intended to facilitate familiarization with Wikivoyage.

License
Wikivoyage uses the Creative Commons Attribution-ShareAlike license, but not the GNU free documentation license . This is intended to facilitate the production of printed guides from a legal point of view. Media files are intended to be published either in the public domain or under multiple licenses (GNU, Creative Commons).

Information structure
The information is built up in a more structured way than usual for encyclopaedias. Articles belonging to a topic are grouped by the categories known from the Mediawiki software as well as through the so-called bread crumb trails which show the geographical connection between the articles.
In the German-language version, different name spaces are used to separate different topics. The main name space contains travel destinations within their geographical hierarchy. Two other important name spaces are reserved for travel topics and travel news , with the intent to allow a tight interconnection between travel destinations and topics.
The content design is decided by consensus of the community of authors.

Languages
At the time of transfer to WMF, the content of Wikivoyage was available in German , Italian , English , French , Dutch , Russian , and Swedish languages. The Russian language project is named Викигид (which translates roughly as "wiki guide").
On January 3, 2013, Portuguese and Spanish versions were created. In March 2013, Polish and Romanian versions followed. In April 2013, Hebrew and Ukrainian versions were added, followed in May 2013 by Greek , the Vietnamese version in August 2013 and the Chinese version on January 2014.

Grand Total

Distribution
The choice of the Creative Commons Attribution-ShareAlike license is intended to allow simplified distribution by mention of the authors, without the need to state the complete license text. [7]
Creative Commons Attribution-ShareAlike allows distribution through mirrors or by other means of modern media. [8] Up-to-date archives are provided on a weekly basis. [9] The files contained in these archives are provided with all the necessary legal licensing information, e.g. the attribution of the authors.

History
The name is a portmanteau of the words " Wiki " (an Internet-based software system that allows change and extension of the text by any user) and " voyage ", meaning travel, journey, or trip. It was retained after extensive voting amongst established editors to decide on the post re-launch name.

Launch
Many Wikivoyage authors and administrators started by working on Wikitravel , which launched its German version on October 7, 2004. On April 20, 2006, Wikitravel was acquired by the for-profit Internet Brands , an operator of media and e-commerce sites. Discontent increased in response to the management style of the new owners, and this led to the decision by most German administrators and authors to continue the project as a fork .
After about six months of preparation, the non-profit association Wikivoyage e.V. was founded and registered, as both the owner of the domain names and operator of the servers. [10] On December 10, 2006, the project went live online with the initial data from the German-language Wikitravel. After seven months, 40% of the articles were new, rising to 50% after 10 months. [11] At this stage there were still major gaps in the coverage, but there were several articles for travel destinations like Egypt, Thailand and Switzerland and for the travel topic "cycling". [12]
The Italian branch of Wikivoyage was launched on December 10, 2007. The organization of media data and the administration of user access were already applicable for use in branches in other languages.
The project garnered some press reports, particularly by Swiss radio and newspapers. The Tages-Anzeiger [13] from Zurich and the Swiss radio station DRS1 reported broadly on the project and discussed its weaknesses. [14] The project was mainly supported by German and Swiss authors.

Additional languages and migration
In 2012, after a lengthy history of dissatisfaction with Wikitravel's host and owner, Internet Brands, it was proposed that the community at Wikitravel fork their work from Wikitravel and Wikitravel Shared and – together with the existing sites at Wikivoyage – merge to create a new travel wiki hosted by the Wikimedia Foundation , the steward of Wikipedia and a large range of other non-profit reference sites based upon a wiki community culture. [15] [16] [17] After lengthy discussion by users of all three communities, comments by their respective hosts, and confirmation by the Wikimedia Foundation that it would host a travel project if users wished, nearly all administrators and bureaucrats at Wikitravel decided to fork their existing work to Wikivoyage. [18]
The contents of Wikitravel in all languages and its related Commons-equivalent site (for images, video, and other media files) were downloaded on August 2, 2012 as a "database dump" in preparation for such a migration. This content became the starting point for all languages excluding German and Italian, which were already hosted by Wikivoyage. Forking is a normal or anticipated activity in wiki communities and is expressly permitted by the Creative Commons–Attribution–Share Alike (CC BY-SA) license in use on sites such as Wikitravel. MediaWiki , the wiki software used for Wikitravel, included that facility, although Internet Brands disabled the function shortly after this date in an attempt to prevent the data migration. [ citation needed ] The community discussion at Wikimedia ended on August 23, 2012 with 540 votes for and 152 votes against the creation of a Wikimedia Foundation travel guide. [19] The project began in beta on WMF servers on November 10, 2012 [20] and was launched fully on January 15, 2013.
As part of the migration, it is planned that current owners and user body "Wikivoyage e.V." will remain in place as an associated organization affiliated with the Wikimedia Foundation "at an organizational level". Wikivoyage stated that, freed of the need to maintain its servers, it would be able to benefit by increasing its work related to outreach, community support, discussion and information, and technical enhancements to the site's software. [21] [22]
In September 2012, Internet Brands filed a lawsuit against one Wikitravel administrator, Ryan Holliday, and one Wikipedia administrator, James Heilman , accusing them of trademark breach and commercial misconduct in the proposals affecting that site, with the defendants and Wikimedia rejecting the case as an example of a SLAPP lawsuit—one that is undertaken without plausible legal grounds for the primary purpose of deterring, overwhelming, or frustrating people engaged in fully lawful actions. [23] On November 19, 2012, the claims by Internet Brands were dismissed by the United States District Court for the Central District of California . [24] [25] [26]

Evolution
English Wikivoyage experiences a constant increase in number of articles (about +1% per month) and a stable amount of activity (about 300 monthly active users and 800 total monthly editors). [27]
After a peak in visits and activity between January and March 2013, [28] confirmed by an Alexa global rank of about 8000 in March for English Wikivoyage, 2013 [29] Wikivoyage has been between the 20,000 and 30,000 position in the global Alexa rank, [30] [31] lower than Wikitravel 's rank which however declined from around 3000th position in 2011 [32] to about 6000th in 2015. [33]

See also
WebPage index: 00101
Wired (magazine)
Wired is a monthly American magazine , published in print and online editions, that focuses on how emerging technologies affect culture , the economy , and politics . Owned by Condé Nast , it is headquartered in San Francisco, California , and has been in publication since its first issue in March/April 1993. [2] Several spin-offs have been launched including: Wired UK , Wired Italia , Wired Japan and Wired Germany .
In its earliest colophons , Wired credited Canadian media theorist Marshall McLuhan as its " patron saint ." From its beginning, the strongest influence on the magazine's editorial outlook came from techno-utopian co-founder Stewart Brand and his associate Kevin Kelly . [3]
From 1998 to 2006, Wired magazine and Wired News (which publishes at Wired.com) had separate owners. However, Wired News remained responsible for republishing Wired magazine's content online due to an agreement when Condé Nast purchased the magazine. In 2006, Condé Nast bought Wired News for $25 million, reuniting the magazine with its website.
Wired is known for coining the terms "the Long Tail " [4] and " crowdsourcing ", [5] as well as its annual tradition of handing out Vaporware Awards which recognize "products, videogames and other nerdy tidbits pitched, promised and hyped, but never delivered". [6]

History
The magazine was founded by American journalist Louis Rossetto and his partner Jane Metcalfe , along with Ian Charles Stewart in 1993 with initial backing from software entrepreneur Charlie Jackson and eclectic academic Nicholas Negroponte of the MIT Media Lab , who was a regular columnist for six years, through 1998 and wrote the book Being Digital . The founding designers were John Plunkett and Barbara Kuhr (Plunkett+Kuhr), beginning with a 1991 prototype and continuing through the first five years of publication, 1993–98.
Wired , which touted itself as "the Rolling Stone of technology," [7] made its debut at the Macworld conference on January 2, 1993. [8] A great success at its launch, it was lauded for its vision, originality, innovation and cultural impact. [ citation needed ] In its first four years, the magazine won two National Magazine Awards for General Excellence and one for Design.
The founding executive editor of Wired , Kevin Kelly , was an editor of the Whole Earth Catalog and the Whole Earth Review , and brought with him contributing writers from those publications. Six authors of the first Wired issue (1.1) had written for Whole Earth Review , most notably Bruce Sterling (who was highlighted on the first cover) [2] and Stewart Brand . Other contributors to Whole Earth appeared in Wired , including William Gibson , who was featured on Wired 's cover in its first year and whose article " Disneyland with the Death Penalty " in issue 1.4 resulted in the publication being banned in Singapore. [9]
Wired co-founder Louis Rossetto claimed in the magazine's first issue that "the Digital Revolution is whipping through our lives like a Bengali typhoon," [10] yet despite the fact that Kelly was involved in launching the WELL , an early source of public access to the Internet and even earlier non-Internet online experience, Wired 's first issue de-emphasized the Internet, and covered interactive games, cell-phone hacking, digital special effects, military simulations, and Japanese otaku . However, the first issue did contain a few references to the Internet, including online-dating and Internet sex, and a tutorial on installing a bozo filter . The last page, a column written by Nicholas Negroponte, was written in the style of an e-mail message, but contained obviously fake, non-standard email addresses. By the third issue in the fall of 1993 the "Net Surf" column began listing interesting FTP sites, Usenet newsgroups , and email addresses, at a time when the numbers of these things were small and this information was still extremely novel to the public. Wired was among the first magazines to list the email address of its authors and contributors.
Associate publisher Kathleen Lyman (formerly of News Corporation and Ziff Davis ) was brought on board to launch Wired with an advertising base of major technology and consumer advertisers. Lyman, along with Simon Ferguson ( Wired 's first advertising manager), introduced revolutionary ad campaigns by a diverse group of industry leaders—such as Apple Computer , Intel , Sony , Calvin Klein , and Absolut —to the readers of the first technology publication with a lifestyle slant.
The magazine was quickly followed by a companion website HotWired , a book publishing division, HardWired, a Japanese edition, and a short-lived British edition, Wired UK . Wired UK was relaunched in April 2009. [11] In 1994, John Battelle , co-founding editor, commissioned Jules Marshall to write a piece on the Zippies . The cover story broke records for being one of the most publicized stories of the year and was used to promote Wired's HotWired news service. [12]
HotWired spawned websites Webmonkey , the search engine HotBot , and a weblog , Suck.com . In June 1998, the magazine launched a stock index, The Wired Index , since July 2003 called The Wired 40 .
The fortune of the magazine and allied enterprises corresponded closely to that of the dot-com bubble . In 1996, Rossetto and the other participants in Wired Ventures attempted to take the company public with an IPO . The initial attempt had to be withdrawn in the face of a downturn in the stock market, and especially the Internet sector, during the summer of 1996. The second try was also unsuccessful.
Rossetto and Metcalfe lost control of Wired Ventures to financial investors Providence Equity Partners in May 1998, who quickly sold off the company in pieces. Wired was purchased by Advance Publications , who assigned it to Advance's subsidiary, New York-based publisher Condé Nast Publications (while keeping Wired 's editorial offices in San Francisco). [13] Wired Digital (wired.com, hotbot.com, webmonkey.com, etc.) was purchased by Lycos and run independently from the rest of the magazine until 2006 when it was sold by Lycos to Advance Publications, returning the websites back to the same company that published the magazine.

The Anderson era
Wired survived the dot-com bubble and found new direction under editor-in-chief Chris Anderson in 2001, making the magazine's coverage "more mainstream." [14]
Under Anderson, Wired has produced some widely noted articles, including the April 2003 "Welcome to the Hydrogen Economy" story, the November 2003 "Open Source Everywhere" issue (which put Linus Torvalds on the cover and articulated the idea that the open source method was taking off outside of software, including encyclopedias as evidenced by Wikipedia), the February 2004 "Kiss Your Cubicle Goodbye" issue (which presented the outsourcing issue from both American and Indian perspectives), and an October 2004 article by Chris Anderson, which coined the popular term " Long Tail ."
The November 2004 issue of Wired was published with The Wired CD . All of the songs on the CD were released under various Creative Commons licenses, an attempt to push alternative copyright into the spotlight. Most of the songs were contributed by major artists, including the Beastie Boys , My Morning Jacket , Paul Westerberg , and David Byrne .
In 2005, Wired received the National Magazine Award for General Excellence in the category of 500,000 to 1,000,000 subscribers. [15] That same year Anderson won Advertising Age 's editor of the year award. [15] In May 2007, the magazine again won the National Magazine Award for General Excellence. [16] In 2008, Wired was nominated for three National Magazine Awards and won the ASME for Design. It also took home 14 Society of Publication Design Awards, including the Gold for Magazine of the Year. In 2009, Wired was nominated for four National Magazine Awards – including General Excellence, Design, Best Section (Start), and Integration – and won three: General Excellence, Design and Best Section (Start). David Rowan from Wired UK was awarded the BSME Launch of the Year 2009 award. [17] On December 14, 2009, Wired magazine was named Magazine of the Decade by the editors of Adweek . [18]
In 2006, writer Jeff Howe and editor Mark Robinson coined the term crowdsourcing in the June issue. [19]
In 2009, Condé Nast Italia launched the Italian edition of Wired and Wired.it. [20] On April 2, 2009, Condé Nast relaunched the UK edition of Wired , edited by David Rowan, and launched Wired.co.uk. [21] Also in 2009, Wired writer Evan Ratliff "vanished" attempting to keep his whereabouts secret saying "I will try to stay hidden for 30 days." A $5,000 reward was offered to his finder(s). [22] Ratliff was found September 8 in New Orleans by a team effort, which was written about by Ratliff in a later issue. In 2010, Wired released its Tablet edition.
In 2012, Limor Fried became the first female engineer featured on the cover of Wired . [23]
In May 2013, Wired joined the Digital Video Network with the announcement of five original web series including the National Security Agency satire Codefellas and the animated advice series Mister Know-It-All . [24] [25]

Website
The Wired website, formerly known as Wired News and HotWired , launched in October 1994. [27] It split off from the magazine when it was purchased by Condé Nast Publishing in the 1990s. Wired News was owned by Lycos not long after the split, until Condé Nast purchased Wired News on July 11, 2006.
Wired.com hosts several technology blogs on topics in transportation, security, business, new products, video games, the " GeekDad " blog on toys, creating websites, cameras, culture and science. It also publishes the Vaporware Awards.

WikiLeaks affair
Wired was criticized [28] [29] for its handling of the Adrian Lamo / Chelsea Manning logs. Wired contributor Kevin Poulsen used Lamo to obtain transcripts of the communications between Lamo and Bradley that led to Manning's arrest over the " WikiLeaks " in 2010. Poulsen released approximately one third of the logs, but he and Wired editor in chief Evan Hansen refused to release more on grounds of privacy. The issue became a subject of controversy, [30] when Poulsen and Hansen attacked Wired critic Glenn Greenwald . [31]

NextFest
From 2004 to 2008, Wired organized an annual "festival of innovative products and technologies". [32] A NextFest for 2009 was canceled. [33]

Supplement

Contributors
Wired 's writers have included Jorn Barger , John Perry Barlow , John Battelle , Paul Boutin , Stewart Brand , Gareth Branwyn , Po Bronson , Scott Carney , Michael Chorost , Douglas Coupland , James Daly , Joshua Davis , J. Bradford DeLong , Mark Dery , David Diamond , Cory Doctorow , Esther Dyson , Mark Frauenfelder , Simson Garfinkel , William Gibson , Dan Gillmor Mike Godwin , George Gilder , Lou Ann Hammond , Chris Hardwick , Danny Hillis , Steven Johnson , Bill Joy , Jon Katz , Leander Kahney , Richard Kadrey , Jaron Lanier , Lawrence Lessig , Paul Levinson , Steven Levy , John Markoff , Wil McCarthy , Russ Mitchell , Glyn Moody , Charles Platt , Josh Quittner , Spencer Reiss , Howard Rheingold , Rudy Rucker , Paul Saffo , Adam Savage , Evan Schwartz , Peter Schwartz , Alex Steffen , Neal Stephenson , Bruce Sterling , John Hodgman , Kevin Warwick , Dave Winer , Belinda Parmar and Gary Wolf .
Guest editors have included Barack Obama , Rem Koolhaas , James Cameron , Will Wright , J. J. Abrams , Christopher Nolan and Serena Williams .

See also
WebPage index: 00102
Text corpus
In linguistics , a corpus (plural corpora ) or text corpus is a large and structured set of texts (nowadays usually electronically stored and processed). They are used to do statistical analysis and hypothesis testing , checking occurrences or validating linguistic rules within a specific language territory.

Overview
A corpus may contain texts in a single language ( monolingual corpus ) or text data in multiple languages ( multilingual corpus ).
Multilingual corpora that have been specially formatted for side-by-side comparison are called aligned parallel corpora . There are two main types of parallel corpora which contain texts in two languages. In a translation corpus , the texts in one language are translations of texts in the other language. In a comparable corpus , the texts are of the same kind and cover the same content, but they are not translations of each other. [1] To exploit a parallel text, some kind of text alignment identifying equivalent text segments (phrases or sentences) is a prerequisite for analysis. Machine translation algorithms for translating between two languages are often trained using parallel fragments comprising a first language corpus and a second language corpus which is an element-for-element translation of the first language corpus. [2]
In order to make the corpora more useful for doing linguistic research, they are often subjected to a process known as annotation . An example of annotating a corpus is part-of-speech tagging , or POS-tagging , in which information about each word's part of speech (verb, noun, adjective, etc.) is added to the corpus in the form of tags . Another example is indicating the lemma (base) form of each word. When the language of the corpus is not a working language of the researchers who use it, interlinear glossing is used to make the annotation bilingual.
Some corpora have further structured levels of analysis applied. In particular, a number of smaller corpora may be fully parsed . Such corpora are usually called Treebanks or Parsed Corpora . The difficulty of ensuring that the entire corpus is completely and consistently annotated means that these corpora are usually smaller, containing around one to three million words. Other levels of linguistic structured analysis are possible, including annotations for morphology , semantics and pragmatics .
Corpora are the main knowledge base in corpus linguistics . The analysis and processing of various types of corpora are also the subject of much work in computational linguistics , speech recognition and machine translation , where they are often used to create hidden Markov models for part of speech tagging and other purposes. Corpora and frequency lists derived from them are useful for language teaching . Corpora can be considered as a type of foreign language writing aid as the contextualised grammatical knowledge acquired by non-native language users through exposure to authentic texts in corpora allows learners to grasp the manner of sentence formation in the target language, enabling effective writing. [3]

Archaeological corpora
Text corpora are also used in the study of historical documents , for example in attempts to decipher ancient scripts, or in Biblical scholarship . Some archaeological corpora can be of such short duration that they provide a snapshot in time. One of the shortest corpora in time, may be the 15–30 year Amarna letters texts ( 1350 BC ). The corpus of an ancient city, (for example the " Kültepe Texts" of Turkey), may go through a series of corpora, determined by their find site dates.

Some notable text corpora

See also
WebPage index: 00103
Word-sense disambiguation
In computational linguistics , word-sense disambiguation ( WSD ) is an open problem of natural language processing and ontology . WSD is identifying which sense of a word (i.e. meaning ) is used in a sentence , when the word has multiple meanings . The solution to this problem impacts other computer-related writing, such as discourse , improving relevance of search engines , anaphora resolution , coherence , inference , etcetera .
The human brain is quite proficient at word-sense disambiguation. The fact that natural language is formed in a way that requires so much of it is a reflection of that neurologic reality. In other words, human language developed in a way that reflects (and also has helped to shape) the innate ability provided by the brain's neural networks . In computer science and the information technology that it enables, it has been a long-term challenge to develop the ability in computers to do natural language processing and machine learning .
To date, a rich variety of techniques have been researched, from dictionary-based methods that use the knowledge encoded in lexical resources, to supervised machine learning methods in which a classifier is trained for each distinct word on a corpus of manually sense-annotated examples, to completely unsupervised methods that cluster occurrences of words, thereby inducing word senses. Among these, supervised learning approaches have been the most successful algorithms to date.
Accuracy of current algorithms is difficult to state without a host of caveats. In English, accuracy at the coarse-grained ( homograph ) level is routinely above 90%, with some methods on particular homographs achieving over 96%. On finer-grained sense distinctions, top accuracies from 59.1% to 69.0% have been reported in recent evaluation exercises (SemEval-2007, Senseval-2), where the baseline accuracy of the simplest possible algorithm of always choosing the most frequent sense was 51.4% and 57%, respectively.

About
Disambiguation requires two strict inputs: a dictionary to specify the senses which are to be disambiguated and a corpus of language data to be disambiguated (in some methods, a training corpus of language examples is also required). WSD task has two variants: " lexical sample " and " all words " task. The former comprises disambiguating the occurrences of a small sample of target words which were previously selected, while in the latter all the words in a piece of running text need to be disambiguated. The latter is deemed a more realistic form of evaluation, but the corpus is more expensive to produce because human annotators have to read the definitions for each word in the sequence every time they need to make a tagging judgement, rather than once for a block of instances for the same target word.
To give a hint how all this works, consider two examples of the distinct senses that exist for the (written) word " bass ":
and the sentences:
To a human, it is obvious that the first sentence is using the word " bass (fish) ", as in the former sense above and in the second sentence, the word " bass (instrument) " is being used as in the latter sense below. Developing algorithms to replicate this human ability can often be a difficult task, as is further exemplified by the implicit equivocation between " bass (sound) " and " bass (musical instrument)".

History
WSD was first formulated into as a distinct computational task during the early days of machine translation in the 1940s, making it one of the oldest problems in computational linguistics. Warren Weaver , in his famous 1949 memorandum on translation, [1] first introduced the problem in a computational context. Early researchers understood the significance and difficulty of WSD well. In fact, Bar-Hillel (1960) used the above example to argue [2] that WSD could not be solved by "electronic computer" because of the need in general to model all world knowledge.
In the 1970s, WSD was a subtask of semantic interpretation systems developed within the field of artificial intelligence, starting with Wilks ' preference semantics. However, since WSD systems were at the time largely rule-based and hand-coded they were prone to a knowledge acquisition bottleneck.
By the 1980s large-scale lexical resources, such as the Oxford Advanced Learner's Dictionary of Current English (OALD), became available: hand-coding was replaced with knowledge automatically extracted from these resources, but disambiguation was still knowledge-based or dictionary-based.
In the 1990s, the statistical revolution swept through computational linguistics, and WSD became a paradigm problem on which to apply supervised machine learning techniques.
The 2000s saw supervised techniques reach a plateau in accuracy, and so attention has shifted to coarser-grained senses, domain adaptation, semi-supervised and unsupervised corpus-based systems, combinations of different methods, and the return of knowledge-based systems via graph-based methods. Still, supervised systems continue to perform best.

Difficulties

Differences between dictionaries
One problem with word sense disambiguation is deciding what the senses are. In cases like the word bass above, at least some senses are obviously different. In other cases, however, the different senses can be closely related (one meaning being a metaphorical or metonymic extension of another), and in such cases division of words into senses becomes much more difficult. Different dictionaries and thesauruses will provide different divisions of words into senses. One solution some researchers have used is to choose a particular dictionary, and just use its set of senses. Generally, however, research results using broad distinctions in senses have been much better than those using narrow ones. [3] [4] However, given the lack of a full-fledged coarse-grained sense inventory, most researchers continue to work on fine-grained WSD.
Most research in the field of WSD is performed by using WordNet as a reference sense inventory for English. WordNet is a computational lexicon that encodes concepts as synonym sets (e.g. the concept of car is encoded as { car, auto, automobile, machine, motorcar }). Other resources used for disambiguation purposes include Roget's Thesaurus [5] and Wikipedia . [6] More recently, BabelNet , a multilingual encyclopedic dictionary, has been used for multilingual WSD. [7]

Part-of-speech tagging
In any real test, part-of-speech tagging and sense tagging are very closely related with each potentially making constraints to the other. And the question whether these tasks should be kept together or decoupled is still not unanimously resolved, but recently scientists incline to test these things separately (e.g. in the Senseval/ SemEval competitions parts of speech are provided as input for the text to disambiguate).
It is instructive to compare the word sense disambiguation problem with the problem of part-of-speech tagging. Both involve disambiguating or tagging with words, be it with senses or parts of speech. However, algorithms used for one do not tend to work well for the other, mainly because the part of speech of a word is primarily determined by the immediately adjacent one to three words, whereas the sense of a word may be determined by words further away. The success rate for part-of-speech tagging algorithms is at present much higher than that for WSD, state-of-the art being around 95% [ citation needed ] accuracy or better, as compared to less than 75% [ citation needed ] accuracy in word sense disambiguation with supervised learning . These figures are typical for English, and may be very different from those for other languages.

Inter-judge variance
Another problem is inter-judge variance . WSD systems are normally tested by having their results on a task compared against those of a human. However, while it is relatively easy to assign parts of speech to text, training people to tag senses is far more difficult. [8] While users can memorize all of the possible parts of speech a word can take, it is often impossible for individuals to memorize all of the senses a word can take. Moreover, humans do not agree on the task at hand – give a list of senses and sentences, and humans will not always agree on which word belongs in which sense. [9]
As human performance serves as the standard, it is an upper bound for computer performance. Human performance, however, is much better on coarse-grained than fine-grained distinctions, so this again is why research on coarse-grained distinctions [10] [11] has been put to test in recent WSD evaluation exercises. [3] [4]

Pragmatics
Some AI researchers like Douglas Lenat argue that one cannot parse meanings from words without some form of common sense ontology . This linguistic issue is called pragmatics . For example, comparing these two sentences:
To properly identify senses of words one must know common sense facts. [12] Moreover, sometimes the common sense is needed to disambiguate such words like pronouns in case of having anaphoras or cataphoras in the text.

Sense inventory and algorithms' task-dependency
A task-independent sense inventory is not a coherent concept: [13] each task requires its own division of word meaning into senses relevant to the task. For example, the ambiguity of ' mouse ' (animal or device) is not relevant in English-French machine translation , but is relevant in information retrieval . The opposite is true of 'river', which requires a choice in French ( fleuve 'flows into the sea', or rivière 'flows into a river').
Also, completely different algorithms might be required by different applications. In machine translation, the problem takes the form of target word selection. Here, the "senses" are words in the target language, which often correspond to significant meaning distinctions in the source language ("bank" could translate to the French "banque"—that is, 'financial bank' or "rive"—that is, 'edge of river'). In information retrieval, a sense inventory is not necessarily required, because it is enough to know that a word is used in the same sense in the query and a retrieved document; what sense that is, is unimportant.

Discreteness of senses
Finally, the very notion of " word sense " is slippery and controversial. Most people can agree in distinctions at the coarse-grained homograph level (e.g., pen as writing instrument or enclosure), but go down one level to fine-grained polysemy , and disagreements arise. For example, in Senseval-2, which used fine-grained sense distinctions, human annotators agreed in only 85% of word occurrences. [14] Word meaning is in principle infinitely variable and context sensitive. It does not divide up easily into distinct or discrete sub-meanings. [15] Lexicographers frequently discover in corpora loose and overlapping word meanings, and standard or conventional meanings extended, modulated, and exploited in a bewildering variety of ways. The art of lexicography is to generalize from the corpus to definitions that evoke and explain the full range of meaning of a word, making it seem like words are well-behaved semantically. However, it is not at all clear if these same meaning distinctions are applicable in computational applications , as the decisions of lexicographers are usually driven by other considerations. Recently, a task – named lexical substitution – has been proposed as a possible solution to the sense discreteness problem. [16] The task consists of providing a substitute for a word in context that preserves the meaning of the original word (potentially, substitutes can be chosen from the full lexicon of the target language, thus overcoming discreteness).

Approaches and methods
As in all natural language processing , there are two main approaches to WSD – deep approaches and shallow approaches .
Deep approaches presume access to a comprehensive body of world knowledge . Knowledge, such as "you can go fishing for a type of fish, but not for low frequency sounds" and "songs have low frequency sounds as parts, but not types of fish", is then used to determine in which sense the word bass is used. These approaches are not very successful in practice, mainly because such a body of knowledge does not exist in a computer-readable format, outside very limited domains. [17] However, if such knowledge did exist, then deep approaches would be much more accurate than the shallow approaches. [ citation needed ] Also, there is a long tradition in computational linguistics , of trying such approaches in terms of coded knowledge and in some cases, it is hard to say clearly whether the knowledge involved is linguistic or world knowledge. The first attempt was that by Margaret Masterman and her colleagues, at the Cambridge Language Research Unit in England, in the 1950s. This attempt used as data a punched-card version of Roget's Thesaurus and its numbered "heads", as an indicator of topics and looked for repetitions in text, using a set intersection algorithm. It was not very successful, [18] but had strong relationships to later work, especially Yarowsky's machine learning optimisation of a thesaurus method in the 1990s.
Shallow approaches don't try to understand the text. They just consider the surrounding words, using information such as "if bass has words sea or fishing nearby, it probably is in the fish sense; if bass has the words music or song nearby, it is probably in the music sense." These rules can be automatically derived by the computer, using a training corpus of words tagged with their word senses. This approach, while theoretically not as powerful as deep approaches, gives superior results in practice, due to the computer's limited world knowledge. However, it can be confused by sentences like The dogs bark at the tree which contains the word bark near both tree and dogs .
There are four conventional approaches to WSD:
Almost all these approaches normally work by defining a window of n content words around each word to be disambiguated in the corpus, and statistically analyzing those n surrounding words. Two shallow approaches used to train and then disambiguate are Naïve Bayes classifiers and decision trees . In recent research, kernel-based methods such as support vector machines have shown superior performance in supervised learning . Graph-based approaches have also gained much attention from the research community, and currently achieve performance close to the state of the art.

Dictionary- and knowledge-based methods
The Lesk algorithm [19] is the seminal dictionary-based method. It is based on the hypothesis that words used together in text are related to each other and that the relation can be observed in the definitions of the words and their senses. Two (or more) words are disambiguated by finding the pair of dictionary senses with the greatest word overlap in their dictionary definitions. For example, when disambiguating the words in "pine cone", the definitions of the appropriate senses both include the words evergreen and tree (at least in one dictionary). A similar approach [20] searches for the shortest path between two words: the second word is iteratively searched among the definitions of every semantic variant of the first word, then among the definitions of every semantic variant of each word in the previous definitions and so on. Finally, the first word is disambiguated by selecting the semantic variant which minimizes the distance from the first to the second word.
An alternative to the use of the definitions is to consider general word-sense relatedness and to compute the semantic similarity of each pair of word senses based on a given lexical knowledge base such as WordNet . Graph-based methods reminiscent of spreading activation research of the early days of AI research have been applied with some success. More complex graph-based approaches have been shown to perform almost as well as supervised methods [21] or even outperforming them on specific domains. [3] [22] Recently, it has been reported that simple graph connectivity measures , such as degree , perform state-of-the-art WSD in the presence of a sufficiently rich lexical knowledge base. [23] Also, automatically transferring knowledge in the form of semantic relations from Wikipedia to WordNet has been shown to boost simple knowledge-based methods, enabling them to rival the best supervised systems and even outperform them in a domain-specific setting. [24]
The use of selectional preferences (or selectional restrictions ) is also useful, for example, knowing that one typically cooks food, one can disambiguate the word bass in "I am cooking basses" (i.e., it's not a musical instrument).

Supervised methods
Supervised methods are based on the assumption that the context can provide enough evidence on its own to disambiguate words (hence, common sense and reasoning are deemed unnecessary). Probably every machine learning algorithm going has been applied to WSD, including associated techniques such as feature selection , parameter optimization , and ensemble learning . Support Vector Machines and memory-based learning have been shown to be the most successful approaches, to date, probably because they can cope with the high-dimensionality of the feature space. However, these supervised methods are subject to a new knowledge acquisition bottleneck since they rely on substantial amounts of manually sense-tagged corpora for training, which are laborious and expensive to create.

Semi-supervised methods
Because of the lack of training data, many word sense disambiguation algorithms use semi-supervised learning , which allows both labeled and unlabeled data. The Yarowsky algorithm was an early example of such an algorithm. [25] It uses the ‘One sense per collocation’ and the ‘One sense per discourse’ properties of human languages for word sense disambiguation. From observation, words tend to exhibit only one sense in most given discourse and in a given collocation.
The bootstrapping approach starts from a small amount of seed data for each word: either manually tagged training examples or a small number of surefire decision rules (e.g., 'play' in the context of 'bass' almost always indicates the musical instrument). The seeds are used to train an initial classifier , using any supervised method. This classifier is then used on the untagged portion of the corpus to extract a larger training set, in which only the most confident classifications are included. The process repeats, each new classifier being trained on a successively larger training corpus, until the whole corpus is consumed, or until a given maximum number of iterations is reached.
Other semi-supervised techniques use large quantities of untagged corpora to provide co-occurrence information that supplements the tagged corpora. These techniques have the potential to help in the adaptation of supervised models to different domains.
Also, an ambiguous word in one language is often translated into different words in a second language depending on the sense of the word. Word-aligned bilingual corpora have been used to infer cross-lingual sense distinctions, a kind of semi-supervised system.

Unsupervised methods
Unsupervised learning is the greatest challenge for WSD researchers. The underlying assumption is that similar senses occur in similar contexts, and thus senses can be induced from text by clustering word occurrences using some measure of similarity of context, [26] a task referred to as word sense induction or discrimination. Then, new occurrences of the word can be classified into the closest induced clusters/senses. Performance has been lower than for the other methods described above, but comparisons are difficult since senses induced must be mapped to a known dictionary of word senses. If a mapping to a set of dictionary senses is not desired, cluster-based evaluations (including measures of entropy and purity) can be performed. Alternatively, word sense induction methods can be tested and compared within an application. For instance, it has been shown that word sense induction improves Web search result clustering by increasing the quality of result clusters and the degree diversification of result lists. [27] [28] It is hoped that unsupervised learning will overcome the knowledge acquisition bottleneck because they are not dependent on manual effort.

Other approaches
Other approaches may vary differently in their methods:

Other languages

Local impediments and summary
The knowledge acquisition bottleneck is perhaps the major impediment to solving the WSD problem. Unsupervised methods rely on knowledge about word senses, which is barely formulated in dictionaries and lexical databases. Supervised methods depend crucially on the existence of manually annotated examples for every word sense, a requisite that can so far be met only for a handful of words for testing purposes, as it is done in the Senseval exercises.
Therefore, one of the most promising trends in WSD research is using the largest corpus ever accessible, the World Wide Web , to acquire lexical information automatically. [41] WSD has been traditionally understood as an intermediate language engineering technology which could improve applications such as information retrieval (IR). In this case, however, the reverse is also true: Web search engines implement simple and robust IR techniques that can be successfully used when mining the Web for information to be employed in WSD. Therefore, the lack of training data provoked appearing some new algorithms and techniques described here:

External knowledge sources
Knowledge is a fundamental component of WSD. Knowledge sources provide data which are essential to associate senses with words. They can vary from corpora of texts, either unlabeled or annotated with word senses, to machine-readable dictionaries, thesauri, glossaries, ontologies, etc. They can be [42] [43] classified as follows:
Structured:
Unstructured:

Evaluation
Comparing and evaluating different WSD systems is extremely difﬁcult, because of the different test sets, sense inventories, and knowledge resources adopted. Before the organization of speciﬁc evaluation campaigns most systems were assessed on in-house, often small-scale, data sets . In order to test one's algorithm, developers should spend their time to annotate all word occurrences. And comparing methods even on the same corpus is not eligible if there is different sense inventories.
In order to define common evaluation datasets and procedures, public evaluation campaigns have been organized. Senseval (now renamed SemEval ) is an international word sense disambiguation competition, held every three years since 1998: Senseval-1 (1998), Senseval-2 (2001), Senseval-3 (2004), and its successor, SemEval (2007). The objective of the competition is to organize different lectures, preparing and hand-annotating corpus for testing systems, perform a comparative evaluation of WSD systems in several kinds of tasks, including all-words and lexical sample WSD for different languages, and, more recently, new tasks such as semantic role labeling , gloss WSD , lexical substitution , etc. The systems submitted for evaluation to these competitions usually integrate different techniques and often combine supervised and knowledge-based methods (especially for avoiding bad performance in lack of training examples).
In recent years 2007-2012 , the WSD evaluation task choices had grown and the criterion for evaluating WSD has changed drastically depending on the variant of the WSD evaluation task. Below enumerates the variety of WSD tasks:

Task design choices
As technology evolves, the Word Sense Disambiguation (WSD) tasks grows in different flavors towards various research directions and for more languages:

Software

See also

Notes

Works cited

External links and suggested reading
WebPage index: 00104
PageRank
PageRank (PR) is an algorithm used by Google Search to rank websites in their search engine results. PageRank was named after Larry Page , [1] one of the founders of Google. PageRank is a way of measuring the importance of website pages. According to Google:
It is not the only algorithm used by Google to order search engine results, but it is the first algorithm that was used by the company, and it is the best-known. [3] [4]

Description
PageRank is a link analysis algorithm and it assigns a numerical weighting to each element of a hyperlinked set of documents, such as the World Wide Web , with the purpose of "measuring" its relative importance within the set. The algorithm may be applied to any collection of entities with reciprocal quotations and references. The numerical weight that it assigns to any given element E is referred to as the PageRank of E and denoted by P R ( E ) . {\displaystyle PR(E).} Other factors like Author Rank can contribute to the importance of an entity.
A PageRank results from a mathematical algorithm based on the webgraph , created by all World Wide Web pages as nodes and hyperlinks as edges, taking into consideration authority hubs such as cnn.com or usa.gov . The rank value indicates an importance of a particular page. A hyperlink to a page counts as a vote of support. The PageRank of a page is defined recursively and depends on the number and PageRank metric of all pages that link to it (" incoming links "). A page that is linked to by many pages with high PageRank receives a high rank itself.
Numerous academic papers concerning PageRank have been published since Page and Brin's original paper. [5] In practice, the PageRank concept may be vulnerable to manipulation. Research has been conducted into identifying falsely influenced PageRank rankings. The goal is to find an effective means of ignoring links from documents with falsely influenced PageRank. [6]
Other link-based ranking algorithms for Web pages include the HITS algorithm invented by Jon Kleinberg (used by Teoma and now Ask.com ),the IBM CLEVER project , the TrustRank algorithm and the hummingbird algorithm . [ citation needed ]

History
The eigenvalue problem was suggested in 1976 by Gabriel Pinski and Francis Narin, who worked on scientometrics ranking scientific journals [7] and in 1977 by Thomas Saaty in his concept of Analytic Hierarchy Process which weighted alternative choices. [8]
PageRank was developed at Stanford University by Larry Page and Sergey Brin in 1996 as part of a research project about a new kind of search engine. [9] Sergey Brin had the idea that information on the web could be ordered in a hierarchy by "link popularity": A page is ranked higher as there are more links to it. [10] It was co-authored by Rajeev Motwani and Terry Winograd . The first paper about the project, describing PageRank and the initial prototype of the Google search engine, was published in 1998: [5] shortly after, Page and Brin founded Google Inc. , the company behind the Google search engine. While just one of many factors that determine the ranking of Google search results, PageRank continues to provide the basis for all of Google's web search tools. [11]
The name "PageRank" plays off of the name of developer Larry Page, as well as the concept of a web page . [12] The word is a trademark of Google, and the PageRank process has been patented ( U.S. Patent 6,285,999 ). However, the patent is assigned to Stanford University and not to Google. Google has exclusive license rights on the patent from Stanford University. The university received 1.8 million shares of Google in exchange for use of the patent; the shares were sold in 2005 for $ 336 million. [13] [14]
PageRank was influenced by citation analysis , early developed by Eugene Garfield in the 1950s at the University of Pennsylvania, and by Hyper Search , developed by Massimo Marchiori at the University of Padua. In the same year PageRank was introduced (1998), Jon Kleinberg published his important work on HITS . Google's founders cite Garfield, Marchiori, and Kleinberg in their original papers. [5] [15]
A small search engine called " RankDex " from IDD Information Services designed by Robin Li was, since 1996, already exploring a similar strategy for site-scoring and page ranking. [16] The technology in RankDex would be patented by 1999 [17] and used later when Li founded Baidu in China. [18] [19] Li's work would be referenced by some of Larry Page's U.S. patents for his Google search methods. [20]

Algorithm
The PageRank algorithm outputs a probability distribution used to represent the likelihood that a person randomly clicking on links will arrive at any particular page. PageRank can be calculated for collections of documents of any size. It is assumed in several research papers that the distribution is evenly divided among all documents in the collection at the beginning of the computational process. The PageRank computations require several passes, called "iterations", through the collection to adjust approximate PageRank values to more closely reflect the theoretical true value.
A probability is expressed as a numeric value between 0 and 1. A 0.5 probability is commonly expressed as a "50% chance" of something happening. Hence, a PageRank of 0.5 means there is a 50% chance that a person clicking on a random link will be directed to the document with the 0.5 PageRank.

Simplified algorithm
Assume a small universe of four web pages: A , B , C and D . Links from a page to itself, or multiple outbound links from one single page to another single page, are ignored. PageRank is initialized to the same value for all pages. In the original form of PageRank, the sum of PageRank over all pages was the total number of pages on the web at that time, so each page in this example would have an initial value of 1. However, later versions of PageRank, and the remainder of this section, assume a probability distribution between 0 and 1. Hence the initial value for each page in this example is 0.25.
The PageRank transferred from a given page to the targets of its outbound links upon the next iteration is divided equally among all outbound links.
If the only links in the system were from pages B , C , and D to A , each link would transfer 0.25 PageRank to A upon the next iteration, for a total of 0.75.
Suppose instead that page B had a link to pages C and A , page C had a link to page A , and page D had links to all three pages. Thus, upon the first iteration, page B would transfer half of its existing value, or 0.125, to page A and the other half, or 0.125, to page C . Page C would transfer all of its existing value, 0.25, to the only page it links to, A . Since D had three outbound links, it would transfer one third of its existing value, or approximately 0.083, to A . At the completion of this iteration, page A will have a PageRank of approximately 0.458.
In other words, the PageRank conferred by an outbound link is equal to the document's own PageRank score divided by the number of outbound links L( ) .
In the general case, the PageRank value for any page u can be expressed as:
i.e. the PageRank value for a page u is dependent on the PageRank values for each page v contained in the set B u (the set containing all pages linking to page u ), divided by the number L ( v ) of links from page v .

Damping factor
The PageRank theory holds that an imaginary surfer who is randomly clicking on links will eventually stop clicking. The probability, at any step, that the person will continue is a damping factor d . Various studies have tested different damping factors, but it is generally assumed that the damping factor will be set around 0.85. [5]
The damping factor is subtracted from 1 (and in some variations of the algorithm, the result is divided by the number of documents ( N ) in the collection) and this term is then added to the product of the damping factor and the sum of the incoming PageRank scores. That is,
So any page's PageRank is derived in large part from the PageRanks of other pages. The damping factor adjusts the derived value downward. The original paper, however, gave the following formula, which has led to some confusion:
The difference between them is that the PageRank values in the first formula sum to one, while in the second formula each PageRank is multiplied by N and the sum becomes N . A statement in Page and Brin's paper that "the sum of all PageRanks is one" [5] and claims by other Google employees [21] support the first variant of the formula above.
Page and Brin confused the two formulas in their most popular paper "The Anatomy of a Large-Scale Hypertextual Web Search Engine", where they mistakenly claimed that the latter formula formed a probability distribution over web pages. [5]
Google recalculates PageRank scores each time it crawls the Web and rebuilds its index. As Google increases the number of documents in its collection, the initial approximation of PageRank decreases for all documents.
The formula uses a model of a random surfer who gets bored after several clicks and switches to a random page. The PageRank value of a page reflects the chance that the random surfer will land on that page by clicking on a link. It can be understood as a Markov chain in which the states are pages, and the transitions, which are all equally probable, are the links between pages.
If a page has no links to other pages, it becomes a sink and therefore terminates the random surfing process. If the random surfer arrives at a sink page, it picks another URL at random and continues surfing again.
When calculating PageRank, pages with no outbound links are assumed to link out to all other pages in the collection. Their PageRank scores are therefore divided evenly among all other pages. In other words, to be fair with pages that are not sinks, these random transitions are added to all nodes in the Web, with a residual probability usually set to d = 0.85, estimated from the frequency that an average surfer uses his or her browser's bookmark feature.
So, the equation is as follows:
where p 1 , p 2 , . . . , p N {\displaystyle p_{1},p_{2},...,p_{N}} are the pages under consideration, M ( p i ) {\displaystyle M(p_{i})} is the set of pages that link to p i {\displaystyle p_{i}} , L ( p j ) {\displaystyle L(p_{j})} is the number of outbound links on page p j {\displaystyle p_{j}} , and N {\displaystyle N} is the total number of pages.
The PageRank values are the entries of the dominant right eigenvector of the modified adjacency matrix . This makes PageRank a particularly elegant metric: the eigenvector is
where R is the solution of the equation
where the adjacency function ℓ ( p i , p j ) {\displaystyle \ell (p_{i},p_{j})} is 0 if page p j {\displaystyle p_{j}} does not link to p i {\displaystyle p_{i}} , and normalized such that, for each j
i.e. the elements of each column sum up to 1, so the matrix is a stochastic matrix (for more details see the computation section below). Thus this is a variant of the eigenvector centrality measure used commonly in network analysis .
Because of the large eigengap of the modified adjacency matrix above, [22] the values of the PageRank eigenvector can be approximated to within a high degree of accuracy within only a few iterations.
Google's founders, in their original paper, [15] reported that the PageRank algorithm for a network consisting of 322 million links (in-edges and out-edges) converges to within a tolerable limit in 52 iterations. The convergence in a network of half the above size took approximately 45 iterations. Through this data, they concluded the algorithm can be scaled very well and that the scaling factor for extremely large networks would be roughly linear in log ⁡ n {\displaystyle \log n} , where n is the size of the network.
As a result of Markov theory , it can be shown that the PageRank of a page is the probability of arriving at that page after a large number of clicks. This happens to equal t − 1 {\displaystyle t^{-1}} where t {\displaystyle t} is the expectation of the number of clicks (or random jumps) required to get from the page back to itself.
One main disadvantage of PageRank is that it favors older pages. A new page, even a very good one, will not have many links unless it is part of an existing site (a site being a densely connected set of pages, such as Wikipedia ).
Several strategies have been proposed to accelerate the computation of PageRank. [23]
Various strategies to manipulate PageRank have been employed in concerted efforts to improve search results rankings and monetize advertising links. These strategies have severely impacted the reliability of the PageRank concept, [ citation needed ] which purports to determine which documents are actually highly valued by the Web community.
Since December 2007, when it started actively penalizing sites selling paid text links, Google has combatted link farms and other schemes designed to artificially inflate PageRank. How Google identifies link farms and other PageRank manipulation tools is among Google's trade secrets .

Computation
PageRank can be computed either iteratively or algebraically. The iterative method can be viewed as the power iteration method [24] [25] or the power method. The basic mathematical operations performed are identical.

Iterative
At t = 0 {\displaystyle t=0} , an initial probability distribution is assumed, usually
At each time step, the computation, as detailed above, yields
or in matrix notation
where R i ( t ) = P R ( p i ; t ) {\displaystyle \mathbf {R} _{i}(t)=PR(p_{i};t)} and 1 {\displaystyle \mathbf {1} } is the column vector of length N {\displaystyle N} containing only ones.
The matrix M {\displaystyle {\mathcal {M}}} is defined as
i.e.,
where A {\displaystyle A} denotes the adjacency matrix of the graph and K {\displaystyle K} is the diagonal matrix with the outdegrees in the diagonal.
The computation ends when for some small ϵ {\displaystyle \epsilon }
i.e., when convergence is assumed.

Algebraic
—For t → ∞ {\displaystyle t\to \infty } (i.e., in the steady state ), the above equation (*) reads
The solution is given by
with the identity matrix I {\displaystyle \mathbf {I} } .
The solution exists and is unique for 0 < d < 1 {\displaystyle 0<d<1} . This can be seen by noting that M {\displaystyle {\mathcal {M}}} is by construction a stochastic matrix and hence has an eigenvalue equal to one as a consequence of the Perron–Frobenius theorem .

Power Method
If the matrix M {\displaystyle {\mathcal {M}}} is a transition probability, i.e., column-stochastic and R {\displaystyle \mathbf {R} } is a probability distribution (i.e., | R | = 1 {\displaystyle |\mathbf {R} |=1} , E R = 1 {\displaystyle \mathbf {E} \mathbf {R} =\mathbf {1} } where E {\displaystyle \mathbf {E} } is matrix of all ones), Eq. (**) is equivalent to
Hence PageRank R {\displaystyle \mathbf {R} } is the principal eigenvector of M ^ {\displaystyle {\widehat {\mathcal {M}}}} . A fast and easy way to compute this is using the power method : starting with an arbitrary vector x ( 0 ) {\displaystyle x(0)} , the operator M ^ {\displaystyle {\widehat {\mathcal {M}}}} is applied in succession, i.e.,
until
Note that in Eq. (***) the matrix on the right-hand side in the parenthesis can be interpreted as
where P {\displaystyle \mathbf {P} } is an initial probability distribution. In the current case
Finally, if M {\displaystyle {\mathcal {M}}} has columns with only zero values, they should be replaced with the initial probability vector P {\displaystyle \mathbf {P} } . In other words,
where the matrix D {\displaystyle {\mathcal {D}}} is defined as
with
In this case, the above two computations using M {\displaystyle {\mathcal {M}}} only give the same PageRank if their results are normalized:
PageRank MATLAB / Octave implementation
Example of code calling the rank function defined above:
This example takes 13 iterations to converge.

Variations

PageRank of an undirected graph
The PageRank of an undirected graph G is statistically close to the degree distribution of the graph G, [26] but they are generally not identical: If R is the PageRank vector defined above, and D is the degree distribution vector
where d e g ( p i ) {\displaystyle deg(p_{i})} denotes the degree of vertex p i {\displaystyle p_{i}} , and E is the edge-set of the graph, then, with Y = 1 N 1 {\displaystyle Y={1 \over N}\mathbf {1} } , by: [27]
1 − d 1 + d ∥ Y − D ∥ 1 ≤ ∥ R − D ∥ 1 ≤ ∥ Y − D ∥ 1 , {\displaystyle {1-d \over 1+d}\|Y-D\|_{1}\leq \|R-D\|_{1}\leq \|Y-D\|_{1},}
that is, the PageRank of an undirected graph equals to the degree distribution vector if and only if the graph is regular, i.e., every vertex has the same degree.

Distributed algorithm for PageRank computation
There are simple and fast random walk -based distributed algorithms for computing PageRank of nodes in a network. [28] They present a simple algorithm that takes O ( log ⁡ n / ϵ ) {\displaystyle O(\log n/\epsilon )} rounds with high probability on any graph (directed or undirected), where n is the network size and ϵ {\displaystyle \epsilon } is the reset probability ( 1 − ϵ {\displaystyle 1-\epsilon } is also called as damping factor) used in the PageRank computation. They also present a faster algorithm that takes O ( log ⁡ n / ϵ ) {\displaystyle O({\sqrt {\log n}}/\epsilon )} rounds in undirected graphs. Both of the above algorithms are scalable, as each node processes and sends only small (polylogarithmic in n, the network size) number of bits per round.

Google Toolbar
The Google Toolbar long had a PageRank feature which displayed a visited page's PageRank as a whole number between 0 and 10. The most popular websites displayed a PageRank of 10. The least showed a PageRank of 0. Google has not disclosed the specific method for determining a Toolbar PageRank value, which is to be considered only a rough indication of the value of a website. In March 2016 Google announced it would no longer support this feature, and the underlying API would soon cease to operate. [29]

SERP rank
The search engine results page (SERP) is the actual result returned by a search engine in response to a keyword query. The SERP consists of a list of links to web pages with associated text snippets. The SERP rank of a web page refers to the placement of the corresponding link on the SERP, where higher placement means higher SERP rank. The SERP rank of a web page is a function not only of its PageRank, but of a relatively large and continuously adjusted set of factors (over 200). [30] Search engine optimization (SEO) is aimed at influencing the SERP rank for a website or a set of web pages.
Positioning of a webpage on Google SERPs for a keyword depends on relevance and reputation, also known as authority and popularity. PageRank is Google’s indication of its assessment of the reputation of a webpage: It is non-keyword specific. Google uses a combination of webpage and website authority to determine the overall authority of a webpage competing for a keyword. [31] The PageRank of the HomePage of a website is the best indication Google offers for website authority. [32]
After the introduction of Google Places into the mainstream organic SERP, numerous other factors in addition to PageRank affect ranking a business in Local Business Results. [33]

Google directory PageRank
The Google Directory PageRank was an 8-unit measurement. Unlike the Google Toolbar, which shows a numeric PageRank value upon mouseover of the green bar, the Google Directory only displayed the bar, never the numeric values. Google Directory was closed on July 20, 2011. [34]

False or spoofed PageRank
In the past, the PageRank shown in the Toolbar was easily manipulated. Redirection from one page to another, either via a HTTP 302 response or a "Refresh" meta tag , caused the source page to acquire the PageRank of the destination page. Hence, a new page with PR 0 and no incoming links could have acquired PR 10 by redirecting to the Google home page. This spoofing technique was a known vulnerability. Spoofing can generally be detected by performing a Google search for a source URL; if the URL of an entirely different site is displayed in the results, the latter URL may represent the destination of a redirection.

Manipulating PageRank
For search engine optimization purposes, some companies offer to sell high PageRank links to webmasters. [35] As links from higher-PR pages are believed to be more valuable, they tend to be more expensive. It can be an effective and viable marketing strategy to buy link advertisements on content pages of quality and relevant sites to drive traffic and increase a webmaster's link popularity. However, Google has publicly warned webmasters that if they are or were discovered to be selling links for the purpose of conferring PageRank and reputation, their links will be devalued (ignored in the calculation of other pages' PageRanks). The practice of buying and selling links is intensely debated across the Webmaster community. Google advises webmasters to use the nofollow HTML attribute value on sponsored links. According to Matt Cutts , Google is concerned about webmasters who try to game the system , and thereby reduce the quality and relevance of Google search results. [35]

Directed Surfer Model
A more intelligent surfer that probabilistically hops from page to page depending on the content of the pages and query terms the surfer that it is looking for. This model is based on a query-dependent PageRank score of a page which as the name suggests is also a function of query. When given a multiple-term query, Q={q1,q2,…}, the surfer selects a q according to some probability distribution, P(q) and uses that term to guide its behavior for a large number of steps. It then selects another term according to the distribution to determine its behavior, and so on. The resulting distribution over visited web pages is QD-PageRank. [36]

Social components
The PageRank algorithm has major effects on society as it contains a social influence. As opposed to the scientific viewpoint of PageRank as an algorithm the humanities instead view it through a lens examining its social components. In these instances, it is dissected and reviewed not for its technological advancement in the field of search engines, but for its societal influences.
[37] Laura Granka discusses PageRank by describing how the pages are not simply ranked via popularity as they contain a reliability that gives them a trustworthy quality. This has led to a development of behavior that is directly linked to PageRank. PageRank is viewed as the definitive rank of products and businesses and thus, can manipulate thinking. The information that is available to individuals is what shapes thinking and ideology and PageRank is the device that displays this information. The results shown are the forum to which information is delivered to the public and these results have a societal impact as they will affect how a person thinks and acts.
[38] Katja Mayer views PageRank as a social network as it connects differing viewpoints and thoughts in a single place. People go to PageRank for information and are flooded with citations of other authors who also have an opinion on the topic. This creates a social aspect where everything can be discussed and collected to provoke thinking. There is a social relationship that exists between PageRank and the people who use it as it is constantly adapting and changing to the shifts in modern society. Viewing the relationship between PageRank and the individual through sociometry allows for an in-depth look at the connection that results.
[39] Matteo Pasquinelli reckons the basis for the belief that PageRank has a social component lies in the idea of attention economy . With attention economy, value is placed on products that receive a greater amount of human attention and the results at the top of the PageRank garner a larger amount of focus then those on subsequent pages. The outcomes with the higher PageRank will therefore enter the human consciousness to a larger extent. These ideas can influence decision-making and the actions of the viewer have a direct relation to the PageRank. They possess a higher potential to attract a user’s attention as their location increases the attention economy attached to the site. With this location they can receive more traffic and their online marketplace will have more purchases. The PageRank of these sites allow them to be trusted and they are able to parlay this trust into increased business.

Other uses
The mathematics of PageRank are entirely general and apply to any graph or network in any domain. Thus, PageRank is now regularly used in bibliometrics, social and information network analysis, and for link prediction and recommendation. It's even used for systems analysis of road networks, as well as biology, chemistry, neuroscience, and physics. [40]
In neuroscience , the PageRank of a neuron in a neural network has been found to correlate with its relative firing rate. [41]
Personalized PageRank is used by Twitter to present users with other accounts they may wish to follow. [42]
Swiftype 's site search product builds a "PageRank that’s specific to individual websites" by looking at each website's signals of importance and prioritizing content based on factors such as number of links from the home page. [43]
A version of PageRank has recently been proposed as a replacement for the traditional Institute for Scientific Information (ISI) impact factor , [44] and implemented at Eigenfactor as well as at SCImago . Instead of merely counting total citation to a journal, the "importance" of each citation is determined in a PageRank fashion.
A similar new use of PageRank is to rank academic doctoral programs based on their records of placing their graduates in faculty positions. In PageRank terms, academic departments link to each other by hiring their faculty from each other (and from themselves). [45]
PageRank has been used to rank spaces or streets to predict how many people (pedestrians or vehicles) come to the individual spaces or streets. [46] [47] In lexical semantics it has been used to perform Word Sense Disambiguation , [48] Semantic similarity , [49] and also to automatically rank WordNet synsets according to how strongly they possess a given semantic property, such as positivity or negativity. [50]
A Web crawler may use PageRank as one of a number of importance metrics it uses to determine which URL to visit during a crawl of the web. One of the early working papers [51] that were used in the creation of Google is Efficient crawling through URL ordering , [52] which discusses the use of a number of different importance metrics to determine how deeply, and how much of a site Google will crawl. PageRank is presented as one of a number of these importance metrics, though there are others listed such as the number of inbound and outbound links for a URL, and the distance from the root directory on a site to the URL.
The PageRank may also be used as a methodology to measure the apparent impact of a community like the Blogosphere on the overall Web itself. This approach uses therefore the PageRank to measure the distribution of attention in reflection of the Scale-free network paradigm. [ citation needed ]
In any ecosystem, a modified version of PageRank may be used to determine species that are essential to the continuing health of the environment. [53]
For the analysis of protein networks in biology PageRank is also a useful tool. [54] [55]
In 2005, in a pilot study in Pakistan, Structural Deep Democracy, SD2 [56] [57] was used for leadership selection in a sustainable agriculture group called Contact Youth. SD2 uses PageRank for the processing of the transitive proxy votes, with the additional constraints of mandating at least two initial proxies per voter, and all voters are proxy candidates. More complex variants can be built on top of SD2, such as adding specialist proxies and direct votes for specific issues, but SD2 as the underlying umbrella system, mandates that generalist proxies should always be used.
Pagerank has recently been used to quantify the scientific impact of researchers. The underlying citation and collaboration networks are used in conjunction with pagerank algorithm in order to come up with a ranking system for individual publications which propagates to individual authors. The new index known as pagerank-index (Pi) is demonstrated to be fairer compared to h-index in the context of many drawbacks exhibited by h-index. [58]

nofollow
In early 2005, Google implemented a new value, " nofollow ", [59] for the rel attribute of HTML link and anchor elements, so that website developers and bloggers can make links that Google will not consider for the purposes of PageRank—they are links that no longer constitute a "vote" in the PageRank system. The nofollow relationship was added in an attempt to help combat spamdexing .
As an example, people could previously create many message-board posts with links to their website to artificially inflate their PageRank. With the nofollow value, message-board administrators can modify their code to automatically insert "rel='nofollow'" to all hyperlinks in posts, thus preventing PageRank from being affected by those particular posts. This method of avoidance, however, also has various drawbacks, such as reducing the link value of legitimate comments. (See: Spam in blogs#nofollow )
In an effort to manually control the flow of PageRank among pages within a website, many webmasters practice what is known as PageRank Sculpting [60] —which is the act of strategically placing the nofollow attribute on certain internal links of a website in order to funnel PageRank towards those pages the webmaster deemed most important. This tactic has been used since the inception of the nofollow attribute, but may no longer be effective since Google announced that blocking PageRank transfer with nofollow does not redirect that PageRank to other links. [61]

Deprecation
PageRank was once available for the verified site maintainers through the Google Webmaster Tools interface. However, on October 15, 2009, a Google employee confirmed that the company had removed PageRank from its Webmaster Tools section, saying that "We've been telling people for a long time that they shouldn't focus on PageRank so much. Many site owners seem to think it's the most important metric for them to track, which is simply not true." [62] In addition, The PageRank indicator is not available in Google's own Chrome browser.
The visible page rank is updated very infrequently. It was last updated in November 2013. In October 2014 Matt Cutts announced that another visible pagerank update would not be coming. [63]
Even though "Toolbar" PageRank is less important for SEO purposes, the existence of back-links from more popular websites continues to push a webpage higher up in search rankings. [64]
Google elaborated on the reasons for PageRank deprecation at Q&A #March and announced Links and Content as the Top Ranking Factors, RankBrain was announced as the #3 Ranking Factor in October 2015 so the Top 3 Factors are now confirmed officially by Google. [65]
On April 15, 2016 Google has officially shut down their Google Toolbar PageRank Data to public. Google had declared their intention to remove the PageRank score from the Google toolbar several months earlier. [66] Google will still be using PageRank score when determining how to rank content in search results. [67]

See also

Notes
WebPage index: 00105
h2g2
The h2g2 website is a British-based collaborative online encyclopedia project engaged in the construction of, in its own words, "an unconventional guide to life , the universe , and everything ", in the spirit of the fictional publication The Hitchhiker's Guide to the Galaxy from the science fiction comedy series of the same name by Douglas Adams . [1] It was founded by Adams in 1999 and was run by the BBC between 2001 and 2011. [2] [3] [4]
The intent is to create an Earth -focused guide that allows members to share information about their geographic area and the local sites, activities and businesses, to help people decide where they want to go and what they may find when they get there. It has grown to contain subjects from restaurants and recipes, to quantum theory and history . Explicit advertising of businesses was forbidden when the site was run by the BBC, but customer reviews were permitted. [5]
The content of the project is written by registered "Researchers" on its website. [6] Articles written by Researchers form the "Guide" as a whole, with an "Edited Guide" being steadily created out of factual articles that have been peer reviewed via the "Peer Review" system. [ citation needed ] The Edited Guide includes both traditional encyclopaedic subjects and more idiosyncratic offerings, and while articles in the Edited Guide sometimes aim for a slightly humorous style, [7] most are correct and well-written treatment of their subject matter by virtue of the Peer Review process.

History
The h2g2 site was founded on 28 April 1999 as the Earth edition of The Hitchhiker's Guide to the Galaxy by the author of the series, Douglas Adams , and his friends and colleagues at The Digital Village . [8] "h2g2" is an abbreviation for the title as well as part of the url. The site was a runner-up for Best Community Site in the Yell.com awards in 2000. [9]
Like other dot-com companies , Adams' company TDV ran into financial difficulties towards the end of 2000 and eventually ceased operations. [4] In January 2001, the management of the site was taken over by the BBC , and moved to bbc.co.uk (then known as BBCi). [3]
21 April 2005 marked the launch of h2g2 Mobile, an edition of the guide produced for PDAs (Personal Digital Assistants) and mobile phones that could access the internet, so that people could read h2g2 entries while on the move. [10] This was done because people wanted h2g2 to be much like the Hitchhiker's Guide described in the books — a mobile, electronic device that anyone could read from anywhere. [8] An earlier attempt at a WAP phone based version of h2g2 started in December 2000 only to end when the BBC took over the site in January 2001. [11]
The site was redesigned for the BBC by Aerian Studios in 2011, [12] bringing it in line with the general appearance of other BBC websites, while maintaining a degree of the site's old character. [13]
On 24 January 2011, the BBC announced cuts of 25% to its online budget, resulting in a £34 million less investment into the site. A number of sites were to be closed, including BBC Switch , BBC Blast and 6-0-6 . As part of this exercise, the BBC chose to sell h2g2. [14] On 21 June 2011, it was announced the winning bid was a joint bid put together by three parties: Robbie Stamp , h2g2c2 ('The h2g2 Community Consortium') and the owners of Lycos Chat (Brian Larholm and Alyson Larholm) [15] On 31 August 2011, it was announced h2g2 was sold to Not Panicking Ltd , a company founded by Robbie Stamp , Brian Larholm and Alyson Larholm, as well as The h2g2 Community Consortium. [16]
On 3 October 2011, the BBC incarnation of h2g2 closed, leaving only an announcement reading "H2G2 has now left the BBC. The new owners of H2G2 are currently preparing the site for relaunch. Soon you will find The Guide to Life, the Universe and Everything at www.h2g2.com" [17]
The post-BBC version of the site went live on 16 October 2011.

Terms and conditions
To contribute to the site it is necessary to register and to agree to the h2g2 "House Rules" and the general Not Panicking Ltd Terms and Conditions. Registered users are called Researchers . Researchers retain the copyright to their articles, but grant Not Panicking Ltd a non-exclusive license to reproduce their work in all formats.

DNA
Part of the software for h2g2 is known as DNA, after the initials of author and site founder Douglas Adams . The DNA technology was introduced a few months after the BBC takeover and is still used for BBC blogs, messageboards and commenting systems. Before that there was a technology which was written mostly in Perl .
Adams was involved in the website in its early days. [18] His account name was DNA, and his user number was 42, a reference to the joke in The Hitchhiker's Guide to the Galaxy that the Answer to the Ultimate Question of Life, the Universe and Everything is 42. Adams's legacy is still felt on h2g2, though it is not a fan site. [2]

See also
WebPage index: 00106
Baidu Baike
Baidu Baike [1] / ˈ b aɪ d uː ˈ b aɪ k ə / ( Chinese : 百度 百科 ; pinyin : Bǎidù Bǎikē ; literally: "Baidu Encyclopedia") is a Chinese-language, collaborative, web-based encyclopedia owned and produced by the Chinese search engine Baidu . Its test version was released on 20 April 2006, and within three weeks the encyclopedia had grown to more than 90,000 articles surpassing the number in Chinese Wikipedia . By 2008, Hudong.com had surpassed both in article count, but Baidu Baike later became number one again. [ when? ] The encyclopedia censors its content in accordance with the requirements of the Chinese government. [2] [3] [4] [5] As of July 2016 [update] , Baidu Baike has more than 13 million articles. [6]
Baidu states officially that Baidu Baike serves as an online encyclopedia as well as information storage space for netizens. Baidu Baike claims "equality", "cooperation", "sharing" and "freedom" spiritually, and connect this online platform with search engines technically in order to fulfil the needs of the users for information of different levels. [7] When searching with the search engine Baidu , the link of the corresponding entry in Baidu Baike, if exists, will be put as the first result or one of the first results. [8]

Conception
Baidu's William Chang said at WWW2008, the conference of the World Wide Web Consortium , "There is, in fact, no reason for China to use Wikipedia ... It's very natural for China to make its own products." [9]

Content restrictions
Articles or comments containing the following types of content are removed: [10] [ need quotation to verify ]

Censorship
Being in the jurisdiction of the Chinese government, Baidu is required to censor content on their encyclopedia in accordance to relevant governmental regulations. All editors need to register accounts using their real names before editing, and administrators filter edits before they go public. [11] Users on microblogging platforms generally perceive Baidu Baike as similar to a governmentally sanctioned information source due to the censorship of its content. [12] As of August 2013, no articles on the 1989 Tiananmen Square Protests , the Xinjiang independence movement , or the Falun Gong appear on the encyclopedia. [3]
PCWorld states that complying with Chinese censorship laws gives Baidu Baike an advantage over its competitors. Since the Chinese version of Wikipedia does not censor its own content, the government may block it while keeping Baidu Baike accessible. [11]

Formation

Front page
The current front page of Baidu Baike was put into use on 6 September 2012. At the top of the page the slogan "Let all humanity learn the world equally," as well as current information on the number of users and entries. On the bottom-left of the front page selected contents are presented; the bottom right contains announcements, plans and projects, etc. The front page information usually includes current hot topics, often related to featured news. Beside those hot topics, there are also one-sentence summaries of the news. Other than the front page, Baidu Baike also includes channels such as nature, culture, geography, and special topics such as core-users, digital museums, etc.

Entries
The entry pages of Baidu Baike include calling cards, texts, and other supporting information. An earlier version allowed users to comment on pages, but this feature was removed after September 2008. [13] The main language used is Chinese, written using the Simplified script ; posts written in Traditional Chinese, Korean, Vietnamese, or Japanese are automatically translated .

Calling cards and texts
Baike calling cards contain two parts - the description of an entry and the basic information. The former is similar to the preface of an article, which provides a general introduction to the whole text; the latter uses a table to summarize basic information and statistics. Both are edited separately from the main body of the article. The main texts are limited to 40 thousand bits, which is equivalent to 20 thousand Chinese characters. At the end of the article is the declaration of exemption. The earlier version of the entry can only contain one image, but now 20 images and 20 albums can be included in an article. The entry can also link to a Baidu map, dynamic screenshots, and videos. Information cited is listed beneath the main text, which does not distinguish footnotes from other details. However, the images cited do not specify the source, but contain image directories.

Supporting information
Most pieces of supporting information are located at the bottom or the right of the page, except scientific terms, which are pushed to the top. Usually, supporting information is automatically generated by the system, with some edited by hand. Supporting information on the right of the page contains personal information, statistics, today in history, contribution of honor, current trends, and pop-links. Statistics include the number of page views, the number of editors, latest updates, creators information, etc. Editors who contribute complex articles are also mentioned and honoured. [14] [ need quotation to verify ]
A user can see a database, the Baidu Dictionary, and related entries at the bottom of each article. Celebrities' articles have appended databases that list their single songs, albums, and videos. The user recommendations section is used to collect feedback about the article. [ citation needed ]

Scope

Contents and categories
Unlike a traditional encyclopedia, the information on Baidu Baike is broader, similar to Chinese Wikipedia and Hudong.com . Baidu Baike also includes food recipes, film products, internet programs, and video games as well as its encyclopedic content. However, unlike Chinese Wikipedia, Baidu Baike never gives a strict definition of the difference between an encyclopedia entry and a dictionary entry. There are also many explanations on diction and common phrases.
In addition to articles, Baidu Baike includes several special pages:
Baidu Baike holds open policies on the addition of entries, and support categorized search functions. The categorization is based on the characteristics of an entry but not the quality, [16] and there is no limit to categorization. Baidu Baike now has an elementary categorized page and tree. Part of the open categorization can be set according to levels, but catalogs having similar meaning can not be merged or redirected.

Numbers of entries
According to the latest list of entries, Baidu Baike has an increase of about 3,800 entries per day. Here are some milestones:

Copyright
Baidu Baike's copyright policy is outlined in the "Terms of Use" section of its help page. It states that by adding content to the site, users agree to assign Baidu rights to their original contributions. It also states that users cannot violate intellectual property law, and that contributions which quote works held under the Creative Commons and/or GNU Free Documentation License (GFDL) must follow the restrictions of those licenses. [23]

See also
WebPage index: 00107
Outline of Wikipedia
Wikipedia – a free , web-based , collaborative and multilingual encyclopedia project supported by the non-profit Wikimedia Foundation . Its more than 20 million articles ( over 5.4 million in English ) have been written collaboratively by volunteers around the world. Almost all of its articles can be edited by anyone with access to the site, [1] and it has about 100,000 regularly active contributors. [2]

What 

Implementation of Wikipedia

Wikipedia community

Viewing Wikipedia off-line

Diffusion of Wikipedia

Websites that use Wikipedia

Websites that mirror Wikipedia

Wikipedia derived encyclopedias

Parodies of Wikipedia

Wikipedia-related media

Books about Wikipedia

Films about Wikipedia

Third-party software related to Wikipedia

Mobile apps

Reliability analysis programs

General Wikipedia concepts

Politics of Wikipedia

History of Wikipedia
History of Wikipedia – Wikipedia was formally launched on 15 January 2001 by Jimmy Wales and Larry Sanger, using the concept and technology of a wiki pioneered by Ward Cunningham. Initially, Wikipedia was created to complement Nupedia, an online encyclopedia project edited solely by experts, by providing additional draft articles and ideas for it. Wikipedia quickly overtook Nupedia, becoming a global project in multiple languages and inspiring a wide range of additional reference projects.

Wikipedia-inspired projects

Wikipedia in culture
Wikipedia in culture –

People in relation to Wikipedia

Critics of Wikipedia

Wikipedia Foundations and Organizations

Wikipedia-related projects

Wikipedia's sister projects
Wikimedia projects

Wikipedias by language

See also
WebPage index: 00108
List of online encyclopedias
This is a list of encyclopedias accessible on the Internet .

General reference

Biography

Antiquities, arts, and literature

Regional interest

Pop culture and fiction

Mathematics

Music

Philosophy

Politics and history

Religion and theology

Science and technology

Life sciences

See also
WebPage index: 00109
The San Diego Union-Tribune
The San Diego Union-Tribune is an American metropolitan daily newspaper , published in San Diego, California .
Its name derives from a 1992 merger between the two major daily newspapers at the time, The San Diego Union and the San Diego Evening Tribune . The name changed to U-T San Diego in 2012 but was changed again to The San Diego Union-Tribune in 2015. [2] In 2015, it was acquired by Tribune Publishing, later renamed Tronc .

History

Predecessors
The predecessor newspapers of the Union-Tribune were: [3] [4]
In addition, the San Diego Union purchased the San Diego Daily Bee in 1888, and for a brief time the combined newspaper was named the San Diego Union and Daily Bee . [6]

Acquisition and merger by Copley Press (and subsequent sale to Platinum Equity)
Both the Union and the Tribune were acquired by Copley Press in 1928 [7] and were merged on February 2, 1992. The merged newspaper was sold to the private investment group Platinum Equity of Beverly Hills , California, on March 18, 2009. [8]

Redesign
On August 17, 2010, the Union-Tribune changed its design to improve "clarity, legibility, and ease of use". Changes included being printed on thinner, 100 percent recycled paper, moving the comics to the back of the business section, and abbreviating the title The San Diego Union-Tribune on the front page to U-T San Diego . [9] The U-T nameplate was created by Jim Parkinson, a type designer who also created nameplates for The Rolling Stone , Esquire , and Newsweek . [10]

Purchase by MLIM Holdings
In November 2011, Platinum Equity sold the newspaper to MLIM Holdings, a company led by Doug Manchester , a San Diego real estate developer and "an outspoken supporter of conservative causes". The purchase price was reportedly in excess of $110 million. [11] Manchester built two landmark downtown hotels, the Manchester Grand Hyatt Hotel and the San Diego Marriott Hotel and Marina . His group also owns the Grand Del Mar luxury resort in San Diego. [12]

Rebranding to 
On January 3, 2012, the newspaper announced that it would use the name U-T San Diego "on all of our media products and communications"; the newspaper's website (formerly SignOnSanDiego.com) would use the name UTSanDiego.com. The official announcement explained the change as being intended to "unify our print and digital products under a single brand with a clear and consistent expectation of quality". [13] [14] [15]

Acquisition of the 
U-T San Diego bought the North County Times in 2012. [16] On October 15, 2012, the North County Times ceased publication and became the U-T North County Times , which was an edition of the U-T with some North County–specific content. [17] Six months later the U-T North County Times name was dropped and the newspaper became a North County edition of the U-T . [ citation needed ]

U-T TV
In June 2012, U-T San Diego launched U-T TV , a television news channel . The network featured news, lifestyle, and editorial content produced by the newspaper's staff, and was created as part of the newspaper's growing emphasis on multi-platform content under Manchester.
By October 2013, just over a year after its launch, the network re-formatted with a focus on news, amidst a number of major departures among the channel's staff.
On February 19, 2014, U-T TV was discontinued, but the network's remaining staff was retained to produce video content for the newspaper's digital properties. [18] [19]

Acquisition of weekly newspapers
In November 2013, the newspaper bought eight more local weekly newspapers in the San Diego area, which continued publication under their own names. [20]

Purchase by Tribune Publishing
On May 7, 2015, it was announced that the Tribune Publishing Company , publisher of the Los Angeles Times , the Chicago Tribune , and other newspapers, had reached a deal to acquire U-T San Diego and its associated properties for $85 million. The sale ended the newspaper's 146 years of private ownership.
The transaction was completed on May 21, 2015. On the same date, the newspaper reintroduced its previous branding as The San Diego Union-Tribune . [2]
The Union-Tribune and the Los Angeles Times became part of a new operating entity known as the California News Group, with both newspapers led by Times publisher and chief executive officer Austin Beutner . The two newspapers reportedly would retain distinct operations, but there would be a larger amount of synergy and content sharing between them.
The acquisition did not include the newspaper's headquarters, which was retained by Manchester and would be leased by the newspaper. [21] [22]
On May 26, 2015, the newspaper announced it would lay off 178 employees, representing about thirty percent of the total staff, as it consolidated its printing operations with the Times in Los Angeles. [23]

Closure of San Diego printing facilities
On June 13, 2015, at 10:02 p.m. PDT the final run of The San Diego Union Tribune was printed at the San Diego headquarters in Mission Valley began. [24] It was to print the Sunday edition newspaper for June 14, 2015. The following Monday's newspaper would be printed at the Los Angeles Times location. The dismantling of the printing presses in Mission Valley began in mid-September 2015.

Gannett offer and name change
In 2016 rival newspaper publisher Gannett Company offered to buy the Tribune Publishing Company. The offer was rejected by management, spurring some shareholder dissatisfaction and a shareholder lawsuit. Meanwhile, the Tribune Publishing Company renamed itself Tronc Inc. Tronc is an acronym for Tribune online content. Effective June 20, the renamed company will trade on the NASDAQ exchange under the symbol TRNC. [25]

Headquarters
The newspaper was originally located in Old Town San Diego, and was moved to downtown San Diego in 1871. In 1973, it moved to a custom-built, brick and stone office and printing plant complex in Mission Valley .
The newspaper moved back downtown in May 2016, to offices on the 9th through 12th floor of a tower at 600 B Street. The Union-Tribune is to be the named tenant of the building, replacing Bridgepoint Education and, before that, Comerica. [26]

Awards

Pulitzer Prizes

Criticisms

Copleys and Platinum Equity
Under the Copleys' ownership, the newspaper had a reliably conservative editorial position, endorsing almost exclusively Republicans for elective office, and sometimes refusing to interview or cover Democratic candidates.
Under Platinum Equity, the newspaper's editorial position "skewed closer to the middle" and showcased multiple viewpoints. [31]

Manchester and Lynch
When Manchester and business partner John Lynch took ownership in 2011, they were open about their desire to use the newspaper to "promote their agenda of downtown development and politically conservative causes", [32] with Lynch stating on KPBS radio that he and Manchester "wanted to be cheerleaders for all that is good in San Diego". [33] Lynch expanded on this position in 2012, saying "We make no apologies. We are doing what a newspaper ought to do, which is to take positions. We are very consistent—pro-conservative, pro-business, pro-military—and we are trying to make a newspaper that gets people excited about this city and its future." [34]
This open promotion of certain viewpoints resulted in criticism from journalism professors and other newspaper editors, who worried that negative news about topics such as the military and business might not be covered. [35] Dean Nelson, director of the journalism program at Point Loma Nazarene University , argued, "Now if you're saying we're going to be the cheerleaders of the military, why would you report on this guy that's taking bribes?... Where's the cheerleading there?" a reference to the Union-Tribune' s Pulitzer Prize winning coverage of the Duke Cunningham bribery scandal. [36] A New York Times writer added, "There is a growing worry that the falling value and failing business models of many American newspapers could lead to a situation where moneyed interests buy papers and use them to prosecute a political and commercial agenda. That future appears to have arrived in San Diego." [34]
Lynch said, "We totally respect the journalistic integrity of our paper and there is a clear line of demarcation between our editorials and our news. Our editor, Jeff Light, calls the shots." However, in November 2011 Lynch told the sports editor that the sports pages should advocate for a new football stadium; when a longtime sportswriter wrote skeptically about the idea, he was fired. [34]

Downtown redevelopment
In January 2012, two months after Manchester bought the U-T , the newspaper featured a front-page proposal for downtown redevelopment, to include a downtown football stadium and an expansion of the San Diego Convention Center . [37] Both properties are adjacent to hotels that Manchester owns. [38]
In September 2012, Investigative Newsource reporter Brooke Williams obtained articles that claimed Lynch "threatened" Port Commissioner Scott Peters , who was running for Congress, "with a newspaper campaign to dismantle the Unified Port of San Diego ". In e-mails obtained by Williams, Lynch was quoted as indicating that if the Dole Food Company obtained a long-term contract, that the Port's independence governance would be questioned in editorial coverage. Williams said the effort showed "the extent to which the newspaper's new owners will go to push their vision for a football stadium on the Tenth Avenue Marine Terminal", [39]

Endorsements and polling
During the 2012 mayoral election the owners of the U-T donated to Republican City Council Member Carl DeMaio 's campaign, [40] and the newspaper ran several prominent editorials favoring DeMaio. Those endorsements were wrapped around the front section of the newspaper on a separate page, "as though they were even more important" than the front page. [41]
In October 2012, a poll was taken by the U-T asking respondents to choose between DeMaio and Democratic Congressman Bob Filner in the mayoral election to be held in November. A rival news outlet noted that "Employees of a newspaper, television / radio station, marketing / public opinion research company or the city of San Diego—or who live with someone employed in one of those fields" were excluded from the poll results, which showed the Republican leading the Democrat, 46 percent to 36 percent. Reporter Kelly Davis of SDCityBeat.com wrote: "Common sense dictates that those votes [by city employees or those living with them] would swing in Filner's favor due to DeMaio's long-running feud with city-employee unions." But U-T assignment editor Michael Smolens replied that "city employees were excluded to avoid political entanglements" in other parts of the ballot as well as in the mayor's race. [42] [43] Despite the newspaper's efforts, DeMaio lost to Filner.
Lynch handed day-to-day operations off to another executive in February 2014, [44] and Editor Jeff Light became company president in January 2015. [45] In 2016, Light was named publisher. [46]

Publishers

Notable people

See also
WebPage index: 00110
The San Diego Union-Tribune
The San Diego Union-Tribune is an American metropolitan daily newspaper , published in San Diego, California .
Its name derives from a 1992 merger between the two major daily newspapers at the time, The San Diego Union and the San Diego Evening Tribune . The name changed to U-T San Diego in 2012 but was changed again to The San Diego Union-Tribune in 2015. [2] In 2015, it was acquired by Tribune Publishing, later renamed Tronc .

History

Predecessors
The predecessor newspapers of the Union-Tribune were: [3] [4]
In addition, the San Diego Union purchased the San Diego Daily Bee in 1888, and for a brief time the combined newspaper was named the San Diego Union and Daily Bee . [6]

Acquisition and merger by Copley Press (and subsequent sale to Platinum Equity)
Both the Union and the Tribune were acquired by Copley Press in 1928 [7] and were merged on February 2, 1992. The merged newspaper was sold to the private investment group Platinum Equity of Beverly Hills , California, on March 18, 2009. [8]

Redesign
On August 17, 2010, the Union-Tribune changed its design to improve "clarity, legibility, and ease of use". Changes included being printed on thinner, 100 percent recycled paper, moving the comics to the back of the business section, and abbreviating the title The San Diego Union-Tribune on the front page to U-T San Diego . [9] The U-T nameplate was created by Jim Parkinson, a type designer who also created nameplates for The Rolling Stone , Esquire , and Newsweek . [10]

Purchase by MLIM Holdings
In November 2011, Platinum Equity sold the newspaper to MLIM Holdings, a company led by Doug Manchester , a San Diego real estate developer and "an outspoken supporter of conservative causes". The purchase price was reportedly in excess of $110 million. [11] Manchester built two landmark downtown hotels, the Manchester Grand Hyatt Hotel and the San Diego Marriott Hotel and Marina . His group also owns the Grand Del Mar luxury resort in San Diego. [12]

Rebranding to 
On January 3, 2012, the newspaper announced that it would use the name U-T San Diego "on all of our media products and communications"; the newspaper's website (formerly SignOnSanDiego.com) would use the name UTSanDiego.com. The official announcement explained the change as being intended to "unify our print and digital products under a single brand with a clear and consistent expectation of quality". [13] [14] [15]

Acquisition of the 
U-T San Diego bought the North County Times in 2012. [16] On October 15, 2012, the North County Times ceased publication and became the U-T North County Times , which was an edition of the U-T with some North County–specific content. [17] Six months later the U-T North County Times name was dropped and the newspaper became a North County edition of the U-T . [ citation needed ]

U-T TV
In June 2012, U-T San Diego launched U-T TV , a television news channel . The network featured news, lifestyle, and editorial content produced by the newspaper's staff, and was created as part of the newspaper's growing emphasis on multi-platform content under Manchester.
By October 2013, just over a year after its launch, the network re-formatted with a focus on news, amidst a number of major departures among the channel's staff.
On February 19, 2014, U-T TV was discontinued, but the network's remaining staff was retained to produce video content for the newspaper's digital properties. [18] [19]

Acquisition of weekly newspapers
In November 2013, the newspaper bought eight more local weekly newspapers in the San Diego area, which continued publication under their own names. [20]

Purchase by Tribune Publishing
On May 7, 2015, it was announced that the Tribune Publishing Company , publisher of the Los Angeles Times , the Chicago Tribune , and other newspapers, had reached a deal to acquire U-T San Diego and its associated properties for $85 million. The sale ended the newspaper's 146 years of private ownership.
The transaction was completed on May 21, 2015. On the same date, the newspaper reintroduced its previous branding as The San Diego Union-Tribune . [2]
The Union-Tribune and the Los Angeles Times became part of a new operating entity known as the California News Group, with both newspapers led by Times publisher and chief executive officer Austin Beutner . The two newspapers reportedly would retain distinct operations, but there would be a larger amount of synergy and content sharing between them.
The acquisition did not include the newspaper's headquarters, which was retained by Manchester and would be leased by the newspaper. [21] [22]
On May 26, 2015, the newspaper announced it would lay off 178 employees, representing about thirty percent of the total staff, as it consolidated its printing operations with the Times in Los Angeles. [23]

Closure of San Diego printing facilities
On June 13, 2015, at 10:02 p.m. PDT the final run of The San Diego Union Tribune was printed at the San Diego headquarters in Mission Valley began. [24] It was to print the Sunday edition newspaper for June 14, 2015. The following Monday's newspaper would be printed at the Los Angeles Times location. The dismantling of the printing presses in Mission Valley began in mid-September 2015.

Gannett offer and name change
In 2016 rival newspaper publisher Gannett Company offered to buy the Tribune Publishing Company. The offer was rejected by management, spurring some shareholder dissatisfaction and a shareholder lawsuit. Meanwhile, the Tribune Publishing Company renamed itself Tronc Inc. Tronc is an acronym for Tribune online content. Effective June 20, the renamed company will trade on the NASDAQ exchange under the symbol TRNC. [25]

Headquarters
The newspaper was originally located in Old Town San Diego, and was moved to downtown San Diego in 1871. In 1973, it moved to a custom-built, brick and stone office and printing plant complex in Mission Valley .
The newspaper moved back downtown in May 2016, to offices on the 9th through 12th floor of a tower at 600 B Street. The Union-Tribune is to be the named tenant of the building, replacing Bridgepoint Education and, before that, Comerica. [26]

Awards

Pulitzer Prizes

Criticisms

Copleys and Platinum Equity
Under the Copleys' ownership, the newspaper had a reliably conservative editorial position, endorsing almost exclusively Republicans for elective office, and sometimes refusing to interview or cover Democratic candidates.
Under Platinum Equity, the newspaper's editorial position "skewed closer to the middle" and showcased multiple viewpoints. [31]

Manchester and Lynch
When Manchester and business partner John Lynch took ownership in 2011, they were open about their desire to use the newspaper to "promote their agenda of downtown development and politically conservative causes", [32] with Lynch stating on KPBS radio that he and Manchester "wanted to be cheerleaders for all that is good in San Diego". [33] Lynch expanded on this position in 2012, saying "We make no apologies. We are doing what a newspaper ought to do, which is to take positions. We are very consistent—pro-conservative, pro-business, pro-military—and we are trying to make a newspaper that gets people excited about this city and its future." [34]
This open promotion of certain viewpoints resulted in criticism from journalism professors and other newspaper editors, who worried that negative news about topics such as the military and business might not be covered. [35] Dean Nelson, director of the journalism program at Point Loma Nazarene University , argued, "Now if you're saying we're going to be the cheerleaders of the military, why would you report on this guy that's taking bribes?... Where's the cheerleading there?" a reference to the Union-Tribune' s Pulitzer Prize winning coverage of the Duke Cunningham bribery scandal. [36] A New York Times writer added, "There is a growing worry that the falling value and failing business models of many American newspapers could lead to a situation where moneyed interests buy papers and use them to prosecute a political and commercial agenda. That future appears to have arrived in San Diego." [34]
Lynch said, "We totally respect the journalistic integrity of our paper and there is a clear line of demarcation between our editorials and our news. Our editor, Jeff Light, calls the shots." However, in November 2011 Lynch told the sports editor that the sports pages should advocate for a new football stadium; when a longtime sportswriter wrote skeptically about the idea, he was fired. [34]

Downtown redevelopment
In January 2012, two months after Manchester bought the U-T , the newspaper featured a front-page proposal for downtown redevelopment, to include a downtown football stadium and an expansion of the San Diego Convention Center . [37] Both properties are adjacent to hotels that Manchester owns. [38]
In September 2012, Investigative Newsource reporter Brooke Williams obtained articles that claimed Lynch "threatened" Port Commissioner Scott Peters , who was running for Congress, "with a newspaper campaign to dismantle the Unified Port of San Diego ". In e-mails obtained by Williams, Lynch was quoted as indicating that if the Dole Food Company obtained a long-term contract, that the Port's independence governance would be questioned in editorial coverage. Williams said the effort showed "the extent to which the newspaper's new owners will go to push their vision for a football stadium on the Tenth Avenue Marine Terminal", [39]

Endorsements and polling
During the 2012 mayoral election the owners of the U-T donated to Republican City Council Member Carl DeMaio 's campaign, [40] and the newspaper ran several prominent editorials favoring DeMaio. Those endorsements were wrapped around the front section of the newspaper on a separate page, "as though they were even more important" than the front page. [41]
In October 2012, a poll was taken by the U-T asking respondents to choose between DeMaio and Democratic Congressman Bob Filner in the mayoral election to be held in November. A rival news outlet noted that "Employees of a newspaper, television / radio station, marketing / public opinion research company or the city of San Diego—or who live with someone employed in one of those fields" were excluded from the poll results, which showed the Republican leading the Democrat, 46 percent to 36 percent. Reporter Kelly Davis of SDCityBeat.com wrote: "Common sense dictates that those votes [by city employees or those living with them] would swing in Filner's favor due to DeMaio's long-running feud with city-employee unions." But U-T assignment editor Michael Smolens replied that "city employees were excluded to avoid political entanglements" in other parts of the ballot as well as in the mayor's race. [42] [43] Despite the newspaper's efforts, DeMaio lost to Filner.
Lynch handed day-to-day operations off to another executive in February 2014, [44] and Editor Jeff Light became company president in January 2015. [45] In 2016, Light was named publisher. [46]

Publishers

Notable people

See also
WebPage index: 00111
PubMed
PubMed is a free search engine accessing primarily the MEDLINE database of references and abstracts on life sciences and biomedical topics. The United States National Library of Medicine (NLM) at the National Institutes of Health maintains the database as part of the Entrez system of information retrieval .
From 1971 to 1997, MEDLINE online access to the MEDLARS Online computerized database primarily had been through institutional facilities, such as university libraries . PubMed, first released in January 1996, ushered in the era of private, free, home- and office-based MEDLINE searching. [1] The PubMed system was offered free to the public in June 1997, when MEDLINE searches via the Web were demonstrated, in a ceremony, by Vice President Al Gore . [2]

Content
In addition to MEDLINE, PubMed provides access to:
Many PubMed records contain links to full text articles, some of which are freely available, often in PubMed Central [4] and local mirrors such as UK PubMed Central . [5]
Information about the journals indexed in MEDLINE, and available through PubMed, is found in the NLM Catalog. [6]
As of 5 January 2017 [update] , PubMed has more than 26.8 million records going back to 1966, selectively to the year 1865, and very selectively to 1809; about 500,000 new records are added each year. As of the same date [update] , 13.1 million of PubMed's records are listed with their abstracts, and 14.2 million articles have links to full-text (of which 3.8 million articles are available, full-text for free for any user). [7]
In 2016, NLM changed the indexing system so that publishers will be able to directly correct typos and errors in PubMed indexed articles. [8]

Characteristics

Standard searches
Simple searches on PubMed can be carried out by entering key aspects of a subject into PubMed's search window.
PubMed translates this initial search formulation and automatically adds field names, relevant MeSH (Medical Subject Headings) terms, synonyms, Boolean operators, and 'nests' the resulting terms appropriately, enhancing the search formulation significantly, in particular by routinely combining (using the OR operator) textwords and MeSH terms.
The examples given in a PubMed tutorial [9] demonstrate how this automatic process works:
Likewise,
The new PubMed interface, launched in October 2009, encourages the use of such quick, Google-like search formulations; they have also been described as 'telegram' searches. [10]

Comprehensive searches
For comprehensive, optimal searches in PubMed, it is necessary to have a thorough understanding of its core component, MEDLINE, and especially of the MeSH (Medical Subject Headings) controlled vocabulary used to index MEDLINE articles. They may also require complex search strategies, use of field names (tags), proper use of limits and other features, and are best carried out by PubMed search specialists or librarians, [11] who are able to select the right type of search and carefully adjust it for precision and recall . [12]

Journal article parameters
When a journal article is indexed, numerous article parameters are extracted and stored as structured information. Such parameters are: Article Type (MeSH terms, e.g., "Clinical Trial"), Secondary identifiers, (MeSH terms), Language, Country of the Journal or publication history (e-publication date, print journal publication date).

Publication Type: Clinical queries/systematic reviews
Publication type parameter enables many special features. A special feature of PubMed is its "Clinical Queries" section, where "Clinical Categories", "Systematic Reviews", and "Medical Genetics" subjects can be searched, with study-type 'filters' automatically applied to identify substantial, robust studies. [13] As these 'clinical girish' can generate small sets of robust studies with considerable precision, it has been suggested that this PubMed section can be used as a 'point-of-care' resource. [14]

Secondary ID
Since July 2005, the MEDLINE article indexing process extracts important identifiers from the article abstract and puts those in a field called Secondary Identifier (SI). The secondary identifier field is to store accession numbers to various databases of molecular sequence data, gene expression or chemical compounds and clinical trial IDs. For clinical trials, PubMed extracts trial IDs for the two largest trial registries: ClinicalTrials.gov (NCT identifier) and the International Standard Randomized Controlled Trial Number Register (IRCTN identifier). [15]

See also
A reference which is judged particularly relevant can be marked and "related articles" can be identified. If relevant, several studies can be selected and related articles to all of them can be generated (on PubMed or any of the other NCBI Entrez databases) using the 'Find related data' option. The related articles are then listed in order of "relatedness". To create these lists of related articles, PubMed compares words from the title and abstract of each citation, as well as the MeSH headings assigned, using a powerful word-weighted algorithm. [16] The 'related articles' function has been judged to be so precise that some researchers suggest it can be used instead of a full search. [17]

Mapping to MeSH headings and subheadings
A strong feature of PubMed is its ability to automatically link to MeSH terms and subheadings. Examples would be: "bad breath" links to (and includes in the search) "halitosis", "heart attack" to "myocardial infarction", "breast cancer" to "breast neoplasms". Where appropriate, these MeSH terms are automatically "expanded", that is, include more specific terms. Terms like "nursing" are automatically linked to "Nursing [MeSH]" or "Nursing [Subheading]". This important feature makes PubMed searches automatically more sensitive and avoids false-negative (missed) hits by compensating for the diversity of medical terminology.

My NCBI
The PubMed optional facility "My NCBI" (with free registration) provides tools for
and a wide range of other options. [18] The "My NCBI" area can be accessed from any computer with web-access. An earlier version of "My NCBI" was called "PubMed Cubby". [19]

LinkOut
LinkOut, a NLM facility to link (and make available full-text) local journal holdings. [20] Some 3,200 sites (mainly academic institutions) participate in this NLM facility (as of March 2010 [update] ), from Aalborg University in Denmark to ZymoGenetics in Seattle. [21] Users at these institutions see their institutions logo within the PubMed search result (if the journal is held at that institution) and can access the full-text.

PubMed Commons
In 2016, PubMed allows authors of articles to comment on articles indexed by PubMed. This feature was initially tested in a pilot mode (since 2013) and was made permanent in 2016. [22]

PubMed for handhelds/mobiles
PubMed/MEDLINE can be accessed via handheld devices, using for instance the "PICO" option (for focused clinical questions) created by the NLM. [23] A "PubMed Mobile" option, providing access to a mobile friendly, simplified PubMed version, is also available. [24]

askMEDLINE
askMEDLINE, a free-text, natural language query tool for MEDLINE/PubMed, developed by the NLM, also suitable for handhelds. [25]

PubMed identifier
A PMID (PubMed identifier or PubMed unique identifier) [26] is a unique integer value , starting at 1 , assigned to each PubMed record. A PMID is not the same as a PMCID which is the identifier for all works published in the free-to-access PubMed Central . [27]
The assignment of a PMID or PMCID to a publication tells the reader nothing about the type or quality of the content. PMIDs are assigned to letters to the editor , editorial opinions, op-ed columns, and any other piece that the editor chooses to include in the journal, as well as peer-reviewed papers. The existence of the identification number is also not proof that the papers have not been retracted for fraud, incompetence, or misconduct. The announcement about any corrections to original papers may be assigned a PMID.

Alternative interfaces
The National Library of Medicine leases the MEDLINE information to a number of private vendors such as Ovid , Dialog , EBSCO , Knowledge Finder and many other commercial, non-commercial, and academic providers. [28] As of October 2008 [update] , more than 500 licenses had been issued, more than 200 of them to providers outside the United States. As licenses to use MEDLINE data are available for free, the NLM in effect provides a free testing ground for a wide range [29] of alternative interfaces and 3rd party additions to PubMed, one of a very few large, professionally curated databases which offers this option.
Lu [29] identifies a sample of 28 current and free Web-based PubMed versions, requiring no installation or registration, which are grouped into four categories:
As most of these and other alternatives rely essentially on PubMed/MEDLINE data leased under license from the NLM/PubMed, the term "PubMed derivatives" has been suggested. [29] Without the need to store about 90 GB of original PubMed Datasets, anybody can write PubMed applications using the eutils-application program interface as described in "The E-utilities In-Depth: Parameters, Syntax and More", by Eric Sayers, PhD. [43]

See also
WebPage index: 00112
Open access
Open access ( OA ) refers to online research outputs that are free of all restrictions on access (e.g. access tolls) and free of many restrictions on use (e.g. certain copyright and license restrictions). [1] Open access can be applied to all forms of published research output, including peer-reviewed and non peer-reviewed academic journal articles, conference papers , theses , [2] book chapters, [1] and monographs . [3]
Two degrees of open access can be distinguished: gratis open access, which is online access free of charge, and libre open access, which is online access free of charge plus various additional usage rights. [4] These additional usage rights are often granted through the use of various specific Creative Commons licenses . [5] Libre open access is equivalent to the definition of open access in the Budapest Open Access Initiative , the Bethesda Statement on Open Access Publishing and the Berlin Declaration on Open Access to Knowledge in the Sciences and Humanities .
There are multiple ways authors can provide open access to their work. One way is to publish it and then self-archive it in a repository where it can be accessed for free, [6] [7] such as their institutional repository , [8] [9] or a central repository such as PubMed Central . This is known as 'green' open access. Some publishers require delays, or an embargo , on when a research output in a repository may be made open access. [10] Several initiatives provide an alternative to the American and English language dominance of existing publication indexing systems, including Index Copernicus , SciELO and Redalyc .
A second way authors can make their work open access is by publishing it in such a way that makes their research output immediately available from the publisher. [11] This is known as 'gold' open access, [12] and within the sciences this often takes the form of publishing an article in either an open access journal , [13] or a hybrid open access journal . The latter is a journal whose business model is at least partially based on subscriptions, and only provide Gold open access for those individual articles for which their authors (or their author's institution or funder) pay a specific fee for publication, often referred to as an article processing charge . [14] Pure open access journals do not charge subscription fees, and may have one of a variety of business models. Many, however, do charge an article processing fee. [15]
Widespread public access to the World Wide Web in the late 1990s and early 2000s fueled the open access movement, and prompted both the green open access way (self-archiving of non-open access journal articles) and the creation of open access journals (gold way). Conventional non-open access journals cover publishing costs through access tolls such as subscriptions, site licenses or pay-per-view charges. Some non-open access journals provide open access after an embargo period of 6–12 months or longer (see delayed open access journals ). [14] Active debate over the economics and reliability of various ways of providing open access continues among researchers, academics, librarians, university administrators, funding agencies, government officials, commercial publishers , editorial staff and society publishers, as open access gradually gains in acceptance. [16]

Definitions
The term "open access" itself was first formulated in three public statements in the 2000s: the Budapest Open Access Initiative in February 2002, the Bethesda Statement on Open Access Publishing in June 2003, and the Berlin Declaration on Open Access to Knowledge in the Sciences and Humanities in October 2003, [17] and the initial concept of open access refers to an unrestricted online access to scholarly research primarily intended for scholarly journal articles.
The Budapest statement defined open access as follows:
The Bethesda and Berlin statements add that for a work to be open access, users must be able to "copy, use, distribute, transmit and display the work publicly and to make and distribute derivative works, in any digital medium for any responsible purpose, subject to proper attribution of authorship."
Despite these statements emerging in the 2000s, the idea and practise of providing free online access to journal articles began at least a decade before the term "open access" was formally coined. Computer scientists had been self-archiving in anonymous ftp archives since the 1970s and physicists had been self-archiving in arxiv since the 1990s. The Subversive Proposal to generalize the practice was posted in 1994.

Gratis and libre open access
In order to reflect actual practice in providing two different degrees of open access, the further distinction between gratis open access and libre open access was added in 2006 by two of the co-drafters of the original BOAI definition. [4] Gratis OA refers to free online access, and libre OA refers to free online access plus some additional re-use rights. [4] The Budapest, Bethesda, and Berlin definitions had corresponded only to libre OA. The re-use rights of libre OA are often specified by various specific Creative Commons licenses ; [5] these almost all require attribution of authorship to the original authors. [4] [17]

Motivations for open access publishing
Open access itself (mostly green and gratis) began to be sought and provided worldwide by researchers when the possibility itself was opened by the advent of Internet and the World Wide Web . The momentum was further increased by a growing movement for academic journal publishing reform, and with it gold and libre OA. Electronic publishing created new benefits as compared to paper publishing but beyond that, it contributed to causing problems in traditional publishing models.
The premises behind open access publishing are that there are viable funding models to maintain traditional peer review standards of quality while also making the following changes:
The open access movement is motivated by the problems of social inequality caused by restricting access to academic research, which favor large and wealthy institutions with the financial means to purchase access to many journals, as well as the economic challenges and perceived unsustainability of academic publishing. [19] [20]

Stakeholders and concerned communities
The intended audience of research articles is usually other researchers. Open access helps researchers as readers by opening up access to articles that their libraries do not subscribe to. One of the great beneficiaries of open access may be users in developing countries , where currently some universities find it difficult to pay for subscriptions required to access the most recent journals. [21] Some schemes exist for providing subscription scientific publications to those affiliated to institutions in developing countries at little or no cost. [22] All researchers benefit from open access as no library can afford to subscribe to every scientific journal and most can only afford a small fraction of them – this is known as the " serials crisis ". [23]
Open access extends the reach of research beyond its immediate academic circle. An open access article can be read by anyone – a professional in the field, a researcher in another field, a journalist , a politician or civil servant , or an interested layperson . Indeed, a 2008 study revealed that mental health professionals are roughly twice as likely to read a relevant article if it is freely available. [24]

Authors and researchers
The main reason authors make their articles openly accessible is to maximize their research impact . [25] A study in 2001 first reported an open access citation impact advantage, [26] and a growing number of studies [27] have confirmed, with varying degrees of methodological rigor, that an open access article is more likely to be used and cited than one behind subscription barriers. [27] For example, a 2006 study in PLoS Biology found that articles published as immediate open access in PNAS were three times more likely to be cited than non-open access papers, and were also cited more than PNAS articles that were only self-archived. [28] This result has been challenged as an artifact of authors self-selectively paying to publish their higher quality articles in hybrid open access journals, [29] whereas a 2010 study found that the open access citation advantage was equally big whether self-archiving was self-selected or mandated. [30]
Scholars are paid by research funders and/or their universities to do research; the published article is the report of the work they have done, rather than an item for commercial gain. The more the article is used, cited, applied and built upon, the better for research as well as for the researcher's career. [31] [32] Open access can reduce publication delays, an obstacle which led some research fields such as high-energy physics to adopt widespread preprint access. [33]
Some professional organizations have encouraged use of open access: in 2001, the International Mathematical Union communicated to its members that "Open access to the mathematical literature is an important goal" and encouraged them to "[make] available electronically as much of our own work as feasible" to "[enlarge] the reservoir of freely available primary mathematical material, particularly helping scientists working without adequate library access." [34]

Research funders and universities
Research funding agencies and universities want to ensure that the research they fund and support in various ways has the greatest possible research impact. [35] As a means of achieving this, research funders are beginning to expect open access to the research they support. Many of them (including all seven UK Research Councils) have already adopted green open access self-archiving mandates, and others are on the way to do so (see ROARMAP ).

Universities
A growing number of universities are providing institutional repositories in which their researchers can deposit their published articles. Some open access advocates believe that institutional repositories will play a very important role in responding to open access mandates from funders. [36] EnablingOpenScholarship (EPS) provides universities with OA policy-building. [37]
In May 2005, 16 major Dutch universities cooperatively launched DAREnet , the Digital Academic Repositories, making over 47,000 research papers available to anyone with internet access. [38] From 1 January 2007, at the completion of the DARE programme, KNAW Research Information has taken over responsibility for the DAREnet portal. On 2 June 2008, DAREnet has been incorporated into the scholarly portal NARCIS. [39] At the end of 2009, NARCIS provided access to 185,000 open access publications from all Dutch universities, KNAW, NWO and a number of scientific institutes.
In 2011, a group of universities in North America formed the Coalition of Open Access Policy Institutions (COAPI). [40] Starting with 21 institutions where the faculty had either established an open access policy or were in the process of implementing one, COAPI now has nearly 50 members. These institutions' administrators, faculty and librarians, and staff support the international work of the Coalition's awareness-raising and advocacy for open access. Members agree to the following COAPI Principles:
In 2012, the Harvard Open Access Project released its guide to good practices for university open-access policies, [42] focusing on rights-retention policies that allow universities to distribute faculty research without seeking permission from publishers.
In 2013 a group of nine Australian universities formed the Australian Open Access Support Group (AOASG) to advocate, collaborate, raise awareness, and lead and build capacity in the open access space in Australia. [43] In 2015, the group expanded to include all eight New Zealand universities and was renamed the Australasian Open Access Support Group. [44]

Libraries and librarians
As information professionals, librarians are vocal and active advocates of open access. These librarians believe that open access promises to remove both the price barriers and the permission barriers that undermine library efforts to provide access to the scholarly record, [45] as well as helping to address the serials crisis . Many library associations have either signed major open access declarations, or created their own. For example, the Canadian Library Association endorsed a Resolution on Open Access in June 2005. [46]
Librarians also lead education and outreach initiatives to faculty, administrators, and others about the benefits of open access. For example, the Association of College and Research Libraries of the American Library Association has developed a Scholarly Communications Toolkit. [47] The Association of Research Libraries has documented the need for increased access to scholarly information, and was a leading founder of the Scholarly Publishing and Academic Resources Coalition (SPARC). [48] [49]
At most universities, the library manages the institutional repository, which provides free access to scholarly work by the university's faculty. The Canadian Association of Research Libraries has a program [50] to develop institutional repositories at all Canadian university libraries.
An increasing number of libraries provide hosting services for open access journals. A 2008 survey by the Association of Research Libraries [51] found that 65% of surveyed libraries either are involved in journal publishing , or are planning to become involved in the very near future. [52]
In 2013, open access activist Aaron Swartz was posthumously awarded the American Library Association's James Madison Award for being an "outspoken advocate for public participation in government and unrestricted access to peer-reviewed scholarly articles". [53] [54] In March 2013, the entire editorial board and the editor-in-chief of the Journal of Library Administration resigned en masse, citing a dispute with the journal's publisher. [55] One board member wrote of a "crisis of conscience about publishing in a journal that was not open access" after the death of Aaron Swartz. [56] [57]
The pioneer of the open access movement in France and one of the first librarians to advocate the self-archiving approach to open access worldwide is Hélène Bosc. [58] Her work is described in her "15-year retrospective". [59]

Public
Open access to scholarly research is argued to be important to the public for a number of reasons. One of the arguments for public access to the scholarly literature is that most of the research is paid for by taxpayers through government grants , who therefore have a right to access the results of what they have funded. This is one of the primary reasons for the creation of advocacy groups such as The Alliance for Taxpayer Access in the US. [60] Examples of people who might wish to read scholarly literature include individuals with medical conditions (or family members of such individuals) and serious hobbyists or 'amateur' scholars who may be interested in specialized scientific literature (e.g. amateur astronomers ). Additionally, professionals in many fields may be interested in continuing education in the research literature of their field, and many businesses and academic institutions cannot afford to purchase articles from or subscriptions to much of the research literature that is published under a toll access model.
Even those who do not read scholarly articles benefit indirectly from open access. [61] For example, patients benefit when their doctor and other health care professionals have access to the latest research. As argued by open access advocates, open access speeds research progress, productivity, and knowledge translation. [62] Every researcher in the world can read an article, not just those whose library can afford to subscribe to the particular journal in which it appears. Faster discoveries benefit everyone. High school and junior college students can gain the information literacy skills critical for the knowledge age. Critics of the various open access initiatives claim that there is little evidence that a significant amount of scientific literature is currently unavailable to those who would benefit from it. [63] While no library has subscriptions to every journal that might be of benefit, virtually all published research can be acquired via interlibrary loan . [64] Note that interlibrary loan may take a day or weeks depending on the loaning library and whether they will scan and email, or mail the article. Open access online, by contrast is faster, often immediate, making it more suitable than interlibrary loan for fast-paced research.

Low-income countries
In developing nations, open access archiving and publishing acquires a unique importance. Scientists, health care professionals, and institutions in developing nations often do not have the capital necessary to access scholarly literature, although schemes exist to give them access for little or no cost. Among the most important is HINARI , [65] the Health InterNetwork Access to Research Initiative, sponsored by the World Health Organization . HINARI, however, also has restrictions. For example, individual researchers may not register as users unless their institution has access, [66] and several countries that one might expect to have access do not have access at all (not even "low-cost" access) (e.g. South Africa). [66]
Many open access projects involve international collaboration. For example, the SciELO (Scientific Electronic Library Online), [67] is a comprehensive approach to full open access journal publishing, involving a number of Latin American countries. Bioline International , a non-profit organization dedicated to helping publishers in developing countries is a collaboration of people in the UK, Canada, and Brazil; the Bioline International Software is used around the world. Research Papers in Economics (RePEc), is a collaborative effort of over 100 volunteers in 45 countries. The Public Knowledge Project in Canada developed the open source publishing software Open Journal Systems (OJS), which is now in use around the world, for example by the African Journals Online group, and one of the most active development groups is Portuguese. This international perspective has resulted in advocacy for the development of open-source appropriate technology and the necessary open access to relevant information for sustainable development . [68] [69]

Implementation practices
There are various ways in which open access can be provided, with the two most common methods usually categorised as either gold or green open access.

Journals: gold open access
One option for authors who wish to make their work openly accessible is to publish in an open access journal ("gold open access"). There are many business models for open access journals. [70] Open access can be provided by traditional publishers, who may publish open access as well as subscription-based journals, or open access publishers such as Public Library of Science (PLOS), who publish only open access journals. An open access journal may or may not charge a publishing fee ; open access publishing does not necessarily mean that the author has to pay. Traditionally, many academic journals levied page charges, long before open access became a possibility. When open access journals do charge processing fees, it is the author's employer or research funder who typically pays the fee, not the individual author, and many journals will waive the fee in cases of financial hardship, or for authors in less-developed countries. Some no-fee journals have institutional subsidies. Examples of open access publishers [13] include BioMed Central and the Public Library of Science .
Roughly 30% [1] of gold open access journals have author fees to cover the cost of publishing (e.g. PLoS fees vary from $1,495 to $2,900 [71] ) instead of reader subscription fees. Advertising revenue and/or funding from foundations and institutions are also used to provide funding.

Self-archiving: green open access
Self-archiving, also known as green open access, refers to the practice of depositing articles in an open access repository , this can be an institutional or a disciplinary repository such as arXiv .
Green open access journal publishers [72] endorse immediate open access self-archiving by their authors. Open access self-archiving was first formally proposed in 1994 [73] [74] by Stevan Harnad in his " Subversive Proposal ". However, self-archiving was already being done by computer scientists in their local FTP archives in the 1980s, [75] later harvested into CiteSeer . What is deposited can be either a preprint , or the peer-reviewed postprint – either the author's refereed, revised final draft or the publisher's version of record.
To find out if a publisher or journal has given a green light to author self-archiving, the author can check the Publisher Copyright Policies and Self-Archiving list [76] on the SHERPA/RoMEO web site. The EPrints site also provides a FAQ [77] on self-archiving. Extensive details and links can also be found in the Open Access Archivangelism blog [78] and the Eprints Open Access site. [79]

Manner of distribution
Like the self-archived green open access articles, most gold open access journal articles are distributed via the World Wide Web , [1] due to low distribution costs, increasing reach, speed, and increasing importance for scholarly communication. Open source software is sometimes used for open access repositories , [80] open access journal websites , [81] and other aspects of open access provision and open access publishing.
Access to online content requires Internet access, and this distributional consideration presents physical and sometimes financial barriers to access. Proponents of open access argue that Internet access barriers are relatively low in many circumstances, that efforts should be made to subsidize universal Internet access, whereas pay-for-access presents a relatively high additional barrier over and above Internet access itself. [ citation needed ]
The Directory of Open Access Journals lists a number of peer-reviewed open access journals for browsing and searching. Open access articles can also often be found with a web search , using any general search engine or those specialized for the scholarly and scientific literature, such as OAIster and Google Scholar .

Policies and mandates
Many universities, research institutions and research funders have adopted mandates requiring their researchers to provide open access to their peer-reviewed research articles by self-archiving them in an open access repository. [82] Some publishers and publisher associations have lobbied against introducing mandates. [83] [84] [85]
The idea of mandating self-archiving was mooted at least as early as 1998. [86] Since 2003 [87] efforts have been focused on open access mandating by the funders of research: governments, [88] research funding agencies, [89] and universities. [82]
The Registry of Open Access Repository Mandatory Archiving Policies (ROARMAP) is a searchable international database charting the growth of open access mandates . As of May 2014, mandates have been adopted by over 200 universities (including Harvard, MIT, Stanford, University College London, and University of Edinburgh) and over 80 research funders worldwide. [8]

Funding issues
The " article processing charges " which are often used for open access journals shift the burden of payment from readers to authors (or their funders), which creates a new set of concerns. [90] One concern is that if a publisher makes a profit from accepting papers, it has an incentive to accept anything submitted, rather than selecting and rejecting articles based on quality. This could be remedied, however, by charging for the peer-review rather than acceptance. [91] Another concern is that institutional budgets may need to be adjusted in order to provide funding for the article processing charges required to publish in many open access journals (e.g. those published by BioMed Central [92] ). It has been argued that this may reduce the ability to publish research results due to lack of sufficient funds, leading to some research not becoming a part of the public record. [93]
Unless discounts are available to authors from countries with low incomes or external funding is provided to cover the cost, article processing charges could exclude authors from developing countries or less well-funded research fields from publishing in open access journals. However, under the traditional model, the prohibitive costs of some non-open access journal subscriptions already place a heavy burden on the research community; and if green open access self-archiving eventually makes subscriptions unsustainable, the cancelled subscription savings can pay the gold open access publishing costs without the need to divert extra money from research. [94] Moreover, many open access publishers offer discounts or publishing fee waivers to authors from developing countries or those suffering financial hardship. Self-archiving of non-open access publications provides a low cost alternative model. [95]
Another concern is the redirection of money by major funding agencies such as the National Institutes of Health and the Wellcome Trust from the direct support of research to the support open access publication. Robert Terry, Senior Policy Advisor at the Wellcome Trust, has said that he feels that 1–2% of their research budget will change from the creation of knowledge to the dissemination of knowledge. [96]
Research institutions could cover the cost of open access by converting to a open access journal cost-recovery model, with the institutions' annual tool access subscription savings being available to cover annual open access publication costs. [97] A 2017 study by the Max Planck Society the annual turnovers of academic publishers amount to approximately EUR 7.6 billion. It is argued that this money comes predominantly from publicly funded scientific libraries as they purchase subscriptions or licenses in order to provide access to scientific journals for their members. The study was presented by the Max Planck Digital Library and found that subscription budgets would be sufficient to fund the open access publication charges. [98]

History

Efforts before Internet
Even before the advent of the Internet various models were proposed to increase access to academic research.
One early proponent of the publisher-pays model was the physicist Leó Szilárd . To help stem the flood of low-quality publications, he jokingly suggested in the 1940s that at the beginning of his career each scientist should be issued with 100 vouchers to pay for his papers. Closer to the present, but still ahead of its time, was Common Knowledge . This was an attempt to share information for the good of all, the brainchild of Brower Murphy , formerly of The Library Corporation. Both Brower and Common Knowledge are recognised in the Library Microcomputer Hall of Fame. [99] One of Mahatma Gandhi 's earliest publications, Hind Swaraj published in Gujarati in 1909 is recognised as the intellectual blueprint of India's freedom movement. The book was translated into English the next year, with a copyright legend that read "No Rights Reserved". [100]
The modern open access movement (as a social movement ) traces its history at least back to the 1950s, with the Letterist International (LI) placing anything in their journal Potlatch in the public domain. As the LI merged to form the Situationist International , Guy Debord wrote to Patrick Straram "All the material published by the Situationist International is, in principle, usable by everyone, even without acknowledgement, without the preoccupations of literary property." This was to facilitate detournement . [101] It became much more prominent in the 1990s with the advent of the Digital Age . With the spread of the Internet and the ability to copy and distribute electronic data at no cost, the arguments for open access gained new importance. The fixed cost of producing the article is separable from the minimal marginal cost of the online distribution.

Early years of online open access
Probably the earliest book publisher to provide open access was the National Academies Press , publisher for the National Academy of Sciences , Institute of Medicine , and other arms of the National Academies . They have provided free online full-text editions of their books alongside priced, printed editions since 1994, and assert that the online editions promote sales of the print editions. As of June 2006 they had more than 3,600 books up online for browsing, searching, and reading.
While Editor-in-Chief of the Journal of Clinical Investigation , Ajit Varki made it the first major biomedical journal to be freely available on the web in 1996. [102] Varki wrote, "The vexing issue of the day is how to appropriately charge users for this electronic access. The nonprofit nature of the JCI allows consideration of a truly novel solution — not to charge anyone at all!" [103]
An explosion of interest and activity in open access journals has occurred since the 1990s, largely due to the widespread availability of Internet access. It is now possible to publish a scholarly article and also make it instantly accessible anywhere in the world where there are computers and Internet connections. The fixed cost of producing the article is separable from the minimal marginal cost of the online distribution.
These new possibilities emerged at a time when the traditional, print-based scholarly journals system was in a crisis. The number of journals and articles produced had been increasing at a steady rate; however the average cost per journal had been rising at a rate far above inflation for decades, and budgets at academic libraries have remained fairly static. [ citation needed ] The result was decreased access – ironically, just when technology has made almost unlimited access a very real possibility, for the first time. Libraries and librarians have played an important part in the open access movement, initially by alerting faculty and administrators to the serials crisis. The Association of Research Libraries developed the Scholarly Publishing and Academic Resources Coalition (SPARC), in 1997, an alliance of academic and research libraries and other organizations, to address the crisis and develop and promote alternatives, such as open access.
The first online-only, free-access journals (eventually to be called "open access journals") began appearing in the late 1980s and early 1990s. These journals typically used pre-existing infrastructure (such as e-mail or newsgroups ) and volunteer labor and were developed without any intent to generate profit. Examples include Bryn Mawr Classical Review , Postmodern Culture , Psycoloquy , and The Public-Access Computer Systems Review . [104]
The first free scientific online archive was arXiv.org , started in 1991, initially a preprint service for physicists, initiated by Paul Ginsparg . Self-archiving has become the norm in physics, with some sub-areas of physics, such as high-energy physics, having a 100% self-archiving rate. The prior existence of a "preprint culture" in high-energy physics is one major reason why arXiv has been successful. [105] arXiv now includes papers from related disciplines including computer science, mathematics, nonlinear sciences, quantitative biology, quantitative finance, and statistics. [106] However, computer scientists mostly self-archive on their own websites and have been doing so for even longer than physicists. arXiv now includes postprints as well as preprints. [107] The two major physics publishers, American Physical Society and Institute of Physics Publishing, have reported that arXiv has had no effect on journal subscriptions in physics; even though the articles are freely available, usually before publication, physicists value their journals and continue to support them. [108]
Computer scientists had been self-archiving on their own FTP sites and then their websites since even earlier than the physicists, as was revealed when Citeseer began harvesting their papers in the late 1990s. Citeseer is a computer science archive that harvests, Google -style, from distributed computer science websites and institutional repositories , and contains almost twice as many papers as arXiv. The 1994 " Subversive Proposal " [109] was to extend self-archiving to all other disciplines; from it arose CogPrints (1997) and eventually the OAI -compliant generic GNU Eprints.org software in 2000. [110]
In 1997, the U.S. National Library of Medicine (NLM) made Medline , the most comprehensive index to medical literature on the planet, freely available in the form of PubMed . Usage of this database increased a tenfold when it became free, strongly suggesting that prior limits on usage were impacted by lack of access. While indexes are not the main focus of the open access movement, Medline is important in that it opened up a whole new form of use of scientific literature – by the public, not just professionals. [111] The Journal of Medical Internet Research ( JMIR ), [112] one of the first open access journals in medicine, was created in 1998, publishing its first issue in 1999.
In 1998, the American Scientist Open Access Forum [113] was launched (and first called the "September98 Forum").
One of the first humanities journals published in open access is CLCWeb: Comparative Literature and Culture [114] founded at the University of Alberta in 1998 with its first issue published in March 1999 and since 2000 published by Purdue University Press .
In 1999, Harold Varmus of the NIH proposed a journal called E-biomed, intended as an open access electronic publishing platform combining a preprint server with peer-reviewed articles. [115] E-biomed later saw light in a revised form [116] as PubMed Central , a postprint archive.
It was also in 1999 that the Open Archives Initiative and its OAI-PMH protocol for metadata harvesting was launched in order to make online archives interoperable.

2000s
In 2000, BioMed Central , a for-profit open access publisher, was launched by the then Current Science Group (the founder of the Current Opinion series, and now known as the Science Navigation Group). [117] In some ways, BioMed Central resembles Harold Varmus ' original E-biomed proposal more closely than does PubMed Central . [118] As of October 2013 BioMed Central publishes over 250 journals. [119]
In 2001, 34,000 [120] scholars around the world signed "An Open Letter to Scientific Publishers", calling for "the establishment of an online public library that would provide the full contents of the published record of research and scholarly discourse in medicine and the life sciences in a freely accessible, fully searchable, interlinked form". [121] Scientists signing the letter also pledged not to publish in or peer-review for non-open access journals. This led to the establishment of the Public Library of Science , an advocacy organization. However, most scientists continued to publish and review for non-open access journals. PLoS decided to become an open access publisher aiming to compete at the high quality end of the scientific spectrum with commercial publishers and other open access journals, which were beginning to flourish. [122] Critics have argued that, equipped with a $10 million grant, PLoS competes with smaller open access journals for the best submissions and risks destroying what it originally wanted to foster. [123]
The first major international statement on open access was the Budapest Open Access Initiative in February 2002, launched by the Open Society Institute . [80] This provided the first definition of open access, and has a growing list of signatories. [124] Two further statements followed: the Bethesda Statement on Open Access Publishing [125] in June 2003 and the Berlin Declaration on Open Access to Knowledge in the Sciences and Humanities in October 2003. Also in 2003, the World Summit on the Information Society included open access in its Declaration of Principles and Plan of Action. [126]
In 2006, a Federal Research Public Access Act was introduced in US Congress by senators John Cornyn and Joe Lieberman . [127] [128] The act continues to be brought up every year since then, but has never made it past committee. [129]
The year 2007 recorded some backlash from non-OA publishers. [130]
In 2008, Ajit Varki worked with David Lipman to create the first viable model for a major Open Access textbook hosted at NCBI, the 2nd. Edition of the Essentials of Glycobiology . [131]
Perhaps the first dedicated publisher of open access monographs in the humanities was re.press who published their first title in that 2006. Two years later in 2008 Open Humanities Press , another publisher of humanities monographs, was launched. Most recently, the Open Library of Humanities launched in September 2015.
In 2008, USENIX , the advanced computing systems association, implemented an open access policy for their conference proceedings. In 2011 they added audio and video recordings of paper presentations to the material to which they provide open access. [132]

2010s
In 2013, John Holdren , Barack Obama 's director of the Office of Science and Technology Policy , issued a memorandum directing United States' Federal Agencies with more than $100 million in annual R&D expenditures to develop plans within six months to make the published results of federally funded research freely available to the public within one year of publication. [133] [134] As of March 2015, two agencies had made their plans public: the Department of Energy [135] and the National Science Foundation . [136]
In 2013, the UK Higher Education Funding Council for England (HEFCE) proposed adopting a mandate that in order to be eligible for submission to the UK Research Excellence Framework (REF) all peer-reviewed journal articles submitted after 2014 must be deposited in the author's institutional repository immediately upon acceptance for publication , regardless of whether the article is published in a subscription journal or in an open access journal . HEFCE expresses no journal preference, places no restriction on authors' choice and requires the deposit itself to be immediate, irrespective of whether the publisher imposes an embargo (for an allowable embargo period that remains to be decided) on the date at which access to the deposit can be made open. [137] [138] The HEFCE/REF mandate proposal complements the recent Research Councils UK (RCUK) mandate that requires all articles resulting from RCUK funding to be made open access by 6 months after publication at the latest (12 months for arts and humanities articles). [139]
HEFCE also provided grants to universities in England [140] wishing to participate in the Pilot Collection of Knowledge Unlatched , a not-for-profit organisation enabling humanities and social sciences monographs to become open access. The Pilot Collection ran from October 2013 to February 2014 and 297 libraries and institutions worldwide participated in 'unlatching' the collection of 28 titles. 61 of these participating institutions were university libraries in England eligible for the HEFCE grant of 50% towards the $1195 participation fee. [141]
The Indian Council of Agricultural Research had adopted an Open Access policy [142] for its publications on 13 September 2013 [143] and announced that each ICAR institute would set-up an open access institutional repository. One such repository is eprints@cmfri , an open access institutional repository of the Central Marine Fisheries Research Institute which was set-up on 25 February 2010 well before the policy was adopted. [144] However, since March 2010, the ICAR is making available its two flagship journals under Open Access [145] on its website and later through an online platform called Indian Agricultural Research Journals using Open Journal Systems .
In 2014, the Department of Biotechnology and Department of Science and Technology , under Ministry of Science and Technology , Government of India jointly announced their open access policy. [146]
In May 2016 the European Union announced that "all scientific articles in Europe must be freely accessible as of 2020" [147] and that the Commission will "develop and encourage measures for optimal compliance with the provisions for open access to scientific publications under Horizon 2020 ". [148] Some ask such measures to include the usage of free and open-source software . [149]

Growth
A study published in 2010 showed that roughly 20% of the total number of peer-reviewed articles published in 2008 could be found openly accessible. [150] Another study found that by 2010, 7.9% of all academic journals with impact factors were gold open access journals and showed a broad distribution of Gold Open Access journals throughout academic disciplines. [151] 8.5% of the journal literature could be found free at the publishers’ sites (gold open access), of which 62% in full open access journals, 14% in delayed-access subscription journals, and 24% as individually open articles in otherwise subscription journals. For an additional 11.9% of the articles, open access full text copies were available via green open access in either subject-based repositories (43%), institutional repositories (24%) or on the home pages of the authors or their departments (33%). These copies were further classified into exact copies of the published article (38%), manuscripts as accepted for publishing (46%) or manuscripts as submitted (15%). [150]
In the 2010 study, of all scientific fields chemistry had the lowest overall share of open access (13%), while Earth Sciences had the highest (33%). In medicine, biochemistry and chemistry gold publishing in open access journals was more common than author self-archiving. In all other fields self-archiving was more common.
In August 2013, a study done for the European Commission reported that 50% of a random sample of all articles published in 2011 as indexed by Scopus were freely accessible online by the end of 2012. [152] [153] [154] A 2017 study by the Max Planck Society put the share of gold access articles in pure open access journals at around 13 percent of total research papers. [155]

Journals
A study on the development of publishing of open access journals from 1993 to 2009 [156] published in 2011 suggests that, measured both by the number of journals as well as by the increases in total article output, direct gold open access journal publishing has seen rapid growth particularly between the years 2000 and 2009. It was estimated that there were around 19,500 articles published open access in 2000, while the number has grown to 191,850 articles in 2009. The journal count for the year 2000 is estimated to have been 740, and 4769 for 2009; numbers which show considerable growth, albeit at a more moderate pace than the article-level growth. These findings support the notion that open access journals have increased both in numbers and in average annual output over time.
The development of the number of active open access journals and the number of research articles published in them during the period 1993–2009 is shown in the figure above. If these gold open access growth curves are extrapolated to the next two decades, the Laakso et al. (Björk) curve would reach 60% in 2022, and the Springer curve would reach 50% in 2029 as shown in the figure below (the reference provides a more optimistic interpretation which does not match with the values shown in the figure). [157]

Self-archiving
The Registry of Open Access Repositories (ROAR) indexes the creation, location and growth of open access open access repositories and their contents. [8] As of December 2015, over 3,500 institutional and cross-institutional repositories have been registered in ROAR. [158]

Finding open access research online
There are various open access aggregators that index open access journals or articles. ROAD synthesizes information about open access journals and is a subset of the ISSN registry. Users may browse to find open access journals by country or by subject. SHERPA/RoMEO lists international publishers that allow the published version of articles to be deposited in institutional repositories . The Directory of Open Access Journals (DOAJ) contains over 8,000 open access journals of varying open access policies that scholars can search and browse. [159] The Open Archives Initiative (OAI) lists 2937 conforming repositories . Searching each open access repository individually is impractical. The resources in these repositories can be harvested, using the OAI Protocol and aggregated into online systems which in-turn provide access to millions of resources from a single online location. [160]

See also
WebPage index: 00113
Florida
Florida i / ˈ f l ɒr ᵻ d ə / (Spanish for "land of flowers") is a state located in the southeastern region of the United States . It is bordered to the west by the Gulf of Mexico , to the north by Alabama and Georgia , to the east by the Atlantic Ocean , and to the south by the Straits of Florida and Cuba . Florida is the 22nd-most extensive , the 3rd-most populous , [9] and the 8th-most densely populated of the U.S. states. Jacksonville is the most populous municipality in the state and is the largest city by area in the contiguous United States . The Miami metropolitan area is Florida's most populous urban area . The city of Tallahassee is the state capital.
A peninsula between the Gulf of Mexico, the Atlantic Ocean, and the Straits of Florida , it has the longest coastline in the contiguous United States , approximately 1,350 miles (2,170 km), and is the only state that borders both the Gulf of Mexico and the Atlantic Ocean. Much of the state is at or near sea level and is characterized by sedimentary soil. The climate varies from subtropical in the north to tropical in the south. [10] The American alligator , American crocodile , Florida panther , and manatee can be found in the Everglades National Park .
Since the first European contact was made in 1513 by Spanish explorer Juan Ponce de León – who named it La Florida ( [la floˈɾiða] "land of flowers") upon landing there in the Easter season , Pascua Florida [11] – Florida was a challenge for the European colonial powers before it gained statehood in the United States in 1845. It was a principal location of the Seminole Wars against the Native Americans , and racial segregation after the American Civil War .
Today, Florida is distinctive for its large Cuban expatriate community and high population growth, as well as for its increasing environmental issues. The state's economy relies mainly on tourism, agriculture, and transportation , which developed in the late 19th century. Florida is also renowned for amusement parks , orange crops, the Kennedy Space Center , and as a popular destination for retirees.
Florida culture is a reflection of influences and multiple inheritance; Native American, European American, Hispanic and Latino, and African American heritages can be found in the architecture and cuisine. Florida has attracted many writers such as Marjorie Kinnan Rawlings , Ernest Hemingway and Tennessee Williams , and continues to attract celebrities and athletes. It is internationally known for golf, tennis, auto racing and water sports .

History
By the 16th century, the earliest time for which there is a historical record, major Native American groups included the Apalachee (of the Florida Panhandle ), the Timucua (of northern and central Florida), the Ais (of the central Atlantic coast), the Tocobaga (of the Tampa Bay area), the Calusa (of southwest Florida) and the Tequesta (of the southeastern coast).

European arrival
Florida was the first part of the continental United States to be visited and settled by Europeans. The earliest known European explorers came with the Spanish conquistador Juan Ponce de León . Ponce de León spotted and landed on the peninsula on April 2, 1513. He named the region La Florida ("land of flowers"). [12] The story that he was searching for the Fountain of Youth is a myth. [13]
"In May 1539, Conquistador Hernando de Soto skirted the coast of Florida, searching for a deep harbor to land. He described seeing a thick wall of red mangroves spread mile after mile, some reaching as high as 70 feet (21 m), with intertwined and elevated roots making landing difficult. Very soon, 'many smokes' appeared 'along the whole coast', billowing against the sky, when the Native ancestors of the Seminole spotted the newcomers and spread the alarm by signal fires". [14] The Spanish introduced Christianity, cattle, horses, sheep, the Castilian language, and more to Florida. [15] [ full citation needed ] Spain established several settlements in Florida, with varying degrees of success. In 1559, Don Tristán de Luna y Arellano established a settlement at present-day Pensacola , making it the first attempted settlement in Florida, but it was mostly abandoned by 1561.
In 1565, the settlement of St. Augustine (San Agustín) was established under the leadership of admiral and governor Pedro Menéndez de Avilés , creating what would become the oldest European settlement in the continental U.S. and establishing the first generation of Floridanos and the government of Florida . [16] Spain maintained tenuous control over the region by converting the local tribes to Christianity.
The geographical area of Florida diminished with the establishment of English settlements to the north and French claims to the west. The English attacked St. Augustine, burning the city and its cathedral to the ground several times. Spain built the Castillo de San Marcos in 1672 and Fort Matanzas in 1742 to defend Florida's capital city from attacks, and to maintain its strategic position in the defense of the Captaincy General of Cuba and the Spanish West Indies .
Florida attracted numerous Africans and African-Americans from adjacent British colonies who sought freedom from slavery. In 1738, Governor Manuel de Montiano established Fort Gracia Real de Santa Teresa de Mose near St. Augustine, a fortified town for escaped slaves to whom Montiano granted citizenship and freedom in return for their service in the Florida militia, and which became the first free black settlement legally sanctioned in North America. [17] [18]
In 1763 , Spain traded Florida to the Kingdom of Great Britain for control of Havana , Cuba, which had been captured by the British during the Seven Years' War . It was part of a large expansion of British territory following their victory in the Seven Years' War . A large portion of the Floridano population left, taking along most of the remaining indigenous population to Cuba. [19] The British soon constructed the King's Road connecting St. Augustine to Georgia . The road crossed the St. Johns River at a narrow point, which the Seminole called Wacca Pilatka and the British named "Cow Ford", both names ostensibly reflecting the fact that cattle were brought across the river there. [20] [21] [22]
The British divided and consolidated the Florida provinces ( Las Floridas ) into East Florida and West Florida , a division the Spanish government kept after the brief British period. [23] The British government gave land grants to officers and soldiers who had fought in the French and Indian War in order to encourage settlement. In order to induce settlers to move to Florida, reports of its natural wealth were published in England. A large number of British settlers who were "energetic and of good character" moved to Florida, mostly coming from South Carolina , Georgia and England. There was also a group of settlers who came from the colony of Bermuda . This would be the first permanent English-speaking population in what is now Duval County , Baker County , St. Johns County and Nassau County . The British built good public roads and introduced the cultivation of sugar cane, indigo and fruits as well the export of lumber. [24] [25]
As a result of these initiatives northeastern Florida prospered economically in a way it never did under Spanish administration. Furthermore, the British governors were directed to call general assemblies as soon as possible in order to make laws for the Floridas and in the meantime they were, with the advice of councils, to establish courts. This would be the first introduction of much of the English-derived legal system which Florida still has today including trial by jury , habeas corpus and county-based government. [24] [25] Neither East Florida nor West Florida would send any representatives to Philadelphia to draft the Declaration of Independence. Florida would remain a Loyalist stronghold for the duration of the American Revolution. [26]
Spain regained both East and West Florida after Britain's defeat in the American Revolution and the subsequent Treaty of Versailles in 1783, and continued the provincial divisions until 1821.

Joining the United States; Indian removal
Defense of Florida's northern border with the United States was minor during the second Spanish period. The region became a haven for escaped slaves and a base for Indian attacks against U.S. territories, and the U.S. pressed Spain for reform.
Americans of English descent and Americans of Scots-Irish descent began moving into northern Florida from the backwoods of Georgia and South Carolina . Though technically not allowed by the Spanish authorities and the Floridan government, they were never able to effectively police the border region and the backwoods settlers from the United States would continue to immigrate into Florida unchecked. These migrants, mixing with the already present British settlers who had remained in Florida since the British period, would be the progenitors of the population known as Florida Crackers . [27]
These American settlers established a permanent foothold in the area and ignored Spanish authorities. The British settlers who had remained also resented Spanish rule, leading to a rebellion in 1810 and the establishment for ninety days of the so-called Free and Independent Republic of West Florida on September 23. After meetings beginning in June, rebels overcame the garrison at Baton Rouge (now in Louisiana ), and unfurled the flag of the new republic: a single white star on a blue field. This flag would later become known as the " Bonnie Blue Flag ".
In 1810, parts of West Florida were annexed by proclamation of President James Madison , who claimed the region as part of the Louisiana Purchase . These parts were incorporated into the newly formed Territory of Orleans . The U.S. annexed the Mobile District of West Florida to the Mississippi Territory in 1812. Spain continued to dispute the area, though the United States gradually increased the area it occupied.
In 1812, came the formation of the Republic of East Florida and ensuing revolution.
Seminole Indians based in East Florida began raiding Georgia settlements, and offering havens for runaway slaves. The United States Army led increasingly frequent incursions into Spanish territory, including the 1817–1818 campaign against the Seminole Indians by Andrew Jackson that became known as the First Seminole War . The United States now effectively controlled East Florida. Control was necessary according to Secretary of State John Quincy Adams because Florida had become "a derelict open to the occupancy of every enemy, civilized or savage, of the United States, and serving no other earthly purpose than as a post of annoyance to them.". [28]
Florida had become a burden to Spain, which could not afford to send settlers or garrisons. Madrid therefore decided to cede the territory to the United States through the Adams-Onís Treaty , which took effect in 1821. [29] President James Monroe was authorized on March 3, 1821 to take possession of East Florida and West Florida for the United States and provide for initial governance. [30] Andrew Jackson , on behalf of the U.S. federal government, served as a military commissioner with the powers of governor of the newly acquired territory for a brief period. [31] On March 30, 1822, the U.S. Congress merged East Florida and part of West Florida into the Florida Territory . [32]
By the early 1800s, Indian removal was a significant issue throughout the southeastern U.S. and also in Florida. In 1830, the U.S. Congress passed the Indian Removal Act and as settlement increased, pressure grew on the United States government to remove the Indians from Florida. Seminoles harbored runaway blacks, known as the Black Seminoles , and clashes between whites and Indians grew with the influx of new settlers. In 1832, the Treaty of Payne's Landing promised to the Seminoles lands west of the Mississippi River if they agreed to leave Florida. Many Seminole left at this time.
Some Seminoles remained, and the U.S. Army arrived in Florida, leading to the Second Seminole War (1835–42). Following the war, approximately 3,000 Seminole and 800 Black Seminole were removed to Indian Territory . A few hundred Seminole remained in Florida in the Everglades .
On March 3, 1845, Florida became the 27th state to join the United States of America. [33] The state was admitted as a slave state and ceased to be a sanctuary for runaway slaves. Initially its population grew slowly.
As European-American settlers continued to encroach on Seminole lands, and the United States intervened to move the remaining Seminoles to the West. The Third Seminole War (1855–58) resulted in the forced removal of most of the remaining Seminoles, although hundreds of Seminole Indians remained in the Everglades. [34]

Slavery, war, and disenfranchisement
American settlers began to establish cotton plantations in northern Florida, which required numerous laborers, which they supplied by buying slaves in the domestic market. By 1860 Florida had only 140,424 people, of whom 44% were enslaved. There were fewer than 1,000 free African Americans before the American Civil War. [35]
In January 1861, nearly all delegates in the Florida Legislature approved an ordinance of secession, declaring Florida to be "a sovereign and independent nation"—an apparent reassertion to the preamble in Florida's Constitution of 1838, in which Florida agreed with Congress to be a "Free and Independent State." Although not directly related to the issue of slavery, the ordinance declared Florida's secession from the Union , allowing it to become one of the founding members of the Confederate States , a looser union of states.
The confederal union received little help from Florida; the 15,000 men it offered were generally sent elsewhere. The largest engagements in the state were the Battle of Olustee , on February 20, 1864, and the Battle of Natural Bridge , on March 6, 1865. Both were Confederate victories. [36] The war ended in 1865.
Following the American Civil War, Florida's congressional representation was restored on June 25, 1868, albeit forcefully after Radical Reconstruction and the installation of unelected government officials under the final authority of federal military commanders. After the Reconstruction period ended in 1876, white Democrats regained power in the state legislature. In 1885 they created a new constitution, followed by statutes through 1889 that disfranchised most blacks and many poor whites. [ citation needed ]
Until the mid-20th century, Florida was the least populous Southern state. In 1900 its population was only 528,542, of whom nearly 44% were African American, the same proportion as before the Civil War. [37] The boll weevil devastated cotton crops.
Forty thousand blacks, roughly one-fifth of their 1900 population, left the state in the Great Migration . They left due to lynchings and racial violence, and for better opportunities. [38] Disfranchisement for most African Americans in the state persisted until the Civil Rights Movement of the 1960s gained federal legislation in 1965 to enforce protection of their constitutional suffrage.

20th-century growth
Historically, Florida's economy was based upon agricultural products such as cattle farming, sugarcane, citrus, tomatoes, and strawberries.
Economic prosperity in the 1920s stimulated tourism to Florida and related development of hotels and resort communities. Combined with its sudden elevation in profile was the Florida land boom of the 1920s , which brought a brief period of intense land development. Devastating hurricanes in 1926 and 1928 , followed by the Great Depression , brought that period to a halt. Florida's economy did not fully recover until the military buildup for World War II .
The climate, tempered by the growing availability of air conditioning , and low cost of living made the state a haven. Migration from the Rust Belt and the Northeast sharply increased Florida's population after the war. In recent decades, more migrants have come for the jobs in a developing economy.
With a population of more than 18 million according to the 2010 census, Florida is the most populous state in the Southeastern United States, and the fourth-most populous in the United States.

Geography
Much of the state of Florida is situated on a peninsula between the Gulf of Mexico, the Atlantic Ocean and the Straits of Florida . Spanning two time zones , it extends to the northwest into a panhandle , extending along the northern Gulf of Mexico. It is bordered on the north by the states of Georgia and Alabama , and on the west, at the end of the panhandle, by Alabama. It is the only state that borders both the Atlantic Ocean and Gulf of Mexico.
Florida is west of The Bahamas and 90 miles (140 km) north of Cuba . Florida is one of the largest states east of the Mississippi River , and only Alaska and Michigan are larger in water area. The water boundary is 3 nautical miles (3.5 mi; 5.6 km) offshore in the Atlantic Ocean [39] and 9 nautical miles (10 mi; 17 km) offshore in the Gulf of Mexico. [39]
At 345 feet (105 m) above mean sea level , Britton Hill is the highest point in Florida and the lowest highpoint of any U.S. state. [40] Much of the state south of Orlando lies at a lower elevation than northern Florida, and is fairly level. Much of the state is at or near sea level.
However, some places such as Clearwater have promontories that rise 50 to 100 ft (15 to 30 m) above the water. Much of Central and North Florida, typically 25 mi (40 km) or more away from the coastline, have rolling hills with elevations ranging from 100 to 250 ft (30 to 76 m). The highest point in peninsular Florida (east and south of the Suwannee River ), Sugarloaf Mountain , is a 312-foot (95 m) peak in Lake County . [41] On average, Florida is the flattest state in the United States. [42]

Climate
The climate of Florida is tempered somewhat by the fact that no part of the state is distant from the ocean. North of Lake Okeechobee , the prevalent climate is humid subtropical ( Köppen : Cfa ), while areas south of the lake (including the Florida Keys ) have a true tropical climate (Köppen: Aw ). [43] Mean high temperatures for late July are primarily in the low 90s Fahrenheit (32–34 °C). Mean low temperatures for early to mid January range from the low 40s Fahrenheit (4–7 °C) in northern Florida to above 60 °F (16 °C) from Miami on southward. With an average daily temperature of 70.7 °F (21.5 °C), it is the warmest state in the U.S. [44]
In the summer, high temperatures in the state seldom exceed 100 °F (38 °C). Several record cold maxima have been in the 30s °F (−1 to 4 °C) and record lows have been in the 10s (−12 to −7 °C). These temperatures normally extend at most a few days at a time in the northern and central parts of Florida. Southern Florida, however, rarely encounters freezing temperatures. [ citation needed ]
The hottest temperature ever recorded in Florida was 109 °F (43 °C), which was set on June 29, 1931 in Monticello . The coldest temperature was −2 °F (−19 °C), on February 13, 1899, just 25 miles (40 km) away, in Tallahassee. [45] [46]
Due to its subtropical and tropical climate, Florida rarely receives snow . However, on rare occasions, a combination of cold moisture and freezing temperatures can result in snowfall in the farthest northern regions. Frost is more common than snow, occurring sometimes in the panhandle. [ citation needed ]
The USDA Plant hardiness zones for the state range from zone 8a (no colder than 10 °F or −12 °C) in the inland western panhandle to zone 11b (no colder than 45 °F or 7 °C) in the lower Florida Keys . [47]
Florida's nickname is the "Sunshine State", but severe weather is a common occurrence in the state. Central Florida is known as the lightning capital of the United States, as it experiences more lightning strikes than anywhere else in the U.S. [54] Florida has one of the highest average precipitation levels of any state, [55] in large part because afternoon thunderstorms are common in much of the state from late spring until early autumn. A narrow eastern part of the state including Orlando and Jacksonville receives between 2,400 and 2,800 hours of sunshine annually. The rest of the state, including Miami, receives between 2,800 and 3,200 hours annually. [56]
Florida leads the United States in tornadoes per area (when including waterspouts ) [57] but they do not typically reach the intensity of those in the Midwest and Great Plains . Hail often accompanies the most severe thunderstorms. [ citation needed ]
Hurricanes pose a severe threat each year during the June 1 to November 30 hurricane season, particularly from August to October. Florida is the most hurricane-prone state, with subtropical or tropical water on a lengthy coastline. Of the category 4 or higher storms that have struck the United States, 83% have either hit Florida or Texas. [58] From 1851 to 2006, Florida was struck by 114 hurricanes, 37 of them major— category 3 and above. [58] It is rare for a hurricane season to pass without any impact in the state by at least a tropical storm. [ citation needed ]
Florida was the site of what was then the costliest weather disaster in U.S. history, Hurricane Andrew , which caused more than $25 billion in damage when it struck in August 1992; it held that distinction until 2005, when Hurricane Katrina surpassed it. Hurricane Wilma — the second-most expensive hurricane in Florida history — landed just south of Marco Island in October 2005. [59] [60]

Fauna
Florida is host to many types of wildlife including:
The only known calving area for the northern right whale is off the coasts of Florida and Georgia. [65]
The native bear population has risen from a historic low of 300 in the 1970s, to 3,000 in 2011. [66]
Since their accidental importation from South America into North America in the 1930s, the red imported fire ant population has increased its territorial range to include most of the Southern United States, including Florida. They are more aggressive than most native ant species and have a painful sting. [67]
A number of non-native snakes and lizards have been released in the wild. In 2010 the state created a hunting season for Burmese and Indian pythons , African rock pythons , green anacondas , and Nile monitor lizards . [68] Green iguanas have also established a firm population in the southern part of the state.
There are about 500,000 feral pigs in Florida. [69]

Flora
There are about 3,000 different types of wildflowers in Florida. This is the third-most diverse state in the union, behind California and Texas, both larger states. [70]
On the east coast of the state, mangroves have normally dominated the coast from Cocoa Beach southward; salt marshes from St. Augustine northward. From St. Augustine south to Cocoa Beach, the coast fluctuates between the two, depending on the annual weather conditions. [64]

Environmental issues
Florida is a low per capita energy user. [71] It is estimated that approximately 4% of energy in the state is generated through renewable resources. [72] Florida's energy production is 6% of the nation's total energy output, while total production of pollutants is lower, with figures of 5.6% for nitrogen oxide , 5.1% for carbon dioxide , and 3.5% for sulfur dioxide . [72]
All potable water resources have been controlled by the state government through five regional water authorities since 1972. [73]
Red tide has been an issue on the southwest coast of Florida, as well as other areas. While there has been a great deal of conjecture over the cause of the toxic algae bloom, there is no evidence that it is being caused by pollution or that there has been an increase in the duration or frequency of red tides. [74]
The Florida panther is close to extinction. A record 23 were killed in 2009 predominately by automobile collisions, leaving about 100 individuals in the wild. The Center for Biological Diversity and others have therefore called for a special protected area for the panther to be established. [75] Manatees are also dying at a rate higher than their reproduction.
Much of Florida has an elevation of less than 12 feet (3.7 m), including many populated areas. Therefore, it is susceptible to rising sea levels associated with global warming . [76] The Atlantic beaches that are vital to the state's economy are being washed out to sea due to rising sea levels caused by climate change. The Miami beach area, close to the continental shelf, is running out of accessible offshore sand reserves. [77]

Geology
The Florida peninsula is a porous plateau of karst limestone sitting atop bedrock known as the Florida Platform . The largest deposits of potash in the United States are found in Florida. [78]
Extended systems of underwater caves, sinkholes and springs are found throughout the state and supply most of the water used by residents. The limestone is topped with sandy soils deposited as ancient beaches over millions of years as global sea levels rose and fell. During the last glacial period , lower sea levels and a drier climate revealed a much wider peninsula, largely savanna . [79] The Everglades , an enormously wide, slow-flowing river encompasses the southern tip of the peninsula. Sinkhole damage claims on property in the state exceeded a total of $2 billion from 2006 through 2010. [80]
Florida is tied for last place as having the fewest earthquakes of any U.S. state. [81] [82] Earthquakes are rare because Florida is not located near any tectonic plate boundaries.

Demographics

Population
The United States Census Bureau estimates that the population of Florida was 20,271,272 on July 1, 2015, a 7.82% increase since the 2010 United States Census . [4] The population of Florida in the 2010 census was 18,801,310. [84] Florida was the seventh fastest-growing state in the U.S. in the 12-month period ending July 1, 2012. [85] In 2010, the center of population of Florida was located between Fort Meade and Frostproof . The center of population has moved less than 5 miles (8 km) to the east and approximately 1 mile (1.6 km) to the north between 1980 and 2010 and has been located in Polk County since the 1960 census . [86] The population exceeded 19.7 million by December 2014, surpassing the population of the state of New York for the first time. [87]
Florida contains the highest percentage of people over 65 (17%). [88] There were 186,102 military retirees living in the state in 2008. [89] About two-thirds of the population was born in another state, the second highest in the U.S. [90]
In 2010, illegal immigrants constituted an estimated 5.7% of the population. This was the sixth highest percentage of any state in the U.S. [91] [92] There were an estimated 675,000 illegal immigrants in the state in 2010. [93]
A 2013 Gallup poll indicated that 47% of the residents agreed that Florida was the best state to live in. Results in other states ranged from a low of 18% to a high of 77%. [94]

Municipalities and metropolitan areas
The legal name in Florida for a city, town or village is "municipality". In Florida there is no legal difference between towns, villages and cities. [95]
In 2012, 75% of the population lived within 10 miles (16 km) of the coastline. [96]
The largest metropolitan area in the state as well as the entire southeastern United States is the Miami metropolitan area , with about 6.06 million people. The Tampa Bay Area , with over 3.02 million people, is the second largest; the Orlando metropolitan area , with over 2.44 million people, is the third; and the Jacksonville metropolitan area , with over 1.47 million people, is fourth.
Florida has 22 Metropolitan Statistical Areas (MSAs) defined by the United States Office of Management and Budget (OMB). 43 of Florida's 67 counties are in a MSA.

Racial and ethnic makeup
Hispanic and Latinos of any race made up 22.5% of the population in 2010. [101] As of 2011, 57% of Florida's population younger than age 1 were minorities (meaning that they had at least one parent who was not non-Hispanic white). [102]
Florida is among the three states with the most severe felony disenfranchisement laws. Florida requires felons to have completed sentencing, parole and/or probation, and then seven years later, to apply individually for restoration of voting privileges. As in other aspects of the criminal justice system, this law has disproportionate effects for minorities. As a result, according to Brent Staples , based on data from The Sentencing Project , the effect of Florida's law is such that in 2014 "[m]ore than one in ten Floridians – and nearly one in four African-American Floridians – are shut out of the polls because of felony convictions." [103]

Ancestry groups
In 2010, 6.9% of the population (1,269,765) considered themselves to be of only American ancestry (regardless of race or ethnicity). [104] [105] Many of these were of English or Scotch-Irish descent; however, their families have lived in the state for so long, that they choose to identify as having "American" ancestry or do not know their ancestry. [106] [107] [108] [109] [110] [111] In the 1980 United States census the largest ancestry group reported in Florida was English with 2,232,514 Floridians claiming that they were of English or mostly English American ancestry. [112] Some of their ancestry went back to the original thirteen colonies .
As of 2010, those of (non-Hispanic white) European ancestry accounted for 57.9% of Florida's population. Out of the 57.9%, the largest groups were 12.0% German (2,212,391), 10.7% Irish (1,979,058), 8.8% English (1,629,832), 6.6% Italian (1,215,242), 2.8% Polish (511,229), and 2.7% French (504,641). [104] [105] White Americans of all European backgrounds are present in all areas of the state. In 1970, non-Hispanic whites were nearly 80% of Florida's population. [113] Those of English and Irish ancestry are present in large numbers in all the urban/suburban areas across the state. Some native white Floridians, especially those who have descended from long-time Florida families, may refer to themselves as " Florida crackers "; others see the term as a derogatory one. Like whites in most of the other Southern states, they descend mainly from English and Scots-Irish settlers, as well as some other British American settlers. [114]
As of 2010, those of Hispanic or Latino ancestry accounted for 22.5% (4,223,806) of Florida's population. Out of the 22.5%, the largest groups were 6.5% (1,213,438) Cuban , 4.5% (847,550) Puerto Rican , 3.3% (629,718) Mexican , and 1.6% (300,414) Colombian . [116] Florida's Hispanic population includes large communities of Cuban Americans in Miami and Tampa, Puerto Ricans in Orlando and Tampa, and Mexican/Central American migrant workers. The Hispanic community continues to grow more affluent and mobile. As of 2011, 57.0% of Florida's children under the age of 1 belonged to minority groups. [117] Florida has a large and diverse Hispanic population, with Cubans and Puerto Ricans being the largest groups in the state. Nearly 80% of Cuban Americans live in Florida, especially South Florida where there is a long-standing and affluent Cuban community. [118] Florida has the second largest Puerto Rican population after New York, as well as the fastest-growing in the nation. [119] Puerto Ricans are more widespread throughout the state, though the heaviest concentrations are in the Orlando area of Central Florida. [120]
As of 2010, those of African ancestry accounted for 16.0% of Florida's population, which includes African Americans . Out of the 16.0%, 4.0% (741,879) were West Indian or Afro-Caribbean American . [104] [105] [116] During the early 1900s, black people made up nearly half of the state's population. [121] In response to segregation, disfranchisement and agricultural depression, many African Americans migrated from Florida to northern cities in the Great Migration , in waves from 1910 to 1940, and again starting in the later 1940s. They moved for jobs, better education for their children and the chance to vote and participate in society. By 1960 the proportion of African Americans in the state had declined to 18%. [122] Conversely large numbers of northern whites moved to the state. [ citation needed ] Today, large concentrations of black residents can be found in northern and central Florida. Aside from blacks descended from African slaves brought to the US south, there are also large numbers of blacks of West Indian , recent African , and Afro-Latino immigrant origins, especially in the Miami/South Florida area. In 2010, Florida had the highest percentage of West Indians in the United States, with 2.0% (378,926) from Haitian ancestry, and 1.3% (236,950) Jamaican . [123] All other (non-Hispanic) Caribbean nations were well below 0.1% of Florida residents. [123] [124]
As of 2010, those of Asian ancestry accounted for 2.4% of Florida's population. [104] [105]

Languages
In 1988 English was affirmed as the state's official language in the Florida Constitution . Spanish is also widely spoken, especially as immigration has continued from Latin America. Twenty percent of the population speak Spanish as their first language. Twenty-seven percent of Florida's population reports speaking a mother language other than English, and more than 200 first languages other than English are spoken at home in the state. [125] [126]
The most common languages spoken in Florida as a first language in 2010 are: [125]

Religion
The 2014 Pew Religious Landscape Survey showed the religious makeup of the state was as follows: [127]
In 2010, the three largest denominational groups in Florida were the Roman Catholic Church , the Southern Baptist Convention , and the United Methodist Church . [128]
Florida is mostly Protestant , but Roman Catholicism is the single largest denomination in the state, due in significant part to the state's large Hispanic population. There is also a sizable Jewish community, located mainly in South Florida ; this is the largest Jewish population in the South and the third-largest in the U.S. behind those of New York and California. [129]

Governance
The basic structure, duties, function, and operations of the government of the state of Florida are defined and established by the Florida Constitution , which establishes the basic law of the state and guarantees various rights and freedoms of the people. The state government consists of three separate branches: judicial, executive, and legislative. The legislature enacts bills, which, if signed by the governor , become law .
The Florida Legislature comprises the Florida Senate , which has 40 members, and the Florida House of Representatives , which has 120 members. The current Governor of Florida is Rick Scott . The Florida Supreme Court consists of a Chief Justice and six Justices.
Florida has 67 counties . Some reference materials may show only 66 because Duval County is consolidated with the City of Jacksonville . There are 379 cities in Florida (out of 411) that report regularly to the Florida Department of Revenue, but there are other incorporated municipalities that do not. The state government's primary source of revenue is sales tax. Florida does not impose a personal income tax . The primary revenue source for cities and counties is property tax; unpaid taxes are subject to tax sales which are held (at the county level) in May and (due to the extensive use of online bidding sites) are highly popular.
There were 800 federal corruption convictions from 1988 to 2007, more than any other state. [130]

Elections history
From 1952 to 1964, most voters were registered Democrats, but the state voted for the Republican presidential candidate in every election except for 1964 . The following year, Congress passed and President Lyndon B. Johnson signed the Voting Rights Act of 1965 , providing for oversight of state practices and enforcement of constitutional voting rights for African Americans and other minorities in order to prevent the discrimination and disenfranchisement that had excluded most of them for decades from the political process.
From the 1930s through much of the 1960s, Florida was essentially a one-party state dominated by white conservative Democrats, who together with other Democrats of the Solid South, exercised considerable control in Congress. They gained federal money from national programs; like other southern states, Florida residents have received more federal monies than they pay in taxes: the state is a net beneficiary. Since the 1970s, the conservative white majority of voters in the state has largely shifted from the Democratic to the Republican Party. It has continued to support Republican presidential candidates through the 20th century, except in 1976 and 1996 , when the Democratic nominee was from the South . They have had "the luxury of voting for presidential candidates who pledge to cut taxes and halt the expansion of government while knowing that their congressional delegations will continue to protect federal spending." [132]
In the 2008 and 2012 presidential elections, Barack Obama carried the state as a northern Democrat, attracting high voter turnout especially among the young, Independents, and minority voters, of whom Hispanics comprise an increasingly large proportion. 2008 marked the first time since 1932, when Franklin D. Roosevelt carried the state, that Florida was carried by a Northern Democrat for president.
The first post- Reconstruction era Republican elected to Congress from Florida was William C. Cramer in 1954 from Pinellas County on the Gulf Coast, [133] where demographic changes were underway. In this period, African Americans were still disenfranchised by the state's constitution and discriminatory practices; in the 19th century they had made up most of the Republican Party. Cramer built a different Republican Party in Florida, attracting local white conservatives and transplants from northern and midwestern states. In 1966 Claude R. Kirk, Jr. was elected as the first post-Reconstruction Republican governor, in an upset election. [134] In 1968 Edward J. Gurney , also a white conservative, was elected as the state's first post-reconstruction Republican US Senator. [135] In 1970 Democrats took the governorship and the open US Senate seat, and maintained dominance for years.
Since the mid-20th century, Florida has been considered a bellwether , voting for 13 successful presidential candidates since 1952. It voted for the loser only three times. [136]
In 1998, Democratic voters dominated areas of the state with a high percentage of racial minorities and transplanted white liberals from the northeastern United States, known colloquially as "snowbirds". [137] South Florida and the Miami metropolitan area are dominated by both racial minorities and white liberals. Because of this, the area has consistently voted as one of the most Democratic areas of the state. The Daytona Beach area is similar demographically and the city of Orlando has a large Hispanic population, which has often favored Democrats. Republicans, made up mostly of white conservatives, have dominated throughout much of the rest of Florida, particularly in the more rural and suburban areas. This is characteristic of its voter base throughout the Deep South . [137]
The fast-growing I-4 corridor area, which runs through Central Florida and connects the cities of Daytona Beach , Orlando , and Tampa / St. Petersburg , has had a fairly even breakdown of Republican and Democratic voters. The area is often seen as a merging point of the conservative northern portion of the state and the liberal southern portion, making it the biggest swing area in the state. Since the late 20th century, the voting results in this area, containing 40% of Florida voters, has often determined who will win the state of Florida in presidential elections. [138]
The Democratic Party has maintained an edge in voter registration, both statewide and in 40 of the 67 counties, including Miami-Dade , Broward , and Palm Beach counties, the state's three most populous. [139]

Elections of 2000 to present
In 2000, George W. Bush won the U.S. Presidential election by a margin of 271–266 in the Electoral College . [140] Of the 271 electoral votes for Bush, 25 were cast by electors from Florida. [141] The Florida results were contested and a recount was ordered by the court, with the results settled in a court decision.
Reapportionment following the 2010 United States Census gave the state two more seats in the House of Representatives. [142] The legislature's redistricting, announced in 2012, was quickly challenged in court, on the grounds that it had unfairly benefited Republican interests. In 2015, the Florida Supreme Court ruled on appeal that the congressional districts had to be redrawn because of the legislature's violation of the Fair District Amendments to the state constitution passed in 2010; it accepted a new map in early December 2015.
The political make-up of congressional and legislative districts has enabled Republicans to control the governorship and most statewide elective offices, and 17 of the state's 27 seats in the 2012 House of Representatives . [143] Florida has been listed as a swing state in Presidential elections since 1950, voting for the losing candidate once in that period of time. [144]
In the closely contested 2000 election , the state played a pivotal role. [140] [141] [145] [146] [147] [148] Out of more than 5.8 million votes for the two main contenders Bush and Al Gore , around 500 votes separated the two candidates for the all-decisive Florida electoral votes that landed Bush the election win. Florida's felony disenfranchisement law is more severe than most European nations or other American states. A 2002 study in the American Sociological Review concluded that "if the state's 827,000 disenfranchised felons had voted at the same rate as other Floridians, Democratic candidate Al Gore would have won Florida—and the presidency—by more than 80,000 votes." [149]
In 2008, delegates of both the Republican Florida primary election and Democratic Florida primary election were stripped of half of their votes when the conventions met in August due to violation of both parties' national rules.
In the 2010 elections, Republicans solidified their dominance statewide, by winning the governor's mansion, and maintaining firm majorities in both houses of the state legislature. They won four previously Democratic-held seats to create a 19–6 Republican-majority delegation representing Florida in the federal House of Representatives.
In 2010, more than 63% of state voters approved the initiated Amendments 5 and 6 to the state constitution, to ensure more fairness in districting. These have become known as the Fair District Amendments. As a result of the 2010 United States Census , Florida gained two House of Representative seats in 2012. [142] The legislature issued revised congressional districts in 2012, which were immediately challenged in court by supporters of the above amendments.
The court ruled in 2014, after lengthy testimony, that at least two districts had to be redrawn because of gerrymandering. After this was appealed, in July 2015 the Florida Supreme Court ruled that lawmakers had followed an illegal and unconstitutional process overly influenced by party operatives, and ruled that at least eight districts had to be redrawn. On December 2, 2015, a 5-2 majority of the Court accepted a new map of congressional districts, some of which was drawn by challengers. Their ruling affirmed the map previously approved by Leon County Judge Terry Lewis, who had overseen the original trial. It particularly makes changes in South Florida. There are likely to be additional challenges to the map and districts. [150]
According to The Sentencing Project , the effect of Florida's felony disenfranchisement law is such that in 2014, "[m]ore than one in ten Floridians – and nearly one in four African-American Floridians – are [were] shut out of the polls because of felony convictions," although they had completed sentences and parole/probation requirements. [151]

Statutes
The state repealed mandatory auto inspection in 1981. [152]
In 1972, the state made personal injury protection auto insurance mandatory for drivers, becoming the second in the nation to enact a no-fault insurance law. The ease of receiving payments under this law is seen as precipitating a major increase in insurance fraud. [153] Auto insurance fraud was the highest in the nation in 2011, estimated at close to $1 billion. [154] Fraud is particularly centered in the Miami-Dade metropolitan and Tampa areas. [155] [156] [157]

Law enforcement
Florida was ranked the fifth-most dangerous state in 2009. Ranking was based on the record of serious felonies committed in 2008. [158] The state was the sixth highest scammed state in 2010. It ranked first in mortgage fraud in 2009. [159]
In 2009, 44% of highway fatalities involved alcohol. [160] Florida is one of seven states that prohibit the open carry of handguns . This law was passed in 1987. [161]
According to the Federal Trade Commission, Florida has the highest per capita rate of both reported fraud and other types of complaints and reported including identity theft complaints. [162]

Economy
In the twentieth century, tourism, industry, construction, international banking, biomedical and life sciences, healthcare research, simulation training, aerospace and defense, and commercial space travel have contributed to the state's economic development. [ citation needed ]
The Gross Domestic Product (GDP) of Florida in 2010 was $748 billion. [165] Its GDP is the fourth largest economy in the United States. [166] In 2010, it became the fourth largest exporter of trade goods. [167] The major contributors to the state's gross output in 2007 were general services, financial services, trade, transportation and public utilities, manufacturing and construction respectively. In 2010–11, the state budget was $70.5 billion, having reached a high of $73.8 billion in 2006–07. [168] Chief Executive Magazine name Florida the third "Best State for Business" in 2011. [169]
The economy is driven almost entirely by its nineteen metropolitan areas. In 2004, they had a combined total of 95.7% of the state's domestic product. [170]

Personal income
In 2011, Florida's per capita personal income was $39,563, ranking 27th in the nation. [171] In February 2011, the state's unemployment rate was 11.5%. [172] Florida is one of seven states that do not impose a personal income tax .
Florida's constitution establishes a state minimum wage that (unique among minimum wage laws) is adjusted for inflation annually. As of January 1, 2015, Florida's minimum wage was $5.03 for tipped positions , and $8.05 for non-tipped positions, which was higher than the federal rate of $7.25. [173]
Florida has 4 cities in the top 25 cities in the U.S. with the most credit card debt. [174] The state also had the second-highest credit card delinquency rate, with 1.45% of cardholders in the state more than 90 days delinquent on one or more credit cards. [175]
There were 2.4 million Floridians living in poverty in 2008. 18.4% of children 18 and younger were living in poverty. [176] Miami is the sixth poorest big city in the United States. [177] In 2010, over 2.5 million Floridians were on food stamps, up from 1.2 million in 2007. To qualify, Floridians must make less than 133% of the federal poverty level, which would be under $29,000 for a family of four. [178]

Real estate
In the early 20th century, land speculators discovered Florida, and businessmen such as Henry Plant and Henry Flagler developed railroad systems, which led people to move in, drawn by the weather and local economies. From then on, tourism boomed, fueling a cycle of development that overwhelmed a great deal of farmland.
Because of the collective effect on the insurance industry of the hurricane claims of 2004, homeowners insurance has risen 40% to 60% and deductibles have risen. [59]
At the end of the third quarter in 2008, Florida had the highest mortgage delinquency rate in the U.S., with 7.8% of mortgages delinquent at least 60 days. [175] A 2009 list of national housing markets that were hard hit in the real estate crash included a disproportionate number in Florida. [179] The early 21st-century building boom left Florida with 300,000 vacant homes in 2009, according to state figures. [180] In 2009, the US Census Bureau estimated that Floridians spent an average 49.1% of personal income on housing-related costs, the third highest percentage in the U.S. [181]
In the third quarter of 2009, there were 278,189 delinquent loans, 80,327 foreclosures. [182] Sales of existing homes for February 2010 was 11,890, up 21% from the same month in 2009. Only two metropolitan areas showed a decrease in homes sold: Panama City and Brevard County . The average sales price for an existing house was $131,000, 7% decrease from the prior year. [183] [ dubious – discuss ]

Tourism
Tourism makes up one of the largest sectors of the state economy, with nearly 1.4 million persons employed in the tourism industry in 2016 (a record for the state, surpassing the 1.2 million employment from 2015). [185] [186] In 2015, Florida broke the 100-million visitor mark for the first time in state history by hosting a record 105 million visitors [186] [187] and broke that record in 2016 with 112.8 million tourists; Florida has set tourism records for six consecutive years. [185]
Many beach towns are popular tourist destinations, particularly during winter and spring break . Twenty-three million tourists visited Florida beaches in 2000, spending $22 billion. [188] The public has a right to beach access under the public trust doctrine , but some areas have access effectively blocked by private owners for a long distance. [189]
Amusement parks , especially in the Greater Orlando area, make up a significant portion of tourism. The Walt Disney World Resort is the most visited vacation resort in the world with over 50 million annual visitors, consisting of four theme parks , 27 themed resort hotels , 9 non–Disney hotels, two water parks , four golf courses and other recreational venues. [190] Other major theme parks in the area include Universal Orlando Resort , SeaWorld Orlando and Busch Gardens Tampa .

Agriculture and fishing
Agriculture is the second largest industry in the state. Citrus fruit, especially oranges, are a major part of the economy, and Florida produces the majority of citrus fruit grown in the United States. In 2006, 67% of all citrus, 74% of oranges, 58% of tangerines, and 54% of grapefruit were grown in Florida. About 95% of commercial orange production in the state is destined for processing (mostly as orange juice, the official state beverage ). [191]
Citrus canker continues to be an issue of concern. From 1997 to 2013, the growing of citrus trees has declined 25%, from 600,000 acres (240,000 ha) to 450,000 acres (180,000 ha). Citrus greening disease is incurable. A study states that it has caused the loss of $4.5 billion between 2006 and 2012. As of 2014, it was the major agricultural concern. [192]
Other products include sugarcane, strawberries, tomatoes and celery. [193] The state is the largest producer of sweet corn and green beans for the U.S. [194]
The Everglades Agricultural Area is a major center for agriculture. The environmental impact of agriculture, especially water pollution , is a major issue in Florida today.
In 2009, fishing was a $6 billion industry, employing 60,000 jobs for sports and commercial purposes. [195]

Industry
Florida is the leading state for sales of power boats. There were $1.96 billion worth of boats sold in 2013. [197]

Mining
Phosphate mining , concentrated in the Bone Valley , is the state's third-largest industry. The state produces about 75% of the phosphate required by farmers in the United States and 25% of the world supply, with about 95% used for agriculture (90% for fertilizer and 5% for livestock feed supplements) and 5% used for other products. [198]
After the watershed events of Hurricane Andrew in 1992, the state of Florida began investing in economic development through the Office of Trade, Tourism, and Economic Development. Governor Jeb Bush realized that watershed events such as Andrew negatively impacted Florida's backbone industry of tourism severely. The office was directed to target Medical/Bio-Sciences among others. Three years later, The Scripps Research Institute (TSRI) announced it had chosen Florida for its newest expansion. In 2003, TSRI announced plans to establish a major science center in Palm Beach, a 364,000 square feet (33,800 m 2 ) facility on 100 acres (40 ha), which TSRI planned to occupy in 2006. [199]

Government
Since the development of the federal NASA Merritt Island launch sites on Cape Canaveral (most notably Kennedy Space Center) in 1962, Florida has developed a sizable aerospace industry .
Another major economic engine in Florida is the United States military . There are 24 military bases in the state, housing three Unified Combatant Commands ; United States Central Command in Tampa, United States Southern Command in Doral , and United States Special Operations Command in Tampa. Some 109,390 U.S. military personnel stationed in Florida, [200] contributing, directly and indirectly, $52 billion a year to the state's economy. [201]
In 2009, there were 89,706 federal workers employed within the state. [202] Tens of thousands more employees work for contractors who have federal contracts, including those with the military.
In 2012, government of all levels was a top employer in all counties in the state, because this classification includes public school teachers and other school staff. School boards employ nearly 1 of every 30 workers in the state. The federal military was the top employer in three counties. [203]

Health
There were 2.7 million Medicaid patients in Florida in 2009. The governor has proposed adding $2.6 billion to care for the expected 300,000 additional patients in 2011. [204] The cost of caring for 2.3 million clients in 2010 was $18.8 billion. [205] This is nearly 30% of Florida's budget. [206] Medicaid paid for 60% of all births in Florida in 2009. [60] The state has a program for those not covered by Medicaid.
In 2013, Florida refused to participate in providing coverage for the uninsured under the Affordable Care Act , popularly called Obamacare. The Florida legislature also refused to accept additional Federal funding for Medicaid, although this would have helped its constituents at no cost to the state. As a result, Florida is second only to Texas in the percentage of its citizens without health insurance. [207]

Architecture
Florida has the largest collection of Art Deco and Streamline Moderne buildings in both the United States and the entire world, most of which are located in the Miami metropolitan area , especially Miami Beach 's Art Deco District , constructed as the city was becoming a resort destination. [208] A unique architectural design found only in Florida is the post-World War II Miami Modern , which can be seen in areas such as Miami 's MiMo Historic District .
Being of early importance as a regional center of banking and finance, the architecture of Jacksonville displays a wide variety of styles and design principles. Many of state's earliest skyscrapers were constructed in Jacksonville, dating as far back as 1902., [209] and last holding a state height record from 1974 to 1981. [210] The city is endowed with one of the largest collections of Prairie School buildings outside of the Midwest. [211] Jacksonville is also noteworthy for its collection of Mid-Century modern architecture. [212]
Some sections of the state feature architectural styles including Spanish revival , Florida vernacular , and Mediterranean Revival . [213] [214] A notable collection of these styles can be found in St. Augustine , the oldest continuously occupied European-established settlement within the borders of the United States. [215]

Media

Education

Primary and secondary education
Florida's public primary and secondary schools are administered by the Florida Department of Education . School districts are organized within county boundaries. Each school district has an elected Board of Education which sets policy, budget, goals, and approves expenditures. Management is the responsibility of a Superintendent of schools .
The Florida Department of Education is required by law to train educators in teaching English for Speakers of Other Languages (ESOL). [216]

Universities
The State University System of Florida was founded in 1905, and is governed by the Florida Board of Governors . During the 2010 academic year, 312,216 students attended one of these twelve universities. The Florida College System comprises 28 public community and state colleges. In 2011–12, enrollment consisted of more than 875,000 students. [217]
Florida's first private university, Stetson University , was founded in 1883. The Independent Colleges and Universities of Florida is an association of 28 private, educational institutions in the state. [218] This Association reported that their member institutions served over 121,000 students in the fall of 2006. [219]
In 2016, Florida charged the second lowest tuition in the nation for four years, $26,000 for in-state students, to $86,000 for out-of-state students. This compares with an average of 34,800 nationally for in-state students. [220]

Transportation

Highways
Florida's highway system contains 1,473 mi (2,371 km) of interstate highway, and 9,934 mi (15,987 km) of non-interstate highway, such as state highways and U.S. Highways. Florida's interstates , state highways , and U.S. Highways are maintained by the Florida Department of Transportation .
In 2011, there were about 9,000 retail gas stations in the state. Floridians consume 21 million gallons of gasoline daily, ranking it third in national use. [221] [222] Motorists have the 45th lowest rate of car insurance in the U.S. 24% are uninsured. [223]
Drivers between 15 and 19 years of age averaged 364 car crashes a year per ten thousand licensed Florida drivers in 2010. Drivers 70 and older averaged 95 per 10,000 during the same time frame. A spokesperson for the non-profit Insurance Institute said that "Older drivers are more of a threat to themselves." [224]
Before the construction of routes under the Federal Aid Highway Act of 1956 , Florida began construction of a long cross-state toll road , Florida's Turnpike . The first section, from Fort Pierce south to the Golden Glades Interchange was completed in 1957. After a second section north through Orlando to Wildwood (near present-day The Villages ), and a southward extension around Miami to Homestead , it was finished in 1974.
Florida's primary interstate routes include:

Airports
Florida has 131 public airports. [225] Florida's seven large hub and medium hub airports, as classified by the FAA, are the following:

Intercity rail
Florida is served by Amtrak , operating numerous lines throughout, connecting the state's largest cities to points north in the United States and Canada. The busiest Amtrak train stations in Florida in 2011 were: Sanford (259,944), Orlando (179,142), Tampa Union Station (140,785), Miami (94,556), and Jacksonville (74,733). [226] Sanford , in Greater Orlando , is the southern terminus of the Auto Train , which originates at Lorton, Virginia , south of Washington, D.C. Until 2005, Orlando was also the eastern terminus of the Sunset Limited , which travels across the southern United States via New Orleans , Houston , and San Antonio to its western terminus of Los Angeles. Florida is served by two additional Amtrak trains (the Silver Star and the Silver Meteor ), which operate between New York City and Miami. Miami Central Station , the city's rapid transit , commuter rail , intercity rail , and bus hub, is under construction.
The Florida Department of Transportation was preparing to build a high-speed rail between Tampa , Lakeland and Orlando . [227] This was to be the first phase of the Florida High Speed Rail system. [228] Soil work began in July 2010 [229] [230] and construction of the line was slated to begin in 2011, with the initial Tampa-Orlando phase completed by 2014. [231] The second phase, would have extended the line to Miami. Governor Scott, however, refused federal funds and the project has been canceled.
All Aboard Florida is a proposed higher-speed rail service that would run between Orlando and Miami at speeds up to 125 mph. Its Miami to Cocoa portion is scheduled to open in 2016, with the final segment to Orlando opening in 2017.

Public transit

Sports
Florida has three NFL teams, two MLB teams, two NBA teams, two NHL teams, and one MLS team. Florida gained its first permanent major-league professional sports team in 1966 when the American Football League added the Miami Dolphins . The state of Florida has given professional sports franchises some subsidies in the form of tax breaks since 1991. [237]
About half of all Major League Baseball teams conduct spring training in the state, with teams informally organized into the " Grapefruit League ". Throughout MLB history, other teams have held spring training in Florida.
NASCAR (headquartered in Daytona Beach ) begins all three of its major auto racing series in Florida at Daytona International Speedway in February, featuring the Daytona 500 , and ends all three Series in November at Homestead-Miami Speedway . Daytona also has the Coke Zero 400 NASCAR race weekend around Independence Day in July. The 24 Hours of Daytona is one of the world's most prestigious endurance auto races. The Grand Prix of St. Petersburg and Grand Prix of Miami have held IndyCar races as well.
The PGA of America is headquartered in Palm Beach Gardens , the PGA Tour is headquartered in Ponte Vedra Beach , and the LPGA is headquartered in Daytona Beach. The Players Championship , WGC-Cadillac Championship , Arnold Palmer Invitational , Honda Classic and Valspar Championship are PGA Tour rounds.
The Miami Masters is an ATP World Tour Masters 1000 and WTA Premier tennis event, whereas the Delray Beach International Tennis Championships is an ATP World Tour 250 event.
Minor league baseball, football , basketball, ice hockey , soccer and indoor football teams are based in Florida. Three of the Arena Football League 's teams are in Florida.
Florida's universities have a number of collegiate sport programs, especially the Florida State Seminoles and Miami Hurricanes of the Atlantic Coast Conference and the Florida Gators of the Southeastern Conference .

Sister states

See also
WebPage index: 00114
Emory University School of Law
Emory University School of Law (also known as Emory Law or ELS ) is a US law school that is part of Emory University in Atlanta, Georgia . It is currently ranked #22 among ABA -approved law schools by the 2018 U.S. News & World Report . [8]

Campus
Emory Law is located in Gambrell Hall, part of Emory ’s 630-acre (2.5 km 2 ) campus in the Druid Hills neighborhood, six miles (10 km) northeast of downtown Atlanta.
Gambrell Hall contains classrooms, faculty offices, administrative offices, student-organization offices, and a 325-seat auditorium. The school provides wireless Internet access throughout its facilities. Gambrell Hall also houses a courtroom. [9] [ not in citation given ]
Emory's five-story Hugh F. MacMillan Law Library opened in August 1995. The library is situated adjacent to Gambrell Hall and includes access to over 400,000 volumes and more than 4,000 serials subscriptions. [10]

Admissions and academics
Admission to the law school is selective. For the class entering in the fall of 2014, 223 JD candidates enrolled. The 25th and 75th LSAT percentiles for the 2014 entering class were 158 and 166, respectively, with a median of 165. The 25th and 75th undergraduate GPA percentiles were 3.30 and 3.85, respectively, with a median of 3.75. [11]
Nearly half of Emory Law students are women, and about 32% are from underrepresented ethnic groups. Approximately 60% of students come from outside the Southeastern U.S. [12]
It is ranked #22, tied with the University of Minnesota Law School, among ABA -approved law schools by the 2017 U.S. News & World Report. [13]
The School of Law offers a three-year, full-time program leading to a Juris Doctor degree. Emory Law is particularly known for its expertise in Bankruptcy Law , Environmental Law , Feminist Legal Theory , Intellectual Property Law, International law , Law and Religion , and Transactional Law .
Emory Law also offers joint-degree programs through cooperation with the Goizueta Business School (JD/MBA and JM/MBA), the Candler School of Theology (JD/MTS and JD/MDiv), the Graduate School of Arts and Sciences (JD/PhD), the Rollins School of Public Health (JD/MPH), the Emory Center for Ethics (JD/MA in Bioethics), and joint JD and Master of Laws degree (JD/LLM) through Emory School of Law.
In partnership with Central European University , Emory also provides an LLM program for students with a U.S. law degree seeking advanced training in international commercial law and international politics. Emory also has a separate LLM program for qualified foreign professionals seeking training in international and comparative law.
Emory Law's Juris Master is a 30-credit hour program that is intended to supplement a student's interest or professional experience in allied fields to law. The program offers a range of customized concentrations to allow students to enhance their skills in their home profession or interest area through a greater understanding of the law, legal concepts and frameworks. The coursework can be completed either full-time in as little as nine months or part-time in up to four years.

Clinics and programs
Students' expertise is developed through several clinics and programs. Emory Law also offers several summer study abroad programs in Budapest at the Central European University (CEU) and throughout the world. [14]
A team from Emory Law's TI:GER IP/patent/technology program, a collaborative program between Emory and Georgia Tech , was featured on CNN Money. [15] Other academic programs at Emory Law include:

Publications

Employment
According to Emory's official 2013 ABA-required disclosures, 62.4% of the Class of 2013 obtained full-time, long-term, JD-required, non-school funded employment nine months after graduation. [20] Emory's Law School Transparency under-employment score is 5.5%, indicating the percentage of the Class of 2013 unemployed, pursuing an additional degree, or working in a non-professional, short-term, or part-time job nine months after graduation, and an additional 21.2% were in school funded positions. [21]

Costs
The total cost of attendance (indicating the cost of tuition, fees, and living expenses) at Emory for the 2013-2014 academic year is $75,716. [22] The Law School Transparency estimated debt-financed cost of attendance for three years is $290,430. [23]

Notable Alumni

Notable Faculty
WebPage index: 00115
RISE Viktoria
RISE Viktoria (previously Viktoria Institute, Viktoria Swedish ICT) was founded in 1997 at the initiative of the local industry in West Sweden. The task is to perform research and development in applied information technology in collaboration with industry, the public sector, and academic institutions. The goal is to help Swedish automotive and transport industry achieve sustainable development and growth.
Viktoria works for making sure that research results and IT applications are disseminated rapidly, come to practical use and contribute to sustainable development of products, services, business and companies. All research projects are funded through competitive applications to, for example, The Swedish Foundation for Strategic Research, Swedish Agency for Innovation Systems, The Knowledge Foundation, The Swedish Research Institute for Information Technology and the EU. This has resulted in high-quality research and a very good reputation of the institute in Sweden and internationally.
By working closely with industry partners, Viktoria will develop user oriented, innovative services, which will have the advantage of being commercially well packaged from the very beginning. Indeed, Viktoria will play the role as supplier of specifications, as well as inspirational source to the companies developing platforms and services. In this way, Viktoria will provide a neutral arena for experimenting with and negotiating innovative solutions.
Today, about 80 people work at Viktoria. Viktoria is structured into five main application areas and two competence centers: Cooperative Systems, Digitalization Strategy, Electromobility, Sustainable Business and Sustainable Transports. Each research area is managed by a senior researcher (PhD or professor).
The Automotive competence area covers applied research with industry actors targeting IT applications and services that are based, or partly based, on in-vehicle computing and communication platforms. Examples of application areas include active safety, diagnostics or remote diagnostics, and nomadic device integration.
The transport competence area covers applied research with industry actors targeting IT applications that support transportation practices. Utilizing mobile devices embedded in vehicles or held by users, open accessible sensor-data, and stationary systems placed in the infrastructure, three examples of IT application areas in the road transport industry are transport management, intelligent transports for people and goods, and remote vehicle diagnostics.

History
The institute was founded as a spin-out from the University of Gothenburg in 1997 as the result of an initiative taken by industry mainly from West Sweden and some academics from Gothenburg. The institute started in 1997 with seven employees, but expanded to 15 people within the first year of its existence and performed research in the following areas:
The institute was organized as a private joint-stock company, owned by the West Sweden IT Association (a non-profit association for corporate members), the University of Gothenburg , and Chalmers University of Technology . Research financing was received through annual membership fees of corporate members, mission oriented research, and public research funds.
The original location was a building shared with the university's department of Informatics at Viktoriagatan 13 in the center of Gothenburg. The street address also gave name to the institute. After the establishment of Lindholmen Science Park in Gothenburg, the institute moved there, sharing its new premises with the IT University .
As part of the re-organization of IT research in Sweden in 2003, the institute was incorporated into the SICS group, a national holding organization for IT research, comprising the Swedish Institute of Computer Science , the Interactive Institute, the Santa Anna Research Institute and the Viktoria Institute. Since 2005, the SICS group been part of a new organization, Swedish ICT Research AB (SICT). SICT is a private joint-stock company, owned by the Swedish state through its holding company IRECO (60%) and two industry associations (20% each).

External links
WebPage index: 00116
PC World
PC World , stylized PCWorld , is a global computer magazine published monthly by IDG . [2] Since 2013 it has been an online-only publication. It offers advice on various aspects of PCs and related items, the Internet , and other personal-technology products and services. In each publication, PC World reviews and tests hardware and software products from a variety of manufacturers, as well as other technology related devices such as still and video cameras , audio devices and televisions.
The current editor of PC World is Jon Phillips, formerly of Wired . In August 2012, he replaced Steve Fox, who had been editorial director since the December 2008 issue of the magazine. Fox replaced the magazine's veteran editor Harry McCracken , who resigned that spring, [3] after some rocky times, including quitting and being rehired over editorial control issues in 2007. [4]
PC World is published under other names such as PC Advisor and PC Welt in some countries. PC World ' s company name is IDG Consumer & SMB, and it is headquartered in San Francisco . [5]
Some of the non-English PC World websites now redirect to other IDG sites; for example, PCWorld.dk (Denmark) is now Computerworld .dk.

History
The publication was announced at the COMDEX trade show in November 1982, and first appeared on newsstands in March 1983; Felix Dennis set up Personal Computer World which he later sold to VNU, and established MacUser which he sold to Ziff Davis Publishing in the mid-eighties. PC Magazine was also acquired by Ziff Davis . [6]
The magazine was founded by David Bunnell and Cheryl Woodard, and its first editor was Andrew Fluegelman .
PC World ' s magazine and web site have won a number of awards from Folio, the American Society of Business Publication Editors, MIN, the Western Publications Association, and other organizations; it is also one of the few technology magazines to have been a finalist for a National Magazine Award .
Many well-known technology writers have contributed to PC World , including Steve Bass, Daniel Tynan , Christina Wood, John C. Dvorak , Stephen Manes , Lincoln Spector, Stewart Alsop , David Coursey, James A. Martin, and others. Editorial leadership has included Harry Miller, Richard Landry, Eric Knorr, Phil Lemmons, Cathryn Baskin, Kevin McKean, and Harry McCracken.
In 2005 the show Digital Duo was slightly rebranded and relaunched as PC World's Digital Duo and ran for an additional 26 episodes.
As of 2006, PC World ' s audited rate base of 750,000 made it the largest-circulation computing magazine in the world. [7]
On July 10, 2013 owner IDG announced [8] that the magazine would cease its 30-year print run. The August 2013 issue was the last printed of PC World magazine, future issues would be digital-only. [9]

Countries
Based in San Francisco, PC World ' s original edition is published in the United States however it is also available in other countries (51 in total), sometimes under a different name:

Controversy
In May 2007, McCracken resigned abruptly under controversial circumstances. According to sources quoted in Wired , McCracken quit abruptly because the new CEO of PC World , Colin Crawford, tried to kill an unfavorable story about Apple and Steve Jobs . [10] Crawford responded, calling media reports of McCracken's resignation "inaccurate." [11] CNET later reported that McCracken had told colleagues that IDG "was pressuring him to avoid stories that were critical of major advertisers." [12] [13] On May 9, Crawford was transferred to another department and McCracken returned to PC World until his departure in 2008. [14]

See also
WebPage index: 00117
Berkman Klein Center for Internet & Society
The Berkman Klein Center for Internet & Society is a research center at Harvard University that focuses on the study of cyberspace . Founded at Harvard Law School , the center traditionally focused on internet-related legal issues. On May 15, 2008, the Center was elevated to an interfaculty initiative of Harvard University as a whole. [2] It is named after the Berkman family, who owned the communications company The Associated Group (later sold to Liberty Media ). [3] On July 5, 2016, the Center added "Klein" to its name following a gift of $15 million from Michael R. Klein, chairman of the Sunlight Foundation. [4]
Sister centers started or inspired by Berkman Klein founders include the Stanford Center for Internet and Society , Oxford Internet Institute , and Bilgi University Institute of Information and Technology Law . Partner institutions, such as the NEXA Center for Internet and Society at the Polytechnic University of Turin in Italy, Zhejiang University of Media and Communications (ZUMC) Center for Internet and Society, and the Bangalore Centre for Internet & Society , have also been founded since the launch of the Berkman Klein Center.

History and mission
The Berkman Klein Center was founded in 1998 by professors Jonathan Zittrain and Charlie Nesson , along with recent Harvard Law School graduates David Marglin and Tom Smuts. [5] [6] [1] Since then, it has grown from a small project within Harvard Law School to a major interdisciplinary center at Harvard University. [7] The Berkman Klein Center seeks to understand how the development of Internet-related technologies is inspired by the social context in which they are embedded and how the use of those technologies affects society in turn. It seeks to use the lessons drawn from this research to inform the design of Internet-related law and pioneer the development of the Internet itself. [8] The Berkman Klein Center sponsors Internet-related events and conferences, and hosts numerous visiting lecturers and research fellows. [9]
Members of the center teach, write books, scientific articles, weblogs with RSS 2.0 feeds (for which the Center holds the specification [10] ), and podcasts (of which the first series took place at the Berkman Klein Center). Its newsletter, The Buzz , is on the Web and available by e-mail, and it hosts a blog community of Harvard faculty, students, and Berkman Klein Center affiliates. [11]
The Berkman Klein Center faculty and staff have also conducted major public policy reviews of pressing issues. In 2008, John Palfrey led a review of child safety online called the Internet Safety Technical Task Force. [12] In 2009, Yochai Benkler led a review of United States broadband policy. [13] In 2010, Urs Gasser, along with Palfrey and others, led a review of Internet governance body ICANN , focusing on transparency, accountability, and public participation. [14]

Projects and initiatives
The Berkman Klein Center's main research topics are Teens and Media, Monitoring , Privacy , Digital art , Internet Governance , Cloud Computing and Internet censorship . The Berkman Klein Center supports events, presentations, and conferences about the Internet and invites scientists to share their ideas.

Digital Media Law Project
The Digital Media Law Project (DMLP) was a project hosted by the Berkman Klein Center for Internet & Society at Harvard Law School. It had previously been known as the Citizen Media Law Project. The purposes of the DMLP were:
In 2014, Berkman Klein Center announced that it would "spin off its most effective initiatives and cease operation as a stand-alone project within the Berkman Klein Center." [16]

Internet and Democracy Project
The Berkman Klein Center operated the now-completed Internet and Democracy Project, which describes itself as an:

StopBadware
In 2006, the Center established the non-profit organization StopBadware, which aims to stop viruses, spyware, and other threats to the open Internet. In 2010, StopBadware became an independent entity supported by Google , PayPal , Mozilla , and Nominum . [ citation needed ]

Digital Public Library of America
The Digital Public Library of America is a project aimed at making a large-scale digital public library accessible to all.

Members
Fellows include or have included John Perry Barlow , danah boyd , John Clippinger , Tamar Frankel , Benjamin Mako Hill , Reynol Junco , Rebecca MacKinnon , James F. Moore , Mayo Fuster Morell , Doc Searls , Amber Case , Wendy Seltzer , Peter Suber , Jimmy Wales , David Weinberger , Dave Winer , and Ethan Zuckerman .
Faculty include Yochai Benkler , William "Terry" Fisher , Urs Gasser, Lawrence Lessig , Charles Nesson , John Palfrey , and Jonathan Zittrain .
The center also has active groups of faculty associates, affiliates [20] and alumni [21] who host and participate in their projects each year.

See also
WebPage index: 00118
Brian Bergstein
Brian Bergstein is the National Technology Editor for the American Associated Press news agency, based in Boston, Massachusetts . [1] His work focuses mainly on the economic, legal, and social implications of upcoming technologies. [2]

Personal
He is a graduate of Northwestern University and lives in Brookline, Massachusetts . Bergstein was raised in Los Angeles, California . He, his wife, and two children currently live in Brookline, Massachusetts . [2]

Awards and honors
From 2004 to 2005, he held one of the Knight Science Journalism Fellowships . [3]

Career
Bergstein has been a technology journalist for 13 years. He has worked on the Web, computing, telecom, and in the business of technology from Silicon Valley, New York , and Boston . [4] Previously a technology writer for the Associated Press's New York bureau, [1] Bergstein was promoted to Technology Editor in mid-2008 when the AP reorganised to cover stories by topic rather than geographical areas. [5] He has worked as a journalism instructor at Boston University . He is also deputy editor of MIT Technology Review. [4]
WebPage index: 00119
Softpedia
Softpedia is a website from Romania that indexes information and provides primarily software information and downloads. Its main sections are Windows, Mac, Linux, Games, Drivers, Mobile, Webscripts, and News. It also covers technology and science topics from both external and in-house sources, and it provides software and game reviews.
Wherever possible, it includes one or more screenshots of each application, often showing the application's menus to help illustrate its features.
Softpedia does not repack software for distribution. It provides direct downloads of software in its original distribution form, links to developers' downloads, or both.
Softpedia is a "popular destination for software downloads". [2] It is owned by SoftNews NET SRL., a Romanian company.
WebPage index: 00120
Frankfurter Allgemeine Zeitung
The Frankfurter Allgemeine Zeitung ( German: [ˈfʁaŋkfʊʁtɐ alɡəˈmaɪnə ˈtsaɪtʊŋ] , Frankfurt General Newspaper ), abbreviated FAZ , is a centre-right , [1] liberal - conservative [2] German newspaper , founded in 1949. It is published daily in Frankfurt am Main . [3] Its Sunday edition is the Frankfurter Allgemeine Sonntagszeitung ( FAS ).
The F.A.Z. runs its own correspondent network. Its editorial policy is not determined by a single editor, but cooperatively by five editors. It is the German newspaper with the widest circulation abroad, with its editors claiming to deliver the newspaper to 148 countries every day.

History
The first edition of the F.A.Z. appeared on 1 November 1949; [4] [5] its founding editor was Erich Welter ( de ) . Some editors had worked for the moderate Frankfurter Zeitung , which had been banned in 1943. However, in their first issue, the F.A.Z. editorial expressly refuted the notion of being the earlier paper's successor or of continuing its legacy:
Until 30 September 1950 the F.A.Z. was printed in Mainz .
Traditionally, many of the headlines in the F.A.Z. were styled in orthodox blackletter format and no photographs appeared on the title page. Some of the rare exceptions were a picture of the celebrating people in front of the Reichstag in Berlin on the German Unity Day on 4 October 1990, and the two pictures in the edition of 12 September 2001 showing the collapsing World Trade Center and the American president George W. Bush .
In the early 2000s, F.A.Z. expanded aggressively, with customized sections for Berlin and Munich. [7] An eight-page six-day-a-week English-language edition was distributed as an insert in The International Herald Tribune (which is owned by The New York Times Company ); the articles were selected and translated from the same day's edition of the parent newspaper by the F.A.Z. staff in Frankfurt. [8] However, F.A.Z. group suffered a loss of 60.6 million euros in 2002. By 2004 the customized sections were scrapped. The English edition shrank to a tabloid published once a week. [7]
On 5 October 2007, the F.A.Z. altered their traditional layout to include color photographs on the front page and exclude blackletter typeface outside the nameplate . Due to its traditionally sober layout, the introduction of colour photographs in the F.A.Z. was controversially discussed by the readers, became the subject of a 2009 comedy film, and was still current three years later. [9]
Currently, the F.A.Z. is produced electronically using the Networked Interactive Content Access (NICA) and Hermes. For its characteristic comment headings, a digital Fraktur font was ordered. The Fraktur has since been abandoned, however, with the above-mentioned change of layout.
After having introduced on 1 August 1999 the new spelling prescribed by the German spelling reform , the F.A.Z. returned exactly one year later to the old spelling, declaring that the reform had failed to achieve the primary goals of improving language mastery and strengthening the unity of the language. [10] After several changes had been made to the new spelling, F.A.Z. accepted it and started using it (in a custom version) on 1 January 2007. [11]

Orientation
The F.A.Z. promotes an image of making its readers think. The truth is stated to be sacred to the F.A.Z. , so care is taken to clearly label news reports and comments as such. Its political orientation is centre right [1] and liberal - conservative , [2] occasionally providing a forum to commentators with different opinions. In particular, the Feuilleton and some sections of the Sunday edition cannot be said to be specifically conservative or liberal at all. [ citation needed ]
In the 2013 elections the paper was among the supporters of the Christian Democrats . [12]

Circulation
The F.A.Z. is one of several high-profile national newspapers in Germany (along with Süddeutsche Zeitung , Die Welt , Die Zeit , Frankfurter Rundschau and Die Tageszeitung ) and among them has the second largest circulation nationwide. It maintains the largest number of foreign correspondents of any European newspaper (53 as of 2002). [13]
The paper is published in Nordisch format. [14]

Ownership
It has the legal form of a GmbH ; the independent FAZIT-Stiftung (FAZIT Foundation) is its majority shareholder (93.7%). [15] The FAZIT-Stiftung was born in 1959 by the transformation of the then FAZ owner "Allgemeine Verlagsgesellschaft mbH" into a private foundation . The FAZIT-Stiftung is 'owned' by up to nine persons who can't sell or buy their share but have to transmit it free of charge to a successor which is co-opted by the remaining shareholders. The foundations statute prescribe that only such persons shall be co-opted as new member, who "by their standing and personality" can guarantee the "independence" of the FAZ. The current group of seven is composed of active or former CEOs , company owners, board members, and corporate lawyers. The FAZIT foundation also owns more than 90% of the shares of the company 'Frankfurter Societät' which in turn is owner of the printing enterprise 'Frankfurter Societätsdruckerei' and the regional paper Frankfurer Neue Presse .

Circulation
The F.A.Z. had a circulation of 382,000 copies during the third quarter of 1992. [3] The 1993 circulation of the paper was 391,013 copies. [16] In 2001 it had a circulation of 409,000 copies. [14] The F.A.Z. had a circulation of 382,000 copies in 2003. [17] The 2007 circulation of the daily was 382,499 copies. [18] The 2016 (IVW II/2016) circulation of the daily was 256,188 copies. [19]

Controversies and bans
In December 1999, future German Chancellor Angela Merkel published a sensational article in the Frankfurter Allgemeine Zeitung , lamenting the ‘‘tragedy’’ that had befallen the party, blaming incumbent Chancellor Helmut Kohl and urging a new course. [20]
In 2006, the F.A.Z. was banned in Egypt for publishing articles which were deemed as "insulting Islam". [21] The paper was again banned in Egypt in February 2008 due to the publication of Prophet Mohammad's cartoons. [22] In November 2012, the paper provoked strong criticism in Spain because of its stance against Spanish immigration to Germany during the economic crisis. [23]

Famous contributors
WebPage index: 00121
Emily Flake
Emily S. Flake (born June 16, 1977) [1] is an American cartoonist and illustrator. Her work has appeared in The New Yorker , The New York Times , Time and many other publications. Her weekly comic strip Lulu Eightball has appeared in numerous alternative newsweeklies since 2002. [2]

Personal life
Flake was born in Manchester , Connecticut . She now lives in Brooklyn, New York.

Awards
In 2007, Flake won a Prism Award for her book These Things Ain't Gonna Smoke Themselves.

Books
WebPage index: 00122
WebCite
WebCite is an on-demand archiving service, designed to digitally preserve scientific and educationally important material on the web by making snapshots of Internet contents as they existed at the time when a blogger, or a scholar or a Wikipedia editor cited or quoted from it. The preservation service enables verifiability of claims supported by the cited sources even when the original web pages are being revised, removed, or disappear for other reasons, an effect known as link rot . [3]

Comparison to other services
The service differs from the short time Google Cache copies by having indefinite archiving and by offering on-the-fly archiving. The Internet Archive , since 2013, [4] also offers immediate archiving, however WebCite has some advantages:
WebCite is a non-profit consortium supported by publishers and editors, and it can be used by individuals without charge. Rather than relying on a web crawler which archives pages in a " random " fashion, authors who want to cite web pages in a scholarly article can initiate the archiving process. They then cite – instead of or in addition to the original URL – the snapshot address archived by WebCite, with an identifier that specifies the cited source. (However, note that the Internet Archive does both a crawler-based archiving and on-demand archiving.)
WebCite can be used to preserve cited Internet content, such as the archived web pages , in addition to citing the original URL of the Internet content. All types of web content, including HTML web pages, PDF files, style sheets , JavaScript and digital images can be preserved. It also archives metadata about the collected resources such as access time, MIME type , and content length.

History
Conceived in 1997 by Gunther Eysenbach , WebCite was publicly described the following year when an article on Internet quality control declared that such a service could also measure the citation impact of web pages. [5] In the next year, a pilot service was set up at the address webcite.net. Although it seemed that the need for WebCite decreased when Google 's short term copies of web pages begun to be offered by Google Cache and the Internet Archive expanded their crawling (which started in 1996 [4] ), WebCite was the only one allowing "on-demand" archiving by users. WebCite also offers interfaces to scholarly journals and publishers to automate the archiving of cited links. By 2008, over 200 journals had begun routinely using WebCite. [6]
WebCite used to be, but is no longer, a member of the International Internet Preservation Consortium . [1] In a 2012 message on Twitter, Eysenbach commented that "WebCite has no funding, and IIPC charges €4000 per year in annual membership fees." [7]
WebCite "feeds its content" to other digital preservation projects, including the Internet Archive . [1] Lawrence Lessig , an American academic who writes extensively on copyright and technology, used WebCite in his amicus brief in the Supreme Court of the United States case of MGM Studios, Inc. v. Grokster, Ltd. [8]

Fundraising
WebCite ran a fund-raising campaign using FundRazr from January 2013 with a target of $22,500, a sum which its operators stated was needed to maintain and modernize the service beyond the end of 2013. [9] This includes relocating the service to Amazon EC2 cloud hosting and legal support. As of 2013 [update] it remained undecided whether WebCite would continue as a non-profit or as a for-profit entity. [10] [11]

Process
WebCite allows on-demand prospective archiving. It is not crawler-based; pages are only archived if the citing author or publisher requests it. No cached copy will appear in a WebCite search unless the author or another person has specifically cached it beforehand.
To initiate the caching and archiving of a page, an author may use WebCite's "archive" menu option or create a WebCite bookmarklet that will allow web surfers to cache pages just by clicking a button in their bookmarks folder.
One can retrieve or cite archived pages through a transparent format such as
where URL is the URL that was archived, and DATE indicates the caching date. For example,
or the alternate short form http://webcitation.org/5W56XTY5h retrieves an archived copy of the URL http://en.wikipedia.org/wiki/Main_Page that is closest to the date of March 4, 2008. The ID (5W56XTY5h) is the UNIX time in base 62 .
It is important to note that WebCite does not work for pages which contain a no-cache tag . WebCite respects the author's request to not have their web page cached.
One can archive a page by simply navigating in their browser to a link formatted like this:
replacing urltoarchive with the full URL of the page to be archived, and youremail with their e-mail address. This is how the WebCite bookmarklet works. [12]

Business model
The term "WebCite" is a registered trademark. [13] WebCite does not charge individual users, journal editors and publishers [14] any fee to use their service. WebCite earns revenue from publishers who want to "have their publications analyzed and cited webreferences archived", [1] and accepts donations. Early support was from the University of Toronto . [1]
According to their policy, WebCite removes the saved pages after submitting DMCA requests from the copyright holders. The removed pages go to the "dark archive" with pay-per-view access ("$200 (up to 5 snapshots) plus $100 for each further 10 snapshots" [15] ) to the copyrighted content.

Copyright issues
WebCite maintains the legal position that its archiving activities [6] are allowed by the copyright doctrines of fair use and implied license . [1] To support the fair use argument, WebCite notes that its archived copies are transformative , socially valuable for academic research, and not harmful to the market value of any copyrighted work. [1] WebCite argues that caching and archiving web pages is not considered a copyright infringement when the archiver offers the copyright owner an opportunity to "opt-out" of the archive system, thus creating an implied license. [1] To that end, WebCite will not archive in violation of Web site "do-not-cache" and "no-archive" metadata , as well as robot exclusion standards , the absence of which creates an "implied license" for web archive services to preserve the content. [1]
In a similar case involving Google 's web caching activities, on January 19, 2006, the United States District Court for the District of Nevada agreed with that argument in the case of Field v. Google (CV-S-04-0413-RCJ-LRL), holding that fair use and an "implied license" meant that Google's caching of Web pages did not constitute copyright violation. [1] The "implied license" referred to general Internet standards. [1]

See also
WebPage index: 00123
Hawaiian language
The Hawaiian language (Hawaiian: ʻ Ōlelo Hawai ʻ i , pronounced [ʔoːˈlɛlo həˈvɐjʔi] ) [5] is a Polynesian language that takes its name from Hawai ʻ i , the largest island in the tropical North Pacific archipelago where it developed. Hawaiian, along with English , is an official language of the State of Hawaii . King Kamehameha III established the first Hawaiian-language constitution in 1839 and 1840.
For various reasons, including territorial legislation establishing English as the official language in schools, the number of native speakers of Hawaiian gradually decreased during the period from the 1830s to the 1950s. Hawaiian was essentially displaced by English on six of seven inhabited islands. In 2001, native speakers of Hawaiian amounted to under 0.1% of the statewide population. Linguists were unsure that Hawaiian and other endangered languages would survive. [6] [7]
Nevertheless, from around 1949 to the present day, there has been a gradual increase in attention to and promotion of the language. Public Hawaiian-language immersion preschools called Pūnana Leo were started in 1984; other immersion schools followed soon after that. The first students to start in immersion preschool have now graduated from college and many are fluent Hawaiian speakers. The federal government has acknowledged this development. For example, the Hawaiian National Park Language Correction Act of 2000 changed the names of several national parks in Hawai ʻ i, observing the Hawaiian spelling. [8]
A pidgin or creole language spoken in Hawai ʻ i is Hawaiian Pidgin (or Hawaii Creole English, HCE). It should not be mistaken for the Hawaiian language nor for a dialect of English.
The Hawaiian alphabet has 12 letters: five vowels (long and short) and seven consonants. It contains an additional consonantal sound ʻ okina , which is a glottal stop .

Name
The Hawaiian language takes its name from the largest island, Hawaii ( Hawai ʻ i in the Hawaiian language). The island name was first written in English in 1778 by British explorer James Cook and his crew members. They wrote it as "Owhyhee" or "Owhyee". Explorers Mortimer (1791) and Otto von Kotzebue (1821) used that spelling. [9]
The initial "O" in the name is a reflection of the fact that unique identity is predicated in Hawaiian by using a copula form, o , immediately before a proper noun. [10] Thus, in Hawaiian, the name of the island is expressed by saying O Hawai ʻ i , which means "[This] is Hawai ʻ i." [11] The Cook expedition also wrote "Otaheite" rather than "Tahiti." [12]
The spelling "why" in the name reflects the [hw] pronunciation of wh in 18th-century English (still in active use in parts of the English-speaking world ). Why was pronounced [hwai] . The spelling "hee" or "ee" in the name represents the sounds [hi] , or [i] . [13]
Putting the parts together, O-why-(h)ee reflects [o-hwai-i] , a reasonable approximation of the native pronunciation, [o hɐwɐiʔi] .
American missionaries bound for Hawai ʻ i used the phrases "Owhihe Language" and "Owhyhee language" in Boston prior to their departure in October 1819 and during their five-month voyage to Hawai ʻ i. [14] They still used such phrases as late as March 1822. [15] However, by July 1823, they had begun using the phrase "Hawaiian Language." [16]
In Hawaiian, ʻ Ōlelo Hawai ʻ i means "Hawaiian language", as adjectives follow nouns. [17]

Family and origin
Hawaiian is a Polynesian member of the Austronesian language family . [18] It is closely related to other Polynesian languages , such as Marquesan , Tahitian , Māori , Rapa Nui (the language of Easter Island ), and less closely to Samoan and Tongan . [ citation needed ]
According to Schütz (1994), the Marquesans colonized the archipelago in roughly 300 CE [19] followed by later waves of immigration from the Society Islands and Samoa - Tonga . Their languages, over time, became the Hawaiian language within the Hawaiian Islands. [20] Kimura and Wilson (1983) also state:

Methods of proving Hawaiian's family relationships
The genetic history of the Hawaiian language is demonstrated primarily through the application of lexicostatistics , which involves quantitative comparison of lexical cognates, and the comparative method. [22] [23] Both the number of cognates and the phonological similarity of cognates are measures of language relationship.
The following table provides a limited lexicostatistical data set for ten numbers. [24] The asterisk (*) is used to show that these are hypothetical, reconstructed forms. In the table, the year date of the modern forms is rounded off to 2000 CE to emphasize the 6000-year time lapse since the PAN era. [ citation needed ]
Note: For the number "10", the Tongan form in the table is part of the word /hoŋo-fulu/ ('ten'). The Hawaiian cognate is part of the word /ana-hulu/ ('ten days'); however, the more common word for "10" used in counting and quantifying is /ʔumi/ , a different root. [ citation needed ]
Application of the lexicostatistical method to the data in the table will show the four languages to be related to one another, with Tagalog having 100% cognacy with PAN, while Hawaiian and Tongan have 100% cognacy with each other, but 90% with Tagalog and PAN. This is because the forms for each number are cognates, except the Hawaiian and Tongan words for the number "1", which are cognate with each other, but not with Tagalog and PAN. When the full set of 200 meanings is used, the percentages will be much lower. For example, Elbert found Hawaiian and Tongan to have 49% (98 ÷ 200) shared cognacy. [25] This points out the importance of data-set size for this method, where less data leads to cruder results, while more data leads to better results. [ citation needed ]
Application of the comparative method will show partly different genetic relationships. It will point out sound changes , [26] such as:
This method will recognize sound change #1 as a shared innovation of Hawaiian and Tongan. It will also take the Hawaiian and Tongan cognates for "1" as another shared innovation. Due to these exclusively shared features, Hawaiian and Tongan are found to be more closely related to one another than either is to Tagalog or PAN. [ citation needed ]
The forms in the table show that the Austronesian vowels tend to be relatively stable, while the consonants are relatively volatile. It is also apparent that the Hawaiian words for "3", "5", and "8" have remained essentially unchanged for 6000 years. [ citation needed ]

History

First European contact
In 1778, British explorer James Cook made the first reported European contact with Hawai ʻ i, beginning a new phase in the development of Hawaiian. During the next forty years, the sounds of Spanish (1789), Russian (1804), French (1816), and German (1816) arrived in Hawai ʻ i via other explorers and businessmen. Hawaiian began to be written for the first time, largely restricted to isolated names and words, and word lists collected by explorers and travelers. [27]
The early explorers and merchants who first brought European languages to the Hawaiian islands also took on a few native crew members who brought the Hawaiian language into new territory. [28] Hawaiians took these nautical jobs because their traditional way of life changed due to plantations, and although there were not enough of these Hawaiian-speaking explorers to establish any viable speech communities abroad, they still had a noticeable presence. [29] One of them, a boy in his teens known as Obookiah ( ʻ Ōpūkaha ʻ ia ), had a major impact on the future of the language. He sailed to New England , where he eventually became a student at the Foreign Mission School in Cornwall, Connecticut . He inspired New Englanders to support a Christian mission to Hawai ʻ i, and provided information on the Hawaiian language to the American missionaries there prior to their departure for Hawai ʻ i in 1819. [30]
Folk Tales
Like all natural spoken languages, the Hawaiian language was originally just an oral language. The native people of the Hawaiian language relayed religion, traditions, history, and views of their world through stories that were handed down from generation to generation. One form of storytelling most commonly associated with the Hawaiian islands is hula . Nathaniel B. Emerson notes that "It kept the communal imagination in living touch with the nation's legendary past". [31]
The islanders' connection with their stories is argued to be one reason why Captain James Cook received a pleasant welcome. Marshall Sahlins has observed that Hawaiian folktales began bearing similar content to those of the Western world in the eighteenth century. [32] He argues this was caused by the timing of Captain Cook's arrival, which was coincidentally when the indigenous Hawaiians were celebrating the Makahiki festival. The islanders' story foretold of the god Lono's return at the time of the Makahiki festival. [33]

Written Hawaiian
In 1820, Protestant missionaries from New England arrived in Hawai ʻ i, inspired by the presence of several young Hawaiian men, especially Obookiah ( ʻ Ōpūkaha ʻ ia ) , at the Foreign Mission School in Cornwall, Connecticut . The missionaries began to learn the Hawaiian language so that they could form relationships with the locals and publish a Hawaiian Bible. To that end, they developed a successful alphabet for Hawaiian by 1826, taught Hawaiians to read and write the language, published various educational materials in Hawaiian, and eventually finished translating the Bible. Missionaries also influenced King Kamehameha III to establish the first Hawaiian-language constitutions in 1839 and 1840. [ citation needed ]
Adelbert von Chamisso might have consulted with a native speaker of Hawaiian in Berlin, Germany , before publishing his grammar of Hawaiian ( Über die Hawaiische Sprache ) in 1837. [34] When Hawaiian King David Kalākaua took a trip around the world, he brought his native language with him. When his wife, Queen Kapi ʻ olani , and his sister, Princess (later Queen) Lili ʻ uokalani , took a trip across North America and on to the British Islands, in 1887, Lili ʻ uokalani's composition Aloha ʻ Oe was already a famous song in the U.S. [35]
In 1834, the first Hawaiian-language newspapers were published by missionaries working with locals. The missionaries also played a significant role in publishing a vocabulary (1836) [36] grammar (1854) [37] and dictionary (1865) [38] of Hawaiian. Literacy in Hawaiian was widespread among the local population, especially ethnic Hawaiians. Use of the language among the general population might have peaked around 1881. Even so, some people worried, as early as 1854, that the language was "soon destined to extinction." [39]
The increase in travel to and from Hawai ʻ i during the 19th century introduced a number of fatal illnesses such as smallpox , influenza , and leprosy , which killed large numbers of native speakers of Hawaiian. Meanwhile, native speakers of other languages, especially English , Chinese , Japanese , Portuguese , and Ilokano , continued to immigrate to Hawai ʻ i. As a result, the actual number, as well as the percentage, of native speakers of Hawaiian in the local population decreased sharply, and continued to fall throughout the nineteenth century. [ citation needed ]
As the status of Hawaiian fell, the status of English in Hawai ʻ i rose. In 1885, the Prospectus of the Kamehameha Schools announced that "instruction will be given only in English language" (see published opinion of the United States Court of Appeals for the Ninth Circuit, Doe v. Kamehameha Schools, case no. 04-15044, page 8928, filed August 2, 2005). Around 1900, students began to be punished for speaking Hawaiian in schools, [40] and the number of native speakers of Hawaiian diminished from 37,000 at the turn of the twentieth century to 1,000 in 1997; half of these remaining are now in their seventies or eighties (see Ethnologue report below for citations). Due to immersion programs the number of speakers has risen to 24,000 according to the 2011 US census.
There has been some controversy over the reasons for this decline. One school of thought claims that the most important cause for the decline of the Hawaiian language was its voluntary abandonment by the majority of its native speakers. According to Mary Kawena Pukui , they wanted their own children to speak English, as a way to promote their success in a rapidly changing modern environment, so they refrained from using Hawaiian with their own children. [41] The Hawaiian language schools disappeared as their enrollments dropped: parents preferred English language schools. Another school of thought emphasizes the importance of other factors that discouraged the use of the language, such as the fact that the English language was made the only medium of instruction in all schools in 1896 and the fact that schools punished the use of Hawaiian (see "Banning" of Hawaiian below.) General prejudice against ethnic Hawaiians ( kanaka ) has also been blamed for the decline of the language.
A new dictionary was published in 1957, a new grammar in 1979, and new second-language textbooks in 1951, 1965, 1977, and 1989. Master's theses and doctoral dissertations on specific facets of Hawaiian appeared in 1951, 1975, 1976, and 1996.

Kaona
According to Mary Kawena Pukui and Samuel Elbert , kaona ( kao-na ) [42] is a "Hidden meaning, as in Hawaiian poetry; concealed reference, as to a person, thing, or place; words with double meanings that might bring good or bad fortune." Pukui lamented, “in spite of years of dedicated work, it is impossible to record any language completely. How true this seems for Hawaiian, with its rich and varied background, its many idioms heretofore undescribed, and its ingenious and sophisticated use of figurative language.” On page xiii of the 1986 dictionary she warned: "Hawaiian has more words with multiple meanings than almost any other language. One wishing to name a child, a house, a T-shirt, or a painting, should be careful that the chosen name does not have a naughty or vulgar meaning. The name of a justly respectable children's school, Hana Hau ʻ oli, means happy activity and suggests a missionary author, but among older Hawaiians it has another, less 'innocent' meaning that should not concern little children. A Honolulu street (and formerly the name of a hotel) is Hale Le ʻ a 'joyous house', but le ʻ a also means orgasm."
Understanding the kaona of the language requires a comprehensive knowledge of Hawaiian legends, history and cosmology.

Banning of Hawaiian
The law cited as banning the Hawaiian language is identified as Act 57, sec. 30 of the 1896 Laws of the Republic of Hawai ʻ i:
This law established English as the medium of instruction for the government-recognized schools both "public and private". While it did not ban or make illegal the Hawaiian language in other contexts, its implementation in the schools had far reaching effects. The banishment was only two years removed from acknowledgement as a legitimate sovereign government. From July 1894 to January 1895, 19 nations, including the United States, recognized Hawai'i as an independent country. [43] Those who had been pushing for English-only schools took this law as licence to extinguish the native language at the early education level. While the law stopped short of making Hawaiian illegal (it was still the dominant language spoken at the time), many children who spoke Hawaiian at school, including on the playground, were disciplined. This included corporal punishment and going to the home of the offending child to strongly advise them to stop speaking it in their home. Moreover, the law specifically provided for teaching languages "in addition to the English language," reducing Hawaiian to the status of a foreign language, subject to approval by the Department. Hawaiian was not taught initially in any school, including the all-Hawaiian Kamehameha Schools . This is largely because when these schools were founded, like Kamehameha Schools founded in 1887 (nine years before this law), Hawaiian was being spoken in the home. Once this law was enacted, individuals at these institutions took it upon themselves to enforce a ban on Hawaiian. Beginning in 1900, Mary Kawena Pukui, who was later the co-author of the Hawaiian–English Dictionary, was punished for speaking Hawaiian by being rapped on the forehead, allowed to eat only bread and water for lunch, and denied home visits on holidays. [44] Winona Beamer was expelled from Kamehameha Schools in 1937 for chanting Hawaiian. [45]
Hawaiian-language newspapers were published for over a hundred years, through the period of the suppression. Very few pro-Hawaiian papers made it through the period of the overthrow of the kingdom and the subsequent Act 57. Most papers that survived that period had a distinctly pro-U.S.Annexation perspective. Pukui & Elbert (1986 :572) list fourteen Hawaiian newspapers. According to them, the newspapers entitled Ka Lama Hawaii and Ke Kumu Hawaii began publishing in 1834, and the one called Ka Hoku o Hawaii ceased publication in 1948. The longest run was that of Ka Nupepa Kuokoa : about 66 years, from 1861 to 1927.

1949 to present
In 1949, the legislature of the Territory of Hawai ʻ i commissioned Mary Pukui and Samuel Elbert to write a new dictionary of Hawaiian, either revising the Andrews-Parker work or starting from scratch. [46] Pukui and Elbert took a middle course, using what they could from the Andrews dictionary, but making certain improvements and additions that were more significant than a minor revision. The dictionary they produced, in 1957, introduced an era of gradual increase in attention to the language and culture.
Efforts to promote the language have increased in recent decades. Hawaiian-language "immersion" schools are now open to children whose families want to reintroduce Hawaiian language for future generations. [47] The ʻ Aha Pūnana Leo ’s Hawaiian language preschools in Hilo, Hawaii , have received international recognition. [48] The local National Public Radio station features a short segment titled "Hawaiian word of the day" and a Hawaiian language news broadcast. Honolulu television station KGMB ran a weekly Hawaiian language program, ʻ Āha ʻ i ʻ Ōlelo Ola , as recently as 2010. [49] Additionally, the Sunday editions of the Honolulu Star-Advertiser , the largest newspaper in Hawaii, feature a brief article called Kauakukalahale written entirely in Hawaiian by teachers, students, and community members.
Today, the number of native speakers of Hawaiian, which was under 0.1% of the statewide population in 1997, has risen to 2,000, out of 24,000 total who are fluent in the language, according to the US 2011 census. On six of the seven permanently inhabited islands, Hawaiian has been largely displaced by English, but on Ni ʻ ihau , native speakers of Hawaiian have remained fairly isolated and have continued to use Hawaiian almost exclusively. [50] [51] [52]

Niʻihau
The isolated island of Niʻihau , located off the southwest coast of Kauai , is the one island where Hawaiian is still spoken as the language of daily life. [50] Children are taught Hawaiian as a first language, and learn English at about age eight. Reasons for the language's predominance on this island include:
Native speakers of Niʻihau Hawaiian have three distinct modes of speaking Hawaiian:
The last mode of speaking may be further restricted to a certain subset of Niʻihauans , and is rarely even overheard by non-Niʻihauans. In addition to being able to speak Hawaiian in several different registers, most Niʻihauans can speak English as well.
Elbert & Pukui (1979 :23) states that "[v]ariations in Hawaiian dialects have not been systematically studied", and that "[t]he dialect of Niʻihau is the most aberrant and the one most in need of study". They recognized that Niʻihauans can speak Hawaiian in substantially different ways. Their statements are based in part on some specific observations made by Newbrand (1951) . (See Hawaiian phonological processes )

Orthography
Hawaiians had no written language prior to western contact, except for petroglyph symbols. The modern Hawaiian alphabet, ka pī ʻ āpā Hawai ʻ i , is based on the Latin script . Hawaiian words end only [54] in vowels, and every consonant must be followed by a vowel. The Hawaiian alphabetical order has all of the vowels before the consonants, [55] as in the following chart.

Origin
This writing system was developed by American Protestant missionaries during 1820–1826. [56] It was the first thing they ever printed in Hawai ʻ i, on January 7, 1822, and it originally included the consonants B, D, R, T, and V, in addition to the current ones ( H, K, L, M, N, P, W ), and it had F, G, S, Y and Z for "spelling foreign words". The initial printing also showed the five vowel letters ( A, E, I, O, U ) and seven of the short diphthongs ( AE, AI, AO, AU, EI, EU, OU ). [57]
In 1826, the developers voted to eliminate some of the letters which represented functionally redundant allophones (called "interchangeable letters"), enabling the Hawaiian alphabet to approach the ideal state of one-symbol-one- phoneme , and thereby optimizing the ease with which people could teach and learn the reading and writing of Hawaiian. [58] For example, instead of spelling one and the same word as pule, bule, pure, and bure (because of interchangeable p/b and l/r ), the word is spelled only as pule .
However, hundreds of words were very rapidly borrowed into Hawaiian from English, Greek, Hebrew, Latin, and Syriac. [59] [60] [61] Although these loan words were necessarily Hawaiianized , they often retained some of their "non-Hawaiian letters" in their published forms. For example, Brazil fully Hawaiianized is Palakila , but retaining "foreign letters" it is Barazila . [62] Another example is Gibraltar , written as Kipalaleka or Gibaraleta . [63] While [z] and [ɡ] are not regarded as Hawaiian sounds, [b] , [ɹ] , and [t] were represented in the original alphabet, so the letters ( b , r , and t ) for the latter are not truly "non-Hawaiian" or "foreign", even though their post-1826 use in published matter generally marked words of foreign origin.

Glottal stop
ʻOkina ( ʻ oki 'cut' + -na '-ing') is the modern Hawaiian name for the symbol (a letter) that represents the glottal stop . [64] It was formerly known as ʻ u ʻ ina ('snap' [65] [66] ).
For examples of the ʻ okina, consider the Hawaiian words Hawai ʻ i and O ʻ ahu (often simply Hawaii and Oahu in English orthography). In Hawaiian, these words can be pronounced [hʌˈʋʌi.ʔi] and [oˈʔʌ.hu] , and can be written with an ʻ okina where the glottal stop is pronounced. [67] [68]
Elbert & Pukui's Hawaiian Grammar says "The glottal stop, ‘, is made by closing the glottis or space between the vocal cords, the result being something like the hiatus in English oh-oh .". [69]

History
As early as 1823, the missionaries made some limited use of the apostrophe to represent the glottal stop, [70] but they did not make it a letter of the alphabet. In publishing the Hawaiian Bible, they used it to distinguish ko ʻ u ('my') from kou ('your'). [71] In 1864, William DeWitt Alexander published a grammar of Hawaiian in which he made it clear that the glottal stop (calling it "guttural break") is definitely a true consonant of the Hawaiian language. [72] He wrote it using an apostrophe. In 1922, the Andrews-Parker dictionary of Hawaiian made limited use of the opening single quote symbol, called "reversed apostrophe" or "inverse comma", to represent the glottal stop. [73] Subsequent dictionaries have preferred to use that symbol. Today, many native speakers of Hawaiian do not bother, in general, to write any symbol for the glottal stop. Its use is advocated mainly among students and teachers of Hawaiian as a second language, and among linguists. [74]

Electronic encoding
The ʻ okina is written in various ways for electronic uses:
Because many people who want to write the ʻokina are not familiar with these specific characters and/or do not have access to the appropriate fonts and input and display systems, it is sometimes written with more familiar and readily available characters:

Macron
A modern Hawaiian name for the macron symbol is kahakō ( kaha 'mark' + kō 'long'). [78] It was formerly known as mekona (Hawaiianization of macron ). It can be written as a diacritical mark which looks like a hyphen or dash written above a vowel, i.e., ā ē ī ō ū and Ā Ē Ī Ō Ū . It is used to show that the marked vowel is a "double", or "geminate", or "long" vowel, in phonological terms. [79] (See: Vowel length )
As early as 1821, at least one of the missionaries, Hiram Bingham , was using macrons (and breves) in making handwritten transcriptions of Hawaiian vowels. [80] The missionaries specifically requested their sponsor in Boston to send them some type (fonts) with accented vowel characters, including vowels with macrons, but the sponsor made only one response and sent the wrong font size (pica instead of small pica). [73] Thus, they could not print ā, ē, ī, ō, nor ū (at the right size), even though they wanted to.

Pronunciation
Due to extensive allophony , Hawaiian has more than 13 phones . Although vowel length is phonemic, long vowels are not always pronounced as such, [79] even though under the rules for assigning stress in Hawaiian, a long vowel will always receive stress. [81] [82]

Phonology

Consonants
Hawaiian is known for having very few consonant phonemes – eight: /p, k ~ t, ʔ, h, m, n, l, w ~ v/ . It is notable that Hawaiian has allophonic variation of [t] with [k] , [83] [84] [85] [86] [w] with [v] , [87] and (in some dialects) [l] with [n] . [88] The [t] – [k] variation is quite unusual among the world's languages, and is likely a product both of the small number of consonants in Hawaiian, and the recent shift of historical *t to modern [t] – [k] , after historical *k had shifted to [ʔ] . In some dialects, /ʔ/ remains as [k] in some words. These variations are largely free, though there are conditioning factors. /l/ tends to [n] especially in words with both /l/ and /n/ , such as in the island name Lāna ʻ i ( [laːˈnɐʔi] – [naːˈnɐʔi] ), though this is not always the case: ʻ ele ʻ ele or ʻ ene ʻ ene "black". The [k] allophone is almost universal at the beginnings of words, whereas [t] is most common before the vowel /i/ . [v] is also the norm after /i/ and /e/ , whereas [w] is usual after /u/ and /o/ . After /a/ and initially, however, [w] and [v] are in free variation. [89] "A consonant occurs only before a vowel; thus two consonants never occur in succession and a syllable always ends with a vowel". [69]

Vowels
Hawaiian has five short and five long vowels , plus diphthongs .

Monophthongs
Hawaiian has five pure vowels. The short vowels are /u, i, o, e, a/ , and the long vowels, if they are considered separate phonemes rather than simply sequences of like vowels, are /uː, iː, oː, eː, aː/ . When stressed, short /e/ and /a/ tend to become [ɛ] and [ɐ] , while when unstressed they are [e] and [ə] . /e/ also tends to become [ɛ] next to /l/ , /n/ , and another [ɛ] , as in Pele [pɛlɛ] . Some grammatical particles vary between short and long vowels. These include a and o "of", ma "at", na and no "for". Between a back vowel /o/ or /u/ and a following non-back vowel ( /a e i/ ), there is an epenthetic [w] , which is generally not written. Between a front vowel /e/ or /i/ and a following non-front vowel ( /a o u/ ), there is an epenthetic [j] (a y sound), which is never written.

Diphthongs
The short-vowel diphthongs are /iu, ou, oi, eu, ei, au, ai, ao, ae/ . In all except perhaps /iu/ , these are falling diphthongs . However, they are not as tightly bound as the diphthongs of English, and may be considered vowel sequences. (The second vowel in such sequences may receive the stress, but in such cases it is not counted as a diphthong.) In fast speech, /ai/ tends to [ei] and /au/ tends to [ou] , conflating these diphthongs with /ei/ and /ou/ .
There are only a limited number of vowels which may follow long vowels, and some authors treat these sequences as diphthongs as well: /oːu, eːi, aːu, aːi, aːo, aːe/ .

Phonotactics
Hawaiian syllable structure is (C)V. All CV syllables occur except for wū ; [90] wu occurs only in two words borrowed from English. [91] [92] As shown by Schütz, [59] [93] [94] Hawaiian word-stress is predictable in words of one to four syllables, but not in words of five or more syllables. Hawaiian phonological processes include palatalization and deletion of consonants, as well as raising, diphthongization, deletion, and compensatory lengthening of vowels. [84] [95] Phonological reduction (or "decay") of consonant phonemes during the historical development of the language has resulted in the phonemic glottal stop. [96] [97] Ultimate loss ( deletion ) of intervocalic consonant phonemes has resulted in Hawaiian long vowels and diphthongs. [97] [98] [99] [100]

Grammar
Hawaiian is an analytic language with verb–subject–object word order. While there is no use of inflection for verbs, in Hawaiian, like other Austronesian personal pronouns , declension is found in the differentiation between a- and o-class genitive case personal pronouns in order to indicate inalienable possession in a binary possessive class system. Also like many Austronesian languages, Hawaiian pronouns employ separate words for inclusive and exclusive we (clusivity), and distinguish singular , dual , and plural . The grammatical function of verbs is marked by adjacent particles (short words) and by their relative positions, that indicate tense–aspect–mood .
Some examples of verb phrase patterns:
[ [69] ]
Nouns can be marked with articles :
ka and ke are singular definite articles. ke is used before words beginning with a-, e-, o- and k-, and with some words beginning ʻ - and p-. ka is used in all other cases. nā is the plural definite article.
To show part of a group, the word kekahi is used. To show a bigger part, mau is inserted to pluralize the subject.
Examples:

See also

Notes
WebPage index: 00124
John T. Riedl
John Thomas Riedl (January 16, 1962 – July 15, 2013) was an American computer scientist and the McKnight Distinguished Professor at the University of Minnesota . [3] His published works include highly influential research on the social web , recommendation systems , and collaborative systems . [4] [5] [6]

Life and work
John Riedl received his B.S. in Mathematics from the University of Notre Dame in 1983 and his M.S. in Computer Science from Purdue University in 1985. He completed his Ph.D. in Computer Science at Purdue University in 1990. He became an Assistant Professor at the University of Minnesota in 1990 and was promoted to Associate Professor in 1996 and again to Professor in 2003. [2]
At the university, he led the GroupLens Research group. In 2012 he was awarded the McKnight Distinguished Professor position. During his time as a professor he advised 16 Ph.D. students who went on to take faculty positions and work at technology companies like Google , PARC , Intel , eBay and the Wikimedia Foundation . He was also the faculty advisor for a long-running project in which twelve undergraduates each year would hone their entrepreneurial and software-development skills by taking charge of the development and maintenance of Chipmark, an online bookmark-sharing service. [7] [8]
He was a founder of the field of recommender systems , social computing , and interactive intelligent user interface systems. In 1996, he co-founded Net Perceptions to commercialize the recommender systems research, which had "an enormous impact on e-commerce and information portals." [6] At the height of the dot-com bubble , Net Perceptions was valued at $1.5 billion and had over 300 employees, [9] but the company was liquidated in 2004. [10]
Riedl died on July 15, 2013 after a 3-year-long battle with melanoma . [11] [12]

Honors and awards
Riedl was honored with the ACM Software System Award in 2010 for his work on recommender systems. [6] He was named an ACM Fellow in 2009 and was also named an IEEE Fellow in 2012. [5] He received numerous awards for his conference publications including best papers at CSCW , IUI , and WikiSym . [13] [14] [15] He has also received commendations for his teaching, including the Outstanding Teacher Award at the University of Minnesota four times (1990–1993, 2010–2011) and the George Taylor Award for Exceptional Contributions to Teaching (1995–96). [2]

Publications

Highly cited articles
WebPage index: 00125
Andrew Dalby
Andrew Dalby (born 1947 in Liverpool ) is an English linguist , translator and historian who has written articles and several books on a wide range of topics including food history , language , and Classical texts .

Education and early career
Dalby studied Latin , French and Greek at the Bristol Grammar School and University of Cambridge . Here he also studied Romance languages and linguistics , earning a bachelor's degree in 1970.
Dalby worked for fifteen years at Cambridge University Library , eventually specialising in Southern Asia. He gained familiarity with some other languages because of his work there, where he had to work with foreign serials and afterwards with South Asia and Southeast Asian materials. He also wrote articles on multilingual topics linked with the library and its collections.
In 1982 and 1983 he collaborated with Sao Saimong in cataloguing the Scott Collection of manuscripts and documents from Burma (especially the Shan States ) and Indochina . Dalby later published a short biography of the colonial civil servant and explorer J. G. Scott , who formed the collection. [1] To help him with this task, he took classes in Cambridge again in Sanskrit , Hindi and Pali and in London in Burmese and Thai .

Regent's College and food writing
After his time at Cambridge, Dalby worked in London helping to start the library at Regent's College and on renovating another library at London House ( Goodenough College ). He also served as Honorary Librarian of the Institute of Linguists , for whose journal The Linguist he writes a regular column. He later did a part-time PhD at Birkbeck College, London in ancient history (in 1987–93), which improved his Latin and Greek. His Dictionary of Languages was published in 1998. Language in Danger, on the extinction of languages and the threatened monolingual future, followed in 2002.
Meanwhile, he began to work on food history and contributed to Alan Davidson 's journal Petits Propos Culinaires ; He was eventually one of Davidson's informal helpers on the Oxford Companion to Food . Dalby's first food history book, Siren Feasts, appeared in 1995 and won a Runciman Award ; it is also well known in Greece, where it was translated as Seireneia Deipna . At the same time he was working with Sally Grainger on The Classical Cookbook, the first historical cookbook to look beyond Apicius to other ancient Greek and Roman sources in which recipes are found.
Dangerous Tastes , on the history of spices, was the Guild of Food Writers Food Book of the Year for 2001. Work on this also led to Dalby's first article for Gastronomica magazine, in which he traced the disastrous exploration of Gonzalo Pizarro in search of La Canela in eastern Ecuador , showing how the myth of the "Valley of Cinnamon" first arose and identifying the real tree species which was at the root of the legend. [2] Dalby's light-hearted biography of Bacchus includes a retelling, rare in English, of the story of Prosymnus and the price he demanded for guiding Dionysus to Hades . His epilogue to Petronius' Satyrica combines a gastronomic commentary on the " Feast of Trimalchio " with a fictional dénouement inspired by the fate of Petronius himself. [3]

Classics
Dalby's Rediscovering Homer developed out of two academic papers from the 1990s in which he argued that the Iliad and Odyssey must be seen as belonging to the same world as that of the early Greek lyric poets but to a less aristocratic genre. [4] Returning to these themes, he spotlit the unknown poet who, long after the time of the traditional Homer , at last saw the Iliad and Odyssey recorded in writing. As he teasingly suggested, based on what we can judge of this poet's interests and on the circumstances in which oral poetry has been recorded elsewhere, "it is possible, and even probable, that this poet was a woman." [5]

Languages
Dalby's book Language in Danger : The Loss of Linguistic Diversity and the Threat to Our Future , focuses on the decline and extinction of languages from ancient times to the modern era. Dalby attributes the loss to the emergence of large centralised political groupings, the spread of communications technologies, and the hegemony of the English language. [6]
Dalby profiles endangered languages and discusses the significance of their disappearance, which he estimates occurs at a rate of one every two weeks. He states that the world is diminished by each language lost because they encapsulate "local knowledge and ways of looking at the human condition that die with the last speaker." He also discusses the way stronger languages "squeeze out" others, using the rise of Latin and the extinctions that occurred around the Mediterranean in classical times as an example, and notes a similar pattern that Irish , Welsh , and various Native American languages and indigenous Australian languages have faced in the English-speaking world, where they "were banned in school to force minority groups to speak the language of the majority". Dalby writes that preferences have shifted toward encouraging minority languages and that many can be saved. His account was described as engrossing by The Wall Street Journal . [7] The book disputes advocacy of a single common language as a means to a happier, more peaceful, and improved world. [8]

Works

Notes

External links
WebPage index: 00126
The Cult of the Amateur
The Cult of the Amateur: How Today's Internet Is Killing Our Culture ( ISBN 0385520808 ) is a 2007 book written by entrepreneur and Internet critic Andrew Keen . Published by Currency , Keen's first book is a critique of the enthusiasm surrounding user generated content , peer production , and other Web 2.0 -related phenomena. [1]
The book was based in part on a controversial essay Keen wrote for The Weekly Standard , criticizing Web 2.0 for being similar to Marxism , for destroying professionalism and for making it impossible to find high quality material amidst all of the user-generated web content. [1] [2] [3]

Contents
Keen argues against the idea of a read-write culture in media, stating that "most of the content being shared— no matter how many times it has been linked, cross-linked, annotated, and copied— was composed or written by someone from the sweat of their creative brow and the disciplined use of their talent." As such, he contrasts companies such as Time Warner and Disney that "create and produce movies, music, magazines, and television" with companies such as Google . He calls the latter "a parasite" since "it creates no content of its own" and "[i]n terms of value creation, there is nothing there apart from its links." [4]
He elaborates on the point by saying, "Of course, every free listing on Craigslist means one less paid listing in a local newspaper. Every visit to Wikipedia's free information hive means one less customer for a professionally researched and edited encyclopedia such as Britannica ." Thus, he concludes that "what is free is actually costing us a fortune." He also refers to changes such as downsizing of newspaper business and the closing of record labels as forms of economic loss caused by internet-based social changes. [4]
Keen quotes social philosopher Jürgen Habermas about the internet and related technologies: "The price we pay for the growth in egalitarianism offered by the Internet is the decentralized access to unedited stories. In this medium, contributions by intellectuals lose their power to create a focus." Keen states that most of modern social culture has existed with specific gatekeepers analyzing and regulating information as it reaches the masses. He views this expert-based filtering process as beneficial, improving the quality of popular discourse, and argues that it is being circumvented. [4]
He also criticizes the ability of the Internet to promote social harms such as gambling and pornography. [1] He writes, "It’s hardly surprising that the increasingly tasteless nature of such self-advertisements have resulted in social networking sites becoming infested with anonymous sexual predators and pedophiles." He sees "cultural standards and moral values" as "at stake" due to new media innovations. [5]
More broadly, Keen remarks that "history has proven that the crowd is not often very wise" and argues against the notion that mass participation in ideas improve their quality. He highlights that popular opinion has supported "slavery, infanticide, George W. Bush’s war in Iraq, Britney Spears” among other things. He warns against a future of "when ignorance meets egoism meets bad taste meets mob rule." [1]

Reviews and reception
The book received mixed reviews. Some traditional sources gave the book positive or neutral reviews while the book received generally negative reactions from bloggers. [1] [6] The New York Times ran an article by Michiko Kakutani calling the book "a shrewdly argued jeremiad " and also saying that the book "is eloquent on the fallout that free, user-generated materials is having on traditional media." She wrote that the author "wanders off his subject in the later chapters of the book" but broadly "writes with acuity and passion". [1] Daily Mail reviewer A. N. Wilson said that the "book will come as a real shock to many. It certainly did to me. ... I had never realised until reading Keen's book that any amateur can write an entry in Wikipedia. ... Keen leaves me very uneasy indeed." [7]
Lawrence Lessig , who was criticized in both the original essay and in the book, wrote an extremely negative review of the book in which he listed what he stated were a multitude of errors in the book including mischaracterizations of Lessig's views and work. [4] Lessig also set up a wiki where users could collaborate in listing problems with the book. [4] [8]
Larry Sanger , the founder of the expert-centered wiki Citizendium , gave the book a mixed review. Sanger said that "The book is provocative, but its argument is unfortunately weakened by the fact that Keen is so over-the-top and presents more of a caricature of a position than carefully reasoned discourse." He said that it was hypocritical for Keen to express support for Citizendium , for incorporating expert opinion, when the inherent point of the project is to supply free content, which Keen so opposes in principle. Sanger stated that the book "combines several different criticisms of Web 2.0, incoherently, under the rubric of `the cult of the amateur'" but the book "is a much-needed Web 2.0 reality check". [9] [10] Tim O'Reilly commented in response to the book, "I find, Andrew Keen's, his whole pitch, I think he was just pure and simple looking for an angle, to create some controversy and sell a book, I don't think there's any substance whatever to his rants." [11] Furthermore, he has said in response to the book, "I think the Internet is often held to another standard. You don't say, 'Why aren't the newspapers writing about Bismarck, he is more important than Pamela Anderson .' But people will say that about Wikipedia. It's just bias." [11]
Anthony Trewavas , professor at the Institute of Molecular Plant Science at the University of Edinburgh , discussed the book in an article in Trends in Biotechnology . Trewavas wrote that Keen's "concern is the blurring of the distinction between the qualified and informed professional and the uninformed and unqualified amateur", expressing concerns that this social change can hold back agricultural development. Trewavas stated as well, "in agriculture, pesticides, food and farming, expert scientific knowledge and experience is seemingly regarded as having no more weight than that of the opinionated, unqualified (and inexperienced) environmentalist." [12] The book has also been discussed in academic publications negatively, expressing how Keen has considered the worst aspects of a complex social movement while ignoring the demonstrable benefits that have been brought through initiatives such as OpenStreetMap , an expression of Volunteered Geographic Information . [13] [14]

Jarvis-Keen debate
Jeff Jarvis , who had previously called the original essay in The Weekly Standard "snobs.com," was challenged to a debate over Web 2.0 issues. [5] [15] Jarvis held a discussion on his blog about whether he should debate Keen and then decided to accept the offer. [3] [5]

See also
WebPage index: 00127
The New York Review of Books
The New York Review of Books (or NYREV or NYRB ) is a semi-monthly magazine [2] with articles on literature, culture, economics, science and current affairs. Published in New York City, it is inspired by the idea that the discussion of important books is an indispensable literary activity. Esquire called it "the premier literary-intellectual magazine in the English language." [3] In 1970 writer Tom Wolfe described it as "the chief theoretical organ of Radical Chic ". [4]
The Review publishes long-form reviews and essays, often by well-known writers, original poetry, and has letters and personals advertising sections that had attracted critical comment. In 1979 the magazine founded the London Review of Books , which soon became independent. In 1990 it founded an Italian edition, la Rivista dei Libri , published until 2010. Robert B. Silvers and Barbara Epstein edited the paper together from its founding in 1963, until her death in 2006. From then until his death in 2017, Silvers was the sole editor. [5] Ian Buruma became editor in 2017. The Review has a book publishing division, established in 1999, called New York Review Books , which publishes classics, collections and children's books. Since 2010, the journal has hosted an online blog written by its contributors.
The Review celebrated its 50th anniversary in 2013, and a Martin Scorsese film called The 50 Year Argument documents the history and influence of the paper.

History and description

Early years
The New York Review was founded by Robert B. Silvers and Barbara Epstein , together with publisher A. Whitney Ellsworth [6] and writer Elizabeth Hardwick . They were backed and encouraged by Epstein's husband, Jason Epstein , a vice president at Random House and editor of Vintage Books , and Hardwick's husband, poet Robert Lowell . In 1959 Hardwick had published an essay, "The Decline of Book Reviewing", in Harper's , [7] where Silvers was then an editor, in a special issue that he edited called "Writing in America". [8] [9] Her essay was an indictment of American book reviews of the time, "light, little article[s]" that she decried as "lobotomized", passionless praise and denounced as "blandly, respectfully denying whatever vivacious interest there might be in books or in literary matters generally." [10] The group was inspired to found a new magazine to publish thoughtful, probing, lively reviews [11] featuring what Hardwick called "the unusual, the difficult, the lengthy, the intransigent, and above all, the interesting ". [7] [12]
During the New York printers' strike of 1963, when The New York Times and six other newspapers suspended publication, Hardwick, Lowell and the Epsteins seized the chance to establish the sort of vigorous book review that Hardwick had imagined. [13] Jason Epstein knew that book publishers would advertise their books in the new publication, since they had no other outlet for promoting new books. [14] The group turned to the Epsteins' friend Silvers, who had been an editor at The Paris Review and was still at Harper's , [15] to edit the publication, and Silvers asked Barbara Epstein to co-edit with him. [9] [13] She was known as the editor at Doubleday of Anne Frank 's Diary of a Young Girl , among other books, and then worked at Dutton, McGraw-Hill and The Partisan Review . [16] Silvers and Epstein sent books to "the writers we knew and admired most. ... We asked for three thousand words in three weeks in order to show what a book review should be, and practically everyone came through. No one mentioned money." [9] The first issue of the Review was published on February 1, 1963 and sold out its printing of 100,000 copies. [3] It prompted nearly 1,000 letters to the editors asking for the Review to continue. [9] The New Yorker called it "surely the best first issue of any magazine ever." [17] After the success of the first issue, the editors assembled a second issue to demonstrate that "the Review was not a one-shot affair". [9] The founders then collected investments from a circle of friends and acquaintances, and Ellsworth joined as publisher. [9] [18] The Review began regular biweekly publication in November 1963. [19]
Silvers said of the editors' philosophy, that "there was no subject we couldn't deal with. And if there was no book [on a subject], we would deal with it anyway. We tried hard to avoid books that were simply competent rehearsals of familiar subjects, and we hoped to find books that would establish something fresh, something original." [9] In particular, "We felt you had to have a political analysis of the nature of power in America – who had it, who was affected". The editors also shared an "intense admiration for wonderful writers". [21] Well-known writers were willing to contribute articles for the initial issues of the Review without pay because it offered them a chance to write a new kind of book review. As Mark Gevisser explained: "The essays ... made the book review form not just a report on the book and a judgment of the book, but an essay in itself. And that, I think, startled everyone – that a book review could be exciting in that way, could be provocative in that way." [8] Early issues included articles by such writers as Hardwick, Lowell, Jason Epstein, Hannah Arendt , W. H. Auden , Saul Bellow , John Berryman , Truman Capote , Paul Goodman , [22] Lillian Hellman , Irving Howe , Alfred Kazin , Dwight Macdonald , Norman Mailer , Mary McCarthy , Norman Podhoretz , Philip Rahv , Adrienne Rich , Susan Sontag , William Styron , Gore Vidal , Robert Penn Warren and Edmund Wilson . The Review pointedly published interviews with European political dissidents , including Alexander Solzhenitsyn , Andrei Sakharov and Václav Havel . [21] [23] But, Silvers noted, it is a mystery whether "reviews have a calculable political and social impact" or will even gain attention: "You mustn't think too much about influence – if you find something interesting yourself, that should be enough." [9]
Salon later commented that the list of contributors "represented a ' shock and awe ' demonstration of the intellectual firepower available for deployment in mid-century America, and, almost equally impressive, of the art of editorial networking and jawboning. This was the party everyone who was anyone wanted to attend, the Black and White Ball of the critical elite." [24] The Review "announced the arrival of a particular sensibility ... the engaged, literary, post-war progressive intellectual, who was concerned with civil rights and feminism as well as fiction and poetry and theater. [23] The first issue projected "a confidence in the unquestioned rightness of the liberal consensus, in the centrality of literature and its power to convey meaning, in the solubility of our problems through the application of intelligence and good will, and in the coherence and clear hierarchy of the intellectual world". [24]

Since 1979
During the year-long lock-out at The Times in London in 1979, the Review founded a daughter publication, the London Review of Books . For the first six months, this journal appeared as an insert in the New York Review of Books , but it became an independent publication in 1980. [25] [26] In 1990 the Review founded an Italian edition, la Rivista dei Libri. It was published for two decades until May 2010. [27]
For over 40 years, Silvers and Epstein edited the Review together. [3] In 1984, Silvers, Epstein and their partners sold the Review to publisher Rea S. Hederman , [28] who still owns the paper, [29] but the two continued as its editors. [15] In 2006, Epstein died of cancer at the age of 77. [30] In awarding to Epstein and Silvers its 2006 Literarian Award for Outstanding Service to the American Literary Community, the National Book Foundation stated: "With The New York Review of Books , Robert Silvers and Barbara Epstein raised book reviewing to an art and made the discussion of books a lively, provocative and intellectual activity." [31]
After Epstein's death, Silvers was the sole editor until his own death in 2017. [5] Asked about who might succeed him as editor, Silvers told The New York Times , "I can think of several people who would be marvelous editors. Some of them work here, some used to work here, and some are just people we know. I think they would put out a terrific paper, but it would be different." [32] In 2008, the Review celebrated its 45th anniversary with a panel discussion at the New York Public Library , moderated by Silvers, discussing "What Happens Now" in the United States after the 2008 election of Barack Obama as president. Panelists included Review contributors such as Didion, Wills, novelist and literary critic Darryl Pinckney , political commentator Michael Tomasky , and Columbia University professor and contributor Andrew Delbanco . [33] The 45th anniversary edition of the Review (November 20, 2008) began with a posthumous piece by Edmund Wilson , who wrote for the paper's first issue in 1963. [21]
In 2008, the paper moved its headquarters from Midtown Manhattan to 435 Hudson Street, located in the West Village . [34] In 2010, it launched a blog section of its website [35] that The New York Times called "lively and opinionated", [32] and it hosts podcasts. [36] [37] Asked in 2013 how social media might affect the subject matter of the Review , Silvers commented: "I might imagine [a] witty, aphoristic, almost Oscar Wildean [anthology of] remarks, drawn from the millions and millions of tweets. Or from comments that follow on blogs. ... Facebook is a medium in which privacy is, or at least is thought to be, in some way crucial. ... And so there seems a resistance to intrusive criticism. We seem at the edge of a vast, expanding ocean of words ... growing without any critical perspective whatever being brought to bear on it. To me, as an editor, that seems an enormous absence." [38] In 2016, Hederman said that he sees the Review continuing to operate unchanged in five years. [29]
The Review began a year-long celebration of its 50th anniversary with a presentation by Silvers and several contributors at The Town Hall in New York City in February 2013. [39] [40] Other events included a program at the New York Public Library in April, called "Literary Journalism: A Discussion", focusing on the editorial process at the Review [41] [42] and a reception in November at the Frick Collection . [43] [44] During the year, Martin Scorsese filmed a documentary about the history and influence of the Review , and the debates that it has spawned, titled The 50 Year Argument , which premiered in June 2014 at the Sheffield Doc/Fest . [45] [46] It was later seen at various film festivals, on BBC television and on HBO in the US. [9] Asked how he maintained his "level of meticulousness and determination" after 50 years, Silvers said that the Review "was and is a unique opportunity ... to do what one wants on anything in the world. Now, that is given to hardly any editor, anywhere, anytime. There are no strictures, no limits. Nobody saying you can't do something. No subject, no theme, no idea that can’t be addressed in-depth. ... Whatever work is involved is minor compared to the opportunity." [38] A special 50th anniversary issue was dated November 7, 2013. Silvers said:
Author Ian Buruma , who had been a regular contributor to the Review since 1985, was named editor in May 2017. Since 2003, Buruma has been a professor of human rights and journalism at Bard College . [47] [48]

Description
The Review has been described as a "kind of magazine ... in which the most interesting and qualified minds of our time would discuss current books and issues in depth ... a literary and critical journal based on the assumption that the discussion of important books was itself an indispensable literary activity." [49] [50] Each issue includes a broad range of subject matter, including "articles on art, science, politics and literature." [32] Early on, the editors decided that the Review would "be interested in everything ... no subject would be excluded. Someone is writing a piece about Nascar racing for us; another is working on Veronese." [12] The Review has focused, however, on political topics; as Silvers commented in 2004: "The pieces we have published by such writers as Brian Urquhart , Thomas Powers , Mark Danner and Ronald Dworkin have been reactions to a genuine crisis concerning American destructiveness, American relations with its allies, American protections of its traditions of liberties. ... The aura of patriotic defiance cultivated by the [Bush] Administration, in a fearful atmosphere, had the effect of muffling dissent." [51] Silvers told The New York Times : "The great political issues of power and its abuses have always been natural questions for us." [32]
The Nation gave its view of the political focus of the New York Review of Books in 2004:
Over the years, the Review has featured reviews and articles by such international writers and intellectuals, in addition to those already noted, as Timothy Garton Ash , Margaret Atwood , Russell Baker , Saul Bellow , Isaiah Berlin , Harold Bloom , Joseph Brodsky , Ian Buruma, Noam Chomsky , J. M. Coetzee , Frederick Crews , Ronald Dworkin , John Kenneth Galbraith , Masha Gessen , Nadine Gordimer , Stephen Jay Gould , Christopher Hitchens , Tim Judah , Murray Kempton , Paul Krugman , Richard Lewontin , Perry Link , Alison Lurie , Peter Medawar , Daniel Mendelsohn , Bill Moyers , Vladimir Nabokov , Ralph Nader , V. S. Naipaul , Peter G. Peterson , Samantha Power , Nathaniel Rich , Felix Rohatyn , Jean-Paul Sartre , John Searle , Zadie Smith , Timothy Snyder , George Soros , I. F. Stone , Desmond Tutu , John Updike , Derek Walcott , Steven Weinberg , Garry Wills and Tony Judt . According to the National Book Foundation : "From Mary McCarthy and Edmund Wilson to Gore Vidal and Joan Didion , The New York Review of Books has consistently employed the liveliest minds in America to think about, write about, and debate books and the issues they raise." [31]
The Review also devotes space in most issues to poetry, and has featured the work of such poets as Robert Lowell , John Berryman , Ted Hughes , John Ashbery , Richard Wilbur , Seamus Heaney , Octavio Paz , and Czeslaw Milosz . [53] For writers, the "depth [of the articles], and the quality of the people writing for it, has made a Review byline a résumé definer. If one wishes to be thought of as a certain type of writer – of heft, style and a certain gravitas – a Review byline is pretty much the gold standard." [54] In editing a piece, Silvers said that he asked himself "if [the point in any sentence could] be clearer, while also respecting the writer’s voice and tone. You have to listen carefully to the tone of the writer’s prose and try to adapt to it, but only up to a point. [No change was made without the writers' permission.] ... Writers deserve the final word about their prose." [38]
In addition to domestic matters, the Review covers issues of international concern. [55] In the 1980s, a British commentator noted: "In the 1960s [the Review ] opposed American involvement in Vietnam; more recently it has taken a line mildly Keynesian in economics, pro-Israeli but Anti-Zionist, sceptical of Reagan 's Latin-American policy". [56] The British newspaper The Independent has described the Review as "the only mainstream American publication to speak out consistently against the war in Iraq." [57] On Middle East coverage, Silvers said, "any serious criticism of Israeli policy will be seen by some as heresy, a form of betrayal. ... [M]uch of what we've published has come from some of the most respected and brilliant Israeli writers ... Amos Elon , Avishai Margalit , David Grossman , David Shulman , among them. What emerges from them is a sense that occupying land and people year after year can only lead to a sad and bad result." [38]
Caricaturist David Levine illustrated The New York Review of Books from 1963 to 2007, giving the paper a distinctive visual image. [34] Levine died in 2009. [58] John Updike , whom Levine drew many times, wrote: "Besides offering us the delight of recognition, his drawings comfort us, in an exacerbated and potentially desperate age, with the sense of a watching presence, an eye informed by an intelligence that has not panicked, a comic art ready to encapsulate the latest apparitions of publicity as well as those historical devils who haunt our unease." [59] Levine contributed more than 3,800 pen-and-ink caricatures of famous writers, artists and politicians for the publication. [59] [60] Silvers said: "David combined acute political commentary with a certain kind of joke about the person. He was immensely sensitive to the smallest details – people’s shoulders, their feet, their elbows. He was able to find character in these details." [61] The New York Times described Levine's illustrations as "macro-headed, somberly expressive, astringently probing and hardly ever flattering caricatures of intellectuals and athletes, politicians and potentates" that were "replete with exaggeratedly bad haircuts, 5 o'clock shadows, ill-conceived mustaches and other grooming foibles ... to make the famous seem peculiar-looking in order to take them down a peg". [58] In later years, illustrators for the Review included James Ferguson of Financial Times . [62]
The Washington Post described the "lively literary disputes" conducted in the 'letters to the editor' column of the Review as "the closest thing the intellectual world has to bare-knuckle boxing". [3] In addition to reviews, interviews and articles, the paper features extensive advertising from publishers promoting newly published books. It also includes a popular "personals" section that "share[s] a cultivated writing style" with its articles. [36] [63] One lonely heart, author Jane Juska , documented the 63 replies to her personal ad in the Review with a 2003 memoir, A Round-Heeled Woman , that became a West End play . [64] [65] In The Washington Post , Matt Schudel called the personal ads "sometimes laughably highbrow" and recalled that they were "spoofed by Woody Allen in the movie Annie Hall ". [66]
Several of the magazine's editorial assistants have become prominent in journalism, academia and literature, including Jean Strouse , Deborah Eisenberg , Mark Danner and A. O. Scott . [67] Another former intern and a contributor to the Review , author Claire Messud , said: "They’re incredibly generous about taking the time to go through things. So much of [business today] is about people doing things quickly, with haste. One of the first things to go out the window is a type of graciousness. ... There’s a whole sort of rhythm and tone of how they deal with people. I’m sure it was always rare. But it feels incredibly precious now." [54]

Critical reaction
The Washington Post calls the Review "a journal of ideas that has helped define intellectual discourse in the English-speaking world for the past four decades. ... By publishing long, thoughtful articles on politics, books and culture, [the editors] defied trends toward glibness, superficiality and the cult of celebrity". [3] Similarly, the Chicago Tribune praised the paper as "one of the few venues in American life that takes ideas seriously. And it pays readers the ultimate compliment of assuming that we do too." [68] In a 2006 New York magazine feature, James Atlas stated: "It's an eclectic but impressive mix [of articles] that has made The New York Review of Books the premier journal of the American intellectual elite". [69] The Atlantic commented in 2011 that the Review is written with "a freshness of perspective", and "much of it shapes our most sophisticated public discourse." [70] In celebrating the 35th birthday of the Review in 1998, The New York Times commented, "The N.Y.R. gives off rogue intimations of being fun to put out. It hasn't lost its sneaky nip of mischief". [71]
In 2008, Britain's The Guardian deemed the Review "scholarly without being pedantic, scrupulous without being dry". [72] The same newspaper wrote in 2004:
In New York magazine, in February 2011, Oliver Sacks stated that the Review is "one of the great institutions of intellectual life here or anywhere." [73] In 2012, The New York Times described the Review as "elegant, well mannered, immensely learned, a little formal at times, obsessive about clarity and factual correctness and passionately interested in human rights and the way governments violate them." [32]
Known throughout its history as a left-liberal journal, what Tom Wolfe called "the chief theoretical organ of radical chic ", [4] the Review has, perhaps, had its most effective voice in wartime. According to a 2004 feature in The Nation ,
Sometimes accused of insularity, the Review has been called "The New York Review of Each Other's Books". [75] Philip Nobile expressed a mordant criticism along these lines in his book Intellectual Skywriting: Literary Politics and the New York Review of Books . [69] The Guardian characterized such accusations as "sour grapes". [15] In 2008, the San Francisco Chronicle wrote, "the pages of the 45th anniversary issue, in fact, reveal the actuality of [the paper's] willfully panoramic view". [21]
The Washington Post called the 2013 50th Anniversary issue "gaudy with intellectual firepower. Four Nobel Laureates have bylines. U.S. Supreme Court Justice Stephen Breyer muses on reading Proust. There's the transcript of a long-lost lecture by T. S. Eliot ." [54] In 2014, Rachel Cooke wrote in The Observer of a recent issue: "The offer of such an embarrassment of riches is wholly amazing in a world where print journalism increasingly operates in the most threadbare of circumstances". [12] America magazine echoed Zoë Heller 's words about the Review : "I like it because it educates me." [76]

Other publications; archives
The book publishing arm of the Review , established in 1999, is New York Review Books , which has three imprints, "NYRB Classics", "NYRB Collections" and "NYR Children's Collection". The NYRB Classics imprint reissues books that have gone out of print in the United States and translations of classics. It has been called "a marvellous literary imprint ... that has put hundreds of wonderful books back on our shelves." [12] NYRB Collections publishes collections of articles from frequent Review contributors. [77]
The New York Public Library purchased the NYRB archives in 2015. [78]

See also

Notes

External links
WebPage index: 00128
Ideas (radio show)
Ideas is a long-running scholarly radio documentary show on CBC Radio One . Co-created by Phyllis Webb and William A. Young, the show premiered in 1965 under the title The Best Ideas You'll Hear Tonight. [1] It is currently hosted by Paul Kennedy and is broadcast between 9:05 and 10:00 P.M. weekday evenings; one episode each week is repeated on Friday afternoons under the title Ideas in the Afternoon .
The show describes itself as a radio program on contemporary thought. The subject matter of the shows varies, but music, philosophy, science, religion, and especially history are common topics. The show has won many plaudits for its quality and depth.
The series is notable for soliciting programming proposals from people who are not professional broadcasters, and having the successful applicants write and host their own documentaries (aided in production by CBC staff producers). Many Ideas programs are multi-part, with two, three, four, or more fifty-five-minute programs devoted to a single topic. Transcripts and audio recordings of many programs are made available, and sold directly by the CBC.
Notable CBC staff producers who have been associated with the program include Bernie Lucht , Geraldine Sherman , Damiano Pietropaolo, Phyllis Webb , and David Cayley . Individual programs are produced at CBC Radio One facilities across Canada. Documentarian William Whitehead also wrote or cowrote a number of shows for Ideas . [2]
A television version for CBC News Network , Ideas on TV, was short-lived. The book Ideas: Brilliant Thinkers Speak Their Minds , edited by Bernie Lucht, commemorated the series' 40th anniversary.
The show broadcasts Canada's annual Massey Lectures , Lafontaine-Baldwin Lecture , and the Munk Debates . Since 2006, they have included the Henry G. Friesen lectures. [3] Audio downloads of many episodes are available from the CBC website, as well as via the CBC Ideas podcast , which was, by popular demand, one of the first to be included in the network's large podcasting initiative begun in 2005. Many episodes are also available for sale on audio CD.
In January 2014, Ideas broadcast a two-part documentary about Wikipedia entitled "The Great Book of Knowledge", produced and narrated by Philip Coulter . Part 1 of the documentary [4] aired on January 15 and Part 2 on January 22. [5] As of February 2014, both episodes were also available in streaming audio on the Ideas website, and via subscription to the Ideas podcast. [6]

Hosts
WebPage index: 00129
New Scientist
New Scientist is a weekly English-language international science magazine, founded in 1956. Since 1996 it has also run a website.
Sold in retail outlets and on subscription, the magazine covers current developments, news, reviews and commentary on science and technology. It also publishes speculative articles, ranging from the technical to the philosophical. A readers' letters section discusses recent articles, and discussions also take place on the website.
Readers contribute observations on examples of pseudoscience to Feedback, and offer questions and answers on scientific and technical topics to Last Word; extracts from the latter have been compiled into several books.
New Scientist , based in London, publishes editions in the UK, the United States, and Australia.

History
The magazine was founded in 1956 by Tom Margerison , Max Raison and Nicholas Harrison [2] as The New Scientist , with Issue 1 on 22 November, priced one shilling (£0.05 as 20 shillings in the £; £1.13 today). [3]
The British monthly science magazine Science Journal , published 1965–71, was merged with New Scientist to form New Scientist and Science Journal . [4]
Originally, the cover of New Scientist had a list of articles rather than a picture. [5] Pages were numbered sequentially for an entire quarterly volume, as is the norm for academic journals (i.e., so that the first page of a March issue could be 651 instead of 1). Later issues numbered pages separately. Until the 1970s, colour was not used except for on the cover. From the beginning of 1961 " The " was dropped from the title. From 1965, the front cover was illustrated. [6]
Since its first issue, New Scientist has written about the applications of science, through its coverage of technology. For example, the first issue included an article "Where next from Calder Hall?" on the future of nuclear power in the UK, a topic that it has covered throughout its history. In 1964 there was a regular "Science in British Industry" section with several items. [7]
An article in the magazine's 10th anniversary issues provides anecdotes on the founding of the magazine. [2]
In 1970, the Reed Group, which went on to become Reed Elsevier , acquired New Scientist when it merged with IPC Magazines . Reed retained the magazine when it sold most of its consumer titles in a management buyout to what is now IPC Media .
Throughout most of its history, New Scientist has published cartoons as light relief and comment on the news, with contributions from such long-time regular contributors as Mike Peyton and David Austin. The Grimbledon Down comic strip, by the renowned cartoonist Bill Tidy , appeared from 1970 to 1994.
Ariadne , which later moved to Nature , commented weekly on the lighter side of science and technology, with the plausible but impractical humorous inventions of (fictitious) inventor Daedalus , often developed by the (fictitious) DREADCO corporation. [8]
Issues of (The) New Scientist from Issue 1 to the end of 1989 have been made free to read online. [9] Subsequent issues require a subscription. [10]
In the first half of 2013, the international circulation of New Scientist averaged 125,172. While this was a 4.3% reduction on the previous year's figure, it was a much smaller reduction in circulation than many mainstream magazines of similar or greater circulation. [11] For the 2014 UK circulation fell by 3.2% but stronger international sales, increased the circulation to 129,585. [12] See also #Website below.
In April 2017, New Scientist changed hands when RELX Group , formerly known as Reed Elsevier, sold the magazine to Kingston Acquisitions. [13] [14]

Editors of 

Modern format
New Scientist currently contains the following sections: Leader, News (Upfront), Technology, Opinion (interviews, point-of-view articles and letters), Features (including cover article), CultureLab (book and event reviews), Feedback (humour), The Last Word (questions and answers) and Jobs & Careers. A Tom Gauld cartoon appears on the Letters page. [15]
There are 51 issues a year; with a Christmas and New Year double issue. The double issue in 2014 was the 3,000th edition of the magazine.

Staff and contributors
The Editor-in-chief is Sumit Paul-Choudhury, Executive Editor is Graham Lawton, Managing Editor is Rowan Hooper and Editor-at-Large is Jeremy Webb. [16]
Consultants include Fred Pearce (environment) and Marcus Chown (cosmology).
Simon Ings and former editor Alun Anderson are contributors. [ citation needed ] )

Website
The New Scientist website carries blogs, reports and news articles; users with free-of-charge registration have limited access to new content and can receive emailed New Scientist newsletters. Subscribers to the print edition have full access to all articles and the archive of past content that has so far been digitised.
Online readership takes various forms. Overall global views of an online database of over 100,000 articles are 8.0m by 3.6m unique users according to Adobe Reports & Analytics , as of September 2014 [update] . On social media there are 1.47m+ Twitter followers, 2.3m+ Facebook likes and 365,000+ Google+ followers as of January 2015 [update] . [17]

Spin-offs
New Scientist has published books derived from its content, many of which are selected questions and answers from the Last Word section of the magazine and website –
Other books published by New Scientist include –
In 2012 Arc , "a new digital quarterly from the makers of New Scientist , exploring the future through the world of science fiction" and fact was launched. [18] In the same year the magazine launched a dating service, NewScientistConnect, operated by The Dating Lab. [ citation needed ]
A Dutch edition of the New Scientist was launched in June 2015, replacing the former Natuurwetenschap & Techniek (NWT) magazine. The monthly magazine is published by Veen Media and sold in the Netherlands and Belgium. [19] [20]

Criticism

Greg Egan's criticism of the EmDrive article
In September 2006, New Scientist was criticised by science fiction writer Greg Egan , who wrote that "a sensationalist bent and a lack of basic knowledge by its writers" was making the magazine's coverage sufficiently unreliable "to constitute a real threat to the public understanding of science". In particular, Egan found himself "gobsmacked by the level of scientific illiteracy" in the magazine's coverage of Roger Shawyer's " electromagnetic drive ", where New Scientist allowed the publication of "meaningless double-talk" designed to bypass a fatal objection to Shawyer's proposed space drive, namely that it violates the law of conservation of momentum . Egan urged others to write to New Scientist and pressure the magazine to raise its standards, instead of "squandering the opportunity that the magazine's circulation and prestige provides". [21]
The editor of New Scientist , then Jeremy Webb, replied defending the article, saying that it is "an ideas magazine—that means writing about hypotheses as well as theories". [22]

"Darwin was wrong" cover
In January 2009, New Scientist ran a cover with the title " Darwin was wrong". [23] The actual story stated that specific details of Darwin's evolution theory had been shown incorrectly, mainly the shape of phylogenetic trees of interrelated species, which should be represented as a web instead of a tree. Some evolutionary biologists who actively oppose the intelligent design movement thought the cover was both sensationalist and damaging to the scientific community. [23] [24] Jerry Coyne , author of the book Why Evolution Is True , called for a boycott of the magazine, which was supported by evolutionary biologists Richard Dawkins and P.Z. Myers . [23]

See also
WebPage index: 00130
Vice (magazine)
Vice is a print magazine and website focused on arts, culture, and news topics. Founded in 1994 in Montreal , Canada, the magazine later expanded into Vice Media , which consists of divisions including the magazine and website, a film production company, a record label, and a publishing imprint . As of February 2015, the magazine's Chief Creative Officer was Eddy Moretti, Andrew Creighton is President, the editor-in-chief is Ellis Jones [2] and Alex Miller was the global head of content. As of October 2014, there were 29 Vice offices on every continent except Africa and Antarctica . [ citation needed ]
The monthly publication is frequently focused on a single theme.

History
Founded by Suroosh Alvi , Gavin McInnes and Shane Smith , [3] [4] the magazine was launched in 1994 as the Voice of Montreal with government funding, and the intention of the founders was to provide work and a community service. [5] When the editors later sought to dissolve their commitments with the original publisher Alix Laurent, they bought him out and changed the name to Vice in 1996. [6]
Richard Szalwinski, a Canadian software millionaire, acquired the magazine and relocated the operation to New York City, U.S. in the late 1990s. Following the relocation, the magazine quickly developed a reputation for provocative and politically incorrect content. Under Szalwinski's ownership, a few retail stores were opened in New York City and customers could purchase fashion items that were advertised in the magazine. However, due to the end of the dot-com bubble , the three founders eventually regained ownership of the Vice brand, followed by the closure of the stores. [3]
The British edition of Vice was launched in 2002 and Andy Capper was its first editor. Capper explained in an interview shortly after the UK debut that the publication's remit was to cover "the things we're meant to be ashamed of," and articles were published on topics such as bukkake and bodily functions. [7]
By the end of 2007, 13 foreign editions of Vice magazine were published, the Vice independent record label was functional, and the online video channel VBS.com had 184,000 unique viewers from the U.S. during the month of August. The media company was still based in New York City, but the magazine began featuring articles on topics that were considered more serious, such as armed conflict in Iraq, than previous content. Alvi explained to The New York Times in November 2007: "The world is much bigger than the Lower East Side and the East Village." [3]
McInnes left the publication in 2008, citing "creative differences" as the primary issue. In an email communication dated 23 January, McInnes explained: "I no longer have anything to do with Vice or VBS or DOs & DON'Ts or any of that. It's a long story but we've all agreed to leave it at "creative differences," so please don't ask me about it." [8]
At the commencement of 2012, an article in Forbes magazine referred to the Vice company as "Vice Media," but the precise time when this title development occurred is not public knowledge. [9] Vice acquired the fashion magazine i-D in December 2012 and, by February 2013, Vice produced 24 global editions of the magazine, with a global circulation of 1,147,000 (100,000 in the UK). By this stage, Alex Miller had replaced Capper as the editor-in-chief of the UK edition. Furthermore, Vice consisted of 800 worldwide employees, including 100 in London, and around 3,500 freelancers also produced content for the company. [7]

Current staff
The full current staff of Vice magazine and VICE.com can be found on their masthead.

Content

Scope
Vice magazine includes the work of journalists, columnists, fiction writers, graphic artists and cartoonists, and photographers. Both Vice ' s online and magazine content has shifted from dealing mostly with independent arts and pop cultural matters to covering more serious news topics. Due to the large array of contributors and the fact that often writers will only submit a small number of articles with the publication, Vice ' s content varies dramatically and its political and cultural stance is often unclear or contradictory. Articles on the site feature a range of subjects, often things not covered as by mainstream media. The magazine's editors have championed the Immersionist school of journalism , which has been passed to other properties of Vice Media such as the documentary television show Balls Deep on the Viceland Channel. This style of journalism is regarded as something of a DIY antithesis to the methods practiced by mainstream news outlets, and has published an entire issue of articles written in accordance with this ethos. Entire issues of the magazine have also been dedicated to the concerns of Iraqi people , [10] Native Americans , [11] Russian people , [12] people with mental disorders , [13] and people with mental disabilities . [14] Vice also publishes an annual guide for students in the United Kingdom . [15]
In 2007, a Vice announcement was published on the Internet: "After umpteen years of putting out what amounted to a reference book every month, we started to get bored with it. Besides, too many other magazines have ripped it and started doing their own lame take on themes. So we're going to do some issues, starting now, that have whatever we feel like putting in them." [16]

Politics
In a March 2008 interview with The Guardian , Smith was asked about the magazine's political allegiances and he stated, "We're not trying to say anything politically in a paradigmatic left/right way ... We don't do that because we don't believe in either side. Are my politics Democrat or Republican ? I think both are horrific. And it doesn't matter anyway. Money runs America; money runs everywhere." [5]
He has also stated: "I grew up being a socialist and I have problems with it because I grew up in Canada [and] I've spent a lot of time in Scandinavia , where I believe countries legislate out creativity. They cut off the tall trees. Everyone's a C-minus. I came to America from Canada because Canada is stultifyingly boring and incredibly hypocritical. Thanks, Canada." [6]

Website
Vice founded its website as Viceland.com in 1996, as Vice.com was already owned. In 2007, it started VBS.tv as a domain, which prioritized videos over print, and had a number of shows for free such as The Vice Guide to Travel . In 2011, Viceland.com and VBS.tv were combined into Vice.com. [18]
The website has expanded and diversified to include a network of online video channels, including TheCreatorsProject.com, Motherboard.tv, Fightland.com , Noisey.com, Thu.mp, and Broadly, which "represents the multiplicity of women's experiences". [19]

Vice News
Vice News is Vice Media's current affairs brand. Launched in December 2013, its presence consists of a YouTube channel and a website. Vice News content primarily consists of documentaries and video news digests, which range from prison systems, such as Guantanamo bay ( Gitmo ), to American market/political corruption , to international drug addiction and far beyond.

Vice books
The magazine has published the collections The DOs and DON'Ts Book and The Vice Guide to Sex and Drugs and Rock and Roll . In 2008, the photograph compilation The Vice Photo Book was released and featured published works from previous editions of the magazine. [20]

See also
WebPage index: 00131
Racial bias on Wikipedia
Wikipedia has been criticized for having a systemic racial bias in its coverage , due to an under-representation of people of color within its editor base. [1] The President of Wikimedia D.C. , James Hare, noted that "a lot of black history is left out" of Wikipedia, due to articles predominately being written by white editors. [2] Articles that do exist on African topics are, according to some critics, largely edited by editors from Europe and North America and thus reflect their knowledge and consumption of media, which "tend to perpetuate a negative image" of Africa. [3] Maira Liriano of the Schomburg Center for Research in Black Culture , has argued that the lack of information regarding black history on Wikipedia "makes it seem like it's not important." [4]
Different theories have been provided to explain these racial discrepancies. Jay Cassano, writing for Fast Company magazine, argued that Wikipedia's small proportion of black editors is a result of the small black presence within the tech sector, and a relative lack of reliable access to the Internet. [4] Katherine Maher , chief communications officer for the Wikimedia Foundation , has argued that focuses in Wikipedia's content are representative of those of society as a whole. She said that Wikipedia could only represent that which was referenced in secondary sources , which historically have been favourable towards white men. [5]
Attempts have been made to rectify racial biases through edit-a-thons , organised events at which Wikipedia editors attempt to improve coverage of certain topics and train new editors. In February 2015, multiple edit-a-thons were organised to commemorate Black History Month in the United States of America . One such edit-a-thon was organized by the White House to create and improve articles on African Americans in STEM . [5] The Schomburg Center , Howard University , and National Public Radio , also coordinated edit-a-thons to improve coverage of black history. [2] And while Wikipedia supports these edit-a-thons, the organization has always stressed that adequate citation must always be present and neutrality always maintained. [6]

See also
WebPage index: 00132
Florence Devouard
Florence Jacqueline Sylvie Devouard , née Nibart (born 10 September 1968) was the Chair of the Board of Trustees of the Wikimedia Foundation between October 2006 and July 2008.

Education
Devouard holds an engineering degree in agronomy from ENSAIAo and a DEA in genetics and biotechnologies from INPL . [1]

Career
On 9 March 2008, Devouard was elected member of the municipal council of Malintrat. [2]
Devouard joined the board of Wikimedia Foundation in June 2004 as Chair of the Board of Trustees, succeeding Jimmy Wales . [3] She was a founder of Wikimedia France in October 2004. She has served on the Advisory Board of the Foundation since July 2008. [4]
Co-founder of Wikimedia France in 2004, she was vice-chair of its board as of 2011 until December 2012. [5]

Recognition
On 16 May 2008, she was made a knight in the French National Order of Merit , proposed by the Ministry of Foreign Affairs as "chair of an international foundation". [6]
WebPage index: 00133
Essjay controversy
The Essjay controversy involved a prominent Wikipedia participant and salaried Wikia employee, known by the username Essjay, [1] [ better source needed ] who later identified himself as Ryan Jordan . Jordan held trusted volunteer positions within Wikipedia known as " administrator ", "bureaucrat", and " arbitrator ".
On July 24, 2006, Wikipedia critic Daniel Brandt started a thread titled "Who is Essjay?" (later retitled "Who is Essjay?, Probably he's Ryan Jordan" after Jordan's self-disclosure) on the forum site Wikipedia Review . [2] The ensuing discussion brought to light contradictions in claims Essjay made about his academic qualifications and professional experiences on his Wikipedia user page. Jordan claimed that he held doctoral degrees in theology and canon law and worked as a tenured professor at a private university. Five days later, The New Yorker published an interview with Essjay which repeated some of the claims. [3] Wikipedia Review found definitive proof that Jordan made false claims about his qualifications and experience, including that he was a "tenured professor", a claim that was used to describe Essjay in the interview for The New Yorker . In January 2007, Daniel Brandt contacted the author of the article in The New Yorker about the discrepancies in Jordan's biography and the exploitation of his supposed qualifications as leverage in internal disputes over Wikipedia content.
The controversy that ensued focused on his falsification of a persona and qualifications, the impact of this deception on perceptions of Wikipedia (and its policies and credibility), and the quality of decisions made in his promotion, support, and employment. [4] [5] [6]
Reactions to the disclosure were diverse, encompassing commentary and articles in the electronic, print, and broadcast media; [7] the Wikipedia community researched Essjay's article edits to check for errors and debated proposals to improve the project's handling of personal identification. In his editorial activities Jordan spent less time editing the content of articles and more time addressing vandalism and resolving editorial disputes. [8]
Wikipedia co-founder [9] Jimmy Wales initially supported Essjay's use of a persona , saying, "I regard it as a pseudonym and I don't really have a problem with it." [10] Later, Wales withdrew his support and asked for Essjay's resignation from his positions with Wikipedia and Wikia. [8] [10] Wales stated that he withdrew his support when he learned "that Essjay used his false credentials in content disputes" on Wikipedia. [11]

Timeline

The New Yorker
Stacy Schiff , a Pulitzer Prize -winning journalist writing for The New Yorker , interviewed Essjay as a source for an article about Wikipedia ("Know It All"; July 31, 2006) after he was recommended to her by a member of the Wikimedia Foundation . According to The New Yorker , Essjay "was willing to describe his work as a Wikipedia administrator but would not identify himself other than by confirming the biographical details that appeared on his user page." [3]
During the interview, Jordan told The New Yorker and had previously stated on his Wikipedia user page that he held doctoral degrees in theology and canon law and worked as a tenured professor at a private university. [12] It was later discovered that he was 24 years old, and had dropped out of community college with no qualifications. [17] The New Yorker published a correction in February 2007, which brought the issue to broader public attention. [3]
The article said that Essjay spent some 14 hours or more a day on Wikipedia but was careful to keep his online life a secret from his colleagues and friends. It portrayed Essjay as often taking his laptop to class so he could be available to other Wikipedians while giving a quiz. He asserted that he required anonymity to avoid cyberstalking . [3]
Jordan, as Essjay, claimed he sent an email to a college professor using his invented persona's credentials, vouching for Wikipedia's accuracy. In the message he wrote in part, "I am an administrator of the online encyclopedia project Wikipedia. I am also a tenured professor of theology; feel free to have a look at my Wikipedia user page (linked below) to gain an idea of my background and credentials." [6] [20]

Identity revealed
When Essjay was hired by Wikia in January 2007, he changed his Wikia profile and "came clean on who he really was," identifying himself as Ryan Jordan. [21] [22] [23] [24] [25] Other Wikipedia editors questioned Essjay on his Wikipedia talk page about the apparent discrepancy between his new Wikia profile and his previously claimed credentials. [13] [26] Essjay posted a detailed explanation in response to the first inquiry, stating that:
He later commented on his Wikipedia user page about having fooled Schiff by "...doing a good job playing the part." [6] [27]
Wikipedia critic Daniel Brandt then wrote a letter [28] reporting the identity discrepancy to Stacy Schiff and The New Yorker . [4] In late February 2007, the magazine updated its article with a correction indicating that "Essjay now says that his real name is Ryan Jordan, that he is twenty-four and holds no advanced degrees, and that he has never taught." [3]
On March 3, 2007, Andrew Lih , Assistant Professor and Director of Technology Journalism and of the Media Studies Centre at the University of Hong Kong , [29] said on his blog that a portion of Essjay's comments on the incident entered "the dangerous domain of defamation and libel" against Stacy Schiff. Lih stated that on Essjay's Wikipedia talk page, Essjay had written, "Further, she [Schiff] made several offers to compensate me for my time, and my response was that if she truly felt the need to do so, she should donate to the Foundation instead." Lih noted: [30]
Lih wrote that he contacted Schiff for comment about whether she had offered to pay Essjay for his time and quoted her return email. In it, Schiff stated that Essjay's assertion was "complete nonsense". [30]

Reaction

Wikipedia community
Speaking personally about Jordan, Wales said, "Mr. Ryan [ sic ] was a friend, and still is a friend. He is a young man, and he has offered me a heartfelt personal apology, which I have accepted. I hope the world will let him go in peace to build an honorable life and reputation." [31]
Essjay had responded at the time with a statement on his Wikipedia page, in part reading:
Reaction from within the Wikipedia community to the Essjay/Jordan identity discrepancy was sharp, voluminous, and mixed. While most editors denounced at least some of his actions, responses ranged from offering complete support to accusing Jordan of fraud. [33]
As the controversy unfolded, the Wikipedia community began a review of Essjay's previous edits and some felt he had relied upon his fictional professorship to influence editorial consideration of edits he made. "People have gone through his edits and found places where he was basically cashing in on his fake credentials to bolster his arguments," said Michael Snow, a Wikipedia administrator and founder of the Wikipedia community newspaper, the Signpost . "Those will get looked at again." [33] For instance, Essjay had recommended sources such as Catholicism for Dummies , [34] a book granted the nihil obstat and imprimatur by the Roman Catholic Church . [35] Essjay defended his use of the book by telling fellow Wikipedia editors in a disagreement over the editing of the article Imprimatur: "This is a text I often require for my students, and I would hang my own Ph.D. on it's [sic] credibility." [33] [36] In another case (a discussion of the liturgical use of the psalms ), he cited personal experience from "the Abbey of Gethsemani , where I was a monk." [37]
Jimmy Wales proposed a credential verification system on Wikipedia following the Essjay controversy, but the proposal was rejected. Wales was "reported to be considering vetting all persons who adjudicate on factual disputes." [38] "I don't think this incident exposes any inherent weakness in Wikipedia, but it does expose a weakness that we will be working to address," Wales added. [31] He insisted that Wikipedia editors still would be able to remain anonymous if they wished. "We always prefer to give a positive incentive rather than absolute prohibition, so that people can contribute without a lot of hassle," Wales commented. However, he also warned that "It's always inappropriate to try to win an argument by flashing your credentials, and even more so if those credentials are inaccurate." [18] However, Florence Devouard , chair of the Wikimedia Foundation, was not supportive of his credential proposal, saying, "I think what matters is the quality of the content, which we can improve by enforcing policies such as 'cite your source,' not the quality of credentials showed by an editor." A formal proposal that users claiming to have academic qualifications would have to provide evidence before citing them in content disputes was eventually rejected by the Wikipedia community, [39] like all previous such proposals.
As a follow-up to his initial comments to The New Yorker , Wales wrote this apology to the magazine, which appeared in its March 19, 2007 issue:
Wales expressed his regret that Essjay had "made a series of very bad judgments." He also commented that he hoped Wikipedia would improve as a result of the controversy. [19]

Wikipedia critics
Andrew Orlowski , a frequent Wikipedia critic and writer for The Register —a British technology news and opinion website—criticized Jimmy Wales for hiring Essjay at the venture-capital -funded Wikia and for appointing him to the Wikipedia Arbitration Committee after Essjay had apparently admitted his previously claimed academic and professional credentials were false. Orlowski added that Essjay's actions betrayed a dangerous community mindset within Wikipedia. [40]
Others to comment negatively included ZDNet writer Mitch Ratcliffe, who asked "why lying about one's background qualifies a person to work for a company like Wikia, which proposes to help communities to record accurate information" and asked for additional details "such as when he fired Jordan and the reasons for the firing, as well as when he endorsed Jordan in public statements." [10]
Larry Sanger , co-founder of Wikipedia who left the project in 2002, called Essjay's response "a defiant non-apology" [41] and elsewhere characterized Essjay's actions as "identity fraud." [40]
Other comments:

Academics
Following the media coverage of the Essjay controversy, a number of academics noted the damage to the credibility of Wikipedia. On March 2, 2007, a report in The Chronicle of Higher Education commented "the incident is clearly damaging to Wikipedia's credibility—especially with professors who will now note that one of the site's most visible academics has turned out to be a fraud." [47] Ross Brann, a professor of Judeo-Islamic studies at Cornell University in Ithaca , stated that Wikipedia lacks a process of scholarly review, saying, "They could make up your life if they wanted to." Brann also said that Wikipedia "has no place in the University," and he believed the Essjay incident would do nothing to change the unfavorable opinion that academics generally hold about the online encyclopedia. [48]
Nicola Pratt, a lecturer in international relations at the University of East Anglia in England, stated, "The ethos of Wikipedia is that anyone can contribute, regardless of status... What's relevant is their knowledge as judged by other readers, not whether they are professors or not—and the fact the student [Essjay] was exposed shows it works." [49] In 2009, a lengthy article was published by the National Council of Teachers of English discussing the challenges of determining textual origins in college compositions, using a detailed history of the Essjay incident to set the context. [50]

See also

Notes and references

Notes
WebPage index: 00134
Protests against SOPA and PIPA
On January 18, 2012, a series of coordinated protests occurred against two proposed laws in the United States Congress —the Stop Online Piracy Act (SOPA) and the PROTECT IP Act (PIPA). These followed smaller protests in late 2011. Protests were based on concerns that the bills, intended to provide more robust responses to copyright infringement (colloquially known as piracy) arising outside the United States, contained measures that could possibly infringe online freedom of speech, websites, and Internet communities. Protesters also argued that there were insufficient safeguards in place to protect sites based upon user-generated content .
The move to a formal protest was initiated when some websites, including Reddit and the English Wikipedia , considered temporarily closing their content and redirecting users to a message opposing the proposed legislation. Others, such as Google , Mozilla , and Flickr , soon featured protests against the acts. Some shut down completely, while others kept some or all of their content accessible. According to protest organizer Fight for the Future , over 115,000 websites joined the Internet protest. [1] In addition to the online protests, there were simultaneous physical demonstrations in several U.S. cities, including New York City, San Francisco and Seattle, and separately during December 2011 a mass boycott of then–supporter Go Daddy . The protests were reported globally.
The January protest, initially planned to coincide with the first SOPA hearing of the year, drew publicity and reaction. Days prior to the action, the White House issued a statement that it would "not support legislation that reduces freedom of expression, increases cybersecurity risk, or undermines the dynamic, innovative global Internet." [2] On January 18 itself, more than 8 million people looked up their representative on Wikipedia, [3] 3 million people emailed Congress to express opposition to the bills, [1] more than 1 million messages were sent to Congress through the Electronic Frontier Foundation , [4] a petition at Google recorded over 4.5 million signatures, [3] Twitter recorded at least 2.4 million SOPA-related tweets, [3] and lawmakers collected "more than 14 million names—more than 10 million of them voters—who contacted them to protest" the bills. [5]
During and after the January protest, a number of politicians who had previously supported the bills expressed concerns with the proposals in their existing form, while others withdrew their support entirely. Internationally, "scathing" criticism of the bills was voiced from World Wide Web inventor Sir Tim Berners-Lee , [6] as well as the European Commissioner for the Digital Agenda . [7] Some observers were critical of the tactics used; the Boston Herald described the service withdrawals as evidence of "how very powerful these cyber-bullies can be." [8] Motion Picture Association of America Chairman Chris Dodd stated that the coordinated shutdown was "an abuse of power given the freedoms these companies enjoy in the marketplace today." [9] Others such as The New York Times saw the protests as "a political coming of age for the tech industry." [10]
By January 20, 2012, the political environment regarding both bills had shifted significantly. The bills were removed from further voting, ostensibly to be revised to take into consideration the issues raised, [5] but according to The New York Times probably "shelved" following a "flight away from the bill". [5] Opposers noted the bills had been "indefinitely postponed" but cautioned they were "not dead" and "would return." [11]

Background

Background to bills
The Stop Online Piracy Act (SOPA) and the PROTECT IP Act (PIPA) are bills that were introduced into the United States House of Representatives and the United States Senate in the last quarter of 2011. Both are responses to the problem of enforcement of U.S. laws against websites outside U.S. jurisdiction. While the Digital Millennium Copyright Act (DMCA) and other existing laws have generally been considered effective against illegal content or activities on U.S.-based sites, [12] action is more difficult against overseas websites. [12] SOPA and PIPA proposed to rectify this by cutting off infringing sites from their U.S. based funding (particularly advertising ), payment processors , appearances on search engines , and visibility on web browsers , instead. Major providers of all these services are predominantly U.S. based. Notably, the provisions also involved modifying the DNS system , a crucial service that underpins the entire Internet and allows computers to locate each other reliably around the world.
Supporters included, but were not limited to, media companies and industry associations such as the Motion Picture Association of America , the Recording Industry Association of America and the Entertainment Software Association . Supporters generally identified a need to have more effective laws to combat the illegal domestic sales of products and services, the counterfeiting and sale of products (such as prescription drugs, athletic shoes, and cosmetics), and worldwide copyright infringing activities which were problematic to prevent inasmuch as they originated outside the United States.
Those opposed included a mixture of technology and Internet firms and associations, content creators such as the Wikipedia community , free software authors, free speech organizations, lawmakers, and other websites and organizations, as well as members of the public using their services. They generally identified two main areas of severe side-effects: (1) effects on Internet websites, communities and user-generated content, and (2) effects on critically fundamental internet architecture and security:
Google 's policy director, Bob Boorstin, stated that a site like YouTube supporting user-generated content "would just go dark immediately" to comply with the legislation. [12] Tumblr , one of the first websites active in grassroots activism against the bills, added a feature that "censored" its website on November 16, 2011, and the social media aggregator Reddit also became deeply involved. [1]

Legislative and protest timeline
On November 16, 2011, a first hearing by the U.S. House Judiciary Committee was marked by online protests involving blackened website banners, popularly described as "American Censorship Day".
On December 15, 2011, the first House Judiciary Committee mark-up hearing took place for SOPA, prior to its eventual move to the House floor. [15] During the markup session, several proposed amendments to address technological and other concerns were defeated. The mark-up process was put on hold to be resumed after the new year.
Around this time, numerous websites began displaying banners and messages promoting their readerships to contact Congress to stop the progress of the bill, and some websites began to discuss or endorse a possible "Internet blackout" before any vote on SOPA in the House, as a means of further protest. [16] Reddit was the first major site to announce an "Internet blackout" for January 18, 2012, and several other sites shortly followed, coordinating actions for that day. [17]
A notable political response to the November 2011 protests was the outlining in early December of a bipartisan third, alternative, bill with the support of technology companies such as Google and Facebook , [18] which unusually had been posted on the Internet to allow public comment and suggestions in light of the widespread protests related to the SOPA and PIPA bills. It was formally introduced as the Online Protection and Enforcement of Digital Trade Act (OPEN) in the Senate on December 17 by Senator Ron Wyden and in the House on January 18 by Representative Darrell Issa . It proposed placing enforcement in the hands of the United States International Trade Commission , keeping provisions that targeted payments and advertising for infringing websites, and tightly targeted wording to avoid many other key areas of concern with SOPA and PIPA. [19]
Online discussions of a blackout and concerns over the bills continued unabated after the markup hearing and increased in prominence. On January 11, Senator Patrick Leahy , the main sponsor for PIPA, said of the DNS filtering provision, "I will therefore propose that the positive and negative effects of this provision be studied before implemented", [20] reported by some papers as removal of those provisions. [21] Opposers deemed this a tactical withdrawal allowing reintroduction at a later stage and ignoring other concerns as well as provisions in PIPA, and evidence that the bill had not been understood or checked by its own creators and that proposals for a blackout were gaining impact. [22] Momentum for the protests continued unchanged [21] since the bills had merely been postponed, and due to their other contentious provisions.

Protests of November 16, 2011 ("American Censorship Day")
On November 16, 2011, SOPA was discussed by the U.S. House Committee on the Judiciary . Tumblr , Mozilla, Techdirt, and the Center for Democracy and Technology were among many Internet companies who protested by participating in 'American Censorship Day', by displaying black banners over their site logos with the words "STOP CENSORSHIP." [23]

December 2011 boycott of Go Daddy
On December 22, 2011, users at Reddit proposed a boycott and a public day for switching away from then–SOPA supporter Go Daddy , [24] the largest ICANN -accredited registrar in the world, known as Move Your Domain Day . [25] The date was later set as December 29, 2011. [26]
Popular websites that moved domains included imgur , [27] the Wikimedia Foundation , [28] and Cheezburger — which stated it would remove over 1,000 domains from Go Daddy if they continued their support of SOPA. [29]
On December 23, Go Daddy withdrew its support for SOPA, releasing a statement saying "Go Daddy will support it when and if the Internet community supports it." [30] [31] CEO Warren Adelman stated when asked, that he couldn’t commit to changing Go Daddy's position on the record in Congress, but said "I’ll take that back to our legislative guys, but I agree that’s an important step"; [32] when pressed, he said "We’re going to step back and let others take leadership roles." [32] Further outrage was due to the fact that many Internet sites would be subject to shutdowns under SOPA, but GoDaddy is in a narrow class of exempted businesses that would have immunity, whereas many other domain operators would not. [33]
On December 26, 2011, a Google bomb was started against Go Daddy to remove them from the #1 place on Google for the term "Domain Registration" in retaliation for supporting SOPA. [34] This was then disseminated through Hacker News . [35] Reddit users noted that by December 22, 2011, SOPA supporters were discovering the backlash that could arise from ignoring social media users. [36]
Reports up to December 29 described Go Daddy as "hemorrhaging" customers. [37] [38] On December 25, 2011 (Christmas Day), Go Daddy lost a net 16,191 domains as a result of the boycott. [39] However, on December 29 itself, Go Daddy gained a net of 20,748 domains, twice as many as it lost that day, attributed by Techdirt to a number of causes, in particular customers having moved early, and an appeased customer response to their change of position over SOPA. [40] [41]

Protests of January 18, 2012

Protestors

Wikimedia community
On December 10, 2011, Wikipedia co-founder Jimmy Wales drew attention to concerns over SOPA, which he described as a "much worse law" than the DDL intercettazioni (Wiretapping Bill) [42] in Italy some months earlier, which was being fast-tracked through the United States Congress under a "misleading title". He stated he was attending high-level meetings on this, and wanted to gauge the sense of the English Wikipedia community on the issue, and specifically on the question of a blackout similar to that held successfully in October 2011 by Italian Wikipedia editors over the proposed media censorship law in that country: [43]
Following initial informal discussions which resulted in a positive response, a formal consultation titled "SOPA Initiative" was opened by the community to consider specific proposals and preferred options. These included matters such as location (United States only or worldwide), and whether content should be disabled completely or still accessible after a click-through page. Eventually, the discussion led to a decision strongly in favor of a 24-hour global blackout of the site on January 18, disabling normal reading and editing functions, affirmed in a vote of approximately 1,800 editors. [44] The blocking action was purposely not complete; users could access Wikipedia content from the mobile interface or mirror sites, or if they disabled JavaScript or other web browser functions. [45] [46] [47] Within hours of the start of the blackout, many websites posted instructions for disabling the banner, by altering URLs, using browser add-ons such as Adblock Plus or Greasemonkey , or interrupting the page from loading completely.
The vote formally affected the English Wikipedia only; other language editions and Wikimedia projects were left free to decide whether to hold their own protests given the potential worldwide impact of the legislation, with technical support on offer from the Foundation. [44] The editor communities of at least 30 other sister projects chose to do so. [48]
On January 17, 2012, Jimmy Wales affirmed the results of the community's decision and that the Wikimedia Foundation , which hosts the English Wikipedia website, would support the community's decision. He called for a "public uprising" against the proposed legislation, which critics fear would threaten free speech. He added that factors such as funding or donations had not been part of the community's considerations, but the matter had arisen as "a principled stand" from the community, and that in his view "our best long-term prospect for Wikipedia in terms of our survival ... depends on us being principled". [49] He commented on editors' reasons for the decision: [49]
Wikimedia Executive Director Sue Gardner posted an announcement of the Foundation's support for the blackout proposal on Wikimedia's blog. The post received over 7000 responses from the general public within the first 24 hours of its posting. [50] The blackout was to run for 24 hours starting at 05:00 UTC ( midnight Eastern Standard Time ) on January 18. [51]
Despite the support of those polled for the action, a small number of Wikipedia editors blacked out their own user profile pages or resigned their administrative positions in protest of the blackout; one editor stated his "main concern is that it puts the organization in the role of advocacy, and that's a slippery slope". [52]

Other websites
According to protest organizer Fight for the Future , more than 115,000 websites participated in the protest, including Google and Wikipedia. [1] Websites that participated in the blackout included Cheezburger , Craigslist , Boing Boing , A Softer World , Cake Wrecks , Cyanide & Happiness , Demand Progress , Destructoid , DeckTech.net , Entertainment Consumers Association . Free Press , Failblog , Newgrounds , Good.is , GOG.com , Gamesradar , Internet Archive , Marxists Internet Archive , Jay is Games , Mojang , MoveOn.org , Mozilla , MS Paint Adventures , Rate Your Music , Reddit , Roblox , Oh No They Didn't , Tucows , blip.tv , Tumblr , TwitPic , Twitter , The Oatmeal , VGMusic , Wikia , WordPress , xkcd as well as the corporate site of the Linux distribution openSUSE and the congressional websites of Silicon Valley representatives Anna Eshoo and Zoe Lofgren . [53] [54] [55] Google announced their intention to join the blackout by altering their logo for US visitors for the day, almost entirely obscuring it with an interactive black redaction swath. Clicking through the specially designed logo took readers to an informational page about the bills, and the opportunity to sign a petition to be sent to Congress stating their concerns. [56]
The Mozilla Foundation altered the default start page of their Firefox web browser, blacking it out and providing links with more information on the SOPA/PIPA bills and the opposition to them, and to allow users to email their Congressional representatives. [57]
TV Tropes posted black bars atop the web page with the message "STOP SOPA".
Mojang 's bestselling game Minecraft made a splash text that said "SOPA means LOSER in Swedish !"
A site called The Spoony Experiment , known for a cute robot mascot called Burton, changed the homepage into a more nightmarish version, involving a nightmarish version of Burton and the words "The Spoony Experiment" replaced by red text proclaiming "The Experiment is Over", signifying the proposed death of several websites.
Wired magazine's online site used Javascript to place black bars on most of the text on their page, as if the text was redacted, outside of their key article regarding SOPA/PIPA; readers could remove the bars with a mouse click. [58] [59]
The photo-sharing website Flickr created the ability for a registered user to "censor" an unlimited number (up from an initial limit of ten) of photos as demonstration of how SOPA/PIPA regulation would affect the site; the user-selected photographs were greyed out, and included informational text. [60]
4chan ran a banner and "censored" posts by users on all image boards, [61] which could be viewed by hovering over them.
StumbleUpon added numerous links to anti-SOPA/PIPA websites.
A video was circulated by the League for Gamers (founded by Mark Kern and supported by ScrewAttack , Extra Credits , and LoadingReadyRun ) protesting the Entertainment Software Association 's support of SOPA by gathering support to boycott the ESA's popular E3 convention . [62]

Physical demonstrations
In addition to the online blackouts, protests in cities such as New York City, San Francisco, and Seattle were held on January 18 to raise awareness of the two bills. [63] [64]
A series of pickets against the bills were held at the U.S. Embassy in Moscow. Two picketers were arrested. [65]

Reaction

Pre-protest
The announcement of the blackout was reported worldwide. Media that covered the story included ABC Australia , [66] CBC , [67] BBC , [68] der Spiegel , [69] Le Figaro , [70] Le Monde , [71] Libération , [72] Fox News , [73] The Guardian , [74] Menafn, [75] News Limited , [76] Sky News , [77] The Age , [78] The Hindu , [79] The New York Times , [80] [81] Taipei Times , [82] The Washington Post , [83] The Wall Street Journal [84] and The Times of India . [85]
Several media organizations including The Washington Post , The Guardian , and NPR encouraged a " crowdsourcing solution for those left searching for answers" during the Wikipedia blackout by inviting users to ask questions on Twitter using the hashtag #altwiki. [86]
An executive of the Motion Picture Association of America (MPAA) dubbed the blackout plan an example of the "gimmicks and distortion" that inflamed passions while failing to solve the problem of copyright infringement by "draw[ing] people away from trying to resolve what is a real problem, which is that foreigners continue to steal the hard work of Americans". [87] Former U.S. Senator and MPAA Director Chris Dodd stated that the coordinated shutdown was "also an abuse of power given the freedoms these companies enjoy in the marketplace today." [9]
Dick Costolo , CEO of social networking site Twitter , rejected calls for Twitter to join the protest, tweeting that "[c]losing a global business in reaction to single-issue national politics is foolish." [88] Originally, some thought Costolo referred to all of the blackout movements on January 18, but afterwards clarified that he was referring to a hypothetical blackout of Twitter, and that he was supportive of the Wikipedia blackout itself. [89]
The sponsor of the bill, Representative Lamar S. Smith , called the blackout a "publicity stunt," and stated with reference to Wikipedia that "it is ironic a website dedicated to providing information is spreading misinformation about the Stop Online Piracy Act." [90]
On January 17, 2012, in response to growing concerns over PIPA and SOPA, the White House stated that it "will not support legislation that reduces freedom of expression, increases cybersecurity risk, or undermines the dynamic, innovative global Internet." [2]

January 18
The Wikimedia Foundation reported that there were over 162 million visits to the blacked-out version of Wikipedia during the 24-hour period, with at least 8 million uses of the site's front page to look up contact information for their U.S. Congressional representatives. [3] [91] The usage of Wikipedia's front page increased enormously during the blackout with 17,535,733 page views recorded, compared with 4,873,388 on the previous day. [92] A petition created and linked to by Google recorded over 4.5 million signatures, [3] while the Electronic Frontier Foundation reported that more than 1 million email messages were sent to congressmen through their site during the blackout. [4] MSNBC reported that over 2.4 million Twitter messages about SOPA, PIPA, and the blackouts were made during a 16-hour period on January 18; this included Facebook founder Mark Zuckerberg , who had not used the service since 2009, to encourage his followers to contact their congressmen. [93] [94] Sen. Ron Wyden (D-OR) , a key opponent of the bills, said that "lawmakers had collected more than 14 million names - more than 10 million of them voters" to protest the legislation. [5]
Time reported that before the day had ended, "the political dominoes began to fall ... then trickle turned into flood". [95] It named ten senators who had announced their switch to opposing the bills and stated that "nearly twice that many House members" had done so. [95]
During the blackout, libraries at several universities used the outage to remind students that the traditional paper encyclopedias were available for research. Students who grew up turning to the internet to look up information were encouraged to visit the library as an alternative source of information. [96] On Twitter, a joke hashtag #factswithoutWikipedia trended with users posting humorous fake "facts." [97] "Startled" Internet users frustrated or angry at their loss of Wikipedia for the day used Twitter as an outlet; politicians likewise turned to Twitter when overwhelmed by the public communications flood in support of the blackout. [95] CTV news in Canada published a "survival guide" for "getting around the blackout" on their national website, citing Wikipedia as the answer to "burning questions such as "Are chinchillas rodents?" and "What does ‘rickrolling' mean?" The guide provided detailed instructions on how to circumvent the ban and access the English Wikipedia during the protest. [98] CTV referred to the protest as " a date that will live in ignorance ." [99] Creative America , a coalition representing movie studios, entertainment unions, and television networks, used the blackout to prompt those affected by it to enjoy other forms of entertainment in place of their normal Internet activities; such ads appeared at Times Square in New York City and on various websites. [64]

Post-protest
The impact of the coordinated action was generally considered to be significant. Yochai Benkler of the Berkman Center for Internet & Society stated that the January 18 blackout was "a very strong public demonstration to suggest that what historically was seen as a technical system of rules that only influences the content industry has become something more," further adding "You've got millions of citizens who care enough to act. That's not trivial." [100] California House member Darrell Issa called the collective effort an unprecedented means for upsetting a backroom lobbying effort, [101] and the immediate political efficacy of the widespread online protest was characterised in terms of a sleeping giant having awakened and of a new player being in town. [102] One Silicon Valley lobbyist said the content industry had "a lot to learn," noting that they don't have grassroots support: "There are no Facebook pages to call your congressman to support PIPA and SOPA." [103] The New York Times , which framed the netizens' revolt in terms of the new economy versus the old economy, [104] headlined the activism as a "political coming of age for the tech industry." [10] (James Grimmelmann, an Associate Professor at New York Law School , opined two months later that "Legal systems are like Soylent Green : they're made out of people. If you want to protect civil liberties using law, you need to get people on your side who share your vision of what law stands for. That's why the SOPA protests were so effective. They converted an argument about justice into real-world political power.") [105]
Newspaper editorials had mixed views. The Boston Herald called the protest a "hissy fit" by "Internet powerhouses" saying, "within hours of the online protest, political supporters of the bill... began dropping like flies, thus proving how very powerful these cyber-bullies can be." [8] The New York Times described the protest as "Noted, but as a Brief Inconvenience" [106] and, as well, offered an opinion about the protest and possible accomplishments. [107] BBC News technology writer Rory Cellan-Jones was of the opinion that the blackout achieved its objectives but possibly at some cost to Wikipedia's reputation. [108] Bill Keller was of the view that "Jimmy Wales... assumed a higher profile as a combatant for the tech industry [and] supplied an aura of credibility to a libertarian alliance that ranged from the money-farming Megatrons of Google to the hacker anarchists of Anonymous." [109]
Media columnist David Carr wrote in the New York Times that there were two lessons, one being that "People who don’t understand the Web should not try to re-engineer it", and the other that while businesses generally prize their relations with their customers, in the struggle between media and technology companies, the latter have "a much more chronic [i.e. ongoing], intimate relationship with consumers" and would more likely prevail. [110]
Motion Picture Association of America chairman Chris Dodd admitted that the content industry had lost the public relations battle with the Internet industry, adding that "[y]ou've got an opponent who has the capacity to reach millions of people with a click of a mouse and there's no fact-checker. They can say whatever they want." [111] Dodd called for Hollywood and Silicon Valley to work out a compromise on the legislation, [112] but was also criticized for a statement on Fox News to the effect that politicians would risk having campaign funding cut off if they did not support media industry proposals. [113] [114] The legal director of public interest group Public Knowledge was quoted on that organization's website as writing: [115]
Among other media industry reactions, Creative America was of the view that "[t]hey've misidentified this issue as an issue about your Internet, your Internet is being jeopardized. In fact their business model is being asked to be subjected to regulation. They're misleading their huge base." [116] Recording Industry Association of America President Cary Sherman noted that the major television networks supported the legislation but, unlike Wikipedia and Google, did not use their platforms to try to shape public opinion: "when Wikipedia and Google purport to be neutral sources of information that is not only not neutral but affirmatively incomplete and misleading, they are duping their users into accepting as truth what are merely self-serving political declarations." [117]
Rep. Lamar Smith, who sponsored SOPA, flatly stated in a commentary on Fox News that "This bill does not threaten the Internet. But it does threaten the profits generated by foreign criminals who target the U.S. market and willfully steal intellectual property by trafficking in counterfeit or pirated goods." [118] While speaking on the Senate floor on January 23 Senator Leahy reiterated his objections to the protests, saying:

International responses
World Wide Web inventor Sir Tim Berners-Lee "scathingly" [6] attacked the SOPA and PIPA legislation. Speaking at an industry event in Florida he praised the protests by major sites for the attention they had drawn, and described the bills as a "grave threat to the openness of the Internet" that "had to be stopped": [6]
Two days later, Vice-President of the European Commission and European Commissioner for the Digital Agenda Neelie Kroes described the bills as "bad legislation" that would "threaten the basic foundation of the success of the web". [7] She also said there "should be safeguarding benefits of open net." "Speeding is illegal too but you don't put speed bumps on the motorway," she said. [120]

Related protests
SOPA and PIPA protests were overlapped and followed by protests against ACTA which has a similar sense. The ACTA treaty was signed by 22 Member States in Europe and was expected to be signed before March 2012 by the other left Cyprus, Estonia, Netherlands and Slovakia, and thus would have gained legal force for the whole European Union. On February 11, more than 200 European cities took part in a widespread protest against ACTA. [121] [122] Although protests were held in Europe, the signing of ACTA was led by USA, Australia, Canada, South Korea, Japan, New Zealand, Morocco, and Singapore, which were first to sign the treaty at a ceremony on October 1, 2011, in Tokyo. [123] However, the concerns of ACTA are much related and raised after the protests against SOPA and PIPA which directed the public attention to bills and acts that may threaten Internet and civic liberties.

Legislative impact and aftermath
During the day of January 18, six of PIPA's sponsors in the Senate, including Marco Rubio , PIPA's co-sponsor, Orrin Hatch , Kelly Ayotte , Roy Blunt , John Boozman , and Mark Kirk , stated that they would withdraw their support for the bills. [124] Several other congressmen issued statements critical of the current versions of both bills. [125] [126]
By the following day, eighteen of the 100 senators, including eleven of the original sponsors of the PIPA bill, had announced that they no longer supported PIPA. [127] By one account, the shift in stated positions on SOPA/PIPA by members of Congress had gone overnight from 80 for and 31 against to 65 for and 101 against. [128] An initial floor vote was scheduled for January 24, prior to the Internet blackout, but following these responses, Senator Majority Leader Harry Reid announced that the vote will be postponed, urging the bill's main sponsor, Senator Patrick Leahy , to work out compromise in the bill "to forge a balance between protecting Americans' intellectual property, and maintaining openness and innovation on the Internet." [5] [129] Similarly, the House Judiciary Subcommittee chairman, Representative Lamar S. Smith , announced that further voting on SOPA would be placed on hold "until there is wider agreement on a solution." [130] [131] Later, an updated The New York Times news story reported that the two bills were "indefinitely shelved." [5] House Committee on Oversight Chairman Darrell Issa commented that "This unprecedented effort has turned the tide against a backroom lobbying effort by interests that aren't used to being told 'no'", describing the events as a "responsible and transparent exercise of freedom of speech". [132] Opposers cautioned that although "postponed," the bills were "not dead" and "would return." [11]
Months after the protests, in July 2012, The New York Times summarized events as follows: [133]
The development of the electronic grassroots campaign has been the subject of academic analysis. [134] [135]

Other proposed laws
According to the Electronic Frontier Foundation (EFF), "SOPA and PIPA are really only the tip of the iceberg. The same forces behind these domestic U.S. laws have continued to both push for other states to pass similar domestic laws, as well as to secretly negotiate international trade agreements that would force signatory nations to conform to the same legal standards." [136]
Examples cited by EFF include: [136]
Examples considered "similar to SOPA/PIPA" by other analyses:

See also
WebPage index: 00135
1Lib1Ref
#1Lib1Ref ("One Librarian, One Reference") was a Wikipedia publicity drive that asked each librarian on Earth to mark the 15th anniversary of the foundation of Wikipedia, on 15 January 2016, by adding a citation to the online encyclopedia . [1] [2] [3]
The organisers estimated that if each librarian on the planet spent 15 minutes adding a citation, then the English Wikipedia's backlog of 350,000 [citation needed] templates would be cleared. [4]
The week long event, which ran from 15-23 January, used the hashtag "#1lib1ref". [5]

Results
As of the end of the campaign, 1,232 revisions on 879 pages, by 327 users in 9 languages used the hashtag #1lib1ref in the edit summary with the total edit count estimated to be around 50% below the actual number. Furthermore the hashtag was used in 1100 Twitter posts by over 630 users. [6]
WebPage index: 00136
WikiNodes
WikiNodes is an app for the Apple iPad built by IDEA.org . [1] WikiNodes was the first tablet app for browsing Wikipedia using a radial tree approach to visualize how articles and subsections of articles are interrelated. The app displays related items (articles or sections of an article), which spread on the screen, as a spiderweb of icons. [2]

Operation
The app uses the SpicyNodes visualization technique which was awarded a "best for teaching and learning" award in 2011 from American Association of School Librarians (AASL) , [3] and voted #edchat's 35 Best Web 2.0 Classroom Tools in 2010. [4]
The user interface is based on two display modes:
As of June 2011, the app supports the 36 top Wikipedia languages (by number of articles).

Reception
The app was highlighted as a "Staff pick" by Apple's U.S. App Store , Week of May 28, 2011; as "New and Noteworthy" by Apple's U.S. App Store, Week of May 5, 2011; and at other times by Apple's app stores for non-US countries. [10] It has been favorably covered by several bloggers, including those in the references below. [11]

See also
WebPage index: 00137
Veropedia
Veropedia was a free, advertising-supported Internet encyclopedia project launched in late October 2007. It was taken down in January 2009, pending creation of a new version. As of this date, [ when? ] there has been no report of a new version. [ citation needed ] [ original research? ]
Veropedia editors chose Wikipedia articles that met the site's reliability standards; information was then scraped , or chosen by an automatic process, and thereafter a stable version of the article was posted on Veropedia.
Any improvements required for articles to reach a standard suitable for Veropedia had to be done on Wikipedia itself. This model was intended to provide benefits to both projects: Wikipedia's open nature and large volume, and Veropedia's stability and perpetuity. [1] [2] [3]
As of October 2008 [update] the site, still in beta , had checked and imported more than 5,800 articles [4] from the English Wikipedia into its public database. [5] Although Veropedia intended to eventually support itself completely through advertising, the project was mainly financed by those involved in the project, [6] and in January 2009 it disabled articles and advertisements and announced a coming "Beta2". The "Beta2" never arrived and the articles were not restored.

History
Veropedia was started by a group of experienced Wikipedia editors, including founder Daniel Wool, who had prior experience editing a variety of reference works including Encyclopedia of the Peoples of the World [7] and was an employee of the Wikimedia Foundation until spring 2007. [8] By November 2007, roughly 100 Wikipedia editors were involved in the project. The help of academics who had worked on Wikipedia was also being sought. [1] [7]
An explanatory page on the site stated that similar projects in languages other than English might be launched; it distinguished Veropedia from "expert-driven" wikis such as Citizendium .
In January 2009, the encyclopedia contents were removed and replaced with a message stating that "The original version of Veropedia has been taken down for now while we work on a new Veropedia. This new Veropedia will have a superior method of handling articles and introduces an improved interface."

Management and legal status
Veropedia was operated by Veropedia, Inc., a for-profit corporation registered in Florida , [3] [9] and founded by Wool, [10] [11] a former co-ordinator at the Wikimedia Foundation , the parent organization of Wikipedia. [8]
As required by its use of Wikipedia material, all Veropedia content was licensed under the GNU Free Documentation License . [4]

Contrast with Wikipedia
The Veropedia editorial community was far smaller than Wikipedia's, and was intended to be geared towards quality article writing, seeing involvement in Veropedia as a means to return to the roots of knowledge building by focusing upon articles rather than editorial difficulties. [12] Other notable differences included:

Evaluation
Veropedia, founded in 2007, was still in its beta stage. In August 2008, it had an Alexa traffic rank of over 1.5 million [14] – indicating it was significantly less popular than Wikipedia , [15] (Traffic rank 6) Citizendium , [16] (Traffic rank of 273,939) and Scholarpedia . [17] (Traffic rank of 171,753)
Nicholas Carr , a critic of Web 2.0 in general and Wikipedia in particular, criticized Veropedia as trying to "scrape" the "cream" of Wikipedia. [1] Carr has also stated that Veropedia had an unclear interface with clicks bouncing one back and forth between Wikipedia and Veropedia. [1]
Tim Blackmore, an associate professor at the Faculty of Information and Media Studies of the University of Western Ontario , expressed scepticism toward the project, since there are already encyclopedias in existence where "content is checked and articles are reviewed". The main lure of the internet, according to him, is "free information" and Wikipedia has already emerged as a pioneer in open content information resources. [18]
A different evaluation in The Australian said Veropedia "seems more likely to succeed" than Citizendium, another then-recently founded online encyclopedia, because "it is less directly competitive" with Wikipedia. The story opined that both Veropedia and Citizendium "should in theory help improve the fairness and accuracy of available online information about many contentious topics although the academic bent to each raises questions over what, exactly, they will construe as fair when it comes to coverage of corporations and their actions." [19]
A story in Wired News discussed whether Veropedia (and Citizendium ) could avoid some of the same problems that Wikipedia has supposedly encountered: "Though office politics and internecine bickering abound at the Wikimedia Foundation – one former insider described the atmosphere as "MySpace meets 'As the World Turns' for geeks" – both Wool and Sanger deny that internal squabbles were why they started their own encyclopedias. Whether their ventures fall prey to the same turf wars, bureaucratic quagmires and academic catfights as the site that spawned them remains to be seen." [8]
In a review of various Wikipedia alternatives, TechNewsWorld argued that Veropedia's estimation of 5000 articles was not credible, as "many of these articles are small and insignificant almanac-type entries that serve mainly as filler". It thus argued that like Citizendium, Veropedia avoided "the tough challenge of handling controversial and time-sensitive subjects" that Wikipedia had taken on. The article also stated that most Veropedia articles were identical to their Wikipedia counterpart. [20]
In January 2008, the St. Petersburg Times , a well known Florida newspaper based in the town from which Daniel Wool operated Veropedia, listed Wool and Terry Foote, a Veropedia investor, as "people to watch in 2008". [6]

See also
WebPage index: 00138
Personal wiki
A personal wiki is a wiki maintained primarily for personal use. Personal wikis allow people to organize information on their desktop or mobile computing devices in a manner similar to community wikis , but without the need for collaboration by multiple users.
Personal wiki software can be broadly divided into multi-user wiki software with personal editions, and those wiki applications that are designed only for single users, not depending on a database engine and a web server. The first class includes wiki applications such as MoinMoin or TWiki , as these can be installed for standalone use as well. This may require installing additional software, for example a web server , a database management system , or a WAMP / LAMP software bundle. Nevertheless, this does not mean the wiki must be accessible to outside users. [1]
Some personal wikis are public but password-protected, running either on their own webservers or hosted by third parties. This has the advantage that the personal space can be accessed and edited from any computer or PDA with a web browser . [2]

Multi-user wiki software
Multi-user wiki applications with personal editions:

Single-user wiki software
There are also wiki applications specifically designed for personal use [4] apps for mobile use, [5] and apps for USB stick use. [6] Their feature set often differs from traditional wikis; examples are:
Notable examples of such software include:

See also
WebPage index: 00139
Comparison of wiki software
The following tables compare general and technical information for a number of wiki software packages.

General information

Target audience

Features 1

Features 2

Installation

See also

Footnotes
WebPage index: 00140
Dariusz Jemielniak
Dariusz Jemielniak (born March 17, 1975, Warsaw , Poland ) is a full professor of management, the head of the Center for Research on Organizations and Workplaces (CROW), and a founder of New Research on Digital Societies (NeRDS) group at Kozminski University . His interests revolve about critical management studies , open collaboration projects (such as Wikipedia or F/LOSS ), narrativity , storytelling , knowledge-intensive organizations , virtual communities , organizational archetypes, all studied by interpretive and qualitative methods. [1] In 2015, he was elected to the Wikimedia Foundation board of trustees.

Career
He is a graduate of the renowned VI Liceum Ogólnokształcące im. Tadeusza Reytana w Warszawie and a 2000 summa cum laude graduate from the Faculty of Management, Warsaw University . In 2004 he earned a Ph.D. in economics from the Kozminski University , with Andrzej Koźmiński being the supervisor of his thesis on IT workers in organisations; study of programmers' professional culture . In 2009, following his thesis Praca oparta na wiedzy [2] he received his habilitation . In 2014, he received his Professor 's degree from the President of Poland .
His research so far has included workplace studies of knowledge-intensive workers on the example of software engineers in the US and in Europe. [3] He has written a book on "the new knowledge workers ", [4] as well as the social organization of Wikipedia [5] [6] titled Common Knowledge?: An Ethnography of Wikipedia , first published in Poland in 2013 as Życie wirtualnych dzikich. Netnografia Wikipedii, największego projektu współtworzonego przez ludzi . [7] The book follows a period of research on identity and roles in open source projects, in the form of participating ethnography [8] [9] [10]
Jemielniak has conducted research projects at Cornell University (2004-2005), Harvard University (2007), Berkeley (2008), and Harvard Law School (2011-2012, 2015-2016). He also conducts research on organisational changes in higher education facilities [11] and is an active participant in the debate on the reform of higher education in Poland. [12] [13] [14] [15]
His experience running Non-governmental organizations includes being an elected chairperson of the "Inkubator" association (organizing a network of young Poles with literary talents) for one term, as well as being an elected chairperson of the Collegium Invisibile association for three terms. In both cases responsibilities included writing grant proposals and active fundraising. Collegium Invisibile experience allowed Dariusz to participate in the Higher Education Support Program within the Soros Foundation network, including a 300h course on fundraising in Budapest, Moscow, and Blagoevgrad . He also has served on the Funds Dissemination Committee of the "English Teaching" program (aimed at improving language skills of English teachers in rural areas of Poland) coordinated by Fundacja Nida from the funds of Polish-American Freedom Foundation over the last 8+ years. [16]
Jemielniak is the Polish Chief Desk of the Annals of Improbable Research , best known for running the annual Ig Nobel Prizes ceremony. [17] He is also author of Ling.pl , one of Poland's largest Internet dictionaries, as well as of angielski.edu.pl , an English-learning website. [18]
Within the Wikimedia movement, Jemielniak is involved in the Polish Wikipedia , where he serves as an administrator, a bureaucrat, and a checkuser, [19] as well as globally as one of the stewards until early 2016. Previously he served one term on the Wikimedia Foundation 's ombudsman commission. He was the only person (aside from Heilman himself) who voted against the removal of James Heilman from the Wikimedia Foundation's board of trustees. [20]
He is a member of the Polish chapter of Wikimedia , but has never held any roles or position in it. He was one of the active participants in the discussion about allowing paid edits to be made on the Wikimedia projects, [21] as well as on reducing the bureaucracy within the projects, [22] [23] and conflict resolution. [24] He is an advocate of wider involvement of women in the Wikimedia movement [25] as well as of deeper involvement of academic circles in using and editing Wikipedia (writing Wikipedia articles by students as criteria for obtaining credit, checking their level of knowledge and creating texts useful to the society at large at the same time). [26] [27] [28] He considers it the moral obligation of all Wikipedia users to correct any mistakes they see, even if they do not support the movement financially. [29]

Selected academic publications

Notes and references

External links
WebPage index: 00141
Oscar van Dillen
Oscar Ignatius Joannes van Dillen (born 25 June 1958 in 's-Hertogenbosch ) is a Dutch composer, conductor, and instrumentalist.

Education
Van Dillen studied North-Indian classical music ( sitar , tabla , vocal ) with Jamaluddin Bhartiya at the Tritantri School in Amsterdam and bansuri with Gurbachan Singh Sachdev at the Bansuri School of Music in Berkeley, California from 1977 to 1980, as well as classical and jazz flute at the Sweelinck Conservatory in Amsterdam between 1982 and 1984. Here, he also received composition lessons from Misha Mengelberg .
After studies of medieval and Renaissance music with Paul Van Nevel in Leuven ( Belgium ), he studied classical composition with, among others, Dick Raaymakers and Gilius van Bergeijk at the Koninklijk Conservatory in The Hague in 1990/1991 and with Klaas de Vries , Peter-Jan Wagemans and René Uijlenhoet at the Rotterdam Conservatory from 1996 to 2002. He also studied composition with Manfred Trojahn at the Robert Schumann College in Düsseldorf in 2001, where he also received lessons in conducting from Lutz Herbig.

Employment and affiliation
Van Dillen teaches World music composition as well as music theory in the jazz, pop and world music department at the Conservatory of Rotterdam. [1] [2] [3] He lives in the same city.
He is a member of Componisten 96, a closely restricted [4] association for the promotion of composing in the Netherlands. [5]
He was the first chairperson of the Dutch chapter of the Wikimedia Foundation and among its founders. [3] [6] From December 2006 to July 2007 he was a member of the Board of Trustees of the Wikimedia Foundation . [3] [7] [8] He is also the vice president of The Netherlands - Turkey Friendship Foundation.
He is a founding member of the Rotterdam School of composers. [9]

Works and projects
On 22 February 2008 his work Paradox – Music for a Sculpture was performed at the Boymans Van Beuningen Museum in Rotterdam. [10]
On 21 November 2008 his * 2 Cameras @ Sea (2008) premiered at Muziekgebouw aan 't IJ , Amsterdam during SHIFT, the Canadian and Dutch Arts Festival. Van Dillen composed the music for the Canadian ensemble "Continuum Contemporary Music".
In April 2009 Continuum Contemporary Music of Toronto premiered 2 Cameras @ Sea , a collaboration between Oscar van Dillen and Canadian filmmaker Clive Holden , commissioned by the Canada Council for the Arts . [11] The performance took place at the Images Festival in Toronto, and was repeated in Amsterdam. [12] [13] [14]
Also in 2009, Original Winds from Breda performed his ballet music in an extended series of music and dance performances in Rotterdam. [15]
Van Dillen wrote Objet Privé , for solo cello, and Forecast , for violin, cello and bayan for cellist John Addison who also has one of the string quartets on his repertoire list. [16]
Doelenkwartet premiered both of his string quartets and continue to play them regularly. [17]

Compositions

Recordings
WebPage index: 00142
Litigation involving the Wikimedia Foundation
The Wikimedia Foundation has been involved in several lawsuits . Some of them have gone in favor of the Foundation, others have gone against it.

Outcomes in favor of the plaintiffs
In May 2011, Louis Bacon obtained a court order against the Wikimedia Foundation to compel it to reveal the identity of the editors who defamed him on Wikipedia . However, the order was obtained in the UK, and is therefore unenforceable in the United States. [1] [2]

Outcomes in favor of the Wikimedia Foundation
Barbara Bauer, a literary agent, sued Wikimedia Foundation (which owns Wikipedia) for defamation. [3] [4] She claimed that a Wikipedia entry branded her the “dumbest” literary agent. [4] But the case was dismissed because of the Communications Decency Act . [5]
Professional golfer Fuzzy Zoeller , who also felt he was defamed on Wikipedia, did not sue Wikipedia because he was told that his suit would not prevail in light of the Communications Decency Act. [6] He then sued the Miami firm from whose computers the edits were made, but later dropped the case. [ citation needed ]
In 2007, three French nationals sued the Wikimedia Foundation when an article on Wikipedia described them as gay activists. [7] [8] A French court dismissed the defamation and privacy case, ruling that the Foundation was not legally responsible for information in Wikipedia articles. [8] The judge ruled that a 2004 French law limited the Foundation's liability, and found that the content had already been removed. [7] [8] He found that the Foundation was not legally required to check the information on Wikipedia, and that "Web site hosts cannot be liable under civil law because of information stored on them if they do not in fact know of their illicit nature." [8] He did not rule whether the information was defamatory. [7] [8]
Sylvia Scott Gibson, author of Latawnya, the Naughty Horse, Learns to Say "No" to Drugs , sued Wikipedia for describing her book. [9] Her suit was dismissed. [10]

Other alleged defamation
John Seigenthaler , an American writer and journalist, contacted Wikipedia in 2005 after his article was edited to incorrectly state that he had been thought for a brief time to be involved in the assassinations of John Kennedy and Bobby Kennedy. The content was present in the article for four months. [4] [11] Seigenthaler called Wikipedia a "flawed and irresponsible research tool," and criticized the protection the Communications Decency Act provided to Wikipedia. [4]
The representative for the American Academy of Financial Management , George Mentz, suggested that either phony or incompetent Wikipedia editors were creating legal problems for Wikimedia in May 2013, because of alleged intentional false claims that had been published on Wikipedia. [12]

DMCA takedown notices
Texas Instruments sent a DMCA takedown notice to the Wikimedia Foundation because certain cryptographic keys were made public in the Texas Instruments signing key controversy article. A Wikipedia editor later filed a counter-notice, Texas Instruments did not reply in 10–14 business days as required by the DMCA , and the keys were restored to the article.
Two other articles also came under the purview of Wikipedia's office actions because of the DMCA: Damon Dash and Conventional PCI . [13]

FBI seal controversy
In July 2010, the FBI sent a letter to the Wikimedia Foundation demanding that it cease and desist from using its seal on Wikipedia. [14] The FBI claimed that such practice was illegal and threatened to sue. In reply, Wikimedia counsel Michael Godwin sent a counter notice to the FBI claiming that Wikipedia was not in the wrong when it displayed the FBI seal on its website. [15] He defended Wikipedia's actions and also refused to remove the seal. [16]

NSA lawsuit
In March 2015, the Wikimedia Foundation, along with other groups, sued the National Security Agency over its upstream mass surveillance program . [17]

See also
WebPage index: 00143
Hungarian Wikipedia
The Hungarian Wikipedia ( Magyar Wikipédia ) is the Hungarian/Magyar version of Wikipedia , the free encyclopedia . Started on July 8, 2003, this version reached the 300,000 article milestone in May 2015. [1] As of 25 May 2017 this edition has 410,727 articles and is the 23rd largest Wikipedia edition. [2]

History
The first Wikipedia related to the Hungarian language was created on September 5, 2001 by Larry Sanger , the English language Wikipedia coordinator at the time. He created the address at http://hu.wikipedia.com/ . At that time Wikipedia was still running on UseModWiki . For many months there was little Hungarian content, and there were problems with vandalism.
The Hungarian Wikipedia as it is known today was launched by Péter Gervai on July 8, 2003. [3] On this day, the opening page was made available with a Hungarian interface and in Hungarian, at its current address of http://hu.wikipedia.org/ . Since its launch it has been growing steadily, moving up in the multilingual ranking from the 34th place in 2003 to 18th place in December 2005 [4] and 17th in December 2009, [5] and 19th in September 2011. [1]
On October 31, 2010, the Hungarian Wikipedia contained 179,894 articles with 8,992,153 edits by 38 administrators, 153,779 registered users as well as many unregistered ones. [6]
On 14 January 2013, the Hungarian Wikipedia became the first to enable the provision of interlanguage links via Wikidata .

Milestones
The Hungarian Wikipedia reached the 50,000-article milestone on February 7, 2007, the 100,000th on July 17, [3] 2008, and the 150,000th on December 25, 2009, by which it matched the size of the first complete Hungarian encyclopedia, the Pallas's Great Lexicon . [7] The 200,000-article milestone was reached in September 2011, [1] and it was marked by a new version of the Wikipedia globe showing 200,000 moving onward.
On June 17, 2010 the number of featured articles reached 500. [8]
On 7 May 2015 Hungarian Wikipedia reached 300,000 articles. [9]
The 400,000th article was born on December 15, 2016. [10]

Most-disputed articles
According to a 2013 Oxford University study, most-disputed article on the Hungarian Wikipedia was " Gypsy crime ". [11]

Sources

External links
WebPage index: 00144
Serbian Wikipedia
The Serbian Wikipedia ( Serbian : Википедија на српском језику/Vikipedija na srpskom jeziku ) is the Serbian-language version of the free online encyclopedia Wikipedia . Created on 16 February 2003, it reached its 100,000th article on 20 November 2009 before getting to another notable milestone with the 200,000th article on 6 July 2013.
It currently has 129,022 registered users (640 active ones) and about 349,000 articles, making it the second largest South Slavic Wikipedia (28th overall), just behind Serbo-Croatian Wikipedia (19th overall). [1]
The Serbian Wikipedia uses ZhengZhu's character mapping program to convert between Cyrillic and Latin scripts.

History
Serbian Wikipedia was created on 16 February 2003 along with the Croatian Wikipedia when both split off from the joint Serbo-Croatian Wikipedia . The main page was translated from English into Serbian on 22 April 2003 by an unknown user with IP address 80.131.158.32 (possibly from Freiburg , Germany), and user Nikola Smolenski finished the translation on 24 May.
During September 2003, Smolenski prepared the main page along with creating some basic article stubs. In the October 2003 issue of the Serbian IT magazine Svet kompjutera his article about wikis and Wikipedia got published, [2] leading to a surge of new users, both registered and anonymous. Around the same time, Smolenski also translated the user interface page into Serbian.

Variants
The Serbian language uses two alphabets , Cyrillic and Latin . It also has two official accents : Ekavian and Ijekavian . Combining the scripts and accents give four written variants (Ekavian Cyrillic, Ijekavian Cyrillic, Ekavian Latin, and Ijekavian Latin).
When the Serbian Wikipedia was founded, it used only the Cyrillic alphabet, and both standard dialects. However, since both alphabets are widely used by Serbian native speakers, an effort began to enable the parallel usage of both Cyrillic and Latin alphabets. The first attempt was to use a bot for dynamic transliteration of every article. About 1,000 articles were transliterated before the action was stopped due to technical difficulties. This concept was later abandoned in favor of a model used by the Chinese Wikipedia . After a few months, the software was completed and now every visitor has the option to choose between two alphabets using tabs at the top of each article. There are special tags used to indicate those words which should not be transliterated (for example, names and words written in foreign languages). Anti-transliteration tags in use are:
Though there are still minor technical issues, Cyrillic-Latin transliteration is working successfully.
Ekavian–Ijekavian conversion, however, is much more complicated, and its implementation is not yet complete (it will probably require extensive tables of words in Ekavian and Ijekavian forms). However, despite the difficulties, this is probably the first successful attempt to develop the software which will allow parallel work on all four variants of the Serbian language.

Community
Ever since the inaugural meeting on Tuesday, 15 February 2005, members of the Serbian wiki community have been holding regular gatherings. As of September 2013, 253 meetings took place — mostly in Belgrade, with about a dozen taking place in Novi Sad , along with a few in Niš , Pančevo , and Pirot .
At first congregating at each other's apartments, bars, restaurants, and public parks, by late 2005 community members began gathering at the Belgrade Youth Center , which provided meeting space free of charge. At the very first of these Youth Center meetings on Saturday, 3 December 2005, the community members founded the Wikimedia Foundation 's local chapter for Serbia and Montenegro called Wikimedia Serbia and Montenegro (Викимедија Србије и Црне Горе). At the time, it was only the fifth local Wikimedia Foundation chapter anywhere in the world .
Following the May 2006 Montenegrin referendum whose outcome led to the breakup of the Serbia and Montenegro state union, the local chapter modified its name to Wikimedia Serbia (Викимедија Србије). It is registered as a non-governmental, non-partisan, and non-profit organization and its stated goals include promotion of the creation, gathering and multiplication of free content in Serbian language as well as promotion of the idea that everyone should have equal access to knowledge and education. Later that year in December, the Serbian chapter hosted the very first Wikimedia regional conference for Southeast Europe.
Three more regional conferences were put together over the next several years, all of them hosted by Wikimedia Serbia.
In February 2012, Wikimedia Serbia organized an event called Open Wiki GLAM of Serbia as part of the bigger project of the same name. Standing for Galleries, Libraries, Archives & Museums, GLAM is devoted to the topics of Serbian cultural and historical heritage as well as protection of intellectual property and copyright on the Internet. Later that year in December, Wikimedia Serbia got its own office space located in downtown Belgrade at the beginning of the King Aleksandar Boulevard where most of the Serbian wiki community meetings now began to take place.

Content
Serbian Wikipedia cooperates with the University of Belgrade 's Faculty of Mathematics, Faculty of Physical Chemistry and Faculty of Philology as well as the University of Montenegro 's Faculty of Electrical Engineering. Students of those faculties have made occasional contributions to the Serbian Wikipedia by editing its articles.
Due to the similarity of the varieties of Serbo-Croatian , one of the features is copying and adapting articles from one language version of Wikipedia to another (Serbian, Croatian , Serbo-Croatian and Bosnian .) Another Serbian language project, Serbian Wikinews (as of October 2010) has more than 52,000 articles, so the Wikipedia articles are often accompanied by latest news.
A controversy erupted in 2006 over some 10,000 articles on French communities created by a bot. The problem was that such articles needed transcription , and that process went slowly.
Some 1500 human-written articles, including articles on a number of topics related to social work that even the English Wikipedia doesn't have, were bot-uploaded to the Serbian Wikipedia from the Dictionary of Social Work , after author Ivan Vidanović offered to release them under the GFDL .
During September and October 2007, new articles on more than 4,300 towns in Serbia and 1,250 in Montenegro were created. Already existing articles (about 1,600 towns from Serbia and 80 towns from Montenegro) were manually merged with bot-created articles.
From July 13 to July 17, 2009, about 2,400 articles on artificial satellites from the Soviet-Russian Cosmos program were created, and in August 2009 an additional 7,840 articles about deep sky objects from the New General Catalogue were added.

Reviews and research

Gallery
WebPage index: 00145
Bulgarian Wikipedia
The Bulgarian Wikipedia ( Bulgarian : Българоезична Уикипедия ) is the Bulgarian-language edition of Wikipedia . It is written in Bulgarian (Cyrillic) Alphabet . As of 25 May 2017 it has 230,443 articles and is the 33rd largest Wikipedia edition. [1] It was founded on 6 December 2003, and on 12 June 2015 it passed the 200,000 articles threshold.

Users
At this moment Bulgarian Wikipedia has 213580 registered users and 729 of them were actively working on articles during the last 30 days. Bulgarian Wikipedia has a Bulgarian alphabet ( Cyrillic ) interface and users can include Cyrillic letters for their Wikipedia names, although the common practice is that the user name is registered in Latin alphabet and the userboxes and all other personal content is in Cyrillic. This is also due to the need of one username for all Wikipedia languages registrations and edits, while many of the Bulgarian Wikipedians also contribute to the English, French, etc. Wikipedias where Cyrillic nicknames may be hard to read or pronounce.

History
The Bulgarian Wikipedia was created on 6 December 2003. In 2005 Bulgarian Wikipedia added its 20,000th article and was the 21st largest Wikipedia at the time. Later in 2007 it was the 30th largest Wikipedia by article count, with over 50,000 articles. [2] [3] On 24 May 2010, the distinctive Wikipedia globe logo for the Bulgarian Wikipedia was temporarily altered to include the number 100,000 to commemorate the 100,000 article milestone, it became the 32nd largest Wikipedia by size and now it holds 33rd place with more than 200,000 articles.

Timeline

Multimedia
As of March 2010 Bulgarian Wikipedia uses only Commons for pictures and multimedia uploads and local uploads are switched off. The existing files are gradually moved to Commons.
WebPage index: 00146
Slovak Wikipedia
The Slovak Wikipedia ( Slovenská Wikipédia ) is the edition of Wikipedia in the Slovak language . It was started on before 23 September 2003, [1] only becoming active in the summer of 2004. It cleared the 15,000-article mark in September 2005 and the 50,000-article mark in August 2006 and the 100,000 article mark in August 2008. The Slovak Wikipedia has over 170,000 articles as of 28 March 2012. It cleared the 200,000-article mark on 5 February 2015. The Slovak Wikipedia is among the largest Slavic-language Wikipedia editions. There is a large amount of short bot-generated articles in the Slovak Wikipedia.

Gallery

External links
WebPage index: 00147
List of Wikipedias
This is a list of the different language editions of Wikipedia ; as of 25 May 2017 there are 296 Wikipedias of which 285 are active.

Wikipedia edition codes
Each Wikipedia has a code, which is used as a subdomain below wikipedia.org. Interlanguage links are sorted by that code. The codes represent the language codes defined by ISO 639-1 and ISO 639-3 , and the decision of which language code to use is usually determined by the IETF language tag policy. Wikipedias also vary by how thinly they slice dialects and variants; for example, the English Wikipedia includes most modern varieties of English (American English, Indian English, South African English, etc.), but does not include other related languages such as Scots , or Anglo-Saxon , all of which have separate Wikipedias. The Spanish Wikipedia includes both Peninsular Castilian and Latin American Spanish ; Malay Wikipedia includes a large number of Malay languages; and so on.
Differences between the ISO mappings and Wikipedia codes include:
Additionally, Wikipedias vary in wikt:orthography at times. Chinese Wikipedia automatically translates from modern Mandarin Chinese into four standard forms: Mainland China and Singapore in simplified Chinese characters, and Taiwan and Hong Kong / Macau in traditional Chinese characters. Belarussian , however, has a separate Wikipedia for the 'normative' orthography (be) and Taraškievica (be-tarask).

List
An approximation to the number of active users is given in powers of ten (see common logarithm ): so "5" means at least 10,000, "4" means at least 1000, "3" means at least 100, and so on.

Detailed list

Notes

Grand total

See also
WebPage index: 00148
Uzbek Wikipedia
The Uzbek Wikipedia ( Uzbek : Oʻzbekcha Vikipediya in Latin script , Ўзбекча Википедия in Cyrillic script ) is the Uzbek-language edition of the free online encyclopedia Wikipedia. It was founded in December 2003. Articles in Uzbek-language edition are written in the Latin script. In August 2012, a Latin-to-Cyrillic converter was added to allow users to view Uzbek Wikipedia's pages in both the Latin and Cyrillic scripts.
The Uzbek Wikipedia is currently blocked in Uzbekistan . [1] [2] [3] While the reasons for the blockage are unrevealed, some believe that the encyclopedia has been blocked because the Uzbek government is concerned about the appearance of articles critical of its actions. [4] Others have speculated that the Uzbek Wikipedia has been blocked simply as an "act of showmanship" because the government of Uzbekistan sees Uzbek-language content as subject to its jurisdiction. [5] The blockage is not very robust: currently, the pages of the Uzbek Wikipedia can be accessed on an HTTPS connection. [6]
Although Uzbekistan has almost 9 million Internet users, [7] there are not many active editors in the Uzbek Wikipedia and a majority of the existing articles are poorly sourced. [4] Since early 2012, however, both the number of active users and well-written articles have increased noticeably. The number of visits to the encyclopedia has also been rising lately. In early 2013, the Uzbek-language Wikipedia ranked first among different editions of Wikipedia in terms of annual page-view growth. [8] The current number of articles in the Uzbek Wikipedia is 128,867.

History
The Uzbek Wikipedia was launched in December 2003. The very first edit was made on the main page of the encyclopedia on 21 December 2003. [9] During 2004, Uzbek Wikipedia remained largely inactive. However, in 2005, the encyclopedia gradually started to grow.
Sometime around the end of September and beginning of October 2011, the Uzbek Wikipedia was blocked in Uzbekistan . Despite the blockage, the encyclopedia started to grow fast in 2012. During that same year, a bot was used to create a large number of articles. In 2013, another bot was used to add all of the articles of the National Encyclopedia of Uzbekistan to the Uzbek Wikipedia.
A mirror of the Uzbek Wikipedia was set up at wiki .zn .uz in 2008. It was created to reduce international traffic for Internet users in Uzbekistan. However, the mirror has been down since June 2012.

Wikiconferences
In 2007, the first Wikiconference ( Uzbek : Vikimajlis ) of Uzbek Wikipedia editors was organized at the Tashkent University of Information Technologies . Participants included those who had not edited Wikipedia before.

WikiSummer
In 2008, a Wikisummer ( Uzbek : VikiYoz ; "yoz" means both "summer" and "to write" in Uzbek) competition was organized with financial support from Uzinfocom, an agency of the State Committee of Communication, Informatization, and Telecommunication Technology of Uzbekistan. The main aim of the competition was to contribute to the expansion and development of the Uzbek Wikipedia. However, few people showed interest in the Wikisummer and the competition was not successful.

OzodWiki
In February 2014, RFE/RL 's Uzbek Service, known locally as Ozodlik radiosi, launched the OzodWiki project to contribute to the development of the Uzbek Wikipedia. [10] [11] A wide range of articles, including interviews with active editors, [11] [12] [13] [14] [15] [16] reviews of existing articles, [12] [13] [17] and lessons on editing Wikipedia [16] were published as part of the project. In addition, selected words and phrases that were used in Ozodlik radiosi's reports were hyperlinked to corresponding entries in the Uzbek Wikipedia to popularize the encyclopedia. [10] The OzodWiki project was "mutually beneficial, enabling Ozodlik users to click through to expanded information resources, while popularizing Wikipedia by driving new topics and audience their way. In addition, Radio Ozodlik recommends current topics for Wikipedia to define, while Wikipedia sources content to Radio Ozodlik." [10]
In a short period of time the OzodWiki project stated to have a positive impact on the Uzbek Wikipedia. [18] While it is unlikely that all of the changes that took place after the launch of OzodWiki were a direct result of the project, a majority of these changes occurred while the project was running. While the main page of the Uzbek Wikipedia was viewed 20,368 times in January 2014, it was viewed 56,274 times in March of the same year. [18] In March 2013, the main page had been visited 20,403 times. In April 2014, the main page was viewed a record 136,592 times. [19] In April 2013, the main page had been visited only 12,134 times.
Compared to January 2014, in February 2014 the Uzbek Wikipedia was edited more actively. While the total number of edits in January was around 1,400, it increased to about 2,200 in February. [18] In other words, the total number of edits increased by 59 percent. As the total number of edits had declined by 32 percent in January compared to the previous month, the 59 percent increase was a significant change.
While in January the average number of new articles created per day was only one, this number increased threefold in February. [20] In other words, on average three articles were created in a given day during the month of February. In October, November, and December 2013, an average of two articles had been created per day.
However, in February there was no significant change in the number of new editors. [21] Compared to the previous month, both in January and February the number of new editors who made at least ten edits increased by only three people: in December 2013, the Uzbek Wikipedia had 183 editors; in January 2014 — 186; and, finally, in February 2014 — 189. Still, there were slight increases in the number of new editors in March and April. The Uzbek Wikipedia gained four and six new editors who made at least ten edits in March and April, respectively.
A total of 33 unique articles were published as part of the project. The final OzodWiki article was published in January 2016.

Policies

Script
While articles in the Uzbek Wikipedia are written using the Latin script , historically the Uzbek language has used many different alphabets. Before 1928, Uzbek was written in an Arabic-based alphabet by the literate population. Between 1928 and 1940, it was written in a Latin alphabet which was different from the Latin alphabet that is used today. Starting from 1940, Uzbek began to be written in the Cyrillic alphabet , which remained the predominant form of writing until 1993.
A new Latin alphabet was introduced to Uzbek after Uzbekistan gained independence from the USSR . Currently, the Latin script is used in school and university textbooks, some newspapers, and in some official papers. Since 2004 some official websites have switched over to using the Latin script when writing in Uzbek. However, the use of Cyrillic is still widespread, especially among older Uzbeks and among Uzbeks who live in other countries.
Currently, the Uzbek Wikipedia has a function ("vikifikator", literally "wikifier") which allows editors to easily convert Cyrillic text into Latin while editing. In August 2012, a Latin-to-Cyrillic converter was added to allow users to view Uzbek Wikipedia's pages in both the Latin and Cyrillic scripts. [22]

Other policies
Whereas in the English Wikipedia autoconfirmed status is required to move pages , edit semi-protected pages , and upload files , in the Uzbek edition these actions are not restricted. At the moment there are only 8 administrators in the Uzbek Wikipedia.

Content
The Uzbek Wikipedia lacks articles on contemporary political life in Uzbekistan. [5] However, in 2012 the Uzbek Wikipedia started to grow fast despite being blocked in Uzbekistan and since that time the number of well-written articles on important subjects has increased significantly. In 2013, the coverage of the Uzbek Wikipedia expanded noticeably after all of the articles of the National Encyclopedia of Uzbekistan were added to it using a bot . Currently, the majority of articles on the Uzbek Wikipedia are about populated places, astronomical objects, people, music, and football. [23]
Like in many other Wikipedias, Uzbek Wikipedia editors jointly determine featured and good articles. Currently, there are eleven featured and 22 good articles on the Uzbek Wikipedia. The most comprehensive articles are entries about stars , philosophy, the Republic of Korea , Tehran , Aleppo , Karabulak , Texas, Encyclopædia Britannica , Ali-Shir Nava'i , Cristiano Ronaldo , and the British Empire .

Statistics
The number of articles in the Uzbek Wikipedia reached 10,000 on 5 June 2012. [24] The 10,000th article was on compass . A month later, on 5 July 2012, the article count of the encyclopedia reached 20,000. [25] The 20,000th article was on the topic of meteorology . The Uzbek Wikipedia reached 50,000 articles on 8 November 2012. [26] [27] The 50,000th article was on quadratic equations . The encyclopedia reached 100,000 articles on 20 March 2013. [28] [29] The 100,000th article was on labor force . These increases in the number of articles were mostly achieved with the help of bots .
As of May 2017, the Uzbek Wikipedia has 128,867 articles. There are 32,089 users, 81 of whom have made at least one edit in the last 30 days. At the moment only 8 users have administrator rights. The total number of pages on the Uzbek Wikipedia (including talk pages, categories, etc.) is 616,045. The total number of edits is 2,658,325. The editing depth of the Uzbek Wikipedia, which is a rough indicator of the encyclopedia's collaborative quality, is 61.7. Based on the List of articles every Wikipedia should have , the Uzbek Wikipedia ranks 64th. [30]

Censorship
The entire Wikipedia has been briefly blocked twice in Uzbekistan, in 2007 and 2008. The Uzbek Wikipedia was blocked in Uzbekistan around the end of September and beginning of October 2011, but caught the attention of the international press only in late February 2012 following RFE/RL 's report about the blockage on 16 February 2012. [3] Initially Internet users in Uzbekistan trying to access Uzbek-language pages got redirected to msn.com of Microsoft . [3] Currently the pages of the encyclopedia simply fail to load. Users in Uzbekistan can easily open Wikipedia articles in other languages, only Uzbek-language articles have been blocked. [2]
The reason for the blockage of the Uzbek Wikipedia is not known. Some have expressed the view that the encyclopedia has been blocked because the Uzbek government is concerned about the appearance of articles critical of its actions. [4] Sarah Kendzior, an American anthropologist , has speculated that the Uzbek Wikipedia has been blocked simply as an "act of showmanship" because the government of Uzbekistan sees Uzbek-language content as subject to its jurisdiction. [5]
The blockage is not very robust: currently, the articles in the Uzbek-language Wikipedia can be accessed on an HTTPS connection. Therefore, in 2013 Google started indexing articles on the Uzbek Wikipedia with HTTPS by default.
WebPage index: 00149
Azerbaijani Wikipedia
The Azerbaijani Wikipedia ( Azerbaijani : Azərbaycanca Vikipediya; آذربایجانجا ویکی‌پدیا ) is a Wikipedia in Azerbaijani language (the editing interface and the main page have been temporarily accessible via South Azerbaijani subtitles), launched in January 2002. [1] As of 30 November 2010 it had 42,518 articles ( size 4, including 20 featured articles ) and 14,523 uploaded files in its content, as well as 23,766 registered users (including seven administrators and two bureaucrats ). [2] The editorial process is being supported by forty bots .
Within the first two years of its existence the article number in Azerbaijani Wikipedia reached 3,000. [3] As of November 2010 the local list of requested articles contains ten entries (seven biographical, two scientific and one unspecified). Pending November 2010 translation requests comprise three English and three Turkish entries.
The categorization is maintained through nine topic categories: culture, geography, history, life, mathematics, nature, science, society and technology. Hidden categories embrace 111 entries. The backlog category contains 14 subcategories. [ citation needed ]
There are also fourteen portals about architecture, biology, chemistry, history, Islam, geography, literature, medicine, philosophy, Azerbaijani cinema, Azerbaijani military, as well as country-specific ones about Georgia, Turkey and Azerbaijan itself. [ citation needed ]
Azerbaijani Wikipedia is constantly increasing its number of articles, but at some point in 2015 this number somewhat decreased returning to values smaller than 100,000.

History
In 2010, Azerbaijani Wikipedia books published by professor Rasim Aliguliyev and senior scientist Irada Alakbarova. [4] The book edited by Alovsat Aliyev. [4]

The article timeline

Community efforts
The first meetup was held in Baku on 6 December 2009. The event was organized in order to establish relations of friendship and familiarity between Wikipedians and a number of other issues – including technical problems and prospects for future development.
Recently in order to solve the problems, an emergency meeting was organized on 23 October 2010 in Shaki . About 9 users participated in it.

Logo
WebPage index: 00150
Malagasy Wikipedia
The Malagasy Wikipedia is the Malagasy language edition of Wikipedia . Launched in April 2004, [1] It currently has 83,444 articles, making it the 63nd largest Wikipedia by article count. This Wikipedia has 3 administrators along with 13,671 registered users and 25 active users. [2]
WebPage index: 00151
List of Wikipedias
This is a list of the different language editions of Wikipedia ; as of 24 May 2017 there are 296 Wikipedias of which 285 are active.

Wikipedia edition codes
Each Wikipedia has a code, which is used as a subdomain below wikipedia.org. Interlanguage links are sorted by that code. The codes represent the language codes defined by ISO 639-1 and ISO 639-3 , and the decision of which language code to use is usually determined by the IETF language tag policy. Wikipedias also vary by how thinly they slice dialects and variants; for example, the English Wikipedia includes most modern varieties of English (American English, Indian English, South African English, etc.), but does not include other related languages such as Scots , or Anglo-Saxon , all of which have separate Wikipedias. The Spanish Wikipedia includes both Peninsular Castilian and Latin American Spanish ; Malay Wikipedia includes a large number of Malay languages; and so on.
Differences between the ISO mappings and Wikipedia codes include:
Additionally, Wikipedias vary in wikt:orthography at times. Chinese Wikipedia automatically translates from modern Mandarin Chinese into four standard forms: Mainland China and Singapore in simplified Chinese characters, and Taiwan and Hong Kong / Macau in traditional Chinese characters. Belarussian , however, has a separate Wikipedia for the 'normative' orthography (be) and Taraškievica (be-tarask).

List
An approximation to the number of active users is given in powers of ten (see common logarithm ): so "5" means at least 10,000, "4" means at least 1000, "3" means at least 100, and so on.

Detailed list

Notes

Grand total

See also
WebPage index: 00152
List of Wikipedias
This is a list of the different language editions of Wikipedia ; as of 25 May 2017 there are 296 Wikipedias of which 285 are active.

Wikipedia edition codes
Each Wikipedia has a code, which is used as a subdomain below wikipedia.org. Interlanguage links are sorted by that code. The codes represent the language codes defined by ISO 639-1 and ISO 639-3 , and the decision of which language code to use is usually determined by the IETF language tag policy. Wikipedias also vary by how thinly they slice dialects and variants; for example, the English Wikipedia includes most modern varieties of English (American English, Indian English, South African English, etc.), but does not include other related languages such as Scots , or Anglo-Saxon , all of which have separate Wikipedias. The Spanish Wikipedia includes both Peninsular Castilian and Latin American Spanish ; Malay Wikipedia includes a large number of Malay languages; and so on.
Differences between the ISO mappings and Wikipedia codes include:
Additionally, Wikipedias vary in wikt:orthography at times. Chinese Wikipedia automatically translates from modern Mandarin Chinese into four standard forms: Mainland China and Singapore in simplified Chinese characters, and Taiwan and Hong Kong / Macau in traditional Chinese characters. Belarussian , however, has a separate Wikipedia for the 'normative' orthography (be) and Taraškievica (be-tarask).

List
An approximation to the number of active users is given in powers of ten (see common logarithm ): so "5" means at least 10,000, "4" means at least 1000, "3" means at least 100, and so on.

Detailed list

Notes

Grand total

See also
WebPage index: 00153
Bengali Wikipedia
The Bengali Wikipedia ( Bengali : বাংলা উইকিপিডিয়া , Bānglā u'ikipiḍiẏā ) is the Bengali language version of Wikipedia , run by the Wikimedia Foundation . It also has a phonetic Latin alphabet to Bengali script tool so Latin alphabet keyboards can be used to type Bengali without downloading any software.
Bengali Wikipedia surpassed 10,000 articles in October 2006, becoming the second South-Asian language to do so. [1] Also, as of November 13, 2009, it was the 67th largest Wikipedia by article count (with more than 20,500 articles). [ citation needed ] As of October 2011 [update] , it has over 33,000 articles and is the 76th largest Wikipedia by article count. [ citation needed ] In 9 May 2017, total article crossed 50,116 articles. [2]
The total article count of the Bengali Wikipedia currently stands at a figure of 50,116 articles. [3]

Users and editors

See also
WebPage index: 00154
Malayalam Wikipedia
The Malayalam Wikipedia ( Malayalam : മലയാളം വിക്കിപീഡിയ ) is the Malayalam language edition of Wikipedia , a free and publicly editable online encyclopedia , and was launched on December 21, 2002. The project is the leading Wikipedia among other South East Asian language Wikipedias in various quality matrices. [1] It has grown to be a wiki containing more than 30,000 articles as of April 2013 [update] . [2]

History കേരളത്തിലെ ആദ്യത്തെ ഇംഗ്ലീഷ് മീഡിയം സ്കൂൾ

Beginning
Malayalam language Wikipedia is available in the wikipedia.org domain from 2002 December 21. User Vinod M. P. had taken initiatives for it. For the two years following its creation, he had been the key person striving to keep the wiki active. Almost all the early users of Malayalam Wikipedia were non-resident Malayalees . The growth of the Wikipedia during these times was heavily constrained due to OS and browser related issues, rendering issues, Unicode related issues, and so on.

Initial growth phase
By the middle of 2004, unicode and input tools had become popular. Blogging in Malayalam became widespread. Wikipedians started to use these tools and the Wikipedia reached 100 articles by December 2004. More users joined by the middle of 2005 and the wiki had its first sysop by September 2005. He became the first bureaucrat of the wiki after a month and the wiki became self-sufficient in terms of administration.
The year 2006 saw a number of users joining the wiki, following the widespread usage of Malayalam computing tools. 500th article was born on April 10, 2006; the following September the article count reached 1000. On January 15, 2007, this became 2000 and on June 30 it became 3000.

Media coverage and increased growth
The first major Media coverage about the Malayalam Wikipedia was on September 2, 2007, when Malayalam daily newspaper Mathrubhumi covered Malayalam Wikipedia project extensively in its Sunday Supplement. [3] This generated significant interest in the Wikipedia project and large number of users joined the project and started to contribute. [4] The subsequent growth was exponential.
While the article count increased, extreme care was taken to maintain the quality of articles. The page depth of the wiki remains high at 301 (as of March 2010 [update] ). [5] When the Wikipedia crossed 10,000 articles on June 1, a number of print and online newspapers covered the story. [6] [7] [8] [9] [10] [11] [12] [13] [14] [15] [16] Recently, a Malayalam newspaper 'Madhyamam' spent an editorial for the contributors of Malayalam Wikipedia. [17] The mobile version of the Malayalam Wikipedia was launched on February 2010.

Fonts and input methods
Although many Malayalam Unicode fonts are available for old and new Malayalam lipi, most users opt for fonts like AnjaliOldLipi, Rachana and Meera which follows the traditional Malayalam writing style. Early editors adopted specialized Malayalam Unicode input tools based on the Varamozhi keyboard , a phonetic transliteration device. The project has an inbuilt input tool integrated to it.malayalam is now losing its population because all the population is using english as a medium of communication.

Users and editors

See also
WebPage index: 00155
West Frisian Wikipedia
The West Frisian Wikipedia ( Frisian : Frysktalige Wikipedy ) is the West Frisian-language edition of the free online encyclopedia , Wikipedia . There are also editions of Wikipedia in the other two Frisian languages , one in North Frisian and the other in Saterland Frisian . It was started on September 2, 2002. On July 11 there were approximately 25,023 articles and 11,584 registered users.

Milestones

External links
WebPage index: 00156
List of Wikipedias
This is a list of the different language editions of Wikipedia ; as of 26 May 2017 there are 296 Wikipedias of which 285 are active.

Wikipedia edition codes
Each Wikipedia has a code, which is used as a subdomain below wikipedia.org. Interlanguage links are sorted by that code. The codes represent the language codes defined by ISO 639-1 and ISO 639-3 , and the decision of which language code to use is usually determined by the IETF language tag policy. Wikipedias also vary by how thinly they slice dialects and variants; for example, the English Wikipedia includes most modern varieties of English (American English, Indian English, South African English, etc.), but does not include other related languages such as Scots , or Anglo-Saxon , all of which have separate Wikipedias. The Spanish Wikipedia includes both Peninsular Castilian and Latin American Spanish ; Malay Wikipedia includes a large number of Malay languages; and so on.
Differences between the ISO mappings and Wikipedia codes include:
Additionally, Wikipedias vary in wikt:orthography at times. Chinese Wikipedia automatically translates from modern Mandarin Chinese into four standard forms: Mainland China and Singapore in simplified Chinese characters, and Taiwan and Hong Kong / Macau in traditional Chinese characters. Belarussian , however, has a separate Wikipedia for the 'normative' orthography (be) and Taraškievica (be-tarask).

List
An approximation to the number of active users is given in powers of ten (see common logarithm ): so "5" means at least 10,000, "4" means at least 1000, "3" means at least 100, and so on.

Detailed list

Notes

Grand total

See also
WebPage index: 00157
Sundanese Wikipedia
Sundanese Wikipedia ( Sundanese Wikipedia basa Sunda ) is the edition of Wikipedia in the Sundanese language . It has 22,694 articles.
WebPage index: 00158
Ossetian Wikipedia
The Ossetian Wikipedia ( Ossetian : Ирон Википеди ) is the Ossetian-language edition of the free online encyclopedia Wikipedia . It was created on 28 February 2005. [2] [3] With approximately 10,000 articles, it is currently the 134th-largest Wikipedia as measured by the number of articles. [4] Since its creation, the Ossetian Wikipedia has been called "what is perhaps the only website written entirely in Ossetian." [5]
On 3 March 2010, the Ossetian Wikipedia made headlines in local newspapers for reaching a double milestone. The edition was 5 years old and had just passed the 5000 articles threshold. [6]

Notes
WebPage index: 00159
Dutch Low Saxon Wikipedia
The Nedersaksische Wikipedie , the Dutch Low Saxon edition of Wikipedia , was started on 24 March 2006. It collects articles written in any Low German dialect indigenous to the Netherlands , as well as dialects from the border region, which are rendered in Dutch-based spelling. As of May 2017, this edition has about 6,700 articles. Among other features, there are spoken articles, "showcased" articles and provincial portals.
The Dutch Low Saxon Wikipedia has been cited in the Dutch press, and Low Saxon institutes in the Netherlands have noted it and contributed to it. [1] [2] [3]

Characteristics
There are a small number of active contributors. Many of the articles, for instance on Dutch Low Saxon writers, do not exist on any other Wikipedia. Existing illustrations on Wikimedia Commons are usually linked to illustrate articles. According to the list of Wikipedias , the Dutch Low Saxon Wikipedia has a relatively large number of edits and images and above-average article depth for Wikipedias that have between 1,000 and 9,999 articles (depth of 23 compared to a mode of 8, a median of 11 and a mean of 18). [4]

History
The Dutch Low Saxon Wikipedia was preceded by three years by a Low German ( Plattdüütsch ) one. [5] The Low German dialects (which are not standardised) stretch from all of Northern Germany to the Northeast Netherlands. In practice, however, the Plattdüütsch Wikipedia was a German undertaking, following German-based spelling conventions. This spelling, combined with the growing divide since modern times between Dutch and German plat(t) due to the influence of the Dutch and German standard languages respectively, left a gap to be filled.
The eventual creation of a Dutch Low Saxon Wikipedia was delayed for many months due to discussion on whether this collection of dialects – recognised and protected by the Dutch government in the framework of the European Charter for Regional or Minority Languages – merited their own Wikipedia. Proponents made the case that the Dutch varieties are too different from the German ones – at least in writing – to be gathered under the same roof. Some vocal opposition centred on the question of whether the often rather distinct Low Saxon dialects spoken in the Netherlands could be taken as a whole and accommodated within one Wikipedia. The same countrywide grouping, however, had been opted for in the case of the Plattdüütsch Wikipedia. [6]
Throughout its existence, the number of structural contributors to the Dutch Low Saxon Wikipedia has remained low, about three to five such users being active in any given period. Most of the structural contributors have administrator rights. The scarcity of committed writers has remained characteristic in spite of several attempts to boost their number, for instance through appeals on radio shows to which individual contributors have been invited.
The frequency of vandalism - creation of nonsense articles, removal of content, disparaging remarks - has varied, it sometimes being a daily occurrence and at other times occurring little. Vandalism is normally remedied within the day, often within hours or minutes.

Accommodation of the various dialects
Once created, the Dutch Low Saxon Wikipedia took off well, with arrangements being made to accommodate the dialect subgroups and their different spelling conventions (there are several established spelling systems [7] ). Each contributor can write in their own dialect, categorizing an article in accordance with the dialect used. If the article is a stub, another contributor can expand it and bring it in line with their own dialect. If the article is longer, a further contributor using a different dialect will ask for their alterations to be rendered into the original dialect.
Another feature is that many articles on animals, plants, objects and activities include an overview of what is often a plethora of local names, differing per region and even per village.

See also
WebPage index: 00160
Assamese Wikipedia
The Assamese Wikipedia is the Assamese language edition of Wikipedia, the free encyclopedia. Its domain came into existence on June 2, 2002. In July 2015, it had reached 3,600 articles. [2] [3]
It now has 4,566 articles with 16,137 registered users.
The first Assamese Wikipedia workshop was organized in Guwahati University on January 29, 2012 and later another on February 1, 2012 at the Tezpur University in Tezpur to inform people how to edit and add to the wiki. Later on many other workshops are organized by the community members in different places Assam . [4] [5] [6] [7]
WebPage index: 00161
Classical Chinese Wikipedia
The Classical Chinese Wikipedia ( Classical Chinese : 文言維基大典; pinyin : Wényán wéijī dàdiǎn ; Written vernacular Chinese : 文言文維基百科; pinyin : Wényánwén wéijī bǎikē ) is the Classical Chinese edition of Wikipedia , run by the Wikimedia Foundation . It started on 31 July 2006, coincidentally the Qixi Festival [1] [2] (7th day of the 7th month on the Chinese calendar ). As of October 2013, the Classical Chinese Wikipedia had a total of 3126 articles, mostly related to the History of China .

Background
Editors are mainly Wikipedians who have studied Classical Chinese , thus resulting in the slow growth of editors with very few readers. With the increase of articles related to the History of China , there are over 500 users as of May 2007, including those with no ability in Classical Chinese but are interested in the language. There are 10 active editors (Registered/signed in users who made 5 or more edits in a month) as of August 2013, according to the statistics provided by the Wikimedia Foundation. [3]

History

Subtitle / Slogan
The Classical Chinese Wikipedia has a subtitle: 文辭雅正 學問淵深 [6] ( Wéncí yǎzhèng, xuéwèn yuānshēn ). It means, "Correct diction, profound knowledge".

Naming and Logo problems
Just like Chinese Wikipedia , there was a need to have an official name for Wikipedia in the Classical Chinese language. There were suggestions to use 百科 ( bǎikē , meaning 'encyclopedia') as part of the name, which is also in use for Chinese Wikipedia. The names Erya (爾雅, ěryǎ ) and 大典 ( dàdiǎn , literally 'great dictionary', meaning 'encyclopedia') were also considered. Finally, the official Classical Chinese name of Wikipedia, 維基大典 was adopted. [7] The name 爾雅 ěryǎ is adopted for the Classical Chinese edition of Wiktionary .
At the start, the logo of the Classical Chinese Wikipedia had been in English . Later, the official name and slogan was inscribed into the logo. [8] The logo is written right-to-left, akin to traditional Chinese writing styles.

Administration
There is only one administrator (有秩 Yǒu zhì ) in Classical Chinese Wikipedia, Itsmine , [9] a Hong Konger . He is elected by Wikipedians with 18 supporting and zero opposing, [10] meeting the criteria of having at least 10 supporting and at least 70% majority. [11] He also has an alternative user name, Kongming Jushi , ( Chinese : 孔明居士 ; pinyin : Kǒngmíng jūshì ; literally: "Householder/layperson Kongming"), which he uses to address himself and to sign off when replying to talk pages. He admires Zhuge Liang , [12] who had the style name of Kongming.
He contributed to the vernacular Chinese edition of Wikipedia with the username of Itsminecookies and is awarded various service awards . [13] He also contributed to the English Wikipedia . [14]
Articles posted in Classical Chinese Wikipedia are classified in three categories, using the flagged revisions policy from German Wikipedia. [15]

Classification of pages
The Classical Chinese Wikipedia follows the way German Wikipedia classifies its pages. The German Wikipedia classifies its pages into eight main portals on the Main Page . [16] [17] The Classical Chinese Wikipedia classifies its pages into four main portals, namely 經 ( jing , Science), 史 ( shi , History), 子 ( zi , Philosophy) and 集 ( ji , Arts), displayed on the Main Page . [18] [19]

Unique traits
Classical Chinese Wikipedia is portrayed as a book or an encyclopedia instead of an online encyclopedia and the terms used are usually archaic , though there are instances where the internet or scientific terms cannot be avoided.
For example, the article about Science is titled as 格致 ( gezhi ), which is the term for Science used in Ancient China, which stands for 格物致知 ( gewu zhizhi ). 科學 kexue , the modern term for Science, derived from Japanese [20] is redirected to the page. While in the article, it states that at the end of the article, "Known as kexue by people today". [21] However in its categories, the term kexue cannot be avoided. [22] It uses ancient terms as much as possible, and the moderm term will be used if not possible. [23] For instance, a mathematician is known as 疇人 Chóu rén instead of 數學家 shuxue jia , while Economics is known as 計學 Jì xué instead of 經濟學 Jingji xue .

Chinese honorifics
Users communicate with each other in talk pages using Chinese honorifics. The second party can be addressed as 君 ( Jūn ), while a user addresses oneself in terms such as 吾 wú . Also, Wikipedia addresses Wikipedians as 子 ( zi ) or 爾 ( er ), polite forms for 'you', for instance when editing a page.

Main Page
The Main Page is known as 卷首 ( Juànshǒu ), [24] 卷 meaning 'volume/book/roll', 首 meaning 'first/head'. Juànshǒu means "forward section of the book/publications/ frontispiece ", and is an ancient Chinese term.

Loading a random article
The tool of loading a random article is known as 清風翻書 Qīngfēng fān shū . The term originated from a poetry written by Xu Jun , an official of the Qing Dynasty , with the two lines: [25] "清風不識字 何必亂翻書？" Qīngfēng bù shí zì, hébì luàn fān shū .
In one of his petitions , he wrote the word "陛下" bixia (Your Majesty) wrongly. Instead, he wrote 陛 as 狴 . Yongzheng Emperor dismissed him immediately. He later sent people to investigate about him and found the two lines of the poem. Based on this, Xu Jun was wrongly accused. The imperial court insisted that 清風 qīngfēng refers to the Qing (清) Dynasty , and thus the poem reads, "(Since) the Qing (Dynasty) are illiterate, why bother flipping books (i.e. read/study)?" Xu Jun was accused of committing the crime of defaming the imperial court, thus sentenced to death.
In fact, the story is, Xu Jun was studying by the window while a gentle breeze (清風) resulted in the pages of the book being flipped. The purpose of the poem is to depict, "The gentle breeze is illiterate, so why bother to flip books?" This is an example of Literary Inquisition .
The tool is known as 清風翻書 in Classical Chinese Wikipedia, such that it is useful for people who are bored to come across an interesting article in an encyclopedia . Instead, it is portrayed as flipping a random page from a book out of boredom , for people who do not like to study.

Page to view sources of a protected page
The View Source link is known as 案 Àn , which means 'bases/proof', literally 'case'.
When one mouses over to the View Source (i.e. 案) link of a protected page, a line of words will pop up:
In English Wikipedia , on a View Source page of a fully protected page, the instructions state,
However, in Classical Chinese Wikipedia, on a View Source page of a protected page, the instructions state: [26]

Log in page
"Log in" is known as 登簿 ( pinyin : Dēng bù ). [27] 登 literally means 'step afoot/check in/enter' while 簿 literally means 'account/register'.
In the log in page , password is known as 符節 ( fujie ), which means Fu (tally) . In the past, a fu is a tally stick as a proof to be authorised to exercise powers on behalf of the Emperor.

Revision history page
Revision history is known as 誌 Zhì , literally annals/record. As mentioned that the Classical Chinese Wikipedia is portrayed as a book and terms used are archaic, we firstly assume that Wikipedia is an ancient book and whatever changes or edits we make to a page of the book, we are required to record down officially the exact date, time, and the changes made.
If we assume the situation to be so, then even vandalism is recorded down automatically without the user's will.
When one mouses over to the View History (i.e. 誌) link, a line of words will pop up:
The Undo link is known as 悔 huǐ , literally 'regret/repent'.
When one tries to undo a page and there happens to have an edit conflict , a line of words will be displayed to the user.
In English Wikipedia , the line of words reads:
In Classical Chinese Wikipedia, the line of words reads:
Assuming a few people are editing a page at the same time, and want to submit the edits. However there are different versions of the edits, causing a dispute between editors. They officially record their disputes in the revision history. A trial is conducted to resolve the dispute or conflict, but there will be an order to seal the conflict , prohibiting the trial to be disclosed.

Poetry appreciation
The Classical Chinese Wikipedia contains a poetry appreciation section for readers to appreciate Chinese poetry and to add on to self-cultivation. [28] The poetry exhibits the beauty in them and benefits readers.

Comparison between Chinese and Classical Chinese Wikipedia term
The following are comparisons of Wikipedia terms used between Chinese Wikipedia and Classical Chinese Wikipedia.

Namespaces
Namespaces are known as 名字空間 Míngzì kōngjiān in written Vernacular Chinese .

Classical Chinese edition of Wiktionary
The Classical Chinese edition of Wiktionary is known as 維基爾雅 ( pinyin : Wéijī ěr yǎ ; literally: "Wiki Erya"). [29] [30] However, interestingly, it is not affiliated to the multi-lingual Wiktionary (www.wiktionary.org). It is located in Classical Chinese Wikipedia itself.
The vernacular Chinese edition of Wiktionary is affiliated to the multi-lingual Wiktionary and records vernacular Chinese terms and explanations in vernacular Chinese. Also, entry pages are also linked to other languages where a Chinese entry is explained in another language other than Chinese. Moreover, words from other languages can be searched in Chinese Wiktionary for explanations in Chinese. Thus, Wiktionary is indeed a multi-lingual dictionary.
The Classical Chinese edition of Wiktionary records Classical Chinese terms and are explained in Classical Chinese according to ancient dictionaries. Ancient terms may be profound, a character has various meanings and ambiguity, and the purpose of Weiji Erya is to benefit Wikipedians and entries are recorded. Modern, scientific and technological terms are also recorded because of Western influence today.
However, only meanings of characters or one-character term are recorded in Weiji Erya . Hence, the Classical Chinese edition of Wiktionary functions as a 'single-character dictionary ' ( Chinese : 字典 ; pinyin : zìdiǎn ; literally: "Character-dictionary").
The Classical Chinese Wiktionary is under the Wikipedia namespace in Classical Chinese Wikipedia. Therefore, "Classical Chinese Wiktionarians" are also or are in fact Wikipedians. Strictly speaking, it is not affiliated to the multi-lingual Wiktionary, thus entries are still articles in Wikipedia. It is still managed by Classical Chinese Wikipedia administrator.
The radical index (Categorising characters according to their radicals ) [31] is provided for people who cannot type the character.

Script
The Classical Chinese Wikipedia retains most of the Chinese culture, tradition and history in the articles and other pages.
It uses and is written in Traditional Chinese characters exclusively. It does not use Arabic numerals but uses unified Chinese numerals . [32] However, due to the limitations in technology [33] of writing scripts in vertical writing format , the Classical Chinese Wikipedia could only publish Classical Chinese poetry and certain articles/pages in vertical writing. At the same time, it adopts modern Chinese punctuation to replace the traditional way of annotating without any punctuations ( simplified Chinese : 句读 ; traditional Chinese : 句讀 ; pinyin : Jùdòu ).

Year numbering
In the articles created, the type of calendar (e.g. Gregorian calendar , Chinese calendar ) used is not unified nor consistent, and the year numbering system also vary.
However, [34] for anything related to the Chinese history or Chinese, it must be displayed in the regnal year of the Chinese era name or other numbering systems, e.g. the Sexagenary cycle , to be linked to the Gregorian year. For instance, [[五八一年|開皇元年]] ( [[581|Kaihuang 1]] ). [35] For Western-related topics, the Gregorian calendar or year can be displayed. For instance, [[五八一年]] ( [[581]] ). But the Chinese calendar/year can also be used.
As for Chinese topics, the day and month of dates before 1 January 1912 (establishment of the Republic of China ) shown must follow the Chinese calendar.

Limitations
As there are fewer people proficient in Literary Chinese writing than those proficient in Modern Chinese , and that majority are used to reading Modern Chinese, with the fact that there are many people not knowing much about Classical Chinese, resulting in the quality of quite a number of edits not reaching the Literary Chinese Standard. This causes limitations in the development of the Classical Chinese Wikipedia. In addition, biographies of living persons cannot be written on Classical Chinese Wikipedia.
Most of the articles are short articles. Due to the lack of Classical Chinese talents and Wikipedians , most of the articles did not have sections. Certain articles cannot be expanded to be more informative or having citations in pages. The development of the Classical Chinese Wikipedia is slow and certain features of Wikipedia cannot be implemented.

Controversies
Classical Chinese Wikipedia received several negative feedback and criticism from the Chinese Wikipedia community, some asking for the removal of it.

Upsuper
The user Upsuper or Crazy Ghost (疯鬼) thinks that it should be removed because an encyclopedia gives readers some references and allows readers to gain new knowledge, and "almost no one would visit the Classical Chinese Wikipedia" for references of contemporary information that the vernacular Chinese edition contains nor gaining new knowledge in the present world.

Related
Classical Chinese is an archaic register of Chinese with grammar and vocabulary drawn from classical works.
Classical Chinese is a member to the family of Chinese languages. For other members, some Wikipedias have seen established among Wikimedia projects.

See also
WebPage index: 00162
List of Wikipedias
This is a list of the different language editions of Wikipedia ; as of 25 May 2017 there are 296 Wikipedias of which 285 are active.

Wikipedia edition codes
Each Wikipedia has a code, which is used as a subdomain below wikipedia.org. Interlanguage links are sorted by that code. The codes represent the language codes defined by ISO 639-1 and ISO 639-3 , and the decision of which language code to use is usually determined by the IETF language tag policy. Wikipedias also vary by how thinly they slice dialects and variants; for example, the English Wikipedia includes most modern varieties of English (American English, Indian English, South African English, etc.), but does not include other related languages such as Scots , or Anglo-Saxon , all of which have separate Wikipedias. The Spanish Wikipedia includes both Peninsular Castilian and Latin American Spanish ; Malay Wikipedia includes a large number of Malay languages; and so on.
Differences between the ISO mappings and Wikipedia codes include:
Additionally, Wikipedias vary in wikt:orthography at times. Chinese Wikipedia automatically translates from modern Mandarin Chinese into four standard forms: Mainland China and Singapore in simplified Chinese characters, and Taiwan and Hong Kong / Macau in traditional Chinese characters. Belarussian , however, has a separate Wikipedia for the 'normative' orthography (be) and Taraškievica (be-tarask).

List
An approximation to the number of active users is given in powers of ten (see common logarithm ): so "5" means at least 10,000, "4" means at least 1000, "3" means at least 100, and so on.

Detailed list

Notes

Grand total

See also
WebPage index: 00163
National Library of the Czech Republic
The National Library of the Czech Republic ( Czech : Národní knihovna České republiky ) is the central library of the Czech Republic . It is directed by the Ministry of Culture . The library's main building is located in the historical Clementinum building in Prague, where approximately half of its books are kept. The other half of the collection is stored in the district of Hostivař . [3] The National Library is the biggest library in the Czech Republic, in its funds there are around 6 million documents. The library has around 60,000 registered readers. As well as Czech texts, the library also stores older material from Turkey, Iran and India. [4] The library also houses books for Charles University in Prague . [5]
The library won international recognition in 2005 as it received the inaugural Jikji Prize from UNESCO via the Memory of the World Programme for its efforts in digitising old texts. [6] [7] The project, which commenced in 1992, involved the digitisation of 1,700 documents in its first 13 years. [4]

Collections
The most precious medieval manuscripts preserved in the National Library are the Codex Vyssegradensis and the Passional of Abbes Kunigunde .

Proposed new building
In 2006 the Czech parliament approved funding for the construction of a new library building on Letna plain, between Hradčanská metro station and Sparta Prague's football ground, Letná stadium . [8] [9] In March 2007, following a request for tender , Czech architect Jan Kaplický was selected by a jury to undertake the project, with a projected completion date of 2011. [10] Later in 2007 the project was delayed following objections regarding its proposed location from government officials including Prague Mayor Pavel Bém and President Václav Klaus . [9] [11] Plans for the building had still not been decided in February 2008, with the matter being referred to the Office for the Protection of Competition in order to determine if the tender had been won fairly. [12] Later in 2008, Minister of Culture Václav Jehlička announced the end of the project, following a ruling from the European Commission that the tender process had not been carried out legally. [13]

Incidents
The library was affected by the 2002 European floods , with some documents moved to upper levels to avoid the excess water. [14] Over 4,000 books were removed from the library in July 2011 following flooding in parts of the main building. [15] There was a fire at the library in December 2012, but nobody was injured in the event. [16]

See also
WebPage index: 00164
Definition of Free Cultural Works
The Definition of Free Cultural Works is a definition of free content from 2006. The project evaluates and recommends compatible free content licenses .

History
The Open Content Project by David A. Wiley in 1998 was a predecessor project which defined open content . In 2003 Wiley joined the Creative Commons as "Director of Educational Licenses" and announced the Creative Commons and their licenses as successor to his Open Content project . [2] [3]
Therefore, Creative Commons' Erik Möller [4] in collaboration with Richard Stallman , Lawrence Lessig , Benjamin Mako Hill , [4] Angela Beesley, [4] and others started in 2006 the Free Cultural Works project for defining free content . The first draft of the Definition of Free Cultural Works was published 3 April 2006. [5] The 1.0 and 1.1 versions were published in English and translated into some languages. [6]
The Definition of Free Cultural Works is used by the Wikimedia Foundation . [7] In 2008, the Attribution and Attribution-ShareAlike Creative Commons licenses were marked as "Approved for Free Cultural Works". [8]
Following in June 2009, Wikipedia migrated to use two licenses : the Creative Commons Attribution-ShareAlike as main license, additionally to the previously used GNU Free Documentation License (which was made compatible [9] ). [10] An improved license compatibility with the greater free content ecosystem was given as reason for the license change. [11] [12]
In October 2014 the Open Knowledge Foundation 's Open Definition 2.0 for Open Works and Open Licenses described "open" as synonymous to the definition of free in the "Definition of Free Cultural Works" (and also the Open Source Definition and Free Software Definition ). [13] A distinct difference is the focus given to the public domain and that it focuses also on the accessibility (" Open access ") and the readability (" open formats "). The same three creative commons licenses are recommended for open content ( CC BY , CC BY-SA , and CC0 [14] [15] [16] ) as additionally three for open data intended own licenses, the Open Data Commons Public Domain Dedication and Licence (PDDL), the Open Data Commons Attribution License (ODC-BY) and the Open Data Commons Open Database License (ODbL).

"Free cultural works" approved licenses

See also
WebPage index: 00165
Berne Convention
The Berne Convention for the Protection of Literary and Artistic Works , usually known as the Berne Convention , is an international agreement governing copyright , which was first accepted in Berne , Switzerland , in 1886.
The Berne Convention formally mandated several aspects of modern copyright law; it introduced the concept that a copyright exists the moment a work is "fixed", rather than requiring registration . It also enforces a requirement that countries recognize copyrights held by the citizens of all other parties to the convention.

Content
The Berne Convention requires its parties to treat the copyright of works of authors from other parties to the convention (known as members of the Berne Union ) at least as well as those of its own nationals. For example, French copyright law applies to anything published or performed in France, regardless of where it was originally created.
In addition to establishing a system of equal treatment that internationalised copyright amongst parties, the agreement also required member states to provide strong minimum standards for copyright law.
Copyright under the Berne Convention must be automatic; it is prohibited to require formal registration. However, when the United States joined the Convention 1 March 1989, [1] it continued to make statutory damages and attorney's fees only available for registered works.

Applicability
Under Article 3, the protection of the Convention applies to nationals and residents of countries that are party to the convention, and to works first published or simultaneously published (under Article 3(4), "simultaneously" is defined as "within 30 days" [2] ) in a country that is party to the convention. [2] Under Article 4, it also applies to cinematic works by persons who have their headquarters or habitual residence in a party country, and to architectural works situated in a party country. [3]

Country of origin
The Convention relies on the concept of "country of origin". Often determining the country of origin is straightforward: when a work is published in a party country and nowhere else, this is the country of origin. However, under Article 5(4), when a work is published simultaneously in several party countries (under Article 3(4), "simultaneously" is defined as "within 30 days" [2] ), the country with the shortest term of protection is defined as the country of origin. [4]
For works simultaneously published in a party country and one or more non-parties, the party country is the country of origin. For unpublished works or works first published in a non-party country (without publication within 30 days in a party country), the author's nationality usually provides the country of origin, if a national of a party country. (There are exceptions for cinematic and architectural works.) [4]
In the Internet age, unrestricted publication online may be considered publication in every sufficiently internet-connected jurisdiction in the world. It is not clear what this may mean for determining "country of origin". In Kernel v. Mosley , a U.S. court "concluded that a work created outside of the United States, uploaded in Australia and owned by a company registered in Finland was nonetheless a U.S. work by virtue of its being published online". However other U.S. courts in similar situations have reached different conclusions, e.g. Håkan Moberg v. 33T LLC . [5] The matter of determining the country of origin for digital publication remains a topic of controversy among law academics as well. [6]

Copyright term
The Berne Convention states that all works except photographic and cinematographic shall be copyrighted for at least 50 years after the author's death, but parties are free to provide longer terms , [7] as the European Union did with the 1993 Directive on harmonising the term of copyright protection . For photography, the Berne Convention sets a minimum term of 25 years from the year the photograph was created, and for cinematography the minimum is 50 years after first showing, or 50 years after creation if it hasn't been shown within 50 years after the creation. Countries under the older revisions of the treaty may choose to provide their own protection terms, and certain types of works (such as phonorecords and motion pictures) may be provided shorter terms.
If the author is unknown, because for example the author was deliberately anonymous or worked under a pseudonym, the Convention provides for a term of 50 years after publication ("after the work has been lawfully made available to the public"). However, if the identity of the author becomes known, the copyright term for known authors (50 years after death) applies. [7]
Although the Berne Convention states that the copyright law of the country where copyright is claimed shall be applied, Article 7(8) states that "unless the legislation of that country otherwise provides, the term shall not exceed the term fixed in the country of origin of the work", [7] i.e., an author is normally not entitled a longer copyright abroad than at home, even if the laws abroad give a longer term. This is commonly known as "the rule of the shorter term ". Not all countries have accepted this rule.

The minimum standards of protection relate to the works and rights to be protected
As to works, protection must include "every production in the literary, scientific and artistic domain, whatever the mode or form of its expression" (Article 2(1) of the Convention).
Subject to certain allowed reservations, limitations or exceptions, the following are among the rights that must be recognized as exclusive rights of authorization:

Fair uses
The Berne Convention authorizes countries to allow "fair" uses of copyrighted works in other publications or broadcasts. [8] [9] Implementations of this part of the treaty fall into the broad categories of fair use and fair dealing .
The Agreed Statement of the parties to the WIPO Copyright Treaty of 1996 states that: "It is understood that the mere provision of physical facilities for enabling or making a communication does not in itself amount to communication within the meaning of this Treaty or the Berne Convention." [10] This language may mean that Internet service providers are not liable for the infringing communications of their users. [10] Critics claim that the convention does not mention any other rights of consumers of works except for fair use. [11]
There is a legal debate about whether the U.S. Fair Use doctrine is lawful under the Three-step test . [12]

History
The Berne Convention was developed at the instigation of Victor Hugo of the Association Littéraire et Artistique Internationale . Thus it was influenced by the French " right of the author " ( droit d'auteur ), which contrasts with the Anglo-Saxon concept of "copyright" which only dealt with economic concerns. Under the Convention, copyrights for creative works are automatically in force upon their creation without being asserted or declared. An author need not "register" or "apply for" a copyright in countries adhering to the Convention. As soon as a work is "fixed", that is, written or recorded on some physical medium, its author is automatically entitled to all copyrights in the work and to any derivative works , unless and until the author explicitly disclaims them or until the copyright expires. Foreign authors are given the same rights and privileges to copyrighted material as domestic authors in any country that ratified the Convention.
Before the Berne Convention, national copyright laws usually only applied for works created within each country. So for example a work published in the United Kingdom by a British national would be covered by copyright there, but could be copied and sold by anyone in France. Dutch publisher Albertus Willem Sijthoff , who rose to prominence in the trade of translated books, wrote to Queen Wilhelmina of the Netherlands in 1899 in opposition to the convention over concerns that its international restrictions would stifle the Dutch print industry. [13]
The Berne Convention followed in the footsteps of the Paris Convention for the Protection of Industrial Property of 1883, which in the same way had created a framework for international integration of the other types of intellectual property: patents, trademarks and industrial designs .
Like the Paris Convention, the Berne Convention set up a bureau to handle administrative tasks. In 1893 these two small bureaux merged and became the United International Bureaux for the Protection of Intellectual Property (best known by its French acronym BIRPI), situated in Berne. In 1960, BIRPI moved to Geneva , to be closer to the United Nations and other international organizations in that city. In 1967 it became the World Intellectual Property Organization (WIPO), and in 1974 became an organization within the United Nations.
The Berne Convention was revised in Paris in 1896 and in Berlin in 1908, completed in Berne in 1914, revised in Rome in 1928, in Brussels in 1948, in Stockholm in 1967 and in Paris in 1971, and was amended in 1979.
The World Intellectual Property Organization Copyright Treaty was adopted in 1996 to address the issues raised by information technology and the Internet, which were not addressed by the Berne Convention.

Adoption and implementation
The first version of the Berne Convention treaty was signed on 9 September 1886, by Belgium , France , Germany , Haiti , Italy , Liberia , Spain , Switzerland , Tunisia , and the United Kingdom . [14] They ratified it on September 5, 1887. [15]
Although the United Kingdom ratified the convention in 1887, it did not implement large parts of it until 100 years later with the passage of the Copyright, Designs and Patents Act 1988 .
The United States acceded to the convention on November 16, 1988, and the convention entered into force for the United States on March 1, 1989. [16] [15] The United States initially refused to become a party to the Convention, since that would have required major changes in its copyright law , particularly with regard to moral rights , removal of the general requirement for registration of copyright works and elimination of mandatory copyright notice. This led first to the U.S. ratifying the Buenos Aires Convention (BAC) in 1910, and later the Universal Copyright Convention (UCC) in 1952 to accommodate the wishes of other countries. With the WIPO's Berna revision on Paris 1971, [17] many other countries joined the treaty, as expressed by Brazil federal law of 1975. [18]
On 1 March 1989, the U.S. Berne Convention Implementation Act of 1988 was enacted, and the U.S. Senate advised and consented to ratification of the treaty, making the United States a party to the Berne Convention, [19] and making the Universal Copyright Convention nearly obsolete. [20] Except for extremely technical points not relevant, with the accession of Nicaragua in 2000, every nation that is a member of the Buenos Aires Convention is also a member of Berne, and so the BAC has also become nearly obsolete and is essentially deprecated as well. [ who? ]
Since almost all nations are members of the World Trade Organization , the Agreement on Trade-Related Aspects of Intellectual Property Rights requires non-members to accept almost all of the conditions of the Berne Convention.
As of September 2016, there are 172 states that are parties to the Berne Convention. This includes 170 UN member states plus the Holy See and Niue .

See also
WebPage index: 00166
The Open Definition
The Open Definition is a document published by the Open Knowledge Foundation (now Open Knowledge International (OKI)), to define openness in relation to data and content . [1] It specifies what licences for such material may and may not stipulate, in order to be considered open licences . [2] The definition was derived from the Open Source Definition . [2]
OKI summarise the document as: [1]
The latest form of the document, published in November 2015, is version 2.1. [2] The use of language in the document is conformant with RFC 2119 . [2]
The document is available under a Creative Commons Attribution 4.0 International License , [1] which itself meets the Open Definition.

History
The first draft of the Open Definition, v0.1, was circulated in August 2005. [3] v1.0 was published in July 2006. [3] v2.0 was published in October 2014. [3]

See also
WebPage index: 00167
Creative Commons license
A Creative Commons ( CC ) license is one of several public copyright licenses that enable the free distribution of an otherwise copyrighted work. A CC license is used when an author wants to give people the right to share, use, and build upon a work that he/she has created. CC provides an author flexibility (for example, he/she might choose to allow only non-commercial uses of his/her own work) and protects the people who use or redistribute an author's work from concerns of copyright infringement as long as they abide by the conditions that are specified in the license by which the author distributes the work.
There are several types of CC licenses. The licenses differ by several combinations that condition the terms of distribution. They were initially released on December 16, 2002 by Creative Commons , a U.S. non-profit corporation founded in 2001. There have also been five versions of the suite of licenses, numbered 1.0 through 4.0. [1] As of 2016 [update] , the 4.0 license suite is the most current.
In October 2014 the Open Knowledge Foundation approved the Creative Commons CC BY , CC BY-SA , and CC0 licenses as conformant with the " Open Definition " for content and data. [2] [3] [4]

Applicable works
Work licensed under a Creative Commons license is governed by applicable copyright law. [5] This allows Creative Commons licenses to be applied to all work falling under copyright, including: books, plays, movies, music, articles, photographs, blogs, and websites. Creative Commons does not recommend the use of Creative Commons licenses for software. [6]
There are over 35,000 works that are available in hardcopy and have a registered ISBN number. Creative Commons splits these works into two categories, one of which encompasses self-published books. [7]
However, application of a Creative Commons license may not modify the rights allowed by fair use or fair dealing or exert restrictions which violate copyright exceptions. [8] Furthermore, Creative Commons licenses are non-exclusive and non-revocable. [9] Any work or copies of the work obtained under a Creative Commons license may continue to be used under that license. [10]
In the case of works protected by multiple Creative Common licenses, the user may choose either. [11]

Types of licenses
The CC licenses all grant the "baseline rights", such as the right to distribute the copyrighted work worldwide for non-commercial purposes, and without modification. [12] The details of each of these licenses depend on the version, and comprises a selection out of four conditions:
The last two clauses are not free content licenses, according to definitions such as DFSG or the Free Software Foundation 's standards, and cannot be used in contexts that require these freedoms, such as Wikipedia . For software , Creative Commons includes three free licenses created by other institutions: the BSD License , the GNU LGPL , and the GNU GPL . [14]
Mixing and matching these conditions produces sixteen possible combinations, of which eleven are valid Creative Commons licenses and five are not. Of the five invalid combinations, four include both the "nd" and "sa" clauses, which are mutually exclusive; and one includes none of the clauses. Of the eleven valid combinations, the five that lack the "by" clause have been retired because 98% of licensors requested attribution, though they do remain available for reference on the website. [15] [16] [17] This leaves six regularly used licenses + the CC0 public domain waiver :

Seven regularly used licenses
[17] [18]
For example, the Creative Commons Attribution (BY) license allows one to share and remix (create derivative works), even for commercial use, so long as attribution is given. [19]

Version 4.0 and international use
The original non-localized Creative Commons licenses were written with the U.S. legal system in mind, therefore the wording may be incompatible with local legislation in other jurisdictions , rendering the licenses unenforceable there. To address this issue, Creative Commons asked its affiliates to translate the various licenses to reflect local laws in a process called " porting ." [20] As of July 2011, Creative Commons licenses have been ported to over 50 jurisdictions worldwide. [21]
The latest version 4.0 of the Creative Commons licenses, released on November 25, 2013, are generic licenses that are applicable to most jurisdictions and do not usually require ports. [22] [23] [24] [25] No new ports have been implemented in version 4.0 of the license. [26] Version 4.0 discourages using ported versions and instead acts as a single global license. [27]

Rights

Attribution
Since 2004, all current licenses (beside the CC0 waiver) require attribution of the original author, the BY component. [16] The attribution must be given to "the best of [one's] ability using the information available". [28] Generally this implies the following:

Non-commercial licenses
The "non-commercial" option included in some Creative Commons licenses is controversial in definition, [29] as it is sometimes unclear what can be considered a non-commercial setting, and application, since its restrictions differ from the principles of open content promoted by other permissive licenses . [30] In 2014 Wikimedia Deutschland published a guide to using Creative Commons licenses as wiki pages for translations and as PDF. [31]

Zero / public domain
Besides licenses, Creative Commons also offers through CC0 a way to release material worldwide into the public domain . [18] CC0 is a legal tool for waiving as many rights as legally possible. [33] Or, when not legally possible, CC0 acts as fallback as public domain equivalent license . [33] Development of CC0 began in 2007 [34] and the tool was released in 2009. [35] [36] A major target of the license was the scientific data community. [37]
In 2010, Creative Commons announced its Public Domain Mark , [38] a tool for labeling works already in the public domain. Together, CC0 and the Public Domain Mark replace the Public Domain Dedication and Certification, [39] which took a U.S.-centric approach and co-mingled distinct operations.
In 2011, the Free Software Foundation added CC0 to its free software licenses , [40] and currently recommends CC0 as the preferred method of releasing software into the public domain . [41]
In February 2012 CC0 was submitted to Open Source Initiative (OSI) for their approval. [42] However, controversy arose over its clause which excluded from the scope of the license any relevant patents held by the copyright holder. This clause was added with scientific data in mind rather than software, but some members of the OSI believed it could weaken users' defenses against software patents . As a result, Creative Commons withdrew their submission, and the license is not currently approved by the OSI. [37] [43]
In 2013, Unsplash began using the CC0 license to distribute free stock photography . [44] [45] It now distributes several million photos a month [46] and has inspired a host of similar sites, including CC0 photography companies and CC0 blogging companies. [47] Lawrence Lessig , the founder of Creative Commons, has contributed to the site. [48] [ dead link ]
In October 2014 the Open Knowledge Foundation approved the Creative Commons CC0 as conformant with the "Open Definition" and recommend the license to dedicate content to the public domain. [3] [4]

Adaptation
Rights in an adaptation can be expressed by a CC license that is compatible with the status or licensing of the original work or works on which the adaptation is based. [49]

Legal aspects
The legal implications of large numbers of works having Creative Commons licensing are difficult to predict, and there is speculation that media creators often lack insight to be able to choose the license which best meets their intent in applying it. [50]
Some works licensed using Creative Commons licenses have been involved in several court cases. [51] Creative Commons itself was not a party to any of these cases; they only involved licensors or licensees of Creative Commons licenses. When the cases went as far as decisions by judges (that is, they were not dismissed for lack of jurisdiction or were not settled privately out of court), they have all validated the legal robustness of Creative Commons public licenses. Here are some notable cases:

Dutch tabloid
In early 2006, podcaster Adam Curry sued a Dutch tabloid who published photos from Curry's Flickr page without Curry's permission. The photos were licensed under the Creative Commons Non-Commercial license. While the verdict was in favor of Curry, the tabloid avoided having to pay restitution to him as long as they did not repeat the offense. Professor Bernt Hugenholtz, main creator of the Dutch CC license and director of the Institute for Information Law of the University of Amsterdam, commented, "The Dutch Court's decision is especially noteworthy because it confirms that the conditions of a Creative Commons license automatically apply to the content licensed under it, and binds users of such content even without expressly agreeing to, or having knowledge of, the conditions of the license." [52] [53] [54] [55]

Virgin Mobile
In 2007, Virgin Mobile Australia launched an Australian bus stop ad campaign promoting their cellphone text messaging service using the work of amateur photographers who uploaded their work to Flickr using a Creative Commons-BY (Attribution) license. Users licensing their images this way freed their work for use by any other entity, as long as the original creator was attributed credit, without any other compensation required. Virgin upheld this single restriction by printing a URL leading to the photographer's Flickr page on each of their ads. However, one picture, depicting 15-year-old Alison Chang at a fund-raising carwash for her church, [56] caused some controversy when she sued Virgin Mobile. The photo was taken by Alison's church youth counselor, Justin Ho-Wee Wong, who uploaded the image to Flickr under the Creative Commons license. [56] In 2008, the case (concerning personality rights rather than copyright as such) was thrown out of a Texas court for lack of jurisdiction. [57] [58]

SGAE vs Fernández
In the fall of 2006, the collecting society Sociedad General de Autores y Editores ( SGAE ) in Spain sued Ricardo Andrés Utrera Fernández, owner of a disco bar located in Badajoz who played CC-licensed music. SGAE argued that Fernández should pay royalties for public performance of the music between November 2002 and August 2005. The Lower Court rejected the collecting society's claims because the owner of the bar proved that the music he was using was not managed by the society. [59]
In February 2006, the Cultural Association Ladinamo (based in Madrid, and represented by Javier de la Cueva ) was granted the use of copyleft music in their public activities. The sentence said: "Admitting the existence of music equipment, a joint evaluation of the evidence practiced, this court is convinced that the defendant prevents communication of works whose management is entrusted to the plaintiff [SGAE], using a repertoire of authors who have not assigned the exploitation of their rights to the SGAE, having at its disposal a database for that purpose and so it is manifested both by the legal representative of the Association and by Manuela Villa Acosta, in charge of the cultural programming of the association, which is compatible with the alternative character of the Association and its integration in the movement called ' copy left '". [60]

GateHouse Media, Inc. vs. That's Great News, LLC
On June 30, 2010 GateHouse Media filed a lawsuit against That's Great News . GateHouse Media owns a number of local newspapers, including Rockford Register Star , which is based in Rockford, Illinois. That's Great News makes plaques out of newspaper articles and sells them to the people featured in the articles. [61] GateHouse sued That's Great News for copyright infringement and breach of contract. GateHouse claimed that TGN violated the non-commercial and no-derivative works restrictions on GateHouse Creative Commons licensed work when TGN published the material on its website. The case was settled on August 17, 2010, though the settlement was not made public. [61] [62]

Drauglis v. Kappa Map Group, LLC
The plaintiff was photographer Art Drauglis, who uploaded several pictures to the photo-sharing website Flickr using Creative Commons Attribution-ShareAlike 2.0 Generic License (CC BY-SA), including one entitled "Swain's Lock, Montgomery Co., MD.". The defendant was Kappa Map Group, a map-making company, which downloaded the image and used it in a compilation entitled "Montgomery Co. Maryland Street Atlas". Though there was nothing on the cover that indicated the origin of the picture, the text " Photo: Swain's Lock, Montgomery Co., MD Photographer: Carly Lesser & Art Drauglis, Creative Commoms [ sic ] , CC-BY-SA-2.0 " appeared at the bottom of the back cover.
The validity of the CC BY-SA 2.0 as a license was not in dispute. The CC BY-SA 2.0 requires that the licensee to use nothing less restrictive than the CC BY-SA 2.0 terms. The atlas was sold commercially and not for free reuse by others. The dispute was whether Drauglis' license terms that would apply to "derivative works" applied to the entire atlas. Drauglis sued the defendants on June 2014 for copyright infringement and license breach, seeking declaratory and injunctive relief, damages, fees, and costs. Drauglis asserted, among other things, that Kappa Map Group "exceeded the scope of the License because defendant did not publish the Atlas under a license with the same or similar terms as those under which the Photograph was originally licensed." [63] The judge dismissed the case on that count, ruling that the atlas was not a derivative work of the photograph in the sense of the license. Since the atlas was not a derivative work of the photograph, Kappa Map Group did not need to license the entire atlas under the CC BY-SA 2.0 license. The judge also determined that the work had been properly attributed. [64]

Verband zum Schutz geistigen Eigentums im Internet (VGSE)
This incident has not been tested in court, but it highlights a potentially disturbing practice. In July 2016, German computer magazine LinuxUser reports that a German blogger Christoph Langner used two CC-BY licensed photographs from Berlin photographer Dennis Skley on his private blog Linuxundich.de . Langner duly mentioned the author and the license and added a link to the original. Langner was later contacted by the Verband zum Schutz geistigen Eigentums im Internet (VGSE) (Association for the Protection of Intellectual Property in the Internet) with a demand for €2300 for failing to provide the full name of the work, the full name of the author, the license text, and a source link, as is apparently required by the fine print in the license. Of this sum, €40 goes to the photographer and remainder is retained by VGSE. [65] [66]

Works with a Creative Commons license
Creative Commons maintains a content directory wiki of organizations and projects using Creative Commons licenses. [67] On its website CC also provides case studies of projects using CC licenses across the world. [68] CC licensed content can also be accessed through a number of content directories and search engines (see CC licensed content directories ).

Retired licenses
Due to either disuse or criticism, a number of previously offered Creative Commons licenses have since been retired, [15] [69] and are no longer recommended for new works. The retired licenses include all licenses lacking the Attribution element other than CC0, as well as the following four licenses:

See also
WebPage index: 00168
Copyright symbol
The copyright symbol , or copyright sign , © (a circled capital letter C for copyright ), is the symbol used in copyright notices for works other than sound recordings (which are indicated with the ℗ symbol ). The use of the symbol is described in United States copyright law , [1] and, internationally, by the Universal Copyright Convention . [2] The symbol is widely recognized, but under the Berne Convention is no longer required to obtain a new copyright in most nations. For instance, the United States eliminated the copyright symbol requirement as of March 1, 1989, but its presence or absence is legally significant on works published previously.

History
Prior symbols indicating a work's copyright status are seen in Scottish almanacs of the 1670s; books included a printed copy of the local coat-of-arms to indicate their authenticity. [3]
A copyright notice was first required in the U.S. by the Copyright Act of 1802. [4] It was lengthy: "Entered according to act of Congress , in the year , by A. B., in the office of the Librarian of Congress , at Washington ." In general, this notice had to appear on the copyrighted work itself, but in the case of a "work of the fine arts", such as a painting, it could instead be inscribed "on the face of the substance on which [the work of art] shall be mounted". [5] The Copyright Act was amended in 1874 to allow a much shortened notice: "Copyright, 18 , by A. B." [6]
The copyright symbol © was introduced in the United States by a 1954 amendment to the Copyright Act of 1909 , section 18. [7] [8]
The Copyright Act of 1909 was meant to be a complete rewrite and overhaul of existing copyright law. As originally proposed in the draft of the bill, copyright protection required putting the word "copyright" or a sanctioned abbreviation on the work of art itself. This included paintings, the argument being that the frame was detachable. In conference sessions among copyright stakeholders on the proposed bill, conducted in 1905 and 1906, representatives of artist organizations objected to this requirement, wishing to put no more on the work itself than the artist's name. As a compromise, the possibility was created to add a relatively unintrusive mark, the capital letter C within a circle, to appear on the work itself next to the artist's name, indicating the existence of a more elaborate copyright notice elsewhere that was still to be allowed to be placed on the mounting. [9] Indeed, the version of the bill that was submitted to Congress in 1906, compiled by the Copyright Commission under the direction of the Librarian of Congress, Herbert Putnam, contained a provision that a special copyright symbol, the letter C inclosed within a circle, could be used instead of the word "copyright" or the abbreviation "copr.", but only for a limited category of copyrightable works, including works of art but not ordinary books or periodicals. [10] The formulation of the 1909 Act was left unchanged when it was incorporated in 1946 as title 17 of the United States Code ; when that title was amended in 1954, the symbol © was allowed as an alternative to "Copyright" or "Copr." in all copyright notices. [11]
In countries party to the Berne Convention for the Protection of Literary and Artistic Works , including the modern-day U.S., a copyright notice is not required to be displayed in order for copyright to be established; rather, the creation of the work automatically establishes copyright. [12] The United States was one of the later accedents to Berne (1989); the majority of nations now belong to Berne , and thus do not require copyright notices to obtain copyright.

U.S. copyright notice
In the United States , the copyright notice required prior to March 1, 1989, consists of: [13]
For example, for a work first published in 2011:
The notice was once required in order to receive copyright protection in the United States, but in countries respecting the Berne convention this is no longer the case. [12] The United States joined the Berne Convention effective March 1, 1989. [14]

Digital representation
Because the © symbol has long been unavailable on typewriters and ASCII -based computer systems, it has been common to approximate this symbol with the characters (C) .
The character is mapped in Unicode as U+00A9 © COPYRIGHT SIGN (HTML &#169; · &copy; ). [15] Unicode also has U+24B8 Ⓒ CIRCLED LATIN CAPITAL LETTER C (HTML &#9400; ) and U+24D2 ⓒ CIRCLED LATIN SMALL LETTER C (HTML &#9426; ). [16] They are sometimes used as a substitute copyright symbol where the actual copyright symbol is not available in the font or in the character set, for example, in some Korean code pages.
On Windows it may be entered by holding the Alt while typing the numbers 0 1 6 9 on the numeric keypad . It can be entered on a Mac by holding the Option key and then pressing the "g" key. On Linux , it can be obtained with the <compose key> O C ComposeKey sequence.

Related symbols

See also
WebPage index: 00169
Open Source Initiative
The Open Source Initiative ( OSI ) is an organization dedicated to promoting open-source software .
The organization was founded in late February 1998 by Bruce Perens and Eric S. Raymond , part of a group inspired by the Netscape Communications Corporation publishing the source code for its flagship Netscape Communicator product. Later, in August 1998, the organization added a board of directors.
Raymond was president from its founding until February 2005, followed briefly by Russ Nelson and then Michael Tiemann . In May 2012, the new board elected Simon Phipps as president [1] and in May 2015 Allison Randal was elected as president [2] when Phipps stepped down in preparation for the 2016 end of his Board term. [3]

History
As a campaign of sorts, "open source" was launched in 1998 by Jon "maddog" Hall , Larry Augustin , Eric S. Raymond , Bruce Perens , and others. [4] [5]
The group adopted the Open Source Definition for open-source software, based on the Debian Free Software Guidelines . They also established the Open Source Initiative (OSI) as a steward organization for the movement. However, they were unsuccessful in their attempt to secure a trademark for 'open source' to control the use of the term. [6] In 2008, in an apparent effort to reform governance of the organization, the OSI Board invited 50 individuals to join a "Charter Members" group; by 26 July 2008, 42 of the original invitees had accepted the invitations. The full membership of the Charter Members has never been publicly revealed, and the Charter Members group communicated by way of a closed-subscription mailing list, "osi-discuss", with non-public archives. [7] Public information indicates that the group included Bradley M. Kuhn , Karl Fogel , Jim Blandy , Chamindra da Silva , Lawrence Rosen , and David Ascher . [8] [9] [10] Then-OSI Board member Danese Cooper was the principal moderator of osi-discuss. [11] Kuhn later recollected that the Charter Membership was a "brouhaha (bordering on a flame fest)" and took no action. [12]
In 2009, the OSI was temporarily suspended from operation as a California corporation, apparently in response to a complaint concerning tax paperwork from earlier years. [13] [ clarification needed ] Its current status is "Active". [14]
In 2012, under the leadership of OSI director and then-president Simon Phipps , the OSI began transitioning towards a membership-based governance structure. The OSI initiated an Affiliate Membership program for "government-recognized non-profit charitable and not-for-profit industry associations and academic institutions anywhere in the world". [15] Subsequently, the OSI announced an Individual Membership program [16] and listed a number of Corporate Sponsors. [17]
On November 8, 2013, OSI appointed Patrick Masson as its General Manager. [18]

Relationship with the free software movement
Both the modern free software movement (launched by Richard Stallman in the early 1980s) and the Open Source Initiative were born from a common history of Unix , Internet free software, and the hacker culture , but their basic goals and philosophy differ. The Open Source Initiative chose the term "open source," in founding member Michael Tiemann 's words, to "dump the moralizing and confrontational attitude that had been associated with 'free software'" and instead promote open source ideas on "pragmatic, business-case grounds." [19]
As early as 1999, OSI co-founder Perens objected to the "schism" that was developing between supporters of Stallman's Free Software Foundation (FSF) and the OSI because of their disparate approaches. (Perens had hoped the OSI would merely serve as an "introduction" to FSF principles for "non-hackers." [20] ) Stallman has sharply criticized the OSI for its pragmatic focus and for ignoring what he considers the central "ethical imperative" and emphasis on "freedom" underlying free software as he defines it. [21] Nevertheless, Stallman has described his free software movement and the Open Source Initiative as separate camps within the same broad free-software community and acknowledged that despite philosophical differences, proponents of open source and free software "often work together on practical projects." [21]

Board members
The current Open Source Initiative board is: [22]
Past board members include:

See also
WebPage index: 00170
Open-source hardware
Open-source hardware , consists of physical artifacts of technology designed and offered by the open design movement. Both free and open-source software (FOSS) as well as open-source hardware is created by this open-source culture movement and applies a like concept to a variety of components. It is sometimes, thus, referred to as FOSH (free and open-source hardware). The term usually means that information about the hardware is easily discerned so that others can make it - coupling it closely to the maker movement . [1] Hardware design (i.e. mechanical drawings, schematics , bills of material , PCB layout data, HDL source code [2] and integrated circuit layout data), in addition to the software that drives the hardware, are all released under free/ libre terms. The original sharer gains feedback and potentially improvements on the design from the FOSH community. There is now significant evidence that such sharing can drive a high return on investment for investors. [3]
Since the rise of reconfigurable programmable logic devices , sharing of logic designs has been a form of open-source hardware. Instead of the schematics, hardware description language (HDL) code is shared. HDL descriptions are commonly used to set up system-on-a-chip systems either in field-programmable gate arrays (FPGA) or directly in application-specific integrated circuit (ASIC) designs. HDL modules, when distributed, are called semiconductor intellectual property cores , or IP cores .

History
The first hardware focused " open source " activities were started around 1997 by Bruce Perens , creator of the Open Source Definition , co-founder of the Open Source Initiative , and a ham radio operator . He launched the Open Hardware Certification Program, which had the goal of allowing hardware manufacturers to self-certify their products as open. [4] [5]
Shortly after the launch of the Open Hardware Certification Program, David Freeman announced the Open Hardware Specification Project (OHSpec), another attempt at licensing hardware components whose interfaces are available publicly and of creating an entirely new computing platform as an alternative to proprietary computing systems. [6] In early 1999, Sepehr Kiani, Ryan Vallance and Samir Nayfeh joined efforts to apply the open source philosophy to machine design applications. Together they established the Open Design Foundation (ODF) as a non-profit corporation, and set out to develop an Open Design Definition. But most of these activities faded out after a few years.
By the mid 2000s open source hardware again became a hub of activity due to the emergence of several major open source hardware projects and companies, such as OpenCores , RepRap ( 3D printing ), Arduino , Adafruit and SparkFun . In 2007, Perens reactivated the openhardware.org website.
Following the Open Graphics Project , an effort to design, implement, and manufacture a free and open 3D graphics chip set and reference graphics card, Timothy Miller suggested the creation of an organization to safeguard the interests of the Open Graphics Project community. Thus, Patrick McNamara founded the Open Hardware Foundation (OHF) in 2007. [7]
The Tucson Amateur Packet Radio Corporation (TAPR), founded in 1982 as a non-profit organization of amateur radio operators with the goals of supporting R&D efforts in the area of amateur digital communications, created in 2007 the first open hardware license, the TAPR Open Hardware License . The OSI president Eric S. Raymond expressed some concerns about certain aspects of the OHL and decided to not review the license. [8]
Around 2010 in context of the Freedom Defined project, the Open Hardware Definition was created as collaborative work of many [9] and is accepted as of 2016 by dozens of organizations and companies. [10]
In July 2011, CERN ( European Organization for Nuclear Research ) released an open source hardware license, CERN OHL . Javier Serrano, an engineer at CERN’s Beams Department and the founder of the Open Hardware Repository, explained: “By sharing designs openly, CERN expects to improve the quality of designs through peer review and to guarantee their users – including commercial companies – the freedom to study, modify and manufacture them, leading to better hardware and less duplication of efforts” . [11] While initially drafted to address CERN-specific concerns, such as tracing the impact of the organization’s research, in its current form it can be used by anyone developing open source hardware. [12]
Following the 2011 Open Hardware Summit, and after heated debates on licenses and what constitutes open source hardware, Bruce Perens abandoned the OSHW Definition and the concerted efforts of those involved with it. [13] Openhardware.org, led by Bruce Perens, promotes and identifies practices that meet all the combined requirements of the Open Source Hardware Definition, the Open Source Definition, and the Four Freedoms of the Free Software Foundation [14] Since 2014 openhardware.org is not online and seems to have ceased activity. [15]
The Open Source Hardware Association (OSHWA) at proposes Open source hardware and acts as hub of open source hardware activity of all genres, while cooperating with other entities such as TAPR, CERN, and OSI. The OSHWA was established as an organization in June 2012 in Delaware and filed for tax exemption status in July 2013. [16] After same debates about trademark interferences with the OSI, in 2012 the OSHWA and the OSI signed a co-existence agreement. [17] [18]
In 2012, after years of skepticism on the relevance of free hardware designs , [19] the Free Software Foundation started the " Respects Your Freedom " (RYF) computer hardware product certification program. It was intended to encourage the creation and sale of hardware that respects users' freedom and privacy, and aims to ensure that users have control over their devices. [20] [21] The FSF's RYF certificate faced criticism for the requirement to comply with the controversial debated FSF terminology, [22] which is seen by some as unrelated topic and unneeded political polarization for a technological certificate. [23] Also FSF's Replicant project suggested in 2016 an alternative "free hardware" definition, derived from the FSF's four freedoms . [24]

Licenses
Rather than creating a new license, some open-source hardware projects simply use existing, free and open-source software licenses. [25] These licenses may not accord well with patent law . [26]
Later, several new licenses have been proposed, designed to address issues specific to hardware designs. [27] In these licenses, many of the fundamental principles expressed in open-source software (OSS) licenses have been "ported" to their counterpart hardware projects. New hardware licenses are often explained as the "hardware equivalent" of a well-known OSS license, such as the GPL , LGPL , or BSD license .
Despite superficial similarities to software licenses , most hardware licenses are fundamentally different: by nature, they typically rely more heavily on patent law than on copyright law, as many hardware designs are not copyrightable. [28] Whereas a copyright license may control the distribution of the source code or design documents, a patent license may control the use and manufacturing of the physical device built from the design documents. This distinction is explicitly mentioned in the preamble of the TAPR Open Hardware License :
Noteworthy licenses include:
The Open Source Hardware Association recommends seven licenses which follow their open-source hardware definition . [34] From the general copyleft licenses the GNU General Public License (GPL) and Creative Commons Attribution-ShareAlike license, from the HW specific copyleft licenses the CERN Open Hardware License (OHL) and TAPR Open Hardware License (OHL) and from the permissive licenses the FreeBSD license , the MIT license , and the Creative Commons Attribution license. [35] Openhardware.org recommended in 2012 the TAPR Open Hardware License, Creative Commons BY-SA 3.0 and GPL 3.0 license. [36]
Organizations tend to rally around a shared license. For example, Opencores prefers the LGPL or a Modified BSD License , [37] FreeCores insists on the GPL , [38] Open Hardware Foundation promotes " copyleft " or other permissive licenses", [39] the Open Graphics Project uses a variety of licenses, including the MIT license , GPL , and a proprietary license, [40] and the Balloon Project wrote their own license. [41]

Development
Extensive discussion has taken place on ways to make open-source hardware as accessible as open-source software . Discussions focus on multiple areas, [42] such as the level at which open-source hardware is defined, [43] ways to collaborate in hardware development, as well as a model for sustainable development by making open-source appropriate technology . [44] [45] In addition there has been considerable work to produce open-source hardware for scientific hardware using a combination of open-source electronics and 3-D printing . [46] [47]
One of the major differences between developing open-source software and developing open-source hardware is that hardware results in tangible outputs, which cost money to prototype and manufacture. As a result, the phrase "free as in speech, not as in beer", [48] [ not in citation given ] more formally known as Gratis versus Libre , distinguishes between the idea of zero cost and the freedom to use and modify information. While open-source hardware faces challenges in minimizing cost and reducing financial risks for individual project developers, some community members have proposed models to address these needs. [49] Given this, there are initiatives to develop sustainable community funding mechanisms, such as the Open Source Hardware Central Bank. [50]
Often vendors of chips and other electronic components will sponsor contests with the provision that the participants and winners must share their designs. Circuit Cellar magazine organizes some of these contests.

Open-source labs
A guide has been published ( Open-Source Lab (book) by Joshua Pearce ) on using open-source electronics and 3D printing to make open-source labs . Today scientists are creating many such labs, examples include:

Open-source electronics
One of the most popular types of open-source hardware is electronics. There are numerous companies that provide large varieties of open-source electronics such as Sparkfun , Adafruit and Seeed. In addition, there are NPOs and companies that provide a specific open-source electronic component such as the Arduino electronics prototyping platform. There are numerous examples of speciality open-source electronics such as low-cost voltage and current GMAW open-source 3-D printer monitor [53] [54] and a robotics-assisted mass spectrometry assay platform. [55] [56] Open-source electronics finds various uses, including automation of chemical procedures. [57] [58]

Business models
Open hardware companies are experimenting with different business models. In one example, littleBits implements open-source business models by making the design files available for the circuit designs in each littleBits module, in accordance with the CERN Open Hardware License Version 1.2. [59] In another example, Arduino has registered its name as a trademark . Others may manufacture their designs but can't put the Arduino name on them. Thus they can distinguish their products from others by appellation. [60] There are many applicable business models for implementing some open-source hardware even in traditional firms. For example, to accelerate development and technical innovation the photovoltaic industry has experimented with partnerships, franchises, secondary supplier and completely open-source models. [61]
Recently, many open source hardware projects were funded via crowdfunding on Indiegogo or Kickstarter . Especially popular is Crowdsupply for crowdfunding open hardware projects. [62]

Reception and impact
Richard Stallman , the founder of the Free Software movement, was in 1999 skeptical on the idea and relevance of Free hardware (his terminology what is now known as open-source hardware). [63] In a 2015 Wired article he adapted his point of view slightly; while he still sees no ethical parallel between free software and free hardware, he acknowledges the importance. [64] Also, Stallman uses and suggest the term free hardware design over open source hardware , a request which is consistent with his earlier rejection of the term open source software (see also Alternative terms for free software ). [64]
Other authors, such as Joshua Pearce have argued there is an ethical imperative for open-source hardware – specifically with respect open-source appropriate technology for sustainable development . [65] In 2014, he also wrote the book Open-Source Lab: How to Build Your Own Hardware and Reduce Research Costs , which details the development of free and open-source hardware primarily for scientists and university faculty . [66] [67]

See also
WebPage index: 00171
Open access
Open access ( OA ) refers to online research outputs that are free of all restrictions on access (e.g. access tolls) and free of many restrictions on use (e.g. certain copyright and license restrictions). [1] Open access can be applied to all forms of published research output, including peer-reviewed and non peer-reviewed academic journal articles, conference papers , theses , [2] book chapters, [1] and monographs . [3]
Two degrees of open access can be distinguished: gratis open access, which is online access free of charge, and libre open access, which is online access free of charge plus various additional usage rights. [4] These additional usage rights are often granted through the use of various specific Creative Commons licenses . [5] Libre open access is equivalent to the definition of open access in the Budapest Open Access Initiative , the Bethesda Statement on Open Access Publishing and the Berlin Declaration on Open Access to Knowledge in the Sciences and Humanities .
There are multiple ways authors can provide open access to their work. One way is to publish it and then self-archive it in a repository where it can be accessed for free, [6] [7] such as their institutional repository , [8] [9] or a central repository such as PubMed Central . This is known as 'green' open access. Some publishers require delays, or an embargo , on when a research output in a repository may be made open access. [10] Several initiatives provide an alternative to the American and English language dominance of existing publication indexing systems, including Index Copernicus , SciELO and Redalyc .
A second way authors can make their work open access is by publishing it in such a way that makes their research output immediately available from the publisher. [11] This is known as 'gold' open access, [12] and within the sciences this often takes the form of publishing an article in either an open access journal , [13] or a hybrid open access journal . The latter is a journal whose business model is at least partially based on subscriptions, and only provide Gold open access for those individual articles for which their authors (or their author's institution or funder) pay a specific fee for publication, often referred to as an article processing charge . [14] Pure open access journals do not charge subscription fees, and may have one of a variety of business models. Many, however, do charge an article processing fee. [15]
Widespread public access to the World Wide Web in the late 1990s and early 2000s fueled the open access movement, and prompted both the green open access way (self-archiving of non-open access journal articles) and the creation of open access journals (gold way). Conventional non-open access journals cover publishing costs through access tolls such as subscriptions, site licenses or pay-per-view charges. Some non-open access journals provide open access after an embargo period of 6–12 months or longer (see delayed open access journals ). [14] Active debate over the economics and reliability of various ways of providing open access continues among researchers, academics, librarians, university administrators, funding agencies, government officials, commercial publishers , editorial staff and society publishers, as open access gradually gains in acceptance. [16]

Definitions
The term "open access" itself was first formulated in three public statements in the 2000s: the Budapest Open Access Initiative in February 2002, the Bethesda Statement on Open Access Publishing in June 2003, and the Berlin Declaration on Open Access to Knowledge in the Sciences and Humanities in October 2003, [17] and the initial concept of open access refers to an unrestricted online access to scholarly research primarily intended for scholarly journal articles.
The Budapest statement defined open access as follows:
The Bethesda and Berlin statements add that for a work to be open access, users must be able to "copy, use, distribute, transmit and display the work publicly and to make and distribute derivative works, in any digital medium for any responsible purpose, subject to proper attribution of authorship."
Despite these statements emerging in the 2000s, the idea and practise of providing free online access to journal articles began at least a decade before the term "open access" was formally coined. Computer scientists had been self-archiving in anonymous ftp archives since the 1970s and physicists had been self-archiving in arxiv since the 1990s. The Subversive Proposal to generalize the practice was posted in 1994.

Gratis and libre open access
In order to reflect actual practice in providing two different degrees of open access, the further distinction between gratis open access and libre open access was added in 2006 by two of the co-drafters of the original BOAI definition. [4] Gratis OA refers to free online access, and libre OA refers to free online access plus some additional re-use rights. [4] The Budapest, Bethesda, and Berlin definitions had corresponded only to libre OA. The re-use rights of libre OA are often specified by various specific Creative Commons licenses ; [5] these almost all require attribution of authorship to the original authors. [4] [17]

Motivations for open access publishing
Open access itself (mostly green and gratis) began to be sought and provided worldwide by researchers when the possibility itself was opened by the advent of Internet and the World Wide Web . The momentum was further increased by a growing movement for academic journal publishing reform, and with it gold and libre OA. Electronic publishing created new benefits as compared to paper publishing but beyond that, it contributed to causing problems in traditional publishing models.
The premises behind open access publishing are that there are viable funding models to maintain traditional peer review standards of quality while also making the following changes:
The open access movement is motivated by the problems of social inequality caused by restricting access to academic research, which favor large and wealthy institutions with the financial means to purchase access to many journals, as well as the economic challenges and perceived unsustainability of academic publishing. [19] [20]

Stakeholders and concerned communities
The intended audience of research articles is usually other researchers. Open access helps researchers as readers by opening up access to articles that their libraries do not subscribe to. One of the great beneficiaries of open access may be users in developing countries , where currently some universities find it difficult to pay for subscriptions required to access the most recent journals. [21] Some schemes exist for providing subscription scientific publications to those affiliated to institutions in developing countries at little or no cost. [22] All researchers benefit from open access as no library can afford to subscribe to every scientific journal and most can only afford a small fraction of them – this is known as the " serials crisis ". [23]
Open access extends the reach of research beyond its immediate academic circle. An open access article can be read by anyone – a professional in the field, a researcher in another field, a journalist , a politician or civil servant , or an interested layperson . Indeed, a 2008 study revealed that mental health professionals are roughly twice as likely to read a relevant article if it is freely available. [24]

Authors and researchers
The main reason authors make their articles openly accessible is to maximize their research impact . [25] A study in 2001 first reported an open access citation impact advantage, [26] and a growing number of studies [27] have confirmed, with varying degrees of methodological rigor, that an open access article is more likely to be used and cited than one behind subscription barriers. [27] For example, a 2006 study in PLoS Biology found that articles published as immediate open access in PNAS were three times more likely to be cited than non-open access papers, and were also cited more than PNAS articles that were only self-archived. [28] This result has been challenged as an artifact of authors self-selectively paying to publish their higher quality articles in hybrid open access journals, [29] whereas a 2010 study found that the open access citation advantage was equally big whether self-archiving was self-selected or mandated. [30]
Scholars are paid by research funders and/or their universities to do research; the published article is the report of the work they have done, rather than an item for commercial gain. The more the article is used, cited, applied and built upon, the better for research as well as for the researcher's career. [31] [32] Open access can reduce publication delays, an obstacle which led some research fields such as high-energy physics to adopt widespread preprint access. [33]
Some professional organizations have encouraged use of open access: in 2001, the International Mathematical Union communicated to its members that "Open access to the mathematical literature is an important goal" and encouraged them to "[make] available electronically as much of our own work as feasible" to "[enlarge] the reservoir of freely available primary mathematical material, particularly helping scientists working without adequate library access." [34]

Research funders and universities
Research funding agencies and universities want to ensure that the research they fund and support in various ways has the greatest possible research impact. [35] As a means of achieving this, research funders are beginning to expect open access to the research they support. Many of them (including all seven UK Research Councils) have already adopted green open access self-archiving mandates, and others are on the way to do so (see ROARMAP ).

Universities
A growing number of universities are providing institutional repositories in which their researchers can deposit their published articles. Some open access advocates believe that institutional repositories will play a very important role in responding to open access mandates from funders. [36] EnablingOpenScholarship (EPS) provides universities with OA policy-building. [37]
In May 2005, 16 major Dutch universities cooperatively launched DAREnet , the Digital Academic Repositories, making over 47,000 research papers available to anyone with internet access. [38] From 1 January 2007, at the completion of the DARE programme, KNAW Research Information has taken over responsibility for the DAREnet portal. On 2 June 2008, DAREnet has been incorporated into the scholarly portal NARCIS. [39] At the end of 2009, NARCIS provided access to 185,000 open access publications from all Dutch universities, KNAW, NWO and a number of scientific institutes.
In 2011, a group of universities in North America formed the Coalition of Open Access Policy Institutions (COAPI). [40] Starting with 21 institutions where the faculty had either established an open access policy or were in the process of implementing one, COAPI now has nearly 50 members. These institutions' administrators, faculty and librarians, and staff support the international work of the Coalition's awareness-raising and advocacy for open access. Members agree to the following COAPI Principles:
In 2012, the Harvard Open Access Project released its guide to good practices for university open-access policies, [42] focusing on rights-retention policies that allow universities to distribute faculty research without seeking permission from publishers.
In 2013 a group of nine Australian universities formed the Australian Open Access Support Group (AOASG) to advocate, collaborate, raise awareness, and lead and build capacity in the open access space in Australia. [43] In 2015, the group expanded to include all eight New Zealand universities and was renamed the Australasian Open Access Support Group. [44]

Libraries and librarians
As information professionals, librarians are vocal and active advocates of open access. These librarians believe that open access promises to remove both the price barriers and the permission barriers that undermine library efforts to provide access to the scholarly record, [45] as well as helping to address the serials crisis . Many library associations have either signed major open access declarations, or created their own. For example, the Canadian Library Association endorsed a Resolution on Open Access in June 2005. [46]
Librarians also lead education and outreach initiatives to faculty, administrators, and others about the benefits of open access. For example, the Association of College and Research Libraries of the American Library Association has developed a Scholarly Communications Toolkit. [47] The Association of Research Libraries has documented the need for increased access to scholarly information, and was a leading founder of the Scholarly Publishing and Academic Resources Coalition (SPARC). [48] [49]
At most universities, the library manages the institutional repository, which provides free access to scholarly work by the university's faculty. The Canadian Association of Research Libraries has a program [50] to develop institutional repositories at all Canadian university libraries.
An increasing number of libraries provide hosting services for open access journals. A 2008 survey by the Association of Research Libraries [51] found that 65% of surveyed libraries either are involved in journal publishing , or are planning to become involved in the very near future. [52]
In 2013, open access activist Aaron Swartz was posthumously awarded the American Library Association's James Madison Award for being an "outspoken advocate for public participation in government and unrestricted access to peer-reviewed scholarly articles". [53] [54] In March 2013, the entire editorial board and the editor-in-chief of the Journal of Library Administration resigned en masse, citing a dispute with the journal's publisher. [55] One board member wrote of a "crisis of conscience about publishing in a journal that was not open access" after the death of Aaron Swartz. [56] [57]
The pioneer of the open access movement in France and one of the first librarians to advocate the self-archiving approach to open access worldwide is Hélène Bosc. [58] Her work is described in her "15-year retrospective". [59]

Public
Open access to scholarly research is argued to be important to the public for a number of reasons. One of the arguments for public access to the scholarly literature is that most of the research is paid for by taxpayers through government grants , who therefore have a right to access the results of what they have funded. This is one of the primary reasons for the creation of advocacy groups such as The Alliance for Taxpayer Access in the US. [60] Examples of people who might wish to read scholarly literature include individuals with medical conditions (or family members of such individuals) and serious hobbyists or 'amateur' scholars who may be interested in specialized scientific literature (e.g. amateur astronomers ). Additionally, professionals in many fields may be interested in continuing education in the research literature of their field, and many businesses and academic institutions cannot afford to purchase articles from or subscriptions to much of the research literature that is published under a toll access model.
Even those who do not read scholarly articles benefit indirectly from open access. [61] For example, patients benefit when their doctor and other health care professionals have access to the latest research. As argued by open access advocates, open access speeds research progress, productivity, and knowledge translation. [62] Every researcher in the world can read an article, not just those whose library can afford to subscribe to the particular journal in which it appears. Faster discoveries benefit everyone. High school and junior college students can gain the information literacy skills critical for the knowledge age. Critics of the various open access initiatives claim that there is little evidence that a significant amount of scientific literature is currently unavailable to those who would benefit from it. [63] While no library has subscriptions to every journal that might be of benefit, virtually all published research can be acquired via interlibrary loan . [64] Note that interlibrary loan may take a day or weeks depending on the loaning library and whether they will scan and email, or mail the article. Open access online, by contrast is faster, often immediate, making it more suitable than interlibrary loan for fast-paced research.

Low-income countries
In developing nations, open access archiving and publishing acquires a unique importance. Scientists, health care professionals, and institutions in developing nations often do not have the capital necessary to access scholarly literature, although schemes exist to give them access for little or no cost. Among the most important is HINARI , [65] the Health InterNetwork Access to Research Initiative, sponsored by the World Health Organization . HINARI, however, also has restrictions. For example, individual researchers may not register as users unless their institution has access, [66] and several countries that one might expect to have access do not have access at all (not even "low-cost" access) (e.g. South Africa). [66]
Many open access projects involve international collaboration. For example, the SciELO (Scientific Electronic Library Online), [67] is a comprehensive approach to full open access journal publishing, involving a number of Latin American countries. Bioline International , a non-profit organization dedicated to helping publishers in developing countries is a collaboration of people in the UK, Canada, and Brazil; the Bioline International Software is used around the world. Research Papers in Economics (RePEc), is a collaborative effort of over 100 volunteers in 45 countries. The Public Knowledge Project in Canada developed the open source publishing software Open Journal Systems (OJS), which is now in use around the world, for example by the African Journals Online group, and one of the most active development groups is Portuguese. This international perspective has resulted in advocacy for the development of open-source appropriate technology and the necessary open access to relevant information for sustainable development . [68] [69]

Implementation practices
There are various ways in which open access can be provided, with the two most common methods usually categorised as either gold or green open access.

Journals: gold open access
One option for authors who wish to make their work openly accessible is to publish in an open access journal ("gold open access"). There are many business models for open access journals. [70] Open access can be provided by traditional publishers, who may publish open access as well as subscription-based journals, or open access publishers such as Public Library of Science (PLOS), who publish only open access journals. An open access journal may or may not charge a publishing fee ; open access publishing does not necessarily mean that the author has to pay. Traditionally, many academic journals levied page charges, long before open access became a possibility. When open access journals do charge processing fees, it is the author's employer or research funder who typically pays the fee, not the individual author, and many journals will waive the fee in cases of financial hardship, or for authors in less-developed countries. Some no-fee journals have institutional subsidies. Examples of open access publishers [13] include BioMed Central and the Public Library of Science .
Roughly 30% [1] of gold open access journals have author fees to cover the cost of publishing (e.g. PLoS fees vary from $1,495 to $2,900 [71] ) instead of reader subscription fees. Advertising revenue and/or funding from foundations and institutions are also used to provide funding.

Self-archiving: green open access
Self-archiving, also known as green open access, refers to the practice of depositing articles in an open access repository , this can be an institutional or a disciplinary repository such as arXiv .
Green open access journal publishers [72] endorse immediate open access self-archiving by their authors. Open access self-archiving was first formally proposed in 1994 [73] [74] by Stevan Harnad in his " Subversive Proposal ". However, self-archiving was already being done by computer scientists in their local FTP archives in the 1980s, [75] later harvested into CiteSeer . What is deposited can be either a preprint , or the peer-reviewed postprint – either the author's refereed, revised final draft or the publisher's version of record.
To find out if a publisher or journal has given a green light to author self-archiving, the author can check the Publisher Copyright Policies and Self-Archiving list [76] on the SHERPA/RoMEO web site. The EPrints site also provides a FAQ [77] on self-archiving. Extensive details and links can also be found in the Open Access Archivangelism blog [78] and the Eprints Open Access site. [79]

Manner of distribution
Like the self-archived green open access articles, most gold open access journal articles are distributed via the World Wide Web , [1] due to low distribution costs, increasing reach, speed, and increasing importance for scholarly communication. Open source software is sometimes used for open access repositories , [80] open access journal websites , [81] and other aspects of open access provision and open access publishing.
Access to online content requires Internet access, and this distributional consideration presents physical and sometimes financial barriers to access. Proponents of open access argue that Internet access barriers are relatively low in many circumstances, that efforts should be made to subsidize universal Internet access, whereas pay-for-access presents a relatively high additional barrier over and above Internet access itself. [ citation needed ]
The Directory of Open Access Journals lists a number of peer-reviewed open access journals for browsing and searching. Open access articles can also often be found with a web search , using any general search engine or those specialized for the scholarly and scientific literature, such as OAIster and Google Scholar .

Policies and mandates
Many universities, research institutions and research funders have adopted mandates requiring their researchers to provide open access to their peer-reviewed research articles by self-archiving them in an open access repository. [82] Some publishers and publisher associations have lobbied against introducing mandates. [83] [84] [85]
The idea of mandating self-archiving was mooted at least as early as 1998. [86] Since 2003 [87] efforts have been focused on open access mandating by the funders of research: governments, [88] research funding agencies, [89] and universities. [82]
The Registry of Open Access Repository Mandatory Archiving Policies (ROARMAP) is a searchable international database charting the growth of open access mandates . As of May 2014, mandates have been adopted by over 200 universities (including Harvard, MIT, Stanford, University College London, and University of Edinburgh) and over 80 research funders worldwide. [8]

Funding issues
The " article processing charges " which are often used for open access journals shift the burden of payment from readers to authors (or their funders), which creates a new set of concerns. [90] One concern is that if a publisher makes a profit from accepting papers, it has an incentive to accept anything submitted, rather than selecting and rejecting articles based on quality. This could be remedied, however, by charging for the peer-review rather than acceptance. [91] Another concern is that institutional budgets may need to be adjusted in order to provide funding for the article processing charges required to publish in many open access journals (e.g. those published by BioMed Central [92] ). It has been argued that this may reduce the ability to publish research results due to lack of sufficient funds, leading to some research not becoming a part of the public record. [93]
Unless discounts are available to authors from countries with low incomes or external funding is provided to cover the cost, article processing charges could exclude authors from developing countries or less well-funded research fields from publishing in open access journals. However, under the traditional model, the prohibitive costs of some non-open access journal subscriptions already place a heavy burden on the research community; and if green open access self-archiving eventually makes subscriptions unsustainable, the cancelled subscription savings can pay the gold open access publishing costs without the need to divert extra money from research. [94] Moreover, many open access publishers offer discounts or publishing fee waivers to authors from developing countries or those suffering financial hardship. Self-archiving of non-open access publications provides a low cost alternative model. [95]
Another concern is the redirection of money by major funding agencies such as the National Institutes of Health and the Wellcome Trust from the direct support of research to the support open access publication. Robert Terry, Senior Policy Advisor at the Wellcome Trust, has said that he feels that 1–2% of their research budget will change from the creation of knowledge to the dissemination of knowledge. [96]
Research institutions could cover the cost of open access by converting to a open access journal cost-recovery model, with the institutions' annual tool access subscription savings being available to cover annual open access publication costs. [97] A 2017 study by the Max Planck Society the annual turnovers of academic publishers amount to approximately EUR 7.6 billion. It is argued that this money comes predominantly from publicly funded scientific libraries as they purchase subscriptions or licenses in order to provide access to scientific journals for their members. The study was presented by the Max Planck Digital Library and found that subscription budgets would be sufficient to fund the open access publication charges. [98]

History

Efforts before Internet
Even before the advent of the Internet various models were proposed to increase access to academic research.
One early proponent of the publisher-pays model was the physicist Leó Szilárd . To help stem the flood of low-quality publications, he jokingly suggested in the 1940s that at the beginning of his career each scientist should be issued with 100 vouchers to pay for his papers. Closer to the present, but still ahead of its time, was Common Knowledge . This was an attempt to share information for the good of all, the brainchild of Brower Murphy , formerly of The Library Corporation. Both Brower and Common Knowledge are recognised in the Library Microcomputer Hall of Fame. [99] One of Mahatma Gandhi 's earliest publications, Hind Swaraj published in Gujarati in 1909 is recognised as the intellectual blueprint of India's freedom movement. The book was translated into English the next year, with a copyright legend that read "No Rights Reserved". [100]
The modern open access movement (as a social movement ) traces its history at least back to the 1950s, with the Letterist International (LI) placing anything in their journal Potlatch in the public domain. As the LI merged to form the Situationist International , Guy Debord wrote to Patrick Straram "All the material published by the Situationist International is, in principle, usable by everyone, even without acknowledgement, without the preoccupations of literary property." This was to facilitate detournement . [101] It became much more prominent in the 1990s with the advent of the Digital Age . With the spread of the Internet and the ability to copy and distribute electronic data at no cost, the arguments for open access gained new importance. The fixed cost of producing the article is separable from the minimal marginal cost of the online distribution.

Early years of online open access
Probably the earliest book publisher to provide open access was the National Academies Press , publisher for the National Academy of Sciences , Institute of Medicine , and other arms of the National Academies . They have provided free online full-text editions of their books alongside priced, printed editions since 1994, and assert that the online editions promote sales of the print editions. As of June 2006 they had more than 3,600 books up online for browsing, searching, and reading.
While Editor-in-Chief of the Journal of Clinical Investigation , Ajit Varki made it the first major biomedical journal to be freely available on the web in 1996. [102] Varki wrote, "The vexing issue of the day is how to appropriately charge users for this electronic access. The nonprofit nature of the JCI allows consideration of a truly novel solution — not to charge anyone at all!" [103]
An explosion of interest and activity in open access journals has occurred since the 1990s, largely due to the widespread availability of Internet access. It is now possible to publish a scholarly article and also make it instantly accessible anywhere in the world where there are computers and Internet connections. The fixed cost of producing the article is separable from the minimal marginal cost of the online distribution.
These new possibilities emerged at a time when the traditional, print-based scholarly journals system was in a crisis. The number of journals and articles produced had been increasing at a steady rate; however the average cost per journal had been rising at a rate far above inflation for decades, and budgets at academic libraries have remained fairly static. [ citation needed ] The result was decreased access – ironically, just when technology has made almost unlimited access a very real possibility, for the first time. Libraries and librarians have played an important part in the open access movement, initially by alerting faculty and administrators to the serials crisis. The Association of Research Libraries developed the Scholarly Publishing and Academic Resources Coalition (SPARC), in 1997, an alliance of academic and research libraries and other organizations, to address the crisis and develop and promote alternatives, such as open access.
The first online-only, free-access journals (eventually to be called "open access journals") began appearing in the late 1980s and early 1990s. These journals typically used pre-existing infrastructure (such as e-mail or newsgroups ) and volunteer labor and were developed without any intent to generate profit. Examples include Bryn Mawr Classical Review , Postmodern Culture , Psycoloquy , and The Public-Access Computer Systems Review . [104]
The first free scientific online archive was arXiv.org , started in 1991, initially a preprint service for physicists, initiated by Paul Ginsparg . Self-archiving has become the norm in physics, with some sub-areas of physics, such as high-energy physics, having a 100% self-archiving rate. The prior existence of a "preprint culture" in high-energy physics is one major reason why arXiv has been successful. [105] arXiv now includes papers from related disciplines including computer science, mathematics, nonlinear sciences, quantitative biology, quantitative finance, and statistics. [106] However, computer scientists mostly self-archive on their own websites and have been doing so for even longer than physicists. arXiv now includes postprints as well as preprints. [107] The two major physics publishers, American Physical Society and Institute of Physics Publishing, have reported that arXiv has had no effect on journal subscriptions in physics; even though the articles are freely available, usually before publication, physicists value their journals and continue to support them. [108]
Computer scientists had been self-archiving on their own FTP sites and then their websites since even earlier than the physicists, as was revealed when Citeseer began harvesting their papers in the late 1990s. Citeseer is a computer science archive that harvests, Google -style, from distributed computer science websites and institutional repositories , and contains almost twice as many papers as arXiv. The 1994 " Subversive Proposal " [109] was to extend self-archiving to all other disciplines; from it arose CogPrints (1997) and eventually the OAI -compliant generic GNU Eprints.org software in 2000. [110]
In 1997, the U.S. National Library of Medicine (NLM) made Medline , the most comprehensive index to medical literature on the planet, freely available in the form of PubMed . Usage of this database increased a tenfold when it became free, strongly suggesting that prior limits on usage were impacted by lack of access. While indexes are not the main focus of the open access movement, Medline is important in that it opened up a whole new form of use of scientific literature – by the public, not just professionals. [111] The Journal of Medical Internet Research ( JMIR ), [112] one of the first open access journals in medicine, was created in 1998, publishing its first issue in 1999.
In 1998, the American Scientist Open Access Forum [113] was launched (and first called the "September98 Forum").
One of the first humanities journals published in open access is CLCWeb: Comparative Literature and Culture [114] founded at the University of Alberta in 1998 with its first issue published in March 1999 and since 2000 published by Purdue University Press .
In 1999, Harold Varmus of the NIH proposed a journal called E-biomed, intended as an open access electronic publishing platform combining a preprint server with peer-reviewed articles. [115] E-biomed later saw light in a revised form [116] as PubMed Central , a postprint archive.
It was also in 1999 that the Open Archives Initiative and its OAI-PMH protocol for metadata harvesting was launched in order to make online archives interoperable.

2000s
In 2000, BioMed Central , a for-profit open access publisher, was launched by the then Current Science Group (the founder of the Current Opinion series, and now known as the Science Navigation Group). [117] In some ways, BioMed Central resembles Harold Varmus ' original E-biomed proposal more closely than does PubMed Central . [118] As of October 2013 BioMed Central publishes over 250 journals. [119]
In 2001, 34,000 [120] scholars around the world signed "An Open Letter to Scientific Publishers", calling for "the establishment of an online public library that would provide the full contents of the published record of research and scholarly discourse in medicine and the life sciences in a freely accessible, fully searchable, interlinked form". [121] Scientists signing the letter also pledged not to publish in or peer-review for non-open access journals. This led to the establishment of the Public Library of Science , an advocacy organization. However, most scientists continued to publish and review for non-open access journals. PLoS decided to become an open access publisher aiming to compete at the high quality end of the scientific spectrum with commercial publishers and other open access journals, which were beginning to flourish. [122] Critics have argued that, equipped with a $10 million grant, PLoS competes with smaller open access journals for the best submissions and risks destroying what it originally wanted to foster. [123]
The first major international statement on open access was the Budapest Open Access Initiative in February 2002, launched by the Open Society Institute . [80] This provided the first definition of open access, and has a growing list of signatories. [124] Two further statements followed: the Bethesda Statement on Open Access Publishing [125] in June 2003 and the Berlin Declaration on Open Access to Knowledge in the Sciences and Humanities in October 2003. Also in 2003, the World Summit on the Information Society included open access in its Declaration of Principles and Plan of Action. [126]
In 2006, a Federal Research Public Access Act was introduced in US Congress by senators John Cornyn and Joe Lieberman . [127] [128] The act continues to be brought up every year since then, but has never made it past committee. [129]
The year 2007 recorded some backlash from non-OA publishers. [130]
In 2008, Ajit Varki worked with David Lipman to create the first viable model for a major Open Access textbook hosted at NCBI, the 2nd. Edition of the Essentials of Glycobiology . [131]
Perhaps the first dedicated publisher of open access monographs in the humanities was re.press who published their first title in that 2006. Two years later in 2008 Open Humanities Press , another publisher of humanities monographs, was launched. Most recently, the Open Library of Humanities launched in September 2015.
In 2008, USENIX , the advanced computing systems association, implemented an open access policy for their conference proceedings. In 2011 they added audio and video recordings of paper presentations to the material to which they provide open access. [132]

2010s
In 2013, John Holdren , Barack Obama 's director of the Office of Science and Technology Policy , issued a memorandum directing United States' Federal Agencies with more than $100 million in annual R&D expenditures to develop plans within six months to make the published results of federally funded research freely available to the public within one year of publication. [133] [134] As of March 2015, two agencies had made their plans public: the Department of Energy [135] and the National Science Foundation . [136]
In 2013, the UK Higher Education Funding Council for England (HEFCE) proposed adopting a mandate that in order to be eligible for submission to the UK Research Excellence Framework (REF) all peer-reviewed journal articles submitted after 2014 must be deposited in the author's institutional repository immediately upon acceptance for publication , regardless of whether the article is published in a subscription journal or in an open access journal . HEFCE expresses no journal preference, places no restriction on authors' choice and requires the deposit itself to be immediate, irrespective of whether the publisher imposes an embargo (for an allowable embargo period that remains to be decided) on the date at which access to the deposit can be made open. [137] [138] The HEFCE/REF mandate proposal complements the recent Research Councils UK (RCUK) mandate that requires all articles resulting from RCUK funding to be made open access by 6 months after publication at the latest (12 months for arts and humanities articles). [139]
HEFCE also provided grants to universities in England [140] wishing to participate in the Pilot Collection of Knowledge Unlatched , a not-for-profit organisation enabling humanities and social sciences monographs to become open access. The Pilot Collection ran from October 2013 to February 2014 and 297 libraries and institutions worldwide participated in 'unlatching' the collection of 28 titles. 61 of these participating institutions were university libraries in England eligible for the HEFCE grant of 50% towards the $1195 participation fee. [141]
The Indian Council of Agricultural Research had adopted an Open Access policy [142] for its publications on 13 September 2013 [143] and announced that each ICAR institute would set-up an open access institutional repository. One such repository is eprints@cmfri , an open access institutional repository of the Central Marine Fisheries Research Institute which was set-up on 25 February 2010 well before the policy was adopted. [144] However, since March 2010, the ICAR is making available its two flagship journals under Open Access [145] on its website and later through an online platform called Indian Agricultural Research Journals using Open Journal Systems .
In 2014, the Department of Biotechnology and Department of Science and Technology , under Ministry of Science and Technology , Government of India jointly announced their open access policy. [146]
In May 2016 the European Union announced that "all scientific articles in Europe must be freely accessible as of 2020" [147] and that the Commission will "develop and encourage measures for optimal compliance with the provisions for open access to scientific publications under Horizon 2020 ". [148] Some ask such measures to include the usage of free and open-source software . [149]

Growth
A study published in 2010 showed that roughly 20% of the total number of peer-reviewed articles published in 2008 could be found openly accessible. [150] Another study found that by 2010, 7.9% of all academic journals with impact factors were gold open access journals and showed a broad distribution of Gold Open Access journals throughout academic disciplines. [151] 8.5% of the journal literature could be found free at the publishers’ sites (gold open access), of which 62% in full open access journals, 14% in delayed-access subscription journals, and 24% as individually open articles in otherwise subscription journals. For an additional 11.9% of the articles, open access full text copies were available via green open access in either subject-based repositories (43%), institutional repositories (24%) or on the home pages of the authors or their departments (33%). These copies were further classified into exact copies of the published article (38%), manuscripts as accepted for publishing (46%) or manuscripts as submitted (15%). [150]
In the 2010 study, of all scientific fields chemistry had the lowest overall share of open access (13%), while Earth Sciences had the highest (33%). In medicine, biochemistry and chemistry gold publishing in open access journals was more common than author self-archiving. In all other fields self-archiving was more common.
In August 2013, a study done for the European Commission reported that 50% of a random sample of all articles published in 2011 as indexed by Scopus were freely accessible online by the end of 2012. [152] [153] [154] A 2017 study by the Max Planck Society put the share of gold access articles in pure open access journals at around 13 percent of total research papers. [155]

Journals
A study on the development of publishing of open access journals from 1993 to 2009 [156] published in 2011 suggests that, measured both by the number of journals as well as by the increases in total article output, direct gold open access journal publishing has seen rapid growth particularly between the years 2000 and 2009. It was estimated that there were around 19,500 articles published open access in 2000, while the number has grown to 191,850 articles in 2009. The journal count for the year 2000 is estimated to have been 740, and 4769 for 2009; numbers which show considerable growth, albeit at a more moderate pace than the article-level growth. These findings support the notion that open access journals have increased both in numbers and in average annual output over time.
The development of the number of active open access journals and the number of research articles published in them during the period 1993–2009 is shown in the figure above. If these gold open access growth curves are extrapolated to the next two decades, the Laakso et al. (Björk) curve would reach 60% in 2022, and the Springer curve would reach 50% in 2029 as shown in the figure below (the reference provides a more optimistic interpretation which does not match with the values shown in the figure). [157]

Self-archiving
The Registry of Open Access Repositories (ROAR) indexes the creation, location and growth of open access open access repositories and their contents. [8] As of December 2015, over 3,500 institutional and cross-institutional repositories have been registered in ROAR. [158]

Finding open access research online
There are various open access aggregators that index open access journals or articles. ROAD synthesizes information about open access journals and is a subset of the ISSN registry. Users may browse to find open access journals by country or by subject. SHERPA/RoMEO lists international publishers that allow the published version of articles to be deposited in institutional repositories . The Directory of Open Access Journals (DOAJ) contains over 8,000 open access journals of varying open access policies that scholars can search and browse. [159] The Open Archives Initiative (OAI) lists 2937 conforming repositories . Searching each open access repository individually is impractical. The resources in these repositories can be harvested, using the OAI Protocol and aggregated into online systems which in-turn provide access to millions of resources from a single online location. [160]

See also
WebPage index: 00172
National Institutes of Health
The National Institutes of Health ( NIH ) is the primary agency of the United States government responsible for biomedical and public health research, founded in the late 1870s. It is part of the United States Department of Health and Human Services with facilities mainly located in Bethesda, Maryland . It conducts its own scientific research through its Intramural Research Program (IRP) and provides major biomedical research funding to non-NIH research facilities through its Extramural Research Program.
As of 2013 [update] , the IRP had 1,200 principal investigators and more than 4,000 postdoctoral fellows in basic, translational, and clinical research, being the largest biomedical research institution in the world, [3] while, as of 2003, the extramural arm provided 28% of biomedical research funding spent annually in the U.S., or about US$26.4 billion. [4]
The NIH comprises 27 separate institutes and centers of different biomedical disciplines and is responsible for many scientific accomplishments, including the discovery of fluoride to prevent tooth decay , the use of lithium to manage bipolar disorder , and the creation of vaccines against hepatitis , Haemophilus influenzae (HIB), and human papillomavirus (HPV). [5]

History
NIH's roots extend back to a Marine Hospital Service in the late 1790s that provided medical relief to sick and disabled men in the U.S. Navy. By 1870, a network of marine hospitals had developed and was placed under the charge of a medical officer within the Bureau of the Treasury Department. In the late 1870s, Congress allocated funds to investigate the causes of epidemics like cholera and yellow fever, and it created the National Board of Health, making medical research an official government initiative. [7]
In 1887, a laboratory for the study of bacteria, the Hygienic Laboratory, was established at the Marine Hospital in New York. [8] [9] In the early 1900s, Congress began appropriating funds for the Marine Hospital Service. By 1922, this organization changed its name to Public Health Services and established a Special Cancer Investigations laboratory at Harvard Medical School . This marked the beginning of a partnership with universities. In 1930, the Hygienic Laboratory was re-designated as the National Institute of Health by the Ransdell Act and was given $750,000 to construct two NIH buildings. Over the next few decades, Congress would increase its funding tremendously to the NIH, and various institutes and centers within the NIH were created for specific research programs. [10] In 1944, the Public Health Service Act was approved, and National Cancer Institute became a division of NIH. In 1948, the name changed from National Institute of Health to National Institutes of Health.
In the 1960s, virologist and cancer researcher Chester M. Southam injected HeLa cancer cells into patients at the Jewish Chronic Disease Hospital. [11] :130 When three doctors resigned after refusing to inject patients without their consent, the experiment gained considerable media attention. [11] :133 The NIH was a major source of funding for Southam’s research and had required all research involving human subjects to obtain their consent prior to any experimentation. [11] :135 Upon investigating all of their grantee institutions, the NIH discovered that the majority of them did not protect the rights of human subjects. From then on, the NIH has required all grantee institutions to approve any research proposals involving human experimentation with review boards. [11] :135
In 1967, the Division of Regional Medical Programs was created to administer grants for research for heart disease, cancer, and strokes. That same year, the NIH director lobbied the White House for increased federal funding in order to increase research and the speed with which health benefits could be brought to the people. An advisory committee was formed to oversee further development of the NIH and its research programs. By 1971, cancer research was in full force and President Nixon signed the National Cancer Act, initiating a National Cancer Program, President's Cancer Panel, National Cancer Advisory Board, and 15 new research, training, and demonstration centers. [ citation needed ]
The funding of NIH has often been a source of contention in Congress, serving as a proxy for the political currents of the time. During the 1980s, President Reagan repeatedly tried to cut funding for research, only to see Congress partly restore funding. The political contention over NIH funding slowed the nation's response to the AIDS epidemic; while AIDS was reported in newspaper articles from 1981, no funding was provided for research on the disease. In 1984, National Cancer Institute scientists found implications that "variants of a human cancer virus called HTLV-III are the primary cause of acquired immunodeficiency syndrome (AIDS)," a new epidemic that gripped the nation. [12] It was not until July 1987, as NIH celebrated its 100th anniversary, that President Reagan announced a committee to research the HIV epidemic. [ citation needed ]
By the 1990s, the NIH committee focus had shifted to DNA research, and launched the Human Genome Project .
In 1992, the NIH encompassed nearly 1 percent of the federal government's operating budget and controlled more than 50 percent of all funding for health research, and 85 percent of all funding for health studies in universities. [13] In 2001 President Bush banned federally funded stem-cell research, a ban which President Obama revoked in 2009. [ citation needed ]
Under the Clinton administration 1993-2001 the NIH budget doubled. Since then, funding essentially remained flat, and during the decade following the financial crisis, the NIH budget struggled to keep up with inflation. [14]

Directors

Locations and campuses
Intramural research is primarily conducted at the main campus in Bethesda, Maryland , and the surrounding communities. The National Institute on Aging and the National Institute on Drug Abuse are located in Baltimore , Maryland, and the National Institute of Environmental Health Sciences is located in the Research Triangle region of North Carolina . The National Institute of Allergy and Infectious Diseases maintains its Rocky Mountain Labs in Hamilton, Montana , [16] with an emphasis on BSL3 and BSL4 laboratory work.

Research
NIH devotes 10% of its funding to research within its own facilities (intramural research). The institution gives 80% of its funding in research grants to extramural (outside) researchers. Of this extramural funding, a certain percentage (2.8% in 2014) must be granted to small businesses under the SBIR/STTR program. [17] The extramural funding consists of about 50,000 grants to more than 325,000 researchers at more than 3000 institutions. [18] In FY 2010 [update] , NIH spent US$10.7bn (not including temporary funding from the American Recovery and Reinvestment Act of 2009 ) on clinical research , US$7.4bn on genetics -related research, US$6.0bn on prevention research, US$5.8bn on cancer, and US$5.7bn on biotechnology . [19]

Public Access Policy
In 2008 a Congressional mandate called for investigators funded by the NIH to submit an electronic version of their final manuscripts to the National Library of Medicine 's research repository, PubMed Central (PMC), no later than 12 months after the official date of publication. [20] The NIH Public Access Policy was the first public access mandate for a U.S. public funding agency. [21]

NIH Interagency Pain Research Coordinating Committee
On February 13, 2012, the National Institutes of Health (NIH) announced a new group of individuals assigned to research pain. This committee is composed of researchers from different organizations and will focus to "coordinate pain research activities across the federal government with the goals of stimulating pain research collaboration… and providing an important avenue for public involvement" ("Members of new," 2012). With a committee such as this research will not be conducted by each individual organization or person but instead a collaborating group which will increase the information available. With this hopefully more pain management will be available including techniques for arthritis sufferers.

NIH Toolbox
In September 2006, the NIH Blueprint for Neuroscience Research started a contract for the NIH Toolbox for the Assessment of Neurological and Behavioral Function to develop a set of state-of-the-art measurement tools to enhance collection of data in large cohort studies. Scientists from more than 100 institutions nationwide contributed. In September 2012, the NIH Toolbox was rolled out to the research community. NIH Toolbox assessments are based, where possible, on Item Response Theory and adapted for testing by computer. [ citation needed ]

Economic impact
In 2000, the Joint Economic Committee of Congress reported NIH research, which was funded at $16 billion a year in 2000, that some econometric studies had given a rate of return of 25 to 40 percent per year by reducing the economic cost of illness in the US. It found that of the 21 drugs with the highest therapeutic impact on society introduced between 1965 and 1992, public funding was "instrumental" for 15. [22] As of 2011 NIH-supported research helped to discover 153 new FDA-approved drugs, vaccines, and new indications for drugs in the 40 years prior. [23] In 2015, the National Bureau of Economic Research estimated $10 million invested in research generated two to three new patents. [24]

Funding
Budget and politics . To allocate funds, the NIH must first obtain its budget from Congress. This process begins with institute and center (IC) leaders collaborating with scientists to determine the most important and promising research areas within their fields. IC leaders discuss research areas with NIH management who then develops a budget request for continuing projects, new research proposals, and new initiatives from the Director. NIH submits its budget request to the Department of Health and Human Services (HHS), and the HHS considers this request as a portion of its budget. Many adjustments and appeals occur between NIH and HHS before the agency submits NIH's budget request to the Office of Management and Budget (OMB). OMB determines what amounts and research areas are approved for incorporation into the President's final budget. The President then sends NIH's budget request to Congress in February for the next fiscal year's allocations. [25] The House and Senate Appropriations Subcommittees deliberate and by fall, Congress usually appropriates funding. This process takes approximately 18 months before the NIH can allocate any actual funds. [26]
In 1999, Congress increased the NIH's budget by $2.3 billion [27] to $17.2 billion in 2000. [28] In 2009 Congress again increased the NIH budget to $31 billion in 2010. [28] In March 2017, President Trump proposed to cut the 2018 budget by 18.3%, or about $5.8 billion to $25.9 billion. [29] :26
Over the last century, the responsibility to allocate funding has shifted from the OD and Advisory Committee to the individual ICs and Congress increasingly set apart funding for particular causes. In the 1970s, Congress began to earmark funds specifically for cancer research, and in the 1980s there was a significant amount allocated for AIDS/HIV research. [27]
Decision criteria . NIH employs five broad decision criteria in its funding policy. First, ensure the highest quality of scientific research by employing an arduous peer review process. Second, seize opportunities that have the greatest potential to yield new knowledge and that will lead to better prevention and treatment of disease . Third, maintain a diverse research portfolio in order to capitalize on major discoveries in a variety of fields such as cell biology, genetics, physics, engineering, and computer science. Fourth, address public health needs according to the disease burden (e.g., prevalence and mortality). And fifth, construct and support the scientific infrastructure (e.g., well-equipped laboratories and safe research facilities) necessary to conduct research. [30]
In 2007 the director of the agency stated "responsibilities for identifying ... FCOIs (financial conflict of interest ) must remain with grantee institutions" but institutions that administer grants have no interest to identify grantee's conflicts of interest. [31]
The NIH issued dozens of waivers for NIH's advisory committee members up to 2012. Such waivers exempt a conflicted government employee from ethics laws. Since 2005 the U.S. Office of Government Ethics had documented only three times where the NIH consulted with the office as required by law, and none of the waivers in question had to do with a member of an advisory committee. [32] Advisory committee members advise the Institute on policy and procedures affecting the external research programs and provide a second level of review for all grant and cooperative agreement applications considered by the Institute for funding. [33]

Research project grants
Researchers outside NIH, that is, at universities or other institutions can apply for research project grants (RPGs) from the NIH.There are numerous funding mechanisms for different project types (e.g., basic research, clinical research etc.) and career stages (e.g., early career, postdoc fellowships etc.). Importantly, the NIH regularly issues requests for applications (RFAs) , e.g., on timely medical problems (such as Zika virus research in early 2016). In addition, researchers can apply for investigator-initiated grants whose subject is completely determined by the scientist.
Number of applicants and funding success . The total number of unique applicants has increased substantially, from about 60,000 investigators who had applied during the period from 1999 to 2003 to slightly less than 90,000 in who had applied during the period from 2011 to 2015. [34] However, the "cumulative investigator rate," that is, the likelihood that unique investigators are funded over a 5-year window has declined from 43% to 31%. [34]
R01 grants are the major funding mechanism and include investigator-initiated projects (as opposed to NIH-initiated requests for applications). The roughly 27,000 to 29,000 R01 applications had a funding success of 17-19% during 2012 though 2014. Similarly, the 13,000 to 14,000 R21 applications had a funding success of 13-14% during the same period. [35]
Government shutdown
When a government shutdown occurs, the NIH continues to treat people who are already enrolled in clinical trials , but does not start any new clinical trials and does not admit new patients who are not already enrolled in a clinical trial, except for the most critically ill, [36] [37] [38] as determined by the NIH Director. [39]
Gender and sex bias
In 2014 it was announced that the NIH is directing scientists to perform their experiments with both female and male animals, or cells derived from females as well as males if they are studying cell cultures, and that the NIH would take the balance of each study design into consideration when awarding grants. [40] However, the announcement also stated that this rule would probably not apply when studying sex-specific diseases (for example, ovarian or testicular cancer). [40]

Grant allocation bias controversy
In 2011 a paper published in Science found that black researchers were 10% less likely to win NIH R01 grants (the oldest and most widely used) than white researchers, after controlling for "educational background, country of origin, training, previous research awards, publication record, and employer characteristics." It also found that black researchers are significantly less likely to resubmit an unapproved grant than white researchers. [41] The study lead and economist Donna Grant said that grant reviewers do not have access to the applicant race, but may infer it from biographies or names. She also speculated that the decreased re-submission rate may be due to lack of mentoring. The study, which was commissioned by the NIH, included in its analysis 83,000 grant applications, made between 2000 and 2006. [42] Dr. Otis W. Brawley, chief medical officer at the American Cancer Society and a black man, commented on the cause of the disparity as one unrelated to racism per se, but rather to the reviewers' unconscious tendency to more likely give the benefit of the doubt to someone they are familiar with, in a scientific world where black researchers tend to keep a lower profile than other groups. The study did not reveal similar difficulties for members of other races and ethnic groups (e.g., Hispanics). [42]

Stakeholders
Many groups are highly invested in NIH funding.

General public
One of the goals of the NIH is to "expand the base in medical and associated sciences in order to ensure a continued high return on the public investment in research." [43] Taxpayer dollars funding NIH are from the taxpayers, making them the primary beneficiaries of advances in research. Thus, the general public is a key stakeholder in the decisions resulting from the NIH funding policy. Congress theoretically represents the public interest as the NIH Advisory Committee allocates to the NIH, and the funds to the Director. [44] However, many in the general public do not feel their interests are being accurately represented. As a result, individuals have formed patient advocacy groups to represent their own interests. [45] Patient advocacy groups tend to focus on specific aspects of health care or diseases. Advocates get involved in many different areas such as organizing awareness campaigns, promoting patients' rights, and enhancing health policy initiatives. Most importantly, patient advocacy groups are often involved with advisory panels to ensure that current projects and those projects being considered for funding will directly impact patients' lives, improve delivery of care, and provide support for tertiary care. Advocacy groups strive to promote a health care system that is beneficial for all parties involved. Through congressional representation, NIH Advisory Committee efforts, and patient advocacy groups, the public is able to influence funding allocation as well as the policy itself. [46]

Extramural researchers and scientists
Other important stakeholders of the NIH funding policy are the researchers and scientists themselves. Extramural researchers differ from intramural researchers in that they are not employed by the NIH but must apply for funding. Throughout the history of the NIH, the amount of funding received has increased, but the proportion to each IC remains relatively constant. The individual ICs then decide who will receive the grant money and how much will be allotted. Research funding is important to extramural researchers for multiple reasons. Without the help of an NIH grant (or a similar type of funding), researchers and scientists are unable to pursue their own research interests but are obliged to follow the agenda of the company or university for which they work. This could potentially hinder discoveries in novel research areas. In 2000, Brian Jacobs and Lars Lefgren researched extensively the impact of NIH grants on basic research and development, and the careers of grant recipients. For the period of 1980–2000, they reviewed all postdoctoral research grants and standard research grants for those who received funding and those who did not. [47] Jacobs and Lefgren found that scientists who received postdoctoral research grants were 20 percent more likely to be published within the first five years after receiving the grant. [47] They also found that scientists who received grants were 11 percent more likely to have one publication and 23 percent more likely to have five publications. Due to the 'publish or perish' standard that many researchers face, NIH funding can have a great impact on researchers' careers. [47] Receiving a standard research grant also has a significant impact on researchers. Young scientists who receive a first-time grant (R01) usually produce more than one additional publication in the five-year period after they receive the grant. Those who receive an NIH grant will typically receive $252,000 more in NIH funding in the following six to ten years, [47] and a statistically significant relationship exists between scientists receiving NIH grants and their research productivity throughout their careers.
Policy changes on who receives funding also significantly affect researchers. For example, the NIH has recently attempted to approve more first-time NIH R01 applicants, or the research grant applications of young scientists. To encourage the participation of young scientists who potentially think outside the box, the application process has been shortened and made easier. [48] In addition, first-time applicants are being offered more funding for their research grants than those who have received grants in the past. [49] Although this change provides greater opportunities for young scientists, it also places older, more experienced scientists at a funding disadvantage.

Commercial partnerships
In 2011 and 2012 the Department of Health and Human Services Office of Inspector General published a series of audit reports revealing that throughout the fiscal years 2000–2010, institutes under the aegis of the NIH, did not comply with the time and amount requirements specified in appropriations statutes, in awarding federal contracts to commercial partners, committing the federal government to tens of millions of dollars of expenditure ahead of appropriation of funds from Congress. [50]

Institutes and centers
The NIH is composed of 27 separate institutes and centers (ICs) that conduct and coordinate research across different disciplines of biomedical science. These are:
In addition, the National Center for Research Resources operated from April 13, 1962 to December 23, 2011.

See also
WebPage index: 00173
Law
Law is a system of rules that are created and enforced through social or governmental institutions to regulate behavior . [2] Law as a system helps regulate and ensure that a community show respect, and equality amongst themselves. State-enforced laws can be made by a collective legislature or by a single legislator, resulting in statutes , by the executive through decrees and regulations , or established by judges through precedent , normally in common law jurisdictions. Private individuals can create legally binding contracts , including arbitration agreements that may elect to accept alternative arbitration to the normal court process. The formation of laws themselves may be influenced by a constitution , written or tacit, and the rights encoded therein. The law shapes politics , economics , history and society in various ways and serves as a mediator of relations between people.
A general distinction can be made between (a) civil law jurisdictions (including Catholic canon law and socialist law ), in which the legislature or other central body codifies and consolidates their laws, and (b) common law systems, where judge-made precedent is accepted as binding law. Historically, religious laws played a significant role even in settling of secular matters, and is still used in some religious communities, particularly Jewish and Islamic. Islamic Sharia law is the world's most widely used religious law, and is used as the primary legal system in some countries, such as Iran and Saudi Arabia . [3]
The adjudication of the law is generally divided into two main areas referred to as (i) Criminal law and (ii) Civil law. Criminal law deals with conduct that is considered harmful to social order and in which the guilty party may be imprisoned or fined. Civil law (not to be confused with civil law jurisdictions above) deals with the resolution of lawsuits (disputes) between individuals or organizations. [4]
Law provides a rich source of scholarly inquiry into legal history , philosophy , economic analysis and sociology . Law also raises important and complex issues concerning equality, fairness, and justice . There is an old saying that ' all are equal before the law ', although Jonathan Swift argued that 'Laws are like cobwebs, which may catch small flies, but let wasps and hornets break through.' In 1894, the author Anatole France said sarcastically, "In its majestic equality, the law forbids rich and poor alike to sleep under bridges, beg in the streets, and steal loaves of bread." [5] Writing in 350 BC, the Greek philosopher Aristotle declared, "The rule of law is better than the rule of any individual ." [6] Mikhail Bakunin said: "All law has for its object to confirm and exalt into a system the exploitation of the workers by a ruling class". [7] Cicero said "more law, less justice". [8] Marxist doctrine asserts that law will not be required once the state has withered away . [9] Regardless of one's view of the law, it remains today a completely central institution.

Definition

Mainstream definitions
Numerous definitions of law have been put forward over the centuries. The Third New International Dictionary from Merriam-Webster [10] defines law as: "Law is a binding custom or practice of a community; a rule or mode of conduct or action that is prescribed or formally recognized as binding by a supreme controlling authority or is made obligatory by a sanction (as an edict, decree, rescript, order, ordinance, statute, resolution, rule, judicial decision, or usage) made, recognized, or enforced by the controlling authority."
The Dictionary of the History of Ideas published by Scribner's in 1973 defined the concept of law accordingly as: "A legal system is the most explicit, institutionalized, and complex mode of regulating human conduct. At the same time, it plays only one part in the congeries of rules which influence behavior, for social and moral rules of a less institutionalized kind are also of great importance." [11]

Whether it is possible or desirable to define law
There have been several attempts to produce "a universally acceptable definition of law". In 1972, one source indicated that no such definition could be produced. [12] McCoubrey and White said that the question "what is law?" has no simple answer. [13] Glanville Williams said that the meaning of the word "law" depends on the context in which that word is used. He said that, for example, " early customary law " and " municipal law " were contexts where the word "law" had two different and irreconcilable meanings. [14] Thurman Arnold said that it is obvious that it is impossible to define the word "law" and that it is also equally obvious that the struggle to define that word should not ever be abandoned. [15] It is possible to take the view that there is no need to define the word "law" (e.g. "let's forget about generalities and get down to cases "). [16]

History
The history of law links closely to the development of civilization . Ancient Egyptian law, dating as far back as 3000 BC, contained a civil code that was probably broken into twelve books. It was based on the concept of Ma'at , characterised by tradition, rhetorical speech, social equality and impartiality. [17] [18] By the 22nd century BC, the ancient Sumerian ruler Ur-Nammu had formulated the first law code , which consisted of casuistic statements ("if … then ..."). Around 1760 BC, King Hammurabi further developed Babylonian law , by codifying and inscribing it in stone. Hammurabi placed several copies of his law code throughout the kingdom of Babylon as stelae , for the entire public to see; this became known as the Codex Hammurabi . The most intact copy of these stelae was discovered in the 19th century by British Assyriologists, and has since been fully transliterated and translated into various languages, including English, Italian, German, and French. [19]
The Old Testament dates back to 1280 BC and takes the form of moral imperatives as recommendations for a good society. The small Greek city-state, ancient Athens , from about the 8th century BC was the first society to be based on broad inclusion of its citizenry, excluding women and the slave class. However, Athens had no legal science or single word for "law", [20] relying instead on the three-way distinction between divine law ( thémis ), human decree ( nomos ) and custom ( díkē ). [21] Yet Ancient Greek law contained major constitutional [ disambiguation needed ] innovations in the development of democracy . [22]
Roman law was heavily influenced by Greek philosophy, but its detailed rules were developed by professional jurists and were highly sophisticated. [23] [24] Over the centuries between the rise and decline of the Roman Empire , law was adapted to cope with the changing social situations and underwent major codification under Theodosius II and Justinian I . [25] Although codes were replaced by custom and case law during the Dark Ages , Roman law was rediscovered around the 11th century when medieval legal scholars began to research Roman codes and adapt their concepts. Latin legal maxims (called brocards ) were compiled for guidance. In medieval England, royal courts developed a body of precedent which later became the common law . A Europe-wide Law Merchant was formed so that merchants could trade with common standards of practice rather than with the many splintered facets of local laws. The Law Merchant, a precursor to modern commercial law, emphasised the freedom to contract and alienability of property. [26] As nationalism grew in the 18th and 19th centuries, the Law Merchant was incorporated into countries' local law under new civil codes. The Napoleonic and German Codes became the most influential. In contrast to English common law, which consists of enormous tomes of case law, codes in small books are easy to export and easy for judges to apply. However, today there are signs that civil and common law are converging. [27] EU law is codified in treaties, but develops through the precedent laid down by the European Court of Justice .
Ancient India and China represent distinct traditions of law, and have historically had independent schools of legal theory and practice. The Arthashastra , probably compiled around 100 AD (although it contains older material), and the Manusmriti (c. 100–300 AD) were foundational treatises in India, and comprise texts considered authoritative legal guidance. [28] Manu's central philosophy was tolerance and pluralism , and was cited across Southeast Asia. [29] This Hindu tradition, along with Islamic law, was supplanted by the common law when India became part of the British Empire . [30] Malaysia, Brunei, Singapore and Hong Kong also adopted the common law. The eastern Asia legal tradition reflects a unique blend of secular and religious influences. [31] Japan was the first country to begin modernising its legal system along western lines, by importing bits of the French , but mostly the German Civil Code. [32] This partly reflected Germany's status as a rising power in the late 19th century. Similarly, traditional Chinese law gave way to westernisation towards the final years of the Qing Dynasty in the form of six private law codes based mainly on the Japanese model of German law. [33] Today Taiwanese law retains the closest affinity to the codifications from that period, because of the split between Chiang Kai-shek 's nationalists, who fled there, and Mao Zedong 's communists who won control of the mainland in 1949. The current legal infrastructure in the People's Republic of China was heavily influenced by Soviet Socialist law , which essentially inflates administrative law at the expense of private law rights. [34] Due to rapid industrialisation, today China is undergoing a process of reform, at least in terms of economic, if not social and political, rights. A new contract code in 1999 represented a move away from administrative domination. [35] Furthermore, after negotiations lasting fifteen years, in 2001 China joined the World Trade Organisation . [36]

Legal theory

Philosophy
The philosophy of law is commonly known as jurisprudence. Normative jurisprudence asks "what should law be?", while analytic jurisprudence asks "what is law?" John Austin 's utilitarian answer was that law is "commands, backed by threat of sanctions, from a sovereign, to whom people have a habit of obedience". [38] Natural lawyers on the other side, such as Jean-Jacques Rousseau , argue that law reflects essentially moral and unchangeable laws of nature. The concept of "natural law" emerged in ancient Greek philosophy concurrently and in connection with the notion of justice, and re-entered the mainstream of Western culture through the writings of Thomas Aquinas , notably his Treatise on Law .
Hugo Grotius , the founder of a purely rationalistic system of natural law, argued that law arises from both a social impulse—as Aristotle had indicated—and reason. [39] Immanuel Kant believed a moral imperative requires laws "be chosen as though they should hold as universal laws of nature". [40] Jeremy Bentham and his student Austin, following David Hume , believed that this conflated the "is" and what "ought to be" problem. Bentham and Austin argued for law's positivism ; that real law is entirely separate from "morality". [41] Kant was also criticised by Friedrich Nietzsche , who rejected the principle of equality, and believed that law emanates from the will to power , and cannot be labelled as "moral" or "immoral". [42] [43] [44]
In 1934, the Austrian philosopher Hans Kelsen continued the positivist tradition in his book the Pure Theory of Law . [45] Kelsen believed that although law is separate from morality, it is endowed with "normativity", meaning we ought to obey it. While laws are positive "is" statements (e.g. the fine for reversing on a highway is €500); law tells us what we "should" do. Thus, each legal system can be hypothesised to have a basic norm ( Grundnorm ) instructing us to obey. Kelsen's major opponent, Carl Schmitt , rejected both positivism and the idea of the rule of law because he did not accept the primacy of abstract normative principles over concrete political positions and decisions. [46] Therefore, Schmitt advocated a jurisprudence of the exception ( state of emergency ), which denied that legal norms could encompass all of political experience. [47]
Later in the 20th century, H. L. A. Hart attacked Austin for his simplifications and Kelsen for his fictions in The Concept of Law . [48] Hart argued law is a system of rules, divided into primary (rules of conduct) and secondary ones (rules addressed to officials to administer primary rules). Secondary rules are further divided into rules of adjudication (to resolve legal disputes), rules of change (allowing laws to be varied) and the rule of recognition (allowing laws to be identified as valid). Two of Hart's students continued the debate: In his book Law's Empire , Ronald Dworkin attacked Hart and the positivists for their refusal to treat law as a moral issue. Dworkin argues that law is an " interpretive concept", [49] that requires judges to find the best fitting and most just solution to a legal dispute, given their constitutional traditions. Joseph Raz , on the other hand, defended the positivist outlook and criticised Hart's "soft social thesis" approach in The Authority of Law . [50] Raz argues that law is authority, identifiable purely through social sources and without reference to moral reasoning. In his view, any categorisation of rules beyond their role as authoritative instruments in mediation are best left to sociology , rather than jurisprudence. [51]

Positive law and non-positive law discussions
One definition is that law is a system of rules and guidelines which are enforced through social institutions to govern behaviour. [2] In The Concept of Law Hart argued law is a "system of rules"; [52] Austin said law was "the command of a sovereign, backed by the threat of a sanction"; [38] Dworkin describes law as an "interpretive concept" to achieve justice in his text titled Law's Empire ; [53] and Raz argues law is an "authority" to mediate people's interests. [50] Holmes said "The prophecies of what the courts will do in fact, and nothing more pretentious, are what I mean by the law." [54] In his Treatise on Law Aquinas argues that law is a rational ordering of things which concern the common good that is promulgated by whoever is charged with the care of the community. [55] This definition has both positivist and naturalist elements. [56]

Economic analysis
In the 18th century Adam Smith presented a philosophical foundation for explaining the relationship between law and economics. [57] The discipline arose partly out of a critique of trade unions and U.S. antitrust law. The most influential proponents, such as Richard Posner and Oliver Williamson and the so-called Chicago School of economists and lawyers including Milton Friedman and Gary Becker , are generally advocates of deregulation and privatisation , and are hostile to state regulation or what they see as restrictions on the operation of free markets . [58]
The most prominent economic analyst of law is 1991 Nobel Prize winner Ronald Coase , whose first major article, The Nature of the Firm (1937), argued that the reason for the existence of firms (companies, partnerships, etc.) is the existence of transaction costs . [60] Rational individuals trade through bilateral contracts on open markets until the costs of transactions mean that using corporations to produce things is more cost-effective. His second major article, The Problem of Social Cost (1960), argued that if we lived in a world without transaction costs, people would bargain with one another to create the same allocation of resources, regardless of the way a court might rule in property disputes. [61] Coase used the example of a nuisance case named Sturges v Bridgman , where a noisy sweetmaker and a quiet doctor were neighbours and went to court to see who should have to move. [62] Coase said that regardless of whether the judge ruled that the sweetmaker had to stop using his machinery, or that the doctor had to put up with it, they could strike a mutually beneficial bargain about who moves that reaches the same outcome of resource distribution. Only the existence of transaction costs may prevent this. [63] So the law ought to pre-empt what would happen, and be guided by the most efficient solution. The idea is that law and regulation are not as important or effective at helping people as lawyers and government planners believe. [64] Coase and others like him wanted a change of approach, to put the burden of proof for positive effects on a government that was intervening in the market, by analysing the costs of action. [65]

Sociology
Sociology of law is a diverse field of study that examines the interaction of law with society and overlaps with jurisprudence, philosophy of law, social theory and more specialised subjects such as criminology . [66] The institutions of social construction , social norms , dispute processing and legal culture are key areas for inquiry in this knowledge field. Sociology of law is sometimes seen as a sub-discipline of sociology, but its ties to the academic discipline of law are equally strong, and it is best seen as a transdisciplinary and multidisciplinary study focused on the theorisation and empirical study of legal practices and experiences as social phenomena. In the United States the field is usually called law and society studies; in Europe it is more often referred to as socio-legal studies. At first, jurists and legal philosophers were suspicious of sociology of law. Kelsen attacked one of its founders, Eugen Ehrlich , who sought to make clear the differences and connections between positive law, which lawyers learn and apply, and other forms of 'law' or social norms that regulate everyday life, generally preventing conflicts from reaching barristers and courts. [67] Contemporary research in sociology of law is much concerned with the way that law is developing outside discrete state jurisdictions, being produced through social interaction in many different kinds of social arenas, and acquiring a diversity of sources of (often competing or conflicting) authority in communal networks existing sometimes within nation states but increasingly also transnationally. [68]
Around 1900 Max Weber defined his "scientific" approach to law, identifying the "legal rational form" as a type of domination, not attributable to personal authority but to the authority of abstract norms. [69] Formal legal rationality was his term for the key characteristic of the kind of coherent and calculable law that was a precondition for modern political developments and the modern bureaucratic state. Weber saw this law as having developed in parallel with the growth of capitalism. [66] Another leading sociologist, Émile Durkheim , wrote in his classic work The Division of Labour in Society that as society becomes more complex, the body of civil law concerned primarily with restitution and compensation grows at the expense of criminal laws and penal sanctions. [70] Other notable early legal sociologists included Hugo Sinzheimer , Theodor Geiger , Georges Gurvitch and Leon Petrażycki in Europe, and William Graham Sumner in the U.S. [71] [72]

Legal systems
In general, legal systems can be split between civil law and common law systems. [73] The term "civil law" referring to a legal system should not be confused with "civil law" as a group of legal subjects distinct from criminal or public law . A third type of legal system—accepted by some countries without separation of church and state —is religious law, based on scriptures . The specific system that a country is ruled by is often determined by its history, connections with other countries, or its adherence to international standards. The sources that jurisdictions adopt as authoritatively binding are the defining features of any legal system. Yet classification is a matter of form rather than substance, since similar rules often prevail.

Civil law
Civil law is the legal system used in most countries around the world today. In civil law the sources recognised as authoritative are, primarily, legislation—especially codifications in constitutions or statutes passed by government—and custom . [74] Codifications date back millennia, with one early example being the Babylonian Codex Hammurabi . Modern civil law systems essentially derive from the legal practice of the 6th-century Eastern Roman Empire whose texts were rediscovered by late medieval Western Europe. Roman law in the days of the Roman Republic and Empire was heavily procedural, and lacked a professional legal class. [75] Instead a lay magistrate , iudex , was chosen to adjudicate. Decisions were not published in any systematic way, so any case law that developed was disguised and almost unrecognised. [76] Each case was to be decided afresh from the laws of the State, which mirrors the (theoretical) unimportance of judges' decisions for future cases in civil law systems today. From 529–534 AD the Byzantine Emperor Justinian I codified and consolidated Roman law up until that point, so that what remained was one-twentieth of the mass of legal texts from before. [77] This became known as the Corpus Juris Civilis . As one legal historian wrote, "Justinian consciously looked back to the golden age of Roman law and aimed to restore it to the peak it had reached three centuries before." [78] The Justinian Code remained in force in the East until the fall of the Byzantine Empire . Western Europe, meanwhile, relied on a mix of the Theodosian Code and Germanic customary law until the Justinian Code was rediscovered in the 11th century, and scholars at the University of Bologna used it to interpret their own laws. [79] Civil law codifications based closely on Roman law, alongside some influences from religious laws such as canon law , continued to spread throughout Europe until the Enlightenment ; then, in the 19th century, both France, with the Code Civil , and Germany, with the Bürgerliches Gesetzbuch , modernised their legal codes. Both these codes influenced heavily not only the law systems of the countries in continental Europe (e.g. Greece), but also the Japanese and Korean legal traditions. [80] [81] Today, countries that have civil law systems range from Russia and China to most of Central and Latin America . [82] With the exception of Louisiana's Civil Code, the United States follows the common law system described below.

Common law and equity
In common law legal systems , decisions by courts are explicitly acknowledged as "law" on equal footing with statutes adopted through the legislative process and with regulations issued by the executive branch . The "doctrine of precedent", or stare decisis (Latin for "to stand by decisions") means that decisions by higher courts bind lower courts, and future decisions of the same court, to assure that similar cases reach similar results. In contrast , in " civil law " systems, legislative statutes are typically more detailed, and judicial decisions are shorter and less detailed, because the judge or barrister is only writing to decide the single case, rather than to set out reasoning that will guide future courts.
Common law originated from England and has been inherited by almost every country once tied to the British Empire (except Malta, Scotland , the U.S. state of Louisiana , and the Canadian province of Quebec ). In medieval England, the Norman conquest the law varied-shire-to-shire, based on disparate tribal customs. The concept of a "common law" developed during the reign of Henry II during the late 12th century, when Henry appointed judges that had authority to create an institutionalized and unified system of law "common" to the country. The next major step in the evolution of the common law came when King John was forced by his barons to sign a document limiting his authority to pass laws. This "great charter" or Magna Carta of 1215 also required that the King's entourage of judges hold their courts and judgments at "a certain place" rather than dispensing autocratic justice in unpredictable places about the country. [83] A concentrated and elite group of judges acquired a dominant role in law-making under this system, and compared to its European counterparts the English judiciary became highly centralized. In 1297, for instance, while the highest court in France had fifty-one judges, the English Court of Common Pleas had five. [84] This powerful and tight-knit judiciary gave rise to a systematized process of developing common law. [85]
However, the system became overly systematized—overly rigid and inflexible. As a result, as time went on, increasing numbers of citizens petitioned the King to override the common law, and on the King's behalf the Lord Chancellor gave judgment to do what was equitable in a case. From the time of Sir Thomas More , the first lawyer to be appointed as Lord Chancellor, a systematic body of equity grew up alongside the rigid common law, and developed its own Court of Chancery . At first, equity was often criticized as erratic, that it varied according to the length of the Chancellor's foot. [86] Over time, courts of equity developed solid principles , especially under Lord Eldon . [87] In the 19th century in England, and in 1937 in the U.S. , the two systems were merged .
In developing the common law, academic writings have always played an important part, both to collect overarching principles from dispersed case law, and to argue for change. William Blackstone , from around 1760, was the first scholar to collect, describe, and teach the common law. [88] But merely in describing, scholars who sought explanations and underlying structures slowly changed the way the law actually worked. [89]

Religious law
[ undue weight? – discuss ]
Religious law is explicitly based on religious precepts. Examples include the Jewish Halakha and Islamic Sharia —both of which translate as the "path to follow"—while Christian canon law also survives in some church communities. Often the implication of religion for law is unalterability, because the word of God cannot be amended or legislated against by judges or governments. [ citation needed ] However a thorough and detailed legal system generally requires human elaboration. For instance, the Quran has some law, and it acts as a source of further law through interpretation, [90] Qiyas (reasoning by analogy), Ijma (consensus) and precedent . This is mainly contained in a body of law and jurisprudence known as Sharia and Fiqh respectively. Another example is the Torah or Old Testament , in the Pentateuch or Five Books of Moses. This contains the basic code of Jewish law, which some Israeli communities choose to use. The Halakha is a code of Jewish law which summarises some of the Talmud's interpretations. Nevertheless, Israeli law allows litigants to use religious laws only if they choose. Canon law is only in use by members of the Catholic Church, [91] the Eastern Orthodox Church and the Anglican Communion .

Sharia law
Until the 18th century, Sharia law was practiced throughout the Muslim world in a non-codified form, with the Ottoman Empire 's Mecelle code in the 19th century being a first attempt at codifying elements of Sharia law. Since the mid-1940s, efforts have been made, in country after country, to bring Sharia law more into line with modern conditions and conceptions. [92] [93] In modern times, the legal systems of many Muslim countries draw upon both civil and common law traditions as well as Islamic law and custom. The constitutions of certain Muslim states, such as Egypt and Afghanistan, recognise Islam as the religion of the state, obliging legislature to adhere to Sharia. [94] Saudi Arabia recognises Quran as its constitution, and is governed on the basis of Islamic law. [95] Iran has also witnessed a reiteration of Islamic law into its legal system after 1979. [96] During the last few decades, one of the fundamental features of the movement of Islamic resurgence has been the call to restore the Sharia, which has generated a vast amount of literature and affected world politics . [97]

Legal institutions
The main institutions of law in industrialised countries are independent courts , representative parliaments, an accountable executive, the military and police, bureaucratic organisation, the legal profession and civil society itself. John Locke, in his Two Treatises of Government , and Baron de Montesquieu in The Spirit of the Laws , advocated for a separation of powers between the political, legislature and executive bodies. [98] Their principle was that no person should be able to usurp all powers of the state , in contrast to the absolutist theory of Thomas Hobbes ' Leviathan . [99]
Max Weber and others reshaped thinking on the extension of state. Modern military, policing and bureaucratic power over ordinary citizens' daily lives pose special problems for accountability that earlier writers such as Locke or Montesquieu could not have foreseen. The custom and practice of the legal profession is an important part of people's access to justice , whilst civil society is a term used to refer to the social institutions, communities and partnerships that form law's political basis.

Judiciary
A judiciary is a number of judges mediating disputes to determine outcome. Most countries have systems of appeal courts, answering up to a supreme legal authority. In the United States, this authority is the Supreme Court ; [100] in Australia, the High Court ; in the UK, the Supreme Court ; [101] in Germany, the Bundesverfassungsgericht ; and in France, the Cour de Cassation . [102] [103] For most European countries the European Court of Justice in Luxembourg can overrule national law, when EU law is relevant. The European Court of Human Rights in Strasbourg allows citizens of the Council of Europe member states to bring cases relating to human rights issues before it. [104]
Some countries allow their highest judicial authority to overrule legislation they determine to be unconstitutional . For example, in Brown v. Board of Education , the United States Supreme Court nullified many state statutes that had established racially segregated schools, finding such statutes to be incompatible with the Fourteenth Amendment to the United States Constitution . [105]
A judiciary is theoretically bound by the constitution, just as all other government bodies are. In most countries judges may only interpret the constitution and all other laws. But in common law countries, where matters are not constitutional, the judiciary may also create law under the doctrine of precedent . The UK, Finland and New Zealand assert the ideal of parliamentary sovereignty , whereby the unelected judiciary may not overturn law passed by a democratic legislature. [106]
In communist states , such as China, the courts are often regarded as parts of the executive, or subservient to the legislature; governmental institutions and actors exert thus various forms of influence on the judiciary. [107] In Muslim countries, courts often examine whether state laws adhere to the Sharia: the Supreme Constitutional Court of Egypt may invalidate such laws, [108] and in Iran the Guardian Council ensures the compatibility of the legislation with the "criteria of Islam". [108] [109]

Legislature
Prominent examples of legislatures are the Houses of Parliament in London, the Congress in Washington D.C., the Bundestag in Berlin, the Duma in Moscow, the Parlamento Italiano in Rome and the Assemblée nationale in Paris. By the principle of representative government people vote for politicians to carry out their wishes. Although countries like Israel, Greece, Sweden and China are unicameral , most countries are bicameral , meaning they have two separately appointed legislative houses. [110]
In the 'lower house' politicians are elected to represent smaller constituencies . The 'upper house' is usually elected to represent states in a federal system (as in Australia, Germany or the United States) or different voting configuration in a unitary system (as in France). In the UK the upper house is appointed by the government as a house of review . One criticism of bicameral systems with two elected chambers is that the upper and lower houses may simply mirror one another. The traditional justification of bicameralism is that an upper chamber acts as a house of review. This can minimise arbitrariness and injustice in governmental action. [110]
To pass legislation, a majority of the members of a legislature must vote for a bill (proposed law) in each house. Normally there will be several readings and amendments proposed by the different political factions. If a country has an entrenched constitution, a special majority for changes to the constitution may be required, making changes to the law more difficult. A government usually leads the process, which can be formed from Members of Parliament (e.g. the UK or Germany). However, in a presidential system, the government is usually formed by an executive and his or her appointed cabinet officials (e.g. the United States or Brazil). [111]

Executive
The executive in a legal system serves as the centre of political authority of the State . In a parliamentary system , as with Britain, Italy, Germany, India, and Japan, the executive is known as the cabinet, and composed of members of the legislature. The executive is led by the head of government , whose office holds power under the confidence of the legislature. Because popular elections appoint political parties to govern, the leader of a party can change in between elections. [112]
The head of state is apart from the executive, and symbolically enacts laws and acts as representative of the nation. Examples include the President of Germany (appointed by members of federal and state legislatures ), the Queen of the United Kingdom (an hereditary office ), and the President of Austria (elected by popular vote). The other important model is the presidential system , found in the United States and in Brazil . In presidential systems, the executive acts as both head of state and head of government, and has power to appoint an unelected cabinet. Under a presidential system, the executive branch is separate from the legislature to which it is not accountable. [112] [113]
Although the role of the executive varies from country to country, usually it will propose the majority of legislation, and propose government agenda. In presidential systems, the executive often has the power to veto legislation. Most executives in both systems are responsible for foreign relations , the military and police, and the bureaucracy. Ministers or other officials head a country's public offices, such as a foreign ministry or defence ministry . The election of a different executive is therefore capable of revolutionising an entire country's approach to government.

Military and police
While military organisations have existed as long as government itself, the idea of a standing police force is a relatively modern concept. For example, Medieval England 's system of traveling criminal courts , or assizes , used show trials and public executions to instill communities with fear to maintain control. [114] The first modern police were probably those in 17th-century Paris, in the court of Louis XIV , [115] although the Paris Prefecture of Police claim they were the world's first uniformed policemen. [116]
Max Weber famously argued that the state is that which controls the monopoly on the legitimate use of force . [117] [118] The military and police carry out enforcement at the request of the government or the courts. The term failed state refers to states that cannot implement or enforce policies; their police and military no longer control security and order and society moves into anarchy, the absence of government. [119]

Bureaucracy
The etymology of "bureaucracy" derives from the French word for "office" ( bureau ) and the Ancient Greek for word "power" ( kratos ). [120] Like the military and police, a legal system's government servants and bodies that make up its bureaucracy carry out the directives of the executive. One of the earliest references to the concept was made by Baron de Grimm , a German author who lived in France. In 1765 he wrote,
Cynicism over "officialdom" is still common, and the workings of public servants is typically contrasted to private enterprise motivated by profit . [122] In fact private companies, especially large ones, also have bureaucracies. [123] Negative perceptions of " red tape " aside, public services such as schooling, health care, policing or public transport are considered a crucial state function making public bureaucratic action the locus of government power. [123]
Writing in the early 20th century, Max Weber believed that a definitive feature of a developed state had come to be its bureaucratic support. [124] Weber wrote that the typical characteristics of modern bureaucracy are that officials define its mission, the scope of work is bound by rules, and management is composed of career experts who manage top down, communicating through writing and binding public servants' discretion with rules. [125]

Legal profession
A corollary of the rule of law is the existence of a legal profession sufficiently autonomous to invoke the authority of the independent judiciary; the right to assistance of a barrister in a court proceeding emanates from this corollary—in England the function of barrister or advocate is distinguished from legal counselor. [127] As the European Court of Human Rights has stated, the law should be adequately accessible to everyone and people should be able to foresee how the law affects them. [128]
In order to maintain professionalism, the practice of law is typically overseen by either a government or independent regulating body such as a bar association , bar council or law society . Modern lawyers achieve distinct professional identity through specified legal procedures (e.g. successfully passing a qualifying examination), are required by law to have a special qualification (a legal education earning the student a Bachelor of Laws , a Bachelor of Civil Law , or a Juris Doctor degree. Higher academic degrees may also be pursued. Examples include a Master of Laws , a Master of Legal Studies , a Bar Professional Training Course or a Doctor of Laws .), and are constituted in office by legal forms of appointment ( being admitted to the bar ). There are few titles of respect to signify famous lawyers, such as Esquire , to indicate barristers of greater dignity, [129] [130] and Doctor of law , to indicate a person who obtained a PhD in Law.
Many Muslim countries have developed similar rules about legal education and the legal profession, but some still allow lawyers with training in traditional Islamic law to practice law before personal status law courts. [131] In China and other developing countries there are not sufficient professionally trained people to staff the existing judicial systems, and, accordingly, formal standards are more relaxed. [132]
Once accredited, a lawyer will often work in a law firm , in a chambers as a sole practitioner, in a government post or in a private corporation as an internal counsel . In addition a lawyer may become a legal researcher who provides on-demand legal research through a library, a commercial service or freelance work. Many people trained in law put their skills to use outside the legal field entirely. [133]
Significant to the practice of law in the common law tradition is the legal research to determine the current state of the law. This usually entails exploring case-law reports , legal periodicals and legislation. Law practice also involves drafting documents such as court pleadings , persuasive briefs , contracts, or wills and trusts. Negotiation and dispute resolution skills (including ADR techniques) are also important to legal practice, depending on the field. [133]

Civil society
The Classical republican concept of "civil society" dates back to Hobbes and Locke. [134] Locke saw civil society as people who have "a common established law and judicature to appeal to, with authority to decide controversies between them." [135] German philosopher Georg Wilhelm Friedrich Hegel distinguished the "state" from "civil society" ( bürgerliche Gesellschaft ) in Elements of the Philosophy of Right . [136]
Hegel believed that civil society and the state were polar opposites, within the scheme of his dialectic theory of history. The modern dipole state–civil society was reproduced in the theories of Alexis de Tocqueville and Karl Marx . [137] [138] Nowadays in post-modern theory civil society is necessarily a source of law, by being the basis from which people form opinions and lobby for what they believe law should be. As Australian barrister and author Geoffrey Robertson QC wrote of international law,
Freedom of speech , freedom of association and many other individual rights allow people to gather, discuss, criticise and hold to account their governments, from which the basis of a deliberative democracy is formed. The more people are involved with, concerned by and capable of changing how political power is exercised over their lives, the more acceptable and legitimate the law becomes to the people. The most familiar institutions of civil society include economic markets, profit-oriented firms, families, trade unions, hospitals, universities, schools, charities, debating clubs , non-governmental organisations, neighbourhoods, churches, and religious associations. [140]

Legal subjects
All legal systems deal with the same basic issues, but jurisdictions categorise and identify its legal subjects in different ways. A common distinction is that between " public law " (a term related closely to the state , and including constitutional, administrative and criminal law), and " private law " (which covers contract, tort and property). [141] In civil law systems, contract and tort fall under a general law of obligations , while trusts law is dealt with under statutory regimes or international conventions . International, constitutional and administrative law, criminal law, contract, tort, property law and trusts are regarded as the "traditional core subjects", [142] although there are many further disciplines .

International law
International law can refer to three things: public international law, private international law or conflict of laws and the law of supranational organisations.

Constitutional and administrative law
Constitutional and administrative law govern the affairs of the state. Constitutional law concerns both the relationships between the executive, legislature and judiciary and the human rights or civil liberties of individuals against the state. Most jurisdictions, like the United States and France , have a single codified constitution with a bill of rights . A few, like the United Kingdom , have no such document. A "constitution" is simply those laws which constitute the body politic , from statute , case law and convention . A case named Entick v Carrington [150] illustrates a constitutional principle deriving from the common law. Mr Entick's house was searched and ransacked by Sheriff Carrington. When Mr Entick complained in court, Sheriff Carrington argued that a warrant from a Government minister, the Earl of Halifax , was valid authority. However, there was no written statutory provision or court authority. The leading judge, Lord Camden , stated that,
The fundamental constitutional principle, inspired by John Locke , holds that the individual can do anything except that which is forbidden by law , and the state may do nothing except that which is authorised by law. [152] [153] Administrative law is the chief method for people to hold state bodies to account. People can sue an agency, local council, public service, or government ministry for judicial review of actions or decisions, to ensure that they comply with the law, and that the government entity observed required procedure. The first specialist administrative court was the Conseil d'État set up in 1799, as Napoleon assumed power in France. [154]

Criminal law
Criminal law, also known as penal law, pertains to crimes and punishment. [155] It thus regulates the definition of and penalties for offences found to have a sufficiently deleterious social impact but, in itself, makes no moral judgment on an offender nor imposes restrictions on society that physically prevent people from committing a crime in the first place. [156] Investigating, apprehending, charging, and trying suspected offenders is regulated by the law of criminal procedure . [157] The paradigm case of a crime lies in the proof, beyond reasonable doubt , that a person is guilty of two things. First, the accused must commit an act which is deemed by society to be criminal, or actus reus (guilty act). [158] Second, the accused must have the requisite malicious intent to do a criminal act, or mens rea (guilty mind). However, for so called " strict liability " crimes, an actus reus is enough. [159] Criminal systems of the civil law tradition distinguish between intention in the broad sense ( dolus directus and dolus eventualis ), and negligence. Negligence does not carry criminal responsibility unless a particular crime provides for its punishment. [160] [161]
Examples of crimes include murder, assault, fraud and theft. In exceptional circumstances defences can apply to specific acts, such as killing in self defence , or pleading insanity . Another example is in the 19th-century English case of R v Dudley and Stephens , which tested a defence of " necessity ". The Mignonette , sailing from Southampton to Sydney, sank. Three crew members and Richard Parker, a 17-year-old cabin boy, were stranded on a raft. They were starving and the cabin boy was close to death. Driven to extreme hunger, the crew killed and ate the cabin boy. The crew survived and were rescued, but put on trial for murder. They argued it was necessary to kill the cabin boy to preserve their own lives. Lord Coleridge , expressing immense disapproval, ruled, "to preserve one's life is generally speaking a duty, but it may be the plainest and the highest duty to sacrifice it." The men were sentenced to hang , but public opinion was overwhelmingly supportive of the crew's right to preserve their own lives. In the end, the Crown commuted their sentences to six months in jail. [162]
Criminal law offences are viewed as offences against not just individual victims, but the community as well. [156] The state, usually with the help of police, takes the lead in prosecution, which is why in common law countries cases are cited as " The People v ..." or " R (for Rex or Regina ) v ...". Also, lay juries are often used to determine the guilt of defendants on points of fact: juries cannot change legal rules. Some developed countries still condone capital punishment for criminal activity, but the normal punishment for a crime will be imprisonment , fines , state supervision (such as probation), or community service . Modern criminal law has been affected considerably by the social sciences, especially with respect to sentencing , legal research, legislation, and rehabilitation . [163] On the international field, 111 countries are members of the International Criminal Court , which was established to try people for crimes against humanity . [164]

Contract law
Contract law concerns enforceable promises, and can be summed up in the Latin phrase pacta sunt servanda (agreements must be kept). [165] In common law jurisdictions, three key elements to the creation of a contract are necessary: offer and acceptance , consideration and the intention to create legal relations. In Carlill v Carbolic Smoke Ball Company a medical firm advertised that its new wonder drug, the smokeball, would cure people's flu, and if it did not, the buyers would get £ 100. Many people sued for their £100 when the drug did not work. Fearing bankruptcy, Carbolic argued the advert was not to be taken as a serious, legally binding offer. It was an invitation to treat , mere puffery, a gimmick. But the Court of Appeal held that to a reasonable man Carbolic had made a serious offer, accentuated by their reassuring statement, "£1000 is deposited". Equally, people had given good consideration for the offer by going to the "distinct inconvenience" of using a faulty product. "Read the advertisement how you will, and twist it about as you will", said Lord Justice Lindley , "here is a distinct promise expressed in language which is perfectly unmistakable". [166]
"Consideration" indicates the fact that all parties to a contract have exchanged something of value. Some common law systems, including Australia, are moving away from the idea of consideration as a requirement. The idea of estoppel or culpa in contrahendo , can be used to create obligations during pre-contractual negotiations. [167] In civil law jurisdictions, consideration is not required for a contract to be binding. [168] In France, an ordinary contract is said to form simply on the basis of a "meeting of the minds" or a "concurrence of wills". Germany has a special approach to contracts, which ties into property law. Their ' abstraction principle ' ( Abstraktionsprinzip ) means that the personal obligation of contract forms separately from the title of property being conferred. When contracts are invalidated for some reason (e.g. a car buyer is so drunk that he lacks legal capacity to contract) [169] the contractual obligation to pay can be invalidated separately from the proprietary title of the car. Unjust enrichment law, rather than contract law, is then used to restore title to the rightful owner. [170]

Tort law
Torts, sometimes called delicts , are civil wrongs. To have acted tortiously, one must have breached a duty to another person, or infringed some pre-existing legal right. A simple example might be accidentally hitting someone with a cricket ball. [171] Under the law of negligence , the most common form of tort, the injured party could potentially claim compensation for their injuries from the party responsible. The principles of negligence are illustrated by Donoghue v Stevenson . [172] A friend of Mrs Donoghue ordered an opaque bottle of ginger beer (intended for the consumption of Mrs Donoghue) in a café in Paisley . Having consumed half of it, Mrs Donoghue poured the remainder into a tumbler. The decomposing remains of a snail floated out. She claimed to have suffered from shock, fell ill with gastroenteritis and sued the manufacturer for carelessly allowing the drink to be contaminated. The House of Lords decided that the manufacturer was liable for Mrs Donoghue's illness. Lord Atkin took a distinctly moral approach, and said,
This became the basis for the four principles of negligence: (1) Mr Stevenson owed Mrs Donoghue a duty of care to provide safe drinks (2) he breached his duty of care (3) the harm would not have occurred but for his breach and (4) his act was the proximate cause of her harm. [172] Another example of tort might be a neighbour making excessively loud noises with machinery on his property. [62] Under a nuisance claim the noise could be stopped. Torts can also involve intentional acts, such as assault , battery or trespass . A better known tort is defamation , which occurs, for example, when a newspaper makes unsupportable allegations that damage a politician's reputation. [174] More infamous are economic torts, which form the basis of labour law in some countries by making trade unions liable for strikes, [175] when statute does not provide immunity. [176]

Property law
Property law governs ownership and possession. Real property , sometimes called 'real estate', refers to ownership of land and things attached to it. [178] Personal property , refers to everything else; movable objects, such as computers, cars, jewelry or intangible rights, such as stocks and shares . A right in rem is a right to a specific piece of property, contrasting to a right in personam which allows compensation for a loss, but not a particular thing back. Land law forms the basis for most kinds of property law, and is the most complex. It concerns mortgages , rental agreements , licences , covenants , easements and the statutory systems for land registration. Regulations on the use of personal property fall under intellectual property, company law , trusts and commercial law . An example of a basic case of most property law is Armory v Delamirie [1722]. [179] A chimney sweep 's boy found a jewel encrusted with precious stones. He took it to a goldsmith to have it valued. The goldsmith's apprentice looked at it, sneakily removed the stones, told the boy it was worth three halfpence and that he would buy it. The boy said he would prefer the jewel back, so the apprentice gave it to him, but without the stones. The boy sued the goldsmith for his apprentice's attempt to cheat him. Lord Chief Justice Pratt ruled that even though the boy could not be said to own the jewel, he should be considered the rightful keeper ("finders keepers") until the original owner is found. In fact the apprentice and the boy both had a right of possession in the jewel (a technical concept, meaning evidence that something could belong to someone), but the boy's possessory interest was considered better, because it could be shown to be first in time. Possession may be nine tenths of the law, but not all.
This case is used to support the view of property in common law jurisdictions, that the person who can show the best claim to a piece of property, against any contesting party, is the owner. [180] By contrast, the classic civil law approach to property, propounded by Friedrich Carl von Savigny , is that it is a right good against the world. Obligations, like contracts and torts, are conceptualised as rights good between individuals. [181] The idea of property raises many further philosophical and political issues. Locke argued that our "lives, liberties and estates" are our property because we own our bodies and mix our labour with our surroundings. [182]

Equity and trusts
Equity is a body of rules that developed in England separately from the "common law". The common law was administered by judges and barristers. The Lord Chancellor on the other hand, as the King's keeper of conscience, could overrule the judge-made law if he thought it equitable to do so. [183] This meant equity came to operate more through principles than rigid rules. For instance, whereas neither the common law nor civil law systems allow people to split the ownership from the control of one piece of property, equity allows this through an arrangement known as a 'trust'. 'Trustees' control property, whereas the 'beneficial' (or 'equitable') ownership of trust property is held by people known as 'beneficiaries'. Trustees owe duties to their beneficiaries to take good care of the entrusted property. [184] In the early case of Keech v Sandford [1722] [185] a child had inherited the lease on a market in Romford , London. Mr Sandford was entrusted to look after this property until the child matured. But before then, the lease expired. The landlord had (apparently) told Mr Sandford that he did not want the child to have the renewed lease. Yet the landlord was happy (apparently) to give Mr Sandford the opportunity of the lease instead. Mr Sandford took it. When the child (now Mr Keech) grew up, he sued Mr Sandford for the profit that he had been making by getting the market's lease. Mr Sandford was meant to be trusted, but he put himself in a position of conflict of interest . The Lord Chancellor , Lord King , agreed and ordered Mr Sandford should disgorge his profits. He wrote,
Of course, Lord King LC was worried that trustees might exploit opportunities to use trust property for themselves instead of looking after it. Business speculators using trusts had just recently caused a stock market crash . Strict duties for trustees made their way into company law and were applied to directors and chief executive officers. Another example of a trustee's duty might be to invest property wisely or sell it. [186] This is especially the case for pension funds, the most important form of trust, where investors are trustees for people's savings until retirement. But trusts can also be set up for charitable purposes , famous examples being the British Museum or the Rockefeller Foundation .

Further disciplines
Law spreads far beyond the core subjects into virtually every area of life. Three categories are presented for convenience, though the subjects intertwine and overlap.

See also

Notes
WebPage index: 00174
Creative Commons license
A Creative Commons ( CC ) license is one of several public copyright licenses that enable the free distribution of an otherwise copyrighted work. A CC license is used when an author wants to give people the right to share, use, and build upon a work that he/she has created. CC provides an author flexibility (for example, he/she might choose to allow only non-commercial uses of his/her own work) and protects the people who use or redistribute an author's work from concerns of copyright infringement as long as they abide by the conditions that are specified in the license by which the author distributes the work.
There are several types of CC licenses. The licenses differ by several combinations that condition the terms of distribution. They were initially released on December 16, 2002 by Creative Commons , a U.S. non-profit corporation founded in 2001. There have also been five versions of the suite of licenses, numbered 1.0 through 4.0. [1] As of 2016 [update] , the 4.0 license suite is the most current.
In October 2014 the Open Knowledge Foundation approved the Creative Commons CC BY , CC BY-SA , and CC0 licenses as conformant with the " Open Definition " for content and data. [2] [3] [4]

Applicable works
Work licensed under a Creative Commons license is governed by applicable copyright law. [5] This allows Creative Commons licenses to be applied to all work falling under copyright, including: books, plays, movies, music, articles, photographs, blogs, and websites. Creative Commons does not recommend the use of Creative Commons licenses for software. [6]
There are over 35,000 works that are available in hardcopy and have a registered ISBN number. Creative Commons splits these works into two categories, one of which encompasses self-published books. [7]
However, application of a Creative Commons license may not modify the rights allowed by fair use or fair dealing or exert restrictions which violate copyright exceptions. [8] Furthermore, Creative Commons licenses are non-exclusive and non-revocable. [9] Any work or copies of the work obtained under a Creative Commons license may continue to be used under that license. [10]
In the case of works protected by multiple Creative Common licenses, the user may choose either. [11]

Types of licenses
The CC licenses all grant the "baseline rights", such as the right to distribute the copyrighted work worldwide for non-commercial purposes, and without modification. [12] The details of each of these licenses depend on the version, and comprises a selection out of four conditions:
The last two clauses are not free content licenses, according to definitions such as DFSG or the Free Software Foundation 's standards, and cannot be used in contexts that require these freedoms, such as Wikipedia . For software , Creative Commons includes three free licenses created by other institutions: the BSD License , the GNU LGPL , and the GNU GPL . [14]
Mixing and matching these conditions produces sixteen possible combinations, of which eleven are valid Creative Commons licenses and five are not. Of the five invalid combinations, four include both the "nd" and "sa" clauses, which are mutually exclusive; and one includes none of the clauses. Of the eleven valid combinations, the five that lack the "by" clause have been retired because 98% of licensors requested attribution, though they do remain available for reference on the website. [15] [16] [17] This leaves six regularly used licenses + the CC0 public domain waiver :

Seven regularly used licenses
[17] [18]
For example, the Creative Commons Attribution (BY) license allows one to share and remix (create derivative works), even for commercial use, so long as attribution is given. [19]

Version 4.0 and international use
The original non-localized Creative Commons licenses were written with the U.S. legal system in mind, therefore the wording may be incompatible with local legislation in other jurisdictions , rendering the licenses unenforceable there. To address this issue, Creative Commons asked its affiliates to translate the various licenses to reflect local laws in a process called " porting ." [20] As of July 2011, Creative Commons licenses have been ported to over 50 jurisdictions worldwide. [21]
The latest version 4.0 of the Creative Commons licenses, released on November 25, 2013, are generic licenses that are applicable to most jurisdictions and do not usually require ports. [22] [23] [24] [25] No new ports have been implemented in version 4.0 of the license. [26] Version 4.0 discourages using ported versions and instead acts as a single global license. [27]

Rights

Attribution
Since 2004, all current licenses (beside the CC0 waiver) require attribution of the original author, the BY component. [16] The attribution must be given to "the best of [one's] ability using the information available". [28] Generally this implies the following:

Non-commercial licenses
The "non-commercial" option included in some Creative Commons licenses is controversial in definition, [29] as it is sometimes unclear what can be considered a non-commercial setting, and application, since its restrictions differ from the principles of open content promoted by other permissive licenses . [30] In 2014 Wikimedia Deutschland published a guide to using Creative Commons licenses as wiki pages for translations and as PDF. [31]

Zero / public domain
Besides licenses, Creative Commons also offers through CC0 a way to release material worldwide into the public domain . [18] CC0 is a legal tool for waiving as many rights as legally possible. [33] Or, when not legally possible, CC0 acts as fallback as public domain equivalent license . [33] Development of CC0 began in 2007 [34] and the tool was released in 2009. [35] [36] A major target of the license was the scientific data community. [37]
In 2010, Creative Commons announced its Public Domain Mark , [38] a tool for labeling works already in the public domain. Together, CC0 and the Public Domain Mark replace the Public Domain Dedication and Certification, [39] which took a U.S.-centric approach and co-mingled distinct operations.
In 2011, the Free Software Foundation added CC0 to its free software licenses , [40] and currently recommends CC0 as the preferred method of releasing software into the public domain . [41]
In February 2012 CC0 was submitted to Open Source Initiative (OSI) for their approval. [42] However, controversy arose over its clause which excluded from the scope of the license any relevant patents held by the copyright holder. This clause was added with scientific data in mind rather than software, but some members of the OSI believed it could weaken users' defenses against software patents . As a result, Creative Commons withdrew their submission, and the license is not currently approved by the OSI. [37] [43]
In 2013, Unsplash began using the CC0 license to distribute free stock photography . [44] [45] It now distributes several million photos a month [46] and has inspired a host of similar sites, including CC0 photography companies and CC0 blogging companies. [47] Lawrence Lessig , the founder of Creative Commons, has contributed to the site. [48] [ dead link ]
In October 2014 the Open Knowledge Foundation approved the Creative Commons CC0 as conformant with the "Open Definition" and recommend the license to dedicate content to the public domain. [3] [4]

Adaptation
Rights in an adaptation can be expressed by a CC license that is compatible with the status or licensing of the original work or works on which the adaptation is based. [49]

Legal aspects
The legal implications of large numbers of works having Creative Commons licensing are difficult to predict, and there is speculation that media creators often lack insight to be able to choose the license which best meets their intent in applying it. [50]
Some works licensed using Creative Commons licenses have been involved in several court cases. [51] Creative Commons itself was not a party to any of these cases; they only involved licensors or licensees of Creative Commons licenses. When the cases went as far as decisions by judges (that is, they were not dismissed for lack of jurisdiction or were not settled privately out of court), they have all validated the legal robustness of Creative Commons public licenses. Here are some notable cases:

Dutch tabloid
In early 2006, podcaster Adam Curry sued a Dutch tabloid who published photos from Curry's Flickr page without Curry's permission. The photos were licensed under the Creative Commons Non-Commercial license. While the verdict was in favor of Curry, the tabloid avoided having to pay restitution to him as long as they did not repeat the offense. Professor Bernt Hugenholtz, main creator of the Dutch CC license and director of the Institute for Information Law of the University of Amsterdam, commented, "The Dutch Court's decision is especially noteworthy because it confirms that the conditions of a Creative Commons license automatically apply to the content licensed under it, and binds users of such content even without expressly agreeing to, or having knowledge of, the conditions of the license." [52] [53] [54] [55]

Virgin Mobile
In 2007, Virgin Mobile Australia launched an Australian bus stop ad campaign promoting their cellphone text messaging service using the work of amateur photographers who uploaded their work to Flickr using a Creative Commons-BY (Attribution) license. Users licensing their images this way freed their work for use by any other entity, as long as the original creator was attributed credit, without any other compensation required. Virgin upheld this single restriction by printing a URL leading to the photographer's Flickr page on each of their ads. However, one picture, depicting 15-year-old Alison Chang at a fund-raising carwash for her church, [56] caused some controversy when she sued Virgin Mobile. The photo was taken by Alison's church youth counselor, Justin Ho-Wee Wong, who uploaded the image to Flickr under the Creative Commons license. [56] In 2008, the case (concerning personality rights rather than copyright as such) was thrown out of a Texas court for lack of jurisdiction. [57] [58]

SGAE vs Fernández
In the fall of 2006, the collecting society Sociedad General de Autores y Editores ( SGAE ) in Spain sued Ricardo Andrés Utrera Fernández, owner of a disco bar located in Badajoz who played CC-licensed music. SGAE argued that Fernández should pay royalties for public performance of the music between November 2002 and August 2005. The Lower Court rejected the collecting society's claims because the owner of the bar proved that the music he was using was not managed by the society. [59]
In February 2006, the Cultural Association Ladinamo (based in Madrid, and represented by Javier de la Cueva ) was granted the use of copyleft music in their public activities. The sentence said: "Admitting the existence of music equipment, a joint evaluation of the evidence practiced, this court is convinced that the defendant prevents communication of works whose management is entrusted to the plaintiff [SGAE], using a repertoire of authors who have not assigned the exploitation of their rights to the SGAE, having at its disposal a database for that purpose and so it is manifested both by the legal representative of the Association and by Manuela Villa Acosta, in charge of the cultural programming of the association, which is compatible with the alternative character of the Association and its integration in the movement called ' copy left '". [60]

GateHouse Media, Inc. vs. That's Great News, LLC
On June 30, 2010 GateHouse Media filed a lawsuit against That's Great News . GateHouse Media owns a number of local newspapers, including Rockford Register Star , which is based in Rockford, Illinois. That's Great News makes plaques out of newspaper articles and sells them to the people featured in the articles. [61] GateHouse sued That's Great News for copyright infringement and breach of contract. GateHouse claimed that TGN violated the non-commercial and no-derivative works restrictions on GateHouse Creative Commons licensed work when TGN published the material on its website. The case was settled on August 17, 2010, though the settlement was not made public. [61] [62]

Drauglis v. Kappa Map Group, LLC
The plaintiff was photographer Art Drauglis, who uploaded several pictures to the photo-sharing website Flickr using Creative Commons Attribution-ShareAlike 2.0 Generic License (CC BY-SA), including one entitled "Swain's Lock, Montgomery Co., MD.". The defendant was Kappa Map Group, a map-making company, which downloaded the image and used it in a compilation entitled "Montgomery Co. Maryland Street Atlas". Though there was nothing on the cover that indicated the origin of the picture, the text " Photo: Swain's Lock, Montgomery Co., MD Photographer: Carly Lesser & Art Drauglis, Creative Commoms [ sic ] , CC-BY-SA-2.0 " appeared at the bottom of the back cover.
The validity of the CC BY-SA 2.0 as a license was not in dispute. The CC BY-SA 2.0 requires that the licensee to use nothing less restrictive than the CC BY-SA 2.0 terms. The atlas was sold commercially and not for free reuse by others. The dispute was whether Drauglis' license terms that would apply to "derivative works" applied to the entire atlas. Drauglis sued the defendants on June 2014 for copyright infringement and license breach, seeking declaratory and injunctive relief, damages, fees, and costs. Drauglis asserted, among other things, that Kappa Map Group "exceeded the scope of the License because defendant did not publish the Atlas under a license with the same or similar terms as those under which the Photograph was originally licensed." [63] The judge dismissed the case on that count, ruling that the atlas was not a derivative work of the photograph in the sense of the license. Since the atlas was not a derivative work of the photograph, Kappa Map Group did not need to license the entire atlas under the CC BY-SA 2.0 license. The judge also determined that the work had been properly attributed. [64]

Verband zum Schutz geistigen Eigentums im Internet (VGSE)
This incident has not been tested in court, but it highlights a potentially disturbing practice. In July 2016, German computer magazine LinuxUser reports that a German blogger Christoph Langner used two CC-BY licensed photographs from Berlin photographer Dennis Skley on his private blog Linuxundich.de . Langner duly mentioned the author and the license and added a link to the original. Langner was later contacted by the Verband zum Schutz geistigen Eigentums im Internet (VGSE) (Association for the Protection of Intellectual Property in the Internet) with a demand for €2300 for failing to provide the full name of the work, the full name of the author, the license text, and a source link, as is apparently required by the fine print in the license. Of this sum, €40 goes to the photographer and remainder is retained by VGSE. [65] [66]

Works with a Creative Commons license
Creative Commons maintains a content directory wiki of organizations and projects using Creative Commons licenses. [67] On its website CC also provides case studies of projects using CC licenses across the world. [68] CC licensed content can also be accessed through a number of content directories and search engines (see CC licensed content directories ).

Retired licenses
Due to either disuse or criticism, a number of previously offered Creative Commons licenses have since been retired, [15] [69] and are no longer recommended for new works. The retired licenses include all licenses lacking the Attribution element other than CC0, as well as the following four licenses:

See also
WebPage index: 00175
Content (media)
In publishing, art, and communication, content is the information and experiences that are directed towards an end-user or audience . [1] Content is "something that is to be expressed through some medium, as speech, writing or any of various arts". [2] Content can be delivered via many different media including the Internet , television, audio CDs, books, magazines, and live events, such as conferences and stage performances.

Content value
Content itself is what the end-user derives value from. Thus, "content" can refer to the information provided through the medium, the way in which the information was presented, as well as the added features included in the medium in which that information was delivered. The medium, however, provides little to no value to the end-user without the information and experiences that make up the content. Communication theory philosopher Marshall McLuhan famously coined the phrase, " The medium is the message ." [3] In the case of content, the channel through which information is delivered, the "medium", affects how the end user perceives content, the "message".
The author, producer, or publisher of an original source of information or experiences may or may not be directly responsible for the entire value that they attain as content in a specific context. For example, part of an original article (such as a headline from a news story) may be rendered on another web page displaying the results of a user's search engine query grouped with headlines from other news publications and related advertisements. The value that the original headline has in this group of query results from the search engine as a medium may be very different from the value that it had as message content in its original article.
Content also leads to influencing other people in creating their own content, sometimes in a way that the original author did not or could not plan or imagine. This feature, adding the option of user innovation in a medium, means that users can develop their own content from existing content.

Technological effects on content
Traditionally, content was edited and tailored for the public through news editors, authors, and other kinds of content creators. However, not all information content requires creative authoring or editing. Through recent technological developments, truth is found in philosopher Marshall McLuhan's idea of a global village ; new technologies allow for instantaneous movement of information from every corner to every point at the same time [4] has caused the globe to be contracted into a village by electric technology, [5] such as mobile phones and automated sensors. These new technologies can record events anywhere for publishing and converting in order to potentially reach a global audience on channels such as YouTube . Such recorded or transmitted information and visuals can be referred to as content. Content is no longer a product of only reputable sources; new technology has made primary sources of content more readily available to all. For example, a video of a politician giving a speech compared to an article written by a reporter who witnessed the speech.
Media production and delivery technology may potentially enhance the value of content by formatting, filtering, and combining original sources of content for new audiences with new contexts. The greatest value for a given source of content for a specific audience is often found through such electronic reworking of content as dynamic and real-time as the trends that fuel its interest. Less emphasis on value from content stored for possible use in its original form, and more emphasis on rapid re-purposing, reuse, and redeployment has led many publishers and media producers to view their primary function less as originators and more as transformers of content. Thus, one finds out that institutions, that used to focus on publishing printed materials, are now publishing both databases and software to combine content from various sources for a wider variety of audiences.

Criticism
While marketing and media interests have broadly adopted the term "content", some writers complain about the term's inherent ambiguity. [6] [7] Others assert that the term devalues the work of authors or sets up a false analogy of information as material objects which biases any discussion using the word, [8] [9] and still others argue that it overemphasizes the work of authors. [10]

See also
WebPage index: 00176
Open Content Alliance
The Open Content Alliance (OCA) is a consortium of organizations contributing to a permanent, publicly accessible archive of digitized texts. Its creation was announced in October 2005 by Yahoo! , the Internet Archive , the University of California , the University of Toronto and others. [1] Scanning for the Open Content Alliance is administered by the Internet Archive , which also provides permanent storage and access through its website.
The OCA is, in part, a response to Google Book Search , which was announced in October 2004. OCA's approach to seeking permission from copyright holders differs significantly from that of Google Book Search. OCA digitizes copyrighted works only after asking and receiving permission from the copyright holder ("opt-in"). By contrast, Google Book Search digitizes copyrighted works unless explicitly told not to do so ("opt-out"), and contends that digitizing for the purposes of indexing is fair use .
Microsoft had a special relationship with the Open Content Alliance until May 2008. Microsoft joined the Open Content Alliance in October 2005 as part of its Live Book Search project. [2] However, in May 2008 Microsoft announced it would be ending the Live Book Search project and no longer funding the scanning of books through the Internet Archive. [3] Microsoft removed any contractual restrictions on the content they had scanned and they relinquished the scanning equipment to their digitization partners and libraries to continue digitization programs. [3] Between about 2006 and 2008 Microsoft sponsored the scanning of over 750,000 books, 300,000 of which are now part of the Internet Archive's on-line collections.

Opposition to Google Book Settlement
Brewster Kahle , a founder of the Open Content Alliance, actively opposed the proposed Google Book Settlement until its defeat in March 2011.

Contributors
The following are contributors to the OCA:
Biodiversity Heritage Library, a cooperative project of:

See also
WebPage index: 00177
Feist Publications, Inc., v. Rural Telephone Service Co.
Feist Publications, Inc., v. Rural Telephone Service Co. , 499 U.S. 340 (1991), was a decision by the Supreme Court of the United States establishing that information alone without a minimum of original creativity cannot be protected by copyright. [1] In the case appealed, Feist had copied information from Rural's telephone listings to include in its own, after Rural had refused to license the information. Rural sued for copyright infringement . The Court ruled that information contained in Rural's phone directory was not copyrightable and that therefore no infringement existed.

Background
Rural Telephone Service Company, Inc. is a telephone cooperative providing services for areas in northwest Kansas, with headquarters in the small town of Lenora , in Norton County . The company was under a statutory obligation to compile a phone directory of all their customers free of charge as a condition of their monopoly franchise.
Feist Publications, Inc. specialized in compiling telephone directories from larger geographic areas than Rural from other areas of Kansas. They had licensed the directory of 11 other local directories, with Rural being the only hold-out in the region. Despite Rural's denial of a license to Feist, Feist copied some 4000 entries from Rural's directory. Because Rural had placed a small number of phony entries to detect copying, Feist was caught.
Prior to this case, the substance of copyright in United States law followed the sweat of the brow doctrine, which gave copyright to anyone who invested significant amount of time and energy into their work. At trial and appeal level the courts followed this doctrine, siding with Rural.

Ruling of the court
The ruling of the court was written by Justice O'Connor . It examined the purpose of copyright and explained the standard of copyrightability as based on originality .
The case centered on two well-established principles in United States copyright law: That facts are not copyrightable, but that compilations of facts can be.
"There is an undeniable tension between these two propositions," Justice O'Connor wrote in her decision. "Many compilations consist of nothing but raw data -- i.e. wholly factual information not accompanied by any original expression. On what basis may one claim a copyright upon such work? Common sense tells us that 100 uncopyrightable facts do not magically change their status when gathered together in one place. … The key to resolving the tension lies in understanding why facts are not copyrightable: The sine qua non of copyright is originality."
Rural claimed a collection copyright in its directory. The court clarified that the intent of copyright law was not, as claimed by Rural and some lower courts, to reward the efforts of persons collecting information — the so-called " sweat of the brow " or "industrious collection" doctrine — but rather "to promote the Progress of Science and useful Arts" ( U.S. Const. Art. I, § 8, cl. 8 ). That is, to encourage creative expression.
The standard for creativity is extremely low. It need not be novel, rather it only needs to possess a "spark" or "minimal degree" of creativity to be protected by copyright.
In regard to collections of facts, Justice O'Connor stated that copyright can only apply to the creative aspects of collection: the creative choice of what data to include or exclude, the order and style in which the information is presented, etc., but not on the information itself. If Feist were to take the directory and rearrange it, it would destroy the copyright owned in the data. "Notwithstanding a valid copyright, a subsequent compiler remains free to use the facts contained in another's publication to aid in preparing a competing work, so long as the competing work does not feature the same selection and arrangement," Justice O'Connor wrote.
The court ruled that Rural's directory was nothing more than an alphabetic list of all subscribers to its service, which it was required to compile under law, and that no creative expression was involved. The fact that Rural spent considerable time and money collecting the data was irrelevant to copyright law, and Rural's copyright claim was dismissed.
While the other justices joined Justice O'Connor's majority opinion, Justice Blackmun only concurred in judgement, but never filed a concurring opinion to explain his reasons. [2]

Implications
The ruling has major implications for any project that serves as a collection of knowledge. Information (that is, facts , discoveries, etc.), from any source, is fair game, but cannot contain any of the "expressive" content added by the source author . That includes not only the author's own comments, but also his choice of which facts to cover, his choice of which links to make among the bits of information, his order of presentation (unless it is something obvious like an alphabetical list), any evaluations he may have made about the quality of various pieces of information, or anything else that might be considered "original creative work" of the author rather than mere facts.
For example, a recipe is a process, and not copyrightable, but the words used to describe it are; see idea-expression divide and Publications International v Meredith Corp. (1996). [3] Therefore, you can rewrite a recipe in your own words and publish it without infringing copyrights. But, if you rewrote every recipe from a particular cookbook , you might still be found to have infringed the author's copyright in the choice of recipes and their "coordination" and "presentation", even if you used different words; however, the West decisions below suggest that this is unlikely unless there is some significant creativity carried over from the original presentation. It should be noted that a sufficiently novel, useful, and unique (i.e. non-obvious) recipe can be granted protection under patent law. [4]
Feist proved most important in the area of copyright of legal case law publications. Although one might assume that the text of U.S. case law is in public domain , Thomson West had claimed a copyright as to the first page citations and internal pin-point page citations of its versions of court opinions (case law) found in its printed versions of the case law ("West's citation claims.") West also had claimed a copyright in the text of its versions of the case law, which included parallel citations and typographical corrections ("West's text claims.") The text claim would have barred anyone from copying the text of a case from a West case law reporter, since the copied text would include West enhancements to which West claimed copyright.
In a pre-Feist case, West's citation copyright claim had been affirmed by the U.S. Court of Appeals for the Eighth Circuit in a preliminary injunction case in 1986 brought by West against Mead Data, owner of Lexis. West v. Mead (1986); [5] however, in a case commenced in 1994 in the U.S. District Court for the Southern District of New York , the U.S. Court of Appeals for the Second Circuit found Feist to have undermined the reasoning in West v. Mead . West's citation claims were challenged in 1994 by legal publisher, Matthew Bender & Company and by a small CD-Rom publisher HyperLaw, Inc. HyperLaw intervened, joining Matthew Bender in the citation challenge and separately challenging West's text copyright claims. West was found by the Second Circuit in 1998 not to have a protectable copyright interest in its citations; neither to the first page citations nor to its internal pagination citations. See Matthew Bender v. West , Citation Appeal. [6] The Second Circuit thereby rejected the 1996 determination of a Minnesota district court in Oasis Publishing Co. v. West Publishing Co. , 924 F.Supp. 918 (D. Minn. 1996), that the outcome of West is not changed by Feist.
In the same case, but in separate decisions in which Matthew Bender was not involved, HyperLaw successfully challenged West's text claims. Judge John S. Martin ruled in favor of HyperLaw against West in a U.S. District Court decision in May, 1996. Matthew Bender v. West , No. 94 Civ. 0589, 1997 WL 266972 (S.D.N.Y. May 19, 1997), aff'd , 158 F. 3d 674 (2nd Cir. 1998), cert. denied sub. nom. West v. Hyperlaw , 526 U.S. 1154 (1999). [7] West lost to HyperLaw in its appeal to the U.S. Court of Appeals for the Second Circuit and certiorari was denied by the U.S. Supreme Court. [8]
After the 1986 West v. Mead decision, Mead Data and Lexis were acquired by Reed Elsevier , a large English-Dutch based publisher. During the Matthew Bender v. West case, Reed Elsevier and Matthew Bender entered into a strategic relationship, culminating in Reed Elsevier's acquisition of Matthew Bender in 1998, just after the Second Circuit appeals were argued. Reed Elsevier now was on the side of West and filed an amicus brief opposing HyperLaw and supporting West. Thus, although the name of the case might suggest that Matthew Bender challenged West on the text claim, by the middle of the case Matthew Bender was on the side of West on the text issue. Reed Elsevier's support of West's claims to a copyright in text was consistent with the initiatives, discussed below, to sidestep Feist by implementing database protection, through legislation and treaties discussed below. Similarly, during the case, West was acquired by the Canadian based international publisher, the Thomson Corporation.
Another case covering this area is Assessment Technologies v. Wiredata (2003), [9] in which the Seventh Circuit Court of Appeals ruled that a copyright holder in a compilation of public domain data cannot use that copyright to prevent others from using the underlying public domain data, but may only restrict the specific format of the compilation, if that format is itself sufficiently creative. Assessment Technologies also held that it is a fair use of a copyrighted work to reverse engineer that work in order to gain access to uncopyrightable facts. Assessment Technologies also created new law, stating that it is a copyright misuse and an abuse of process if one attempts to use a contract or license agreement based on one's copyright to protect uncopyrightable facts.
In the late 1990s, Congress attempted to pass laws which would protect collections of data , [10] but these measures failed. [11] By contrast, the European Union has a sui generis (specific to that type of work) intellectual property protection for collections of data .

Other countries
The applicability of copyright to phone directories has come up in several other countries.
In Canada, the appeal-level case of Tele-Direct (Publications) Inc. v. American Business Information Inc. (1997) 76 C.P.R. (3d) 296 (F.C.A.) reached a similar result to that of Feist . However, the Supreme Court partially backed away from the originality doctrine in CCH Canadian Ltd. v. Law Society of Upper Canada . Under the CCH ruling, someone may assert protection in a database where the facts are themselves not copied from another source. For example, a person may assert protection in a collection of her own recipes, but she may not assert protection in a database of facts about persons and their ancestry compiled from census records.
In Australia, the Federal Court decision of Telstra v. Desktop Marketing Systems [2002] FCAFC 112 followed the UK approach in Walter v. Lane and ruled that copyright law did, in fact, follow the "sweat of the brow" doctrine. However, Desktop v. Telstra held, as did CCH Canadian , that collections of facts must not be copied from other sources to be eligible for protection. In 2010, the Telstra decision was overturned in a ruling by Justice Gordon by Telstra v. Phone Directories [12] using the precedent of IceTV v. Nine Network .

Relation with treaties
Congress has been considering whether to implement a treaty negotiated at the World Trade Organization . Part of the Uruguay Round Agreement resulted in text which states, in Part II, Section 1, Article 10:
The text mirrors that of Article 2(5) of the Berne Convention , which applies to "collections of literary or artistic works".
This treaty provision is broadly in line with the United States Copyright Act and the Act's case law , which protects compilations of data whose "selection and arrangement" is sufficiently original. See 17 U.S.C. § 101 ("compilation" as defined by the United States Copyright Act includes compilations of data). The standard for such originality is fairly low; for example, business listings have been found to meet this standard when deciding which companies should be listed and categorizing those companies required some kind of expert judgment. See Key Publ'ns, Inc. v. Chinatown Today Pub. Enters. , 945 F.2d 509 (2d Cir. 1991) (applying Feist ). As such, implementation of this treaty would not overrule Feist .

See also
WebPage index: 00178
Jargon File
The Jargon File is a glossary and usage dictionary of computer programmer slang . The original Jargon File was a collection of terms from technical cultures such as the MIT AI Lab , the Stanford AI Lab (SAIL) and others of the old ARPANET AI / LISP / PDP-10 communities, including Bolt, Beranek and Newman , Carnegie Mellon University , and Worcester Polytechnic Institute . It was published in paperback form in 1983 as The Hacker's Dictionary (edited by Guy Steele ), revised in 1991 as The New Hacker's Dictionary (ed. Eric S. Raymond ; third edition published 1996).

1975 to 1983
The Jargon File (referred to here as "Jargon-1" or "the File") was made by Raphael Finkel at Stanford in 1975. From that time until the plug was finally pulled on the SAIL computer in 1991, the File was named "AIWORD.RF[UP,DOC]" ("[UP,DOC]" was a system directory for "User Program DOCumentation" on the WAITS operating system). Some terms, such as frob , foo and mung are believed to date back to the early 1950s from the Tech Model Railroad Club at MIT and documented in the 1959 Dictionary of the TMRC Language compiled by Peter Samson. [1] The revisions of Jargon-1 were all unnumbered and may be collectively considered "version 1". Note that it was always called "AIWORD" or "the Jargon file", never "the File"; the latter term was coined by Eric Raymond.
In 1976, Mark Crispin , having seen an announcement about the File on the SAIL computer, FTPed a copy of the File to the MIT AI Lab. He noticed that it was hardly restricted to "AI words" and so stored the file on his directory, named as "AI:MRC;SAIL JARGON" ("AI" lab computer, directory "MRC", file "SAIL JARGON").
Raphael Finkel dropped out of active participation shortly thereafter and Don Woods became the SAIL contact for the File (which was subsequently kept in duplicate at SAIL and MIT, with periodic resynchronizations).
The File expanded by fits and starts until 1983. Richard Stallman was prominent among the contributors, adding many MIT and ITS -related coinages. The Incompatible Timesharing System (ITS) was named to distinguish it from another early MIT computer operating system, The Compatible Timesharing System.
In 1981, a hacker named Charles Spurgeon got a large chunk of the File published in Stewart Brand 's CoEvolution Quarterly (issue 29, pages 26–35) with illustrations by Phil Wadler and Guy Steele (including a couple of Steele's Crunchly cartoons). This appears to have been the File's first paper publication.
A late version of Jargon-1, expanded with commentary for the mass market, was edited by Guy Steele into a book published in 1983 as The Hacker's Dictionary (Harper & Row CN 1082, ISBN 0-06-091082-8 ). It included all of Steele's Crunchly cartoons. The other Jargon-1 editors (Raphael Finkel, Don Woods, and Mark Crispin) contributed to this revision, as did Stallman and Geoff Goodfellow . This book (now out of print) is hereafter referred to as "Steele-1983" and those six as the Steele-1983 coauthors.

1983 to 1990
Shortly after the publication of Steele-1983, the File effectively stopped growing and changing. Originally, this was due to a desire to freeze the file temporarily to ease the production of Steele-1983, but external conditions caused the "temporary" freeze to become permanent.
The AI Lab culture had been hit hard in the late 1970s by funding cuts and the resulting administrative decision to use vendor-supported hardware and associated proprietary software instead of homebrew whenever possible. At MIT, most AI work had turned to dedicated Lisp machines . At the same time, the commercialization of AI technology lured some of the AI Lab's best and brightest away to startups along the Route 128 strip in Massachusetts and out west in Silicon Valley . The startups built Lisp machines for MIT; the central MIT-AI computer became a TWENEX system rather than a host for the AI hackers' beloved ITS.
The Stanford AI Lab had effectively ceased to exist by 1980, although the SAIL computer continued as a computer science department resource until 1991. Stanford became a major TWENEX site, at one point operating more than a dozen TOPS-20 systems, but by the mid-1980s, most of the interesting software work was being done on the emerging BSD Unix standard.
In May 1983, the PDP-10 -centered cultures that had nourished the File were dealt a death-blow by the cancellation of the Jupiter project at DEC . The File's compilers, already dispersed, moved on to other things. Steele-1983 was partly a monument to what its authors thought was a dying tradition; no one involved realized at the time just how wide its influence was to be. [ citation needed ]
As mentioned in some editions: [2]

1990 and later
A new revision was begun in 1990, which contained nearly the entire text of a late version of Jargon-1 (a few obsolete PDP-10-related entries were dropped after consultation with the editors of Steele-1983). It merged in about 80% of the Steele-1983 text, omitting some framing material and a very few entries introduced in Steele-1983 that are now only of historical interest.
The new version cast a wider net than the old Jargon File; its aim was to cover not just AI or PDP-10 hacker culture but all of the technical computing cultures in which the true hacker-nature is manifested. More than half of the entries now derived from Usenet and represent jargon then current in the C and Unix communities, but special efforts were made to collect jargon from other cultures including IBM PC programmers, Amiga fans, Mac enthusiasts, and even the IBM mainframe world. [ citation needed ]
Eric Raymond maintained the new File with assistance from Guy Steele, and is the credited editor of the print version of it, The New Hacker's Dictionary (published by MIT Press in 1991); hereafter Raymond-1991. Some of the changes made under his watch were controversial; early critics accused Raymond of unfairly changing the file's focus to the Unix hacker culture instead of the older hacker cultures where the Jargon File originated. Raymond has responded by saying that the nature of hacking had changed and the Jargon File should report on hacker culture, and not attempt to enshrine it. [3] After the second edition of NHD (MIT Press, 1993; hereafter Raymond-1993), Raymond was accused of adding terms reflecting his own politics and vocabulary, [4] even though he says that entries to be added are checked to make sure that they are in live use, not "just the private coinage of one or two people". [5]
The Raymond version was revised again, to include terminology from the nascent subculture of the public Internet and the World Wide Web, and published by MIT Press as The New Hacker's Dictionary , Third Edition, in 1996 (hereafter Raymond-1996).
As of January 2016 [update] , no updates have been made to the official Jargon File since 2003. A volunteer editor produced two updates, reflecting later influences (mostly excoriated) from text messaging , LOLspeak , and Internet slang in general; the last was produced in January 2012. [6]

Impact and reception

Influence
Despite its tongue-in-cheek approach, multiple other style guides and similar works have cited The New Hacker's Dictionary as a reference, and even recommended following some of its "hackish" best practices. The Oxford English Dictionary has used the NHD as a source for computer-related neologisms . [7] The Chicago Manual of Style , the leading American academic and book-publishing style guide, beginning with its 15th edition (2003) explicitly defers, for "computer writing", to the quotation punctuation style – logical quotation –  recommended by the essay "Hacker Writing Style" in The New Hacker's Dictionary (and cites NHD for nothing else). [8] The 16th edition (2010, and the current issue as of 2016 [update] ) does likewise. [9] The National Geographic Style Manual lists NHD among only 8 specialized dictionaries, out of 22 total sources, on which it is based. That manual is the house style of NGS publications, and has been available online for public browsing since 1995. [10] The NGSM does not specify what, in particular, it drew from the NHD or any other source.
Aside from these guides and the Encyclopedia of New Media , the Jargon file, especially in print form, is frequently cited for both its definitions and its essays, by books and other works on hacker history, cyberpunk subculture, computer jargon and online style, and the rise of the Internet as a public medium, in works as diverse as the 20th edition of A Bibliography of Literary Theory, Criticism and Philology edited by José Ángel García Landa (2015); Wired Style: Principles of English Usage in the Digital Age by Constance Hale and Jessie Scanlon of Wired magazine (1999); Transhumanism: The History of a Dangerous Idea by David Livingstone (2015); Mark Dery's Flame Wars: The Discourse of Cyberculture (1994) and Escape Velocity: Cyberculture at the End of the Century (2007); Beyond Cyberpunk! A Do-it-yourself Guide to the Future by Gareth Branwyn and Peter Sugarman (1991); and numerous others.
Time magazine used The New Hacker's Dictionary (Raymond-1993) as the basis for an article about online culture in the November 1995 inaugural edition of the "Time Digital" department. NHD was cited by name on the front page of The Wall Street Journal . [ when? ] Upon the release of the second edition, Newsweek used it as a primary source, and quoted entries in a sidebar, for a major article on the Internet and its history. [ when? ] The MTV show This Week in Rock used excerpts from the Jargon File in its "CyberStuff" segments. Computing Reviews used one of the Jargon File's definitions on its December 1991 cover.
On October 23, 2003, The New Hacker's Dictionary was used in a legal case. SCO Group cited the Raymond-1996 definition of "FUD" ( fear, uncertainty and doubt ), which dwelt on questionable IBM business practices, in a legal filing in the civil lawsuit SCO Group, Inc. v. International Business Machines Corp. . [11] (In response, Raymond added SCO to the entry in a revised copy of the Jargon File , feeling that SCO's own practices deserved similar criticism. [12] )

Defense of the term 
The book is particularly noted for helping (or at least trying) to preserve the distinction between a hacker (a consummate programmer) and a cracker (a computer criminal ); even though not reviewing the book in detail, both the London Review of Books [13] and MIT Technology Review [14] remarked on it in this regard. In a substantial entry on the work, the Encyclopedia of New Media by Steve Jones (2002) observed that this defense of the term hacker was a motivating factor for both Steele's and Raymond's print editions: [15]

Reviews and reactions
PC Magazine in 1984, stated that The Hacker's Dictionary was superior to most other computer-humor books, and noted its authenticity to "hard-core programmers' conversations", especially slang from MIT and Stanford. [16] Reviews quoted by the publisher include: William Safire of New York Times referring to the Raymond-1991 NHD as a "sprightly lexicon" and recommending it as a nerdy gift that holiday season [17] (this reappeared in his "On Language" column again in mid-October 1992); Hugh Kenner in Byte suggesting that it was so engaging that one's reading of it should be "severely timed if you hope to get any work done"; [18] and Mondo 2000 describing it as "slippery, elastic fun with language", as well as "not only a useful guidebook to very much un-official technical terms and street tech slang, but also a de facto ethnography of the early years of the hacker culture." [19] Positive reviews were also published in academic as well as computer-industry publications, including IEEE Spectrum , New Scientist , PC Magazine , PC World , Science , and (repeatedly) Wired .
US game designer Steve Jackson , writing for bOING bOING magazine in its pre-blog, print days, described NHD 's essay "A Portrait of J. Random Hacker" as "a wonderfully accurate pseudo-demographic description of the people who make up the hacker culture." He was nevertheless critical of Raymond's tendency to editorialize, even " flame ", and of the Steele cartoons, which Jackson described as "sophomoric, and embarrassingly out of place beside the dry and sophisticated humor of the text." He wound down his review with some rhetorical questions: "where else will you find, for instance, that one attoparsec per microfortnight is approximately equal to one inch per second? Or an example of the canonical use of canonical ? Or a definition like 'A cuspy but bogus raving story about N random broken people'?" [20]
The third print edition garnered additional coverage, in the usual places like Wired (August 1996), and even in very populist venues like People magazine (October 21, 1996). [7]
WebPage index: 00179
International Standard Serial Number
An International Standard Serial Number ( ISSN ) is an eight-digit serial number used to uniquely identify a serial publication . [1] The ISSN is especially helpful in distinguishing between serials with the same title. ISSN are used in ordering, cataloging, interlibrary loans, and other practices in connection with serial literature. [2]
The ISSN system was first drafted as an International Organization for Standardization (ISO) international standard in 1971 and published as ISO 3297 in 1975. [3] ISO subcommittee TC 46/SC 9 is responsible for maintaining the standard.
When a serial with the same content is published in more than one media type , a different ISSN is assigned to each media type. For example, many serials are published both in print and electronic media . The ISSN system refers to these types as print ISSN ( p-ISSN ) and electronic ISSN ( e-ISSN ), respectively. [ citation needed ] Conversely, as defined in ISO 3297:2007, every serial in the ISSN system is also assigned a linking ISSN ( ISSN-L ), typically the same as the ISSN assigned to the serial in its first published medium, which links together all ISSNs assigned to the serial in every medium. [4]

Code format
The format of the ISSN is an eight digit code, divided by a hyphen into two four-digit numbers. [1] As an integer number , it can be represented by the first seven digits. [5] The last code digit, which may be 0-9 or an X, is a check digit . Formally, the general form of the ISSN code (also named "ISSN structure" or "ISSN syntax") can be expressed as follows: [6]
or by a PCRE regular expression : [7]
The ISSN of the journal Hearing Research , for example, is 0378-5955, where the final 5 is the check digit, that is C =5 . To calculate the check digit, the following algorithm may be used:
To confirm the check digit, calculate the sum of all eight digits of the ISSN multiplied by its position in the number, counting from the right (if the check digit is X, then add 10 to the sum). The modulus 11 of the sum must be 0.
There is an online ISSN checker that can validate an ISSN, based on the above algorithm. [8] [9]

Code assignment
ISSN codes are assigned by a network of ISSN National Centres, usually located at national libraries and coordinated by the ISSN International Centre based in Paris . The International Centre is an intergovernmental organization created in 1974 through an agreement between UNESCO and the French government. The International Centre maintains a database of all ISSNs assigned worldwide, the ISDS Register (International Serials Data System) otherwise known as the ISSN Register . At the end of 2016 [update] , the ISSN Register contained records for 1,943,572 items. [10]

Comparison with other identifiers
ISSN and ISBN codes are similar in concept, where ISBNs are assigned to individual books . An ISBN might be assigned for particular issues of a serial, in addition to the ISSN code for the serial as a whole. An ISSN, unlike the ISBN code, is an anonymous identifier associated with a serial title, containing no information as to the publisher or its location . For this reason a new ISSN is assigned to a serial each time it undergoes a major title change.
Since the ISSN applies to an entire serial a new identifier, the Serial Item and Contribution Identifier (SICI), was built on top of it to allow references to specific volumes, articles, or other identifiable components (like the table of contents ).

Media 
Separate ISSNs are needed for serials in different media (except reproduction microforms ). Thus, the print and electronic media versions of a serial need separate ISSNs. [11] Also, a CD-ROM version and a web version of a serial require different ISSNs since two different media are involved. However, the same ISSN can be used for different file formats (e.g. PDF and HTML ) of the same online serial.
This "media-oriented identification" of serials made sense in the 1970s. In the 1990s and onward, with personal computers, better screens, and the Web, it makes sense to consider only content , independent of media. This "content-oriented identification" of serials was a repressed demand during a decade, but no ISSN update or initiative occurred. A natural extension for ISSN, the unique-identification of the articles in the serials, was the main demand application. An alternative serials' contents model arrived with the indecs Content Model and its application, the digital object identifier (DOI), as ISSN-independent initiative, consolidated in the 2000s.
Only later, in 2007, ISSN-L was defined in the new ISSN standard (ISO 3297:2007) as an "ISSN designated by the ISSN Network to enable collocation or versions of a continuing resource linking among the different media". [12]

Availability
The ISSN Register is not freely available for interrogation on the web, but is available by subscription. There are several routes to the identification and verification of ISSN codes for the public:

Use in URNs
An ISSN can be encoded as a uniform resource name (URN) by prefixing it with " urn:ISSN: ". [13] For example, Rail could be referred to as " urn:ISSN:0953-4563 ". URN namespaces are case-sensitive, and the ISSN namespace is all caps. [14] If the checksum digit is "X" then it is always encoded in uppercase in a URN.

Problems
The util URNs are content-oriented , but ISSN is media-oriented:
A unique URN for serials simplifies the search, recovery and delivery of data for various services including, in particular, search systems and knowledge databases . [12] ISSN-L (see Linking ISSN below) was created to fill this gap.

ISSN variants and labels
There are two most popular media types that adopted special labels (indicating below in italics), and one in fact ISSN-variant, with also an optional label. All are used in standard metadata context like JATS , and the labels also, frequently, as abbreviations.

Print ISSN
p-ISSN is a standard label for "Print ISSN", the ISSN for the print media (paper) version of a serial. Usually it is the "default media", so the "default ISSN".

Electronic ISSN
e-ISSN (or eISSN) is a standard label for "Electronic ISSN", the ISSN for the electronic media (online) version of a serial.

Linking ISSN
ISSN-L is a unique identifier for all versions of the serial containing the same content across different media. As defined by ISO 3297:2007 , the "linking ISSN (ISSN-L)" provides a mechanism for collocation or linking among the different media versions of the same continuing resource.
The ISSN-L is one ISSN number among the existing ISSNs, so, does not change the use or assignment of "ordinary" ISSNs; [16] it is based on the ISSN of the first published medium version of the publication. If the print and online versions of the publication are published at the same time, the ISSN of the print version is chosen as the basis of the ISSN-L .
With ISSN-L is possible to designate one single ISSN for all those media versions of the title. The use of ISSN-L facilitates search, retrieval and delivery across all media versions for services like OpenURL , library catalogues , search engines or knowledge bases . [17]

See also
WebPage index: 00180
National Health and Medical Research Council
The National Health and Medical Research Council (NHMRC) is Australia's peak funding body for medical research , with a budget of roughly $900 million a year. [2] [3] The Council was established to develop and maintain health standards and is responsible for implementing the National Health and Medical Research Council Act 1992 . [4]
NHMRC is a material agency and is incorporated under the federal Financial Management and Accountability Act 1997 . It was a part of the Department of Health and Ageing portfolio until mid-2007 when it became a self-governing statutory authority.
Along with the Australian Research Council (ARC), NHMRC is one of the Australian government's two main agencies for allocating competitively research funding to academics and researchers at Australian universities.

Medical research grading
NHMRC research grading is commonly used to assess medical publications. These include, from the most reliable to least: systematic review, randomized control trial, cohort study, case control, case series. [5]

History
In 2010 the NHMRC's new online system for grant applications was the subject of criticism after a series of technical problems. [6]
The community organisation Organisation Intersex International Australia criticised the NHMRC for funding research programs that pathologise intersex variations as disorders. [7] [8]

See also
WebPage index: 00181
Copyright infringement
WebPage index: 00182
Mashup (culture)
Participants in an online music scene who rearrange spliced parts of musical pieces form Mashup Culture . The audio-files are normally in MP3 format and spliced with audio-editing software online. The new, edited song is called mashup. [1] The expression mashup culture is also strongly connected to mashup in music . Even though it is originally not a political community, the producer of mash-up music are related to the issue of copyright . Mashup Culture is even regarded as "a response to larger technological, institutional, and social contexts". [2]

History
The history of mashup culture in general can be dated back to the beginnings of dada and conceptual art . Artists such as Marcel Duchamp were the first to introduce already existing objects, which they rearranged and combined in collages , to the world of higher art. [3] These artists believed that even though certain artifacts were ascribed a certain meaning, this meaning could be altered through rearranging them and putting them into a new context. However, it was still quite a long way to the beginning of mashup culture in music. From the early 2000s on, music was more and more distributed through the internet. With the introduction of MP3 audio files, it became much easier to access and download music. Not only could music be accessed easier, it could also be transformed and mixed in ways that were not possible before. Especially for younger people, this new gained freedom when it came to the accessibility of audio files lead to the development of a new form of cult around the transformation of musical pieces. [1] This "reworking MP3 recordings pulled from the Internet" was turning into more than just a fashion just as "the Internet is more than just a means of distribution, it becomes a raison d’eˆtre for a culture based on audio data’’ states Alistair Riddell in 2001. [4] During that time, the first versions of mashup music were published, sometimes not under the term mashup but under the name of " creative bootleg " or " bastard pop ". [1] [2] Even though the creation of a new song by combining at least two samples of different songs was also used in other music styles such as Hip Hop before, [2] it was only with the rise of the MP3 audio file along with easy-to-use computer mash-up programs that mash-up was transformed into an own culture as such. Especially peer-to-peer sharing was contributing to this phenomenon: People who create mashup music can easily distribute it and share it with other people through online programs. [1]

Forms of mashup culture
Mashup culture is motivated by a number of different factors.

Statement of art
Mashup culture is sometimes regarded as a cultural movement against common, existing music that is published by the music industry. In 2002, a Newsweek article described the mashup of songs as a strategy of Londoner DJs to transform music they considered bad into something they could appreciate and were willing to listen to. [1] Even though mashup culture has its origin in online communities, it entered a more art-related realm. It is art used as a statement against the content music industry provides. [2]

Political statement
It can even be considered as a political statement against copyright laws and restrictions imposed by the government as well. "Reframing of the original narrative" is regarded as a way to create a new and unique product which leads to a "fresh perspective" of the original. [3] Murray states "there’s the question of the kind of Internet we want moving forward – one increasingly controlled by corporate gatekeepers who get to sanction what creative expression looks like, or one in which the freedom of this expression is valued above share price". Copyright issues have always been limiting the possibilities of mashup culture. Those implications by law have led to the problem of online piracy . [1] Mashups are often created with illegally acquired content. This closeness to illegality has become part of this culture. "In some cases, the illegality of piracy contributes to the appeal of unauthorized copies online" states Shiga in her article Copy-and-Persist: The Logic of Mash-Up Culture . Even though copyright laws were intentionally supposed to stop illegal downloads, they contributed to the appeal of mashup and to the culture existing around it.

Do-it-yourself culture
Another important aspect of the success of mashup culture nowadays lies in the do-it-yourself or DIY trend. Consumers are turning into producers as well, especially due to the simplification of online tools that help creating personalized content.

Mashup in current literature

DJ Spooky
Paul D. Miller, also known as DJ Spooky , published a book about how mashup culture transforms our society. In Sound Unbound: Sampling Digital Music and Culture , he investigates "the nature of that transformation" stating that "rapid advances in technology [that] have transformed art and communication". [5] He describes sampling as an essential part of our society. The book was published by the MIT Press and highly recognized by scholars such as Lawrence Lessig . [5]

See also

Literature
WebPage index: 00183
Societal views on patents
Legal scholars, economists, activists, policymakers, industries, and trade organizations have held differing views on patents and engaged in contentious debates on the subject. Critical perspectives emerged in the nineteenth century that were especially based on the principles of free trade . [1] :262–263 Contemporary criticisms have echoed those arguments, claiming that patents block innovation and waste resources that could otherwise be used productively, [2] and also block access to an increasingly important "commons" of enabling technologies (a phenomenon called the tragedy of the anticommons ), [3] apply a "one size fits all" model to industries with differing needs, [4] that is especially unproductive for industries other than chemicals and pharmaceuticals and especially unproductive for the software industry. [5] Enforcement by patent trolls of poor quality patents has led to criticism of the patent office as well as the system itself; [6] Patents on pharmaceuticals have also been a particular focus of criticism, as the high prices they enable puts life-saving drugs out of reach of many people. [7] Alternatives to patents have been proposed, such Joseph Stiglitz 's suggestion of providing "prize money" (from a "prize fund" sponsored by the government) as a substitute for the lost profits associated with abstaining from the monopoly given by a patent. [8]
These debates are part of a larger discourse on intellectual property protection which also reflects differing perspectives on copyright .

History
Criticism of patents reached an early peak in Victorian Britain between 1850 and 1880, in a campaign against patenting that expanded to target copyright too and, in the judgment of historian Adrian Johns, "remains to this day the strongest [campaign] ever undertaken against intellectual property", coming close to abolishing patents. [1] :247 Its most prominent activists - Isambard Kingdom Brunel , William Robert Grove , William Armstrong and Robert A. MacFie - were inventors and entrepreneurs, and it was also supported by radical laissez-faire economists ( The Economist published anti-patent views), law scholars, scientists (who were concerned that patents were obstructing research) and manufacturers. [1] :249, 267, 270 Johns summarizes some of their main arguments as follows: [1] :273 [9] [10]
Similar debates took place during that time in other European countries such as France, Prussia , Switzerland and the Netherlands (but not in the USA). [1] :248
Based on the criticism of patents as state-granted monopolies inconsistent with free trade , the Netherlands abolished patents in 1869 (having established them in 1817), and did not reintroduce them until 1912. [11] In Switzerland, criticism of patents delayed the introduction of patent laws until 1907. [1] :248 [11]

Contemporary arguments
Contemporary arguments have focused on ways that patents can slow innovation by: blocking researchers' and companies' access to basic, enabling technology, and particularly following the explosion of patent filings in the 1990s, through the creation of "patent thickets"; wasting productive time and resources fending off enforcement of low-quality patents that should not have existed, particularly by "patent trolls"; and wasting money on patent litigation. Patents on pharmaceuticals have been a particular focus of criticism, as the high prices they enable puts life-saving drugs out of reach of many people.

Blocking innovation
The most general argument against patents is that "intellectual property" in all its forms represents an effort to claim something that should not be owned, and harms society by slowing innovation and wasting resources. [2]
Law professors Michael Heller and Rebecca Sue Eisenberg have described an ongoing tragedy of the anticommons with regard to the proliferation of patents in the field of biotechnology , wherein intellectual property rights have become so fragmented that, effectively, no one can take advantage of them as to do so would require an agreement between the owners of all of the fragments. [3] Joshua Pearce has applied the "tragedy of the anticommons" argument to the field of nanotechnology . [12] [13]
Some public campaigns for improving access to medicines and genetically modified food have expressed a concern for "preventing the over-reach" of IP protection including patent protection, and "to retain a public balance in property rights". [14] [15] Some economists [2] and scientists [12] and law professors [16] have raised concerns that patents retard technical progress and innovation. Others claim that patents have had no effect on research, based on surveys of scientists. [17] [18]

Poor patent quality and patent trolls
Patents have also been criticized for being granted on already-known inventions, with some complaining in the United States that the USPTO fails "to do a serious job of examining patents, thus allowing bad patents to slip through the system." [19] On the other hand, some argue that because of low number of patents going into litigation, increasing quality of patents at patent prosecution stage increases overall legal costs associated with patents, and that current USPTO policy is a reasonable compromise between full trial on examination stage on one hand, and pure registration without examination, on the other hand. [19]
Enforcement of patents – especially patents perceived as being overly broad – by patent trolls , has brought criticism of the patent system, [6] [20] though some commentators suggest that patent trolls are not bad for the patent system at all but instead realign market participant incentives, make patents more liquid, and clear the patent market. [21]
Some patents granted in Russia have been denounced as pseudoscientific (for example, health-related patents using lunar phase or religious icons). [22] [23] [24]

Litigation costs
According to James Bessen , the costs of patent litigation exceed their investment value in all industries except chemistry and pharmaceuticals. For example, in the software industry, litigation costs are twice the investment value. [25] Bessen and Meurer also note that software and business model litigation accounts for a disproportionate share (almost 40 percent) of patent litigation cost, and the poor performance of the patent system negatively affects these industries. [5] [26]

Different industries but one law
Richard Posner noted that the most controversial feature of US patent law is that it covers all industries in the same way, but not all industries benefit from the time-limited monopoly a patent provides in order to spur innovation. [4] He said that while the pharmaceutical industry is "poster child" for the need for a twenty year monopoly, since costs to bring to a market are high, the time of development is often long, and the risks are high, in other industries like software the cost and risk of innovation is much lower and the cycle of innovation is quicker, and obtaining and enforcing patents and defending against patent litigation is generally a waste of resources in those industries. [4]

Pharmaceutical patents
Some have raised ethical objections specifically with respect to pharmaceutical patents and the high prices for medication that they enable their proprietors to charge, which poor people in the developed world, and developing world, cannot afford. [7] [27] Critics also question the rationale that exclusive patent rights and the resulting high prices are required for pharmaceutical companies to recoup the large investments needed for research and development. [7] One study concluded that marketing expenditures for new drugs often doubled the amount that was allocated for research and development. [28]
In 2003, World Trade Organization (WTO) reached an agreement, which provides a developing country with options for obtaining needed medications under compulsory licensing or importation of cheaper versions of the drugs, even before patent expiration. [29]
In 2007 the government of Brazil declared Merck 's efavirenz anti-retroviral drug a "public interest" medicine, and challenged Merck to negotiate lower prices with the government or have Brazil strip the patent by issuing a compulsory license. [30] [31] [32]
It is reported that Ghana , Tanzania , the Democratic Republic of Congo and Ethiopia have similar plans to produce generic antiviral drugs. Western pharmaceutical companies initially responded with legal challenges, but some have now promised to introduce alternative pricing structures for developing countries and NGOs. [31] [32]
In July 2008 Nobel Prize-winning scientist Sir John Sulston called for an international biomedical treaty to clear up issues over patents. [33]
In response to these criticisms, one review concluded that less than 5 percent of medicines on the World Health Organization's list of essential drugs are under patent. [34] Also, the pharmaceutical industry has contributed US$2 billion for healthcare in developing countries, providing HIV/AIDS drugs at lower cost or even free of charge in certain countries, and has used differential pricing and parallel imports to provide medication to the poor. [34] Other groups are investigating how social inclusion and equitable distribution of research and development findings can be obtained within the existing intellectual property framework, although these efforts have received less exposure. [34]
Quoting a World Health Organisation report, Trevor Jones (director of research and development at the Wellcome Foundation, as of 2006) argued in 2006 that patent monopolies do not create monopoly pricing . He argued that the companies given monopolies "set prices largely on the willingness/ability to pay, also taking into account the country, disease and regulation" instead of receiving competition from legalized generics. [31]

Proposed alternatives to the patent system
Alternatives have been discussed to address the issue of financial incentivization to replace patents. Mostly, they are related to some form of direct or indirect government funding. One example is Joseph Stiglitz 's idea of providing "prize money" (from a "prize fund" sponsored by the government) as a substitute for the lost profits associated with abstaining from the monopoly given by a patent. [8] Another approach is to remove the issue of financing development from the private sphere altogether, and to cover the costs with direct government funding. [35]

See also
WebPage index: 00184
All rights reversed
All rights reversed is a phrase that indicates a release of copyright or a copyleft licensing status. It is a pun on the common copyright disclaimer " All rights reserved ", a copyright formality originally required by the Buenos Aires Convention of 1910. [1] "All Rights Reversed" (sometimes spelled rites ) was used by author Gregory Hill to authorize the free reprinting of his Principia Discordia in the late 1960s. Hill's disclaimer was accompanied by the kosher "Ⓚ" (for kallisti ) symbol, a play on ©, the copyright symbol . [2]
In 1984/5 programmer Don Hopkins sent Richard Stallman a letter labeled "Copyleft—all rights reversed". Stallman chose the phrase to identify his free software method of distribution. [3] It is often accompanied by a reversed version of the copyright symbol ( see illustration ). [4] That said, this usage is considered legally risky by the Free Software Foundation . [5]
"All Rights Reversed", its homophone, "All Rites Reversed", and/or the "Copyleft" symbol, are occasionally used among those who publish or produce media (or any other material that might normally be copyrighted) as a clever means of saying "This is not copyrighted. Please, do with it what you will." and encouraging the duplication and use of the "copy-lefted" material thereof. An additional meaning is that if the material is labeled with the copyleft symbol, it should stay open under the credits of the original creator. It may be edited, but only if credit is given. It may also not be redistributed under copyright laws.
The open source character Jenny Everywhere is released under an "All rights reversed" licence.
WebPage index: 00185
Commons-based peer production
Commons-based peer production is a term coined by Harvard Law School professor Yochai Benkler . [1] It describes a new model of socioeconomic production in which large numbers of people work cooperatively (usually over the Internet ). Commons -based projects generally have less rigid hierarchical structures than those under more traditional business models. Often—but not always—commons-based projects are designed without a need for financial compensation for contributors.
The term is often used interchangeably with the term social production .

Overview
Benkler contrasts commons -based peer production with firm production , in which tasks are delegated based on a central decision-making process, and market-based production , in which allocating different prices to different tasks serves as an incentive to anyone interested in performing a task.
Benkler first introduced the term in his 2002 paper "Coase's Penguin, or Linux and the Nature of the Firm", [2] whose title refers to the Linux mascot and to Ronald Coase , who originated the transaction costs theory of the firm that provides the methodological template for the paper's analysis of peer production. The paper cites Eben Moglen as the originator of the concept. [2]
In his book The Wealth of Networks (2006), Benkler significantly expands on his definition of commons-based peer production. According to Benkler, what distinguishes commons-based production is that it doesn't rely upon or propagate proprietary knowledge: "The inputs and outputs of the process are shared, freely or conditionally, in an institutional form that leaves them equally available for all to use as they choose at their individual discretion." To ensure that the knowledge generated is available for free use, commons-based projects are often shared under an open license .
Not all commons-based production necessarily qualifies as commons-based peer production. According to Benkler, peer production is defined not only by the openness of its outputs, but also by a decentralized, participant-driven working method of working. [3]
Peer production enterprises have two primary advantages over traditional hierarchical approaches to production:
In Wikinomics , Don Tapscott and Anthony D. Williams suggest an incentive mechanism behind common-based peer production. "People participate in peer production communities," they write, "for a wide range of intrinsic and self-interested reasons....basically, people who participate in peer production communities love it. They feel passionate about their particular area of expertise and revel in creating something new or better." [5]
Aaron Krowne offers another definition:

Principles
First, the potential goals of peer production must be modular . In other words, objectives must be divisible into components, or modules, each of which can be independently produced. That allows participants to work asynchronously, without having to wait for each other's contributions or coordinate with each other in person. [7]
Second, the granularity of the modules is essential. Granularity refers to the degree to which objects are broken down into smaller pieces (module size). [7] Different levels of granularity will allow people with different levels of motivation to work together by contributing small or large grained modules, consistent with their level of interest in the project and their motivation. [7]
Third, a successful peer-production enterprise must have low-cost integration —the mechanism by which the modules are integrated into a whole end product. Thus, integration must include both quality controls over the modules and a mechanism for integrating the contributions into the finished product at relatively low cost. [7]

Examples
Examples of projects using commons-based peer production include:

Outgrowths
Several outgrowths have been:

Related concepts
Interrelated concepts to Commons-based peer production are the processes of peer governance and peer property. To begin with, peer governance is a new mode of governance and bottom-up mode of participative decision-making that is being experimented in peer projects, such as Wikipedia and FLOSS ; thus peer governance is the way that peer production, the process in which common value is produced, is managed. [8] Peer Property indicates the innovative nature of legal forms such as the General Public License, the Creative Commons, etc. Whereas traditional forms of property are exclusionary ("if it is mine, it is not yours"), peer property forms are inclusionary. It is from all of us, i.e. also for you, provided you respect the basic rules laid out in the license, such as the openness of the source code for example. [9]
The ease of entering and leaving an organization is a feature of adhocracies .
The principle of commons-based peer production is similar to collective invention , a model of open innovation in economics coined by Robert Allen. [10]
Also related: Open-source economics and Commercial use of copyleft works .

Criticism
Some [11] believe that the commons-based peer production (CBPP) vision, while powerful and groundbreaking, needs to be strengthened at its root because of some allegedly wrong assumptions concerning free and open source software (FOSS).
The CBPP literature regularly and explicitly quotes FOSS products as examples of artifacts “emerging” by virtue of mere cooperation, with no need for supervising leadership (without «market signals or managerial commands», in Benkler’s words).
It can be argued, however, that in the development of any less than trivial piece of software, irrespective of whether it be FOSS or proprietary, a subset of the (many) participants always play -explicitly and deliberately- the role of leading system and subsystem designers, determining architecture and functionality, while most of the people work “underneath” them in a logical, functional sense. [ weasel words ]

See also
WebPage index: 00186
Patentleft
Patentleft (also patent left , copyleft-style patent license or open patent ) is the practice of licensing patents (especially biological patents ) for royalty -free use, on the condition that adopters license related improvements they develop under the same terms. Copyleft-style licensors seek "continuous growth of a universally accessible technology commons" from which they, and others, will benefit. [1] [2]
Patentleft is analogous to copyleft , a license which allows distribution of a copyrighted work and derived works, but only under the same terms.

Uses
The Biological Innovation for Open Society (BiOS) project implemented a patentleft system to encourage re-contribution and collaborative innovation of their technology. BiOS holds a patented technology for transferring genes in plants, and licenses the technology under the terms that, if a license holder improves the gene transfer tool and patents the improvement, then their improvement must be made available to all the other license holders. [3]
The open patent idea is designed to be practiced by consortia of research-oriented companies [4] and increasingly by standards bodies . These also commonly use open trademark methods to ensure some compliance with a suite of compatibility tests, e.g. Java , X/Open both of which forbid use of the mark by the non-compliant. [ citation needed ]
On October 12, 2001 the Free Software Foundation and Finite State Machine Labs Inc. (FSMLabs) announced a GPL - compliant open-patent license for FSMLabs' software patent , US 5995745 . Titled the Open RTLinux patent license Version 2, it provides for usage of this patent in accordance with the GPL. [5]

Example
Person A has a patent, and licenses it under a patentleft license.
Person B has two patents in her product and wants to use Person A's patents in that product. Person B also wants to charge royalties for her two patents. She decides to use Person A's patent, but now must license her patents, royalty-free, under the same terms as Person A's patent.
Person C has three patents in his product and wants to use Person B's two patents in that product, but doesn't want to use Person A's patent. Person C also wants to charge royalties for his three patents. He decides to use Person B's patent, but now must license his patents, royalty-free, under the same terms as Person A's patent.

See also
WebPage index: 00187
Access to Knowledge movement
The Access to Knowledge ( A2K ) movement is a loose collection of civil society groups, governments , and individuals converging on the idea that access to knowledge should be linked to fundamental principles of justice , freedom , and economic development .

History
The Berlin Declaration on Open Access to Knowledge in the Sciences and Humanities from 2003 is a major declaration reflecting the goals of the movement in relation to academic publishing.
In October 2004, the Geneva declaration on the future of the World Intellectual Property Organization emerged from a call from Brazil and Argentina for a development agenda for the World Intellectual Property Organization , and was supported by hundreds organizations. [1] Supporters included the Free Software Foundation , with a statement Towards a "World Intellectual Wealth Organisation": Supporting the Geneva Declaration. [2]
One of the proposals of the declaration was to a «call for a Treaty on Access to Knowledge and Technology. The Standing Committee on Patents and the Standing Committee on Copyright and Related Rights should solicit views from member countries and the public on elements of such a treaty». [3]
A shared discussion platform on A2K issues is the mailing list of that name, which was initiated around discussion of the Geneva declaration. [4] A draft "A2K treaty" was later produced. [5] The proposed treaty is intended to ease the transfer of knowledge to developing nations, and to secure the viability of open innovation systems all over the world. [6]

Human rights debate
Access to knowledge and science is protected by Article 27 of the Universal Declaration of Human Rights . The article balances the right of access with a right to protection of moral and material interests:
A2K academics argue that “material interests” are not simply equivalent to current intellectual property provisions, not least because these rights are saleable and transferable, and therefore not “inalienable”. The right to access is ultimately the more important part of the right. Current levels of IP protection seem out of balance with Article 27, according to A2K theorists:

Supporters

Knowledge Ecology International
CP Tech (now Knowledge Ecology International ) say: "the A2K (Access to Knowledge) movement takes concerns with copyright law and other regulations that affect knowledge and places them within an understandable social need and policy platform: access to knowledge goods." [8]

Consumers International
Many different groups refer to the A2K movement. Consumers International is particularly prominent, running a dedicated domain, [9] and defines the movement as:

See also
WebPage index: 00188
Open Rights Group
The Open Rights Group ( ORG ) is a UK-based organisation that works to preserve digital rights and freedoms by campaigning on digital rights issues and by fostering a community of grassroots activists. It campaigns on numerous issues including mass surveillance, internet filtering and censorship, and intellectual property rights.

History
The organisation was started by Danny O'Brien , Cory Doctorow , Ian Brown , Rufus Pollock , James Cronin , Stefan Magdalinski , Louise Ferguson and Suw Charman after a panel discussion at Open Tech 2005. [1] O'Brien created a pledge on PledgeBank , placed on 23 July 2005, with a deadline of 25 December 2005: "I will create a standing order of 5 pounds per month to support an organisation that will campaign for digital rights in the UK but only if 1,000 other people will too." The pledge reached 1000 people on 29 November 2005. [2] [3] The Open Rights Group was launched at a "sell-out" meeting in Soho , London. [4] [5]

Work
The group has made submissions to the All Party Internet Group (APIG) inquiry into digital rights management [6] [7] and the Gowers Review of Intellectual Property . [8] [9]
The group was honoured in the 2008 Privacy International Big Brother Awards alongside No2ID , Liberty , Genewatch UK and others, as a recognition of their efforts to keep state and corporate mass surveillance at bay. [10]
In 2010 the group worked with 38 Degrees [11] to oppose the introduction of the Digital Economy Act , which was passed in April 2010. [12]

Goals

Areas of interest
The organisation, though focused on the impact of digital technology on the liberty of UK citizens, operates with an apparently wide range of interests within that category. Its interests include: [13] [14]

Access to knowledge

Free speech and censorship

Government and democracy

Privacy, surveillance and censorship

Structure
ORG has a paid staff, [15] whose members include:
Former staff include Suw Charman-Anderson and Becky Hogge , both Executive Directors, e-voting coordinator Jason Kitcat, campaigner Peter Bradwell, grassroots campaigner Katie Sutton and administrator Katerina Maniadaki. [16] The group's patron is Neil Gaiman . [17] As of February 2011 they have 22,000 supporters of which 1,400 are paying contributors [18]

Advisory council and board of directors
In addition to staff members and volunteers, there is an advisory panel of over thirty members, and a Board of Directors, which oversees the group's work, staff, fundraising and policy. [19] The current board members are:
In January 2015, the Open Rights Group announced the formation of a Scottish Advisory Council which will be handling matters relating to Scottish digital rights and campaigns. The Advisory Council is made up of:
From the existing UK Advisory Council:
And from the Open Rights Group Board:
One of the first projects is to raise awareness and opposition to the Scottish Identity Database.

ORGCON
ORGCON was the first ever conference dedicated to digital rights in the UK, [20] marketed as "a crash course in digital rights". It was held for the first time in 2010 at City University in London and included keynote talks from Cory Doctorow , politicians and similar pressure groups including Liberty , NO2ID and Big Brother Watch . ORGCON has since been held in 2012, 2013 and 2014.

See also
WebPage index: 00189
Sci-Hub
Sci-Hub is an online search engine with over 62,000,000 academic papers [2] and articles available for direct download, bypassing publisher paywalls. New papers are uploaded daily when accessed through educational institution proxies, and papers that have been accessed through Sci-Hub are stored in the LibGen repository.
Sci-hub was founded by Kazakh graduate student Alexandra Elbakyan in 2011, as a reaction to the high cost of research papers behind paywalls , typically US$30 each when bought on a per-paper basis.
In 2015 academic publisher Elsevier filed a legal complaint in New York City against Sci-hub alleging copyright infringement by Sci-Hub, and the subsequent lawsuit led to a loss of the original sci-hub.org domain. Following the first domain loss, Sci-Hub has cycled through a number of domains, some of which have been blocked in certain countries. Sci-Hub has been highly controversial, lauded by parts of the scientific and academic communities and condemned by a number of publishers.

History
The Sci-Hub project started running on 5 September 2011, [5] created by Alexandra Elbakyan , a software developer and neurotechnology researcher from Kazakhstan. [6] Her stated goal is to help spread knowledge by allowing more people to access otherwise paywalled content. [7] Elbakyan claims she could not have performed research at a Kazakh university had she not similarly "pirated" articles through research sharing forums, given the need to access hundreds of articles at 32 dollars each. [8] [9] After getting highly involved with and sending out hundreds of papers she started developing ways to automate the process. [8] [9]
The project's original domain name , Sci-Hub.org, was suspended in November 2015 following a court order. [10] The project resurfaced again that same month under a .io domain. [11] The .io address was subsequently also seized after a complaint from Elsevier was directed to the site's Chinese registrar , Now.cn . [ citation needed ] Despite the takedown by the registrar, Sci-Hub remains reachable via alternate domain at .ac (domain at .bz closed in Jan–Feb 2017); however, as of April 2017, domains .io, .cc, .bz, and .ac all appear to be active. It can also be accessed by directly entering the IP address (80.82.77.83), or through a .onion Tor Hidden Service (scihub22266oqcxt.onion), which TorrentFreak states is "pretty much immune" to takedown requests. [12] [13] Articles can also be retrieved using a bot in the instant messaging service Telegram . [14]
The site has seen widespread popularity in both developed countries and developing countries such as the United States , India , Indonesia , [15] Pakistan , Iran , China , Russia and Brazil . [16] As of early 2016, data released by Elbakyan shows usage in developed countries is high, with a large proportion of the downloads coming from the US and countries within the European Union . [4]
Part of a larger network of sites which provide free access to scholarly papers, Sci-Hub is the first to automate the process of bypassing paywalls. [17] [18] Other methods are requesting papers manually by direct email to paper authors or other academics or by requesting them via online research forums or social networks, like the #ICanHazPDF Twitter tag. [17] [19] [20]

Website
The Sci-Hub website provides access to articles without requiring subscription or payment. [21] :10 In February 2016, the website claimed to serve over 200,000 requests per day [4] —an increase from an average of 80,000 per day before the "sci-hub.org" was blocked in 2015. [22] As of November 2016 [update] the website has 58 million papers in its collection. [23]
The site bypasses publishers' paywalls using a collection of credentials (user IDs and passwords) belonging to educational institutions which have purchased access to the journals. In April 2016, Elbakyan told Science that many anonymous academics from around the world donate their credentials voluntarily, [4] while publishers have claimed that Sci-Hub relies on credentials obtained by phishing . [4] Sci-Hub has credentials to access papers through platforms owned by JSTOR, Springer, Sage, and Elsevier, among others. [24] In 2013 Sci-Hub began collaborating with Library Genesis (LibGen), a repository of educational books and documents hosted in Russia. [25] If a requested paper is available through that database, it will be deployed to the user without needing to utilize any credentials. If a paper is not accessible through LibGen, Sci-Hub will download it and store a copy in the LibGen database for any future requests. [21] :11 Estimates suggest Sci-Hub has downloaded between 50–51 million different articles, which are now stored in the LibGen database. [12] [26] The site's operation is financed by user donations [4] paid in bitcoin . [27]
Though Sci-Hub does not limit itself to scientific papers, published research in the humanities and social sciences may not be as frequently downloaded (accounting for ten percent of Sci-Hub's top 100 downloads). [28]

Lawsuit
The site is involved in a legal case with Elsevier : Elsevier et al. v. Sci-Hub et al. [29] Court documents which discuss technical details of how the site functions are available to the public. [30] Elsevier claims that Sci-Hub illegally accesses accounts of students and academic institutions to provide free access to articles through their platform ScienceDirect . [16] The case is complicated by the fact that the site is hosted in St. Petersburg , Russia , making it difficult to target within the US legal system. [16] A similar case is also being run against the site Library Genesis (LibGen), [15] [16] which may be based in either the Netherlands [16] or also in Russia. [31] As of May 2017, Elsevier is demanding $15,000,000 in damages. [32]
Responding to the lawsuits Elbakyan said:
In her defense Alexandra Elbakyan has cited Article 27 (1.) of the UN Declaration of Human Rights " to share in scientific advancement and its benefits ", which she claims is hindered by publishers demanding payment despite putting in minimal effort in publishing the academic papers, which she says are essentially donated by researchers. [34] [35] The Electronic Frontier Foundation has expressed support for Sci-Hub and its sister site LibGen. [36] The lawsuit has generated criticism of Elsevier, and following the 2015 verdict, a group consisting of researchers, writers, and artists wrote an open letter in support of Sci-Hub and Library Genesis, calling the lawsuit a "big blow" to researchers around the world and stating that "it devalues us, authors, editors, and readers alike. It parasites on our labor, it thwarts our service to the public, it denies us access". [19] [37] Elsevier was criticized for simultaneously striking down on access while donating accounts to Wikipedians in order to increase the visibility of their content on the web. [38] [39]

Reception
Sci-Hub has been lauded as "transformative" and having "changed how we access knowledge", [40] [41] while a number of publishers have been very critical, so far as to claim that Sci-Hub is undermining more widely accepted open access initiatives, [42] and that it ignores how publishers work hard to make access for third-world nations easier. [42] It has also been criticized by library researchers for disincentivizing the use of interlibrary loans . [18]
However, claims have been made that even prominent western institutions such as Harvard and Cornell have had to cut down their access due to ever increasing subscription costs, [43] potentially causing some of the highest use of Sci-Hub to be in American cities with well-known universities (this may however be down to the convenience of the site rather than a lack of access). [4] Sci-Hub can be seen as one venue in a general trend in which research is becoming more accessible. [44] According to The Washington Post , many academics, university librarians and longtime advocates for open scholarly research believe Elbakyan is "giving academic publishers their Napster moment", referring to the illegal music-sharing service that "disrupted and permanently altered the industry". [45]
For her actions in creating Sci-Hub, Elbakyan has been called a hero and "spiritual successor to Aaron Swartz ". [9] [46] She has also been compared to Edward Snowden , because she is hiding in Russia after having "leaked" files in violation of American law. [46] She has also been called a modern-day " Robin Hood " and a "Robin Hood of science". [17] [47]
In August 2016, Gabriel J. Gardner, a researcher at California State University who has written papers on Sci-Hub and similar sites, was sent a letter from the Association of American Publishers . The letter asked Gardner to stop promoting the site after he had discussed the site at a session at the American Library Association . [48] In response the publishing institution was highly criticized for trying to silence legitimate research into the topic, and the letter has since been published in full, and responded to by the dean of library services at Cal State Long Beach who supported Gardner's work. [49] In December 2016, Nature Publishing Group named Alexandra Elbakyan as one of the ten people who most mattered in 2016. [50]

See also
WebPage index: 00190
Peter Sunde
Peter Sunde Kolmisoppi (born 13 September 1978), alias brokep , is a Swedish entrepreneur and politician. Sunde is of Norwegian and Finnish ancestry. [1] [2] He is best known for being a co-founder and ex-spokesperson of The Pirate Bay , a BitTorrent search engine. [3] He is an equality advocate and has expressed concerns over issues of centralization of power to the European Union in his blog. [4] Sunde also participates in the Pirate Party of Finland and describes himself as a socialist . [5] As of April 2017, Sunde has been working on a new venture called Njalla, a privacy oriented domain name registrar. [6]

Personal life
Before the founding of the Pirate Bay Sunde worked for a large German medical company. In 2003 he became a member of Sweden's Piratbyrån (The Pirate Bureau) and a few months later Sunde, Fredrik Neij and Gottfrid Svartholm started The Pirate Bay with Sunde as the spokesperson. [7] He remained The Pirate Bay's spokesperson until late 2009 (three years after the ownership of the site transferred to Reservella). In August 2011 Sunde and fellow Pirate Bay co-founder Fredrik Neij launched file-sharing site BayFiles , that aims to legally share. [8] Sunde is vegan [9] and speaks Swedish, Finnish, Norwegian, English and German.
Peter Sunde ran for European Parliament in 2014 election with the Pirate Party of Finland . [10]
On 31 May 2014, just days after the EU elections and exactly eight years after the police raided The Pirate Bay servers, Sunde was arrested at a farm in Oxie , Malmö to serve his prison sentence for the Pirate Bay case. [11] He was released five months later after having served two-thirds of his eight-month sentence. [12]

The Pirate Bay trial
On 31 January 2008, The Pirate Bay operators – Sunde, Fredrik Neij , Gottfrid Svartholm and Carl Lundström ( CEO of The Pirate Bay's former ISP ) – were charged with "assisting [others in] copyright infringement" . [13] The trial began on 16 February 2009. On 17 April 2009, Sunde and his co-defendants were found to be guilty of "assisting in making copyright content available" in the Stockholm District Court . Each defendant was sentenced to one year in prison and ordered to pay damages of 30 million SEK (approximately € 2,740,900 or US$ 3,620,000), to be apportioned among the four defendants. [14] After the verdict a press conference was held where Sunde held up a handwritten IOU statement claiming that is all the damages he will pay, adding "Even if I had any money I would rather burn everything I own and not even give them the ashes. They could have the job of picking them up. That's how much I hate the media industry." [15]
The defendants' lawyers appealed to the Svea Court of Appeal together with a request for a retrial in the district court claiming bias on the part of judge Tomas Norström. [16] The district court ruled there was no bias and denied the request for a retrial. On appeal, the jail sentences were reduced, but the damages increased. The supreme court of Sweden subsequently refused to hear any further appeal. The European Court of Human Rights also later rejected an appeal. [17]
Segments of an interview with Sunde talking about copyright , the Internet , and culture are featured in the 2007 documentary Steal This Film and 2013 documentary TPB AFK .

Flattr
Flattr is a micropayments system started by Sunde and Linus Olsson, which enables viewers of websites to make small donations to the developer by clicking a "Flattr this" button. At the time of the projects's announcement in February 2010, Sunde explained that "the money you pay each month will be spread evenly among the buttons you click in a month. We want to encourage people to share money as well as content." [18] Flattr itself takes a 10% administration fee. [18]
After Wikileaks 's initial publication of the U.S. Diplomatic Cables, companies including Visa, MasterCard, PayPal and Moneybookers blocked donations and money transfers to the site. Flattr, however, continued allowing donations to Wikileaks. Sunde commented "We [Flattr] think their work is exactly what is needed and if we can help just a little bit, we will." [19]
On 5 April 2017, Adblock Plus publisher Eyeo GmbH announced that it had acquired Flattr for an undisclosed amount. [20]

Hemlis
On 9 July 2013, Peter Sunde, together with Leif Högberg and Linus Olsson, announced a fundraising campaign for Hemlis. [21] Their goal was to launch a mass market messenger that was private, secure and beautiful. [22]
On 22 April 2015, the Hemlis team announced that they were discontinuing the development of the Hemlis messaging platform. [23]

Kopimashin
By 14 December 2015, Sunde released a video [24] on his Vimeo account of a device called " Kopimashin " - a machine made with a Raspberry Pi console that runs a Python routine to produce 100 copies per second of Gnarls Barkley 's single " Crazy ", storing the copies in /dev/null repository (where the data is discarded), surpassing the eight million copies per day.
The following day, Sunde published the full description of the device and project at Konsthack as the first art project of the site's portfolio. [25]
The machine has an LCD screen (as shown in the video) that calculates a running tally of the damages it has supposedly inflicted upon the record industry through its use, accordingly to what RIAA claims on their website. [26] If RIAA's claims are valid, it also means the record industry is supposed to bankrupt soon due to Kopimashin [27] and that's one of the points the project wants to prove with a physical example.
A few days later, Sunde told news site TorrentFreak that Kopimashin was created to ″ ...show the absurdity on the process of putting a value to a copy... ″ and that ″ ...putting a price to a copy is futile. ″, [28] also noting that it's merely an act against the record industry itself and recording labels, who have claimed millions of dollars in losses to the music industry to be caused by Sunde and The Pirate Bay over the past years.
WebPage index: 00191
Good Copy Bad Copy
Good Copy Bad Copy (subtitled "A documentary about the current state of copyright and culture") is a 2007 documentary film about copyright and culture in the context of Internet , peer-to-peer file sharing and other technological advances, directed by Andreas Johnsen, Ralf Christensen, and Henrik Moltke . It features interviews with many people with various perspectives on copyright , including copyright lawyers, producers, artists and filesharing service providers.
A central point of the documentary is the thesis that "creativity itself is on the line" and that a balance needs to be struck, or that there is a conflict between protecting the right of those who own intellectual property and the rights of future generations to create.

Content
Artists interviewed include Girl Talk and Danger Mouse , popular musicians of the mashup scene who cut and remix sounds from other songs into their own. The interviews with these artists reveal an emerging understanding of digital works and the obstacle to their authoring copyright presents.
The interviews featured in Good Copy Bad Copy acknowledge a recent shift towards user-generated content , mashup music and video culture. The documentary opens with explaining the current legal situation concerning sampling , licensing and copyright .
Good Copy Bad Copy documents the conflict between current copyright law and recent technological advances that enable the sampling of music, as well as the distribution of copyrighted material via peer-to-peer file sharing search engines such as The Pirate Bay . MPAA ( Motion Picture Association of America ) CEO Dan Glickman is interviewed in connection with a raid by the Swedish police against The Pirate Bay in May 2006. Glickman concedes that piracy will never be stopped, but states that they will try to make it as difficult and tedious as possible. Gottfrid Svartholm and Fredrik Neij from The Pirate Bay are also interviewed, with Neij stating that The Pirate Bay is illegal according to US law, but not Swedish law.
The interviews document attitudes towards art , culture and copyright in a number of countries, including the United States , Sweden , Russia , Nigeria , and Brazil .
The situation in Nigeria and Brazil is documented in terms of innovative business models that have developed in response to new technological possibilities and changing markets.
In Nigeria the documentary interviews individuals working within the Nigerian film industry, or Nollywood . Charles Igwe , a film producer in Lagos, is interviewed at length about his views on the Nigerian film industry, the nature of Nigerian films, and copyright in the context of digital video technology. Mayo Ayilaran, from the Copyright Society of Nigeria , explains the Nigerian government's approach to copyright enforcement.
In Brazil the Tecno brega industry and its unique approach to copyright and sampling is documented, featuring interviews with amongst others Ronaldo Lemos , Professor of Law FGV Brazil. Lemos explains that CDs or recorded music is treated merely as an advertisement for parties and concerts that generate revenue.
Good Copy Bad Copy also includes interview segments with copyright activist and academic Lawrence Lessig . [1] [2]

Credits

Distribution
Originally created for the Danish National Broadcasting Television network , the film was eventually released for free on the internet as a BitTorrent download. The filmmakers hope that releasing Good Copy Bad Copy for free will raise awareness and lead to other local broadcasting networks to show the documentary. [3]
The documentary first appeared on The Pirate Bay and then it was officially released under a Creative Commons Attribution-NonCommercial license on the Blip.tv video sharing site. [4]
On 8 May 2008, Good Copy Bad Copy was shown on SVT2 , Swedish Television. [5]
On 8 September 2009, Good Copy Bad Copy was shown on YLE FST5 , Finnish Television.
On 5 January 2010, Good Copy Bad Copy was shown on SBS , Australian Television.
The documentary and an unofficial trailer are available on YouTube , [6] [7] and the full film is on Google Video. [8]

See also
WebPage index: 00192
Conservation status
The conservation status of a group of organisms (for instance, a species ) indicates whether the group still exists and how likely the group is to become extinct in the near future. Many factors are taken into account when assessing conservation status: not simply the number of individuals remaining, but the overall increase or decrease in the population over time, breeding success rates, and known threats. Various systems of conservation status exist and are in use at international, multi-country, national and local levels as well as for consumer use.

International systems

IUCN Red List of Threatened Species
The IUCN Red List of Threatened Species is the best known worldwide conservation status listing and ranking system. Species are classified by the IUCN Red List into nine groups set through criteria such as rate of decline, population size, area of geographic distribution, and degree of population and distribution fragmentation. [1] [2]
Also included are species that have gone extinct since 500 AD. [ citation needed ] When discussing the IUCN Red List, the official term " threatened " is a grouping of three categories: critically endangered, endangered, and vulnerable.

The Convention on International Trade in Endangered Species of Wild Fauna and Flora
The Convention on International Trade in Endangered Species of Wild Fauna and Flora (CITES) aims to ensure that international trade in specimens of wild animals and plants does not threaten their survival. Many countries require CITES permits when importing plants and animals listed on CITES.

Multi-country systems
In the European Union (EU), the Birds and Habitats Directives are the legal instruments that evaluate the conservation status within the EU of species and habitats.
NatureServe conservation status focuses on Latin America , United States, Canada, and the Caribbean . It has been developed by scientists from NatureServe , The Nature Conservancy , and the network of natural heritage programs and data centers. It is increasingly integrated with the IUCN Red List system. Its categories for species include: presumed extinct (GX), possibly extinct (GH), critically imperiled (G1), imperiled (G2), vulnerable (G3), apparently secure (G4), and secure (G5). [3] The system also allows ambiguous or uncertain ranks including inexact numeric ranks (e.g. G2?), and range ranks (e.g. G2G3) for when the exact rank is uncertain. NatureServe adds a qualifier for captive or cultivated only (C), which has a similar meaning to the IUCN Red List extinct in the wild (EW) status.
The Red Data Book of the Russian Federation is used within the Russian Federation, and also accepted in parts of Africa.

National systems
In Australia, the Environment Protection and Biodiversity Conservation Act 1999 (EPBC Act) describes lists of threatened species, ecological communities and threatening processes. The categories resemble those of the 1994 IUCN Red List Categories & Criteria (version 2.3). Prior to the EPBC Act, a simpler classification system was used by the Endangered Species Protection Act 1992 . Some state and territory governments also have their own systems for conservation status. [ citation needed ]
In Belgium, the Flemish Research Institute for Nature and Forest publishes an online set of more than 150 nature indicators in Dutch. [4]
In Canada, the Committee on the Status of Endangered Wildlife in Canada (COSEWIC) is a group of experts that assesses and designates which wild species are in some danger of disappearing from Canada. [5] Under the Species at Risk Act (SARA), it is up to the federal government, which is politically accountable, to legally protect species assessed by COSEWIC.
In China, the State, provinces and some counties have determined their key protected wildlife species. There is the China red data book.
In Finland , a large number of species are protected under the Nature Conservation Act, and through the EU Habitats Directive and EU Birds Directive. [6]
In Germany, the Federal Agency for Nature Conservation publishes "red lists of endangered species".
India has the Wild Life Protection Act, 1972, Amended 2003 and the Biological Diversity Act, 2002 .
In Japan, the Ministry of Environment publishes a Threatened Wildlife of Japan Red Data Book. [7]
In the Netherlands , the Dutch Ministry of Agriculture, Nature and Food Quality publishes a list of threatened species, and conservation is enforced by the Nature Conservation Act 1998. Species are also protected through the Wild Birds and Habitats Directives.
In New Zealand, the Department of Conservation publishes the New Zealand Threat Classification System lists. Under this system threatened species or subspecies are assigned one of seven categories: Nationally Critical, Nationally Endangered, Nationally Vulnerable, Serious Decline, Gradual Decline, Sparse, or Range Restricted. While the classification looks only at a national level, many species are unique to New Zealand, and species which are secure overseas are noted as such.
In Russia, the Red Book of Russian Federation came out in 2001, it contains categories defining preservation status for different species. In it there are 8 taxa of amphibians, 21 taxa of reptiles, 128 taxa of birds, and 74 taxa of mammals, in total 231. There are also more than 30 regional red books, for example the red book of the Altaic region which came out in 1994.
In South Africa , The South African National Biodiversity Institute, established under the National Environmental Management: Biodiversity Act, 2004, [8] is responsible for drawing up lists of affected species, and monitoring compliance with CITES decisions. It is envisaged that previously diverse Red lists would be more easily kept current, both technically and financially.
In Thailand , the Wild Animal Reservation and Protection Act of BE 2535 defines fifteen reserved animal species and two classes of protected species, of which hunting, breeding, possession, and trade are prohibited or restricted by law. The National Park, Wildlife and Plant Conservation Department of the Ministry of Natural Resources and Environment is responsible for the regulation of these activities.
In Ukraine , the Ministry of Environment Protection maintains list of endangered species (divided into seven categories from "0" - extinct to "VI" - rehabilitated) and publishes it in the Red Book of Ukraine.
In the United States of America , the Endangered Species Act created the Endangered Species List .

Consumer guides
Some consumer guides for seafood , such as Seafood Watch , divide fish and other sea creatures into three categories, analogous to conservation status categories:
The categories do not simply reflect the imperilment of individual species, but also consider the environmental impacts of how and where they are fished, such as through bycatch or ocean bottom trawlers . Often groups of species are assessed rather than individual species (e.g. squid , prawns ).
The Marine Conservation Society has five levels of ratings for seafood species, as displayed on their FishOnline website. [10]

See also
WebPage index: 00193
Taxonomy (biology)
Taxonomy (from Ancient Greek : τάξις taxis , "arrangement", and -νομία -nomia , " method " [1] ) is the science of defining and naming groups of biological organisms on the basis of shared characteristics. Organisms are grouped together into taxa (singular: taxon) and these groups are given a taxonomic rank ; groups of a given rank can be aggregated to form a super group of higher rank, thus creating a taxonomic hierarchy. [2] [3] The Swedish botanist Carl Linnaeus is regarded as the father of taxonomy, as he developed a system known as Linnaean classification for categorization of organisms and binomial nomenclature for naming organisms.
With the advent of such fields of study as phylogenetics , cladistics , and systematics , the Linnaean system has progressed to a system of modern biological classification based on the evolutionary relationships between organisms, both living and extinct.

Definition
The exact definition of taxonomy varies from source to source, but the core of the discipline remains: the conception, naming, and classification of groups of organisms. [4] As points of reference, recent definitions of taxonomy are presented below:
The varied definitions either place taxonomy as a sub-area of systematics (definition 2), invert that relationship (definition 6), or appear to consider the two terms synonymous. There is some disagreement as to whether biological nomenclature is considered a part of taxonomy (definitions 1 and 2), or a part of systematics outside taxonomy. [9] For example, definition 6 is paired with the following definition of systematics that places nomenclature outside taxonomy: [7]
A whole set of terms including taxonomy, systematic biology , systematics , biosystematics , scientific classification, biological classification, and phylogenetics have at times had overlapping meanings – sometimes the same, sometimes slightly different, but always related and intersecting. [4] [10] The broadest meaning of "taxonomy" is used here. The term itself was introduced in 1813 by Candolle , in his Théorie élémentaire de la botanique . [11]

Alpha and beta taxonomy
The term " alpha taxonomy " is primarily used today to refer to the discipline of finding, describing, and naming taxa , particularly species. [12] In earlier literature, the term had a different meaning, referring to morphological taxonomy, and the products of research through the end of the 19th century. [13]
William Bertram Turrill introduced the term "alpha taxonomy" in a series of papers published in 1935 and 1937 in which he discussed the philosophy and possible future directions of the discipline of taxonomy. [14]
Turrill thus explicitly excludes from alpha taxonomy various areas of study that he includes within taxonomy as a whole, such as ecology, physiology, genetics, and cytology. He further excludes phylogenetic reconstruction from alpha taxonomy (pages 365–366).
Later authors have used the term in a different sense, to mean the delimitation of species (not subspecies or taxa of other ranks), using whatever investigative techniques are available, and including sophisticated computational or laboratory techniques. [15] [12] Thus, Ernst Mayr in 1968 defined beta taxonomy as the classification of ranks higher than species. [16]

Microtaxonomy and macrotaxonomy
How species should be defined in a particular group of organisms gives rise to practical and theoretical problems that are referred to as the species problem . The scientific work of deciding how to define species has been called microtaxonomy. [17] [18] [12] By extension, macrotaxonomy is the study of groups at higher taxonomic ranks, from subgenus and above only, than species. [12]

History
While some descriptions of taxonomic history attempt to date taxonomy to ancient civilizations, a truly scientific attempt to classify organisms did not occur until the 18th century. Earlier works were primarily descriptive, and focused on plants that were useful in agriculture or medicine. There are a number of stages in this scientific thinking. Early taxonomy was based on arbitrary criteria, the so-called "artificial systems", including Linnaeus's system of sexual classification. Later came systems based on a more complete consideration of the characteristics of taxa, referred to as "natural systems", such as those of de Jussieu (1789), de Candolle (1813) and Bentham and Hooker (1862–1863). These were pre- evolutionary in thinking. The publication of Charles Darwin 's On the Origin of Species (1859) led to new ways of thinking about classification based on evolutionary relationships. This was the concept of phyletic systems, from 1883 onwards. This approach was typified by those of Eichler (1883) and Engler (1886–1892). The advent of molecular genetics and statistical methodology allowed the creation of the modern era of "phylogenetic systems" based on cladistics , rather than morphology alone. [19] [20] [21]

Pre-Linnaean

Early taxonomists
Naming and classifying our surroundings has probably been taking place as long as mankind has been able to communicate. It would always have been important to know the names of poisonous and edible plants and animals in order to communicate this information to other members of the family or group. [ citation needed ]
Medicinal plant illustrations show up in Egyptian wall paintings from c. 1500 BC. [22] The paintings clearly show that these societies valued and communicated the uses of different species, and therefore had a basic taxonomy in place. [ citation needed ]

Srimadbhagavata Purana
In Canto 3, chapter 10 of Srimadbhagavata purana 6 types of trees are recognised and they are: [23]

Ancient times
Organisms were first classified by Aristotle ( Greece , 384–322 BC), during his stay on the Island of Lesbos . [24] [25] [26] He classified beings by their parts, or in modern terms attributes, such as having live birth, having four legs, laying eggs, having blood, or being warm-bodied. [27] He divided all living things into two groups: plants and animals. [25] Some of his groups of animals, such as Anhaima (animals without blood, translated as invertebrates ) and Enhaima (animals with blood, roughly the vertebrates ), as well as groups like the selachians and cetaceans , are still commonly used today. [28] His student Theophrastus (Greece, 370–285 BC) carried on this tradition, mentioning some 500 plants and their uses in his Historia Plantarum . Again, several plant groups currently still recognized can be traced back to Theophrastus, such as Cornus , Crocus , and Narcissus . [25]

Medieval
Taxonomy in the Middle Ages was largely based on the Aristotelian system , [27] with additions concerning the philosophical and existential order of creatures. This included concepts such as the Great chain of being in the Western scholastic tradition, [27] again deriving ultimately from Aristotle . Aristotelian system did not classify plants or fungi, due to the lack of microscope at the time, [26] as his ideas were based on arranging the complete world in a single continuum, as per the scala naturae (the Natural Ladder). [25] This, as well, was taken into consideration in the Great chain of being. [25] Advances were made by scholars such as Procopius , Timotheos of Gaza , Demetrios Pepagomenos , and Thomas Aquinas . Medieval thinkers used abstract philosophical and logical categorizations more suited to abstract philosophy than to pragmatic taxonomy. [25]

Renaissance and Early Modern
During the Renaissance , the Age of Reason , and the Enlightenment, categorizing organisms became more prevalent, [25] and taxonomic works became ambitious enough to replace the ancient texts. This is sometimes credited to the development of sophisticated optical lenses, which allowed the morphology of organisms to be studied in much greater detail. One of the earliest authors to take advantage of this leap in technology was the Italian physician Andrea Cesalpino (1519–1603), who has been called "the first taxonomist". [29] His magnum opus De Plantis came out in 1583, and described more than 1500 plant species. [30] [31] Two large plant families that he first recognized are still in use today: the Asteraceae and Brassicaceae . [32] Then in the 17th century John Ray ( England , 1627–1705) wrote many important taxonomic works. [26] Arguably his greatest accomplishment was Methodus Plantarum Nova (1682), [33] in which he published details of over 18,000 plant species. At the time, his classifications were perhaps the most complex yet produced by any taxonomist, as he based his taxa on many combined characters. The next major taxonomic works were produced by Joseph Pitton de Tournefort ( France , 1656–1708). [34] His work from 1700, Institutiones Rei Herbariae , included more than 9000 species in 698 genera, which directly influenced Linnaeus, as it was the text he used as a young student. [22]

The Linnaean era
The Swedish botanist Carl Linnaeus (1707–1778) [27] ushered in a new era of taxonomy. With his major works Systema Naturae 1st Edition in 1735, [35] Species Plantarum in 1753, [36] and Systema Naturae 10th Edition , [37] he revolutionized modern taxonomy. His works implemented a standardized binomial naming system for animal and plant species, [38] which proved to be an elegant solution to a chaotic and disorganized taxonomic literature. He not only introduced the standard of class, order, genus, and species, but also made it possible to identify plants and animals from his book, by using the smaller parts of the flower. [38] Thus the Linnaean system was born, and is still used in essentially the same way today as it was in the 18th century. [38] Currently, plant and animal taxonomists regard Linnaeus' work as the "starting point" for valid names (at 1753 and 1758 respectively). [39] Names published before these dates are referred to as "pre-Linnaean", and not considered valid (with the exception of spiders published in Svenska Spindlar [40] ). Even taxonomic names published by Linnaeus himself before these dates are considered pre-Linnaean. [22]

Modern system of classification
Whereas Linnaeus classified for ease of identification, the idea of the Linnaean taxonomy as translating into a sort of dendrogram of the Animal - and Plant Kingdoms was formulated toward the end of the 18th century, well before On the Origin of Species was published. [26] Among early works exploring the idea of a transmutation of species were Erasmus Darwin 's 1796 Zoönomia and Jean-Baptiste Lamarck 's Philosophie Zoologique of 1809. [12] The idea was popularised in the Anglophone world by the speculative but widely read Vestiges of the Natural History of Creation , published anonymously by Robert Chambers in 1844. [41]
With Darwin's theory, a general acceptance quickly appeared that a classification should reflect the Darwinian principle of common descent . [42] Tree of Life representations became popular in scientific works, with known fossil groups incorporated. One of the first modern groups tied to fossil ancestors was birds . [43] Using the then newly discovered fossils of Archaeopteryx and Hesperornis , Thomas Henry Huxley pronounced that they had evolved from dinosaurs, a group formally named by Richard Owen in 1842. [44] [45] The resulting description, that of dinosaurs "giving rise to" or being "the ancestors of" birds, is the essential hallmark of evolutionary taxonomic thinking. As more and more fossil groups were found and recognized in the late 19th and early 20th centuries, palaeontologists worked to understand the history of animals through the ages by linking together known groups. [46] With the modern evolutionary synthesis of the early 1940s, an essentially modern understanding of the evolution of the major groups was in place. As evolutionary taxonomy is based on Linnaean taxonomic ranks, the two terms are largely interchangeable in modern use. [ citation needed ]
The cladistic method (or cladism) has emerged since the 1960s. [42] In 1958, Julian Huxley used the term clade. [12] Later, in 1960, Cain and Harrison introduced the term cladistic. [12] The salient feature is arranging taxa in a hierarchical evolutionary tree , ignoring ranks. [42] A taxon is called monophyletic, if it includes all the descendants of an ancestral form. [47] [48] Groups that have descendant groups removed from them (e.g. dinosaurs , with birds as offspring group) are termed paraphyletic , [47] while groups representing more than one branch from the tree of life are called polyphyletic . [47] [48] The International Code of Phylogenetic Nomenclature or PhyloCode is intended to regulate the formal naming of clades . [49] [50] Linnaean ranks will be optional under the PhyloCode , which is intended to coexist with the current, rank-based codes. [50]

Kingdoms and domains
Well before Linnaeus, plants and animals were considered separate Kingdoms . [51] Linnaeus used this as the top rank, dividing the physical world into the plant, animal and mineral kingdoms. As advances in microscopy made classification of microorganisms possible, the number of kingdoms increased, five and six-kingdom systems being the most common.
Domains are a relatively new grouping. First proposed in 1977, Carl Woese 's three-domain system was not generally accepted until later. [52] One main characteristic of the three-domain method is the separation of Archaea and Bacteria , previously grouped into the single kingdom Bacteria (a kingdom also sometimes called Monera ), [51] with the Eukaryota for all organisms whose cells contain a nucleus . [53] A small number of scientists include a sixth kingdom, Archaea, but do not accept the domain method. [51]
Thomas Cavalier-Smith , who has published extensively on the classification of protists , has recently proposed that the Neomura , the clade that groups together the Archaea and Eucarya , would have evolved from Bacteria , more precisely from Actinobacteria . His 2004 classification treated the archaeobacteria as part of a subkingdom of the Kingdom Bacteria, i.e. he rejected the three-domain system entirely. [54] Stefan Luketa in 2012 proposed a five "dominion" system, adding Prionobiota (acellular and without nucleic acid) and Virusobiota (acellular but with nucleic acid) to the traditional three domains. [55]

Recent comprehensive classifications
Partial classifications exist for many individual groups of organisms and are revised and replaced as new information becomes available, however comprehensive treatments of most or all life are rarer; two recent examples are that of Adl et al., 2012, [62] which covers eukaryotes only with an emphasis on protists , and Ruggiero et al., 2015, [63] covering both eukaryotes and prokaryotes to the rank of Order, although both exclude fossil representatives. [ citation needed ]

Application
Biological taxonomy is a sub-discipline of biology , and is generally practiced by biologists known as "taxonomists", though enthusiastic naturalists are also frequently involved in the publication of new taxa. The work carried out by taxonomists is crucial for the understanding of biology in general. Two fields of applied biology in which taxonomic work is of fundamental importance are the studies of biodiversity and conservation . [64] Without a working classification of the organisms in any given area, estimating the amount of diversity present is unrealistic, making informed conservation decisions impossible. [ citation needed ]

Classifying organisms
Biological classification is a critical component of the taxonomic process. As a result, it informs the user as to what the relatives of the taxon are hypothesized to be. Biological classification uses taxonomic ranks, including among others (in order from most inclusive to least inclusive): Domain , Kingdom , Phylum , Class , Order , Family , Genus , and Species . [65] [Note 1]

Taxonomic descriptions
The "definition" of a taxon is encapsulated by its description or its diagnosis or by both combined. There are no set rules governing the definition of taxa, but the naming and publication of new taxa is governed by sets of rules. [9] In zoology , the nomenclature for the more commonly used ranks ( superfamily to subspecies ), is regulated by the International Code of Zoological Nomenclature ( ICZN Code ). [66] In the fields of botany , phycology , and mycology , the naming of taxa is governed by the International Code of Nomenclature for algae, fungi, and plants ( ICN ). [67]
The initial description of a taxon involves five main requirements: [68]
However, often much more information is included, like the geographic range of the taxon, ecological notes, chemistry, behavior, etc. How researchers arrive at their taxa varies: depending on the available data, and resources, methods vary from simple quantitative or qualitative comparisons of striking features, to elaborate computer analyses of large amounts of DNA sequence data. [69]

Author citation
An "authority" may be placed after a scientific name. [70] The authority is the name of the scientist or scientists who first validly published the name. [70] For example, in 1758 Linnaeus gave the Asian elephant the scientific name Elephas maximus , so the name is sometimes written as " Elephas maximus Linnaeus, 1758". [71] The names of authors are frequently abbreviated: the abbreviation L., for Linnaeus, is commonly used. In botany, there is, in fact, a regulated list of standard abbreviations (see list of botanists by author abbreviation ). [72] The system for assigning authorities differs slightly between botany and zoology . [9] However, it is standard that if a species' name or placement has been changed since the original description, the original authority's name is placed in parentheses. [73]

Phenetics
In phenetics, also known as taximetrics, or numerical taxonomy, organisms are classified based on overall similarity, regardless of their phylogeny or evolutionary relationships. [12] It results in a measure of evolutionary "distance" between taxa. Phenetic methods have become relatively rare in modern times, largely superseded by cladistic analyses, as phenetic methods do not distinguish plesiomorphic [ clarification needed ] from apomorphic traits. [74] However, certain phenetic methods, such as neighbor joining , have found their way into cladistics, as a reasonable approximation of phylogeny when more advanced methods (such as Bayesian inference ) are too computationally expensive. [75]

Databases
Modern taxonomy uses database technologies to search and catalogue classifications and their documentation. [76] While there is no commonly used database, there are comprehensive databases such as the Catalogue of Life , which attempts to list every documented species. [77] The catalogue listed 1.64 million species for all kingdoms as of April 2016, claiming coverage of more than three quarters of the estimated species known to modern science. [78]

See also

Notes
WebPage index: 00194
Mammal
Mammals are any vertebrates within the class Mammalia ( / m ə ˈ m eɪ l i . ə / from Latin mamma "breast"), a clade of endothermic amniotes distinguished from reptiles (including birds ) by the possession of a neocortex (a region of the brain), hair , three middle ear bones and mammary glands . Females of all mammal species nurse their young with milk , secreted from the mammary glands.
Mammals include the biggest animals on the planet, the great whales . The basic body type is a terrestrial quadruped , but some mammals are adapted for life at sea , in the air , in trees , underground or on two legs . The largest group of mammals, the placentals , have a placenta , which enables the feeding of the fetus during gestation. Mammals range in size from the 30–40 mm (1.2–1.6 in) bumblebee bat to the 30-meter (98 ft) blue whale . With the exception of the five species of monotreme (egg-laying mammals), all modern mammals give birth to live young. Most mammals, including the six most species-rich orders , belong to the placental group. The largest orders are the rodents , bats and Soricomorpha (shrews and allies). The next three biggest orders, depending on the biological classification scheme used, are the Primates ( apes and monkeys ), the Cetartiodactyla ( whales and even-toed ungulates ), and the Carnivora ( cats , dogs , seals , and allies).
Living mammals are divided into the Yinotheria ( platypus and echidnas ) and Theriiformes (all other mammals). There are around 5450 species of mammal, depending on which authority is cited. In some classifications, extant mammals are divided into two subclasses: the Prototheria , that is, the order Monotremata; and the Theria, or the infraclasses Metatheria and Eutheria . The marsupials constitute the crown group of the Metatheria, and include all living metatherians as well as many extinct ones; the placentals are the crown group of the Eutheria. While mammal classification at the family level has been relatively stable, several contending classifications regarding the higher levels—subclass, infraclass and order, especially of the marsupials—appear in contemporaneous literature. Much of the changes reflect the advances of cladistic analysis and molecular genetics . Findings from molecular genetics, for example, have prompted adopting new groups, such as the Afrotheria , and abandoning traditional groups, such as the Insectivora .
The mammals represent the only living Synapsida , which together with the Sauropsida form the Amniota clade. The early synapsid mammalian ancestors were sphenacodont pelycosaurs , a group that produced the non-mammalian Dimetrodon . At the end of the Carboniferous period, this group diverged from the sauropsid line that led to today's reptiles and birds. The line following the stem group Sphenacodontia split-off several diverse groups of non-mammalian synapsids—sometimes referred to as mammal-like reptiles—before giving rise to the proto-mammals ( Therapsida ) in the early Mesozoic era. The modern mammalian orders arose in the Paleogene and Neogene periods of the Cenozoic era, after the extinction of non-avian dinosaurs , and have been among the dominant terrestrial animal groups from 66 million years ago to the present.
Some mammals are intelligent , with some possessing large brains, self-awareness and tool use . Mammals can communicate and vocalize in several different ways, including the production of ultrasound , scent-marking , alarm signals , singing , and echolocation . Mammals can organize themselves into fission-fusion societies , harems , and hierarchies , but can also be solitary and territorial . Most mammals are polygynous , but some can be monogamous or polyandrous .
In human culture, domesticated mammals played a major role in the Neolithic revolution , causing farming to replace hunting and gathering , and leading to a major restructuring of human societies with the first civilizations . They provided, and continue to provide, power for transport and agriculture, as well as various commodities such as meat , dairy products , wool , and leather . Mammals are hunted or raced for sport, and are used as model organisms in science. Mammals have been depicted in art since Palaeolithic times, and appear in literature, film, mythology, and religion. Defaunation of mammals is primarily driven by anthropogenic factors, such as poaching and habitat destruction , though there are efforts to combat this.

Classification
Mammal classification has been through several iterations since Carl Linnaeus initially defined the class. No classification system is universally accepted; McKenna & Bell (1997) and Wilson & Reader (2005) provide useful recent compendiums. [1] George Gaylord Simpson 's "Principles of Classification and a Classification of Mammals" (AMNH Bulletin v. 85, 1945) provides systematics of mammal origins and relationships that were universally taught until the end of the 20th century. Since Simpson's classification, the paleontological record has been recalibrated, and the intervening years have seen much debate and progress concerning the theoretical underpinnings of systematization itself, partly through the new concept of cladistics . Though field work gradually made Simpson's classification outdated, it remains the closest thing to an official classification of mammals. [2]
Most mammals, including the six most species-rich orders , belong to the placental group. The three largest orders in numbers of species are Rodentia : mice , rats , porcupines , beavers , capybaras and other gnawing mammals; Chiroptera : bats; and Soricomorpha : shrews , moles and solenodons . The next three biggest orders, depending on the biological classification scheme used, are the Primates including the apes , monkeys and lemurs ; the Cetartiodactyla including whales and even-toed ungulates ; and the Carnivora which includes cats , dogs , weasels , bears , seals and allies. [3] According to Mammal Species of the World , 5,416 species were identified in 2006. These were grouped into 1,229 genera , 153 families and 29 orders. [3] In 2008, the International Union for Conservation of Nature (IUCN) completed a five-year Global Mammal Assessment for its IUCN Red List , which counted 5,488 species. [4]

Definitions 
The word " mammal " is modern, from the scientific name Mammalia coined by Carl Linnaeus in 1758, derived from the Latin mamma ("teat, pap"). In an influential 1988 paper, Timothy Rowe defined Mammalia phylogenetically as the crown group of mammals, the clade consisting of the most recent common ancestor of living monotremes ( echidnas and platypuses ) and therian mammals ( marsupials and placentals ) and all descendants of that ancestor. [5] Since this ancestor lived in the Jurassic period, Rowe's definition excludes all animals from the earlier Triassic , despite the fact that Triassic fossils in the Haramiyida have been referred to the Mammalia since the mid-19th century. [6] If Mammalia is considered as the crown group, its origin can be roughly dated as the first known appearance of animals more closely related to some extant mammals than to others. Ambondro is more closely related to monotremes than to therian mammals while Amphilestes and Amphitherium are more closely related to the therians; as fossils of all three genera are dated about 167 million years ago in the Middle Jurassic , this is a reasonable estimate for the appearance of the crown group. [7]
T. S. Kemp has provided a more traditional definition: " synapsids that possess a dentary – squamosal jaw articulation and occlusion between upper and lower molars with a transverse component to the movement" or, equivalently in Kemp's view, the clade originating with the last common ancestor of Sinoconodon and living mammals. [8] The earliest known synapsid satisfying Kemp's definitions is Tikitherium , dated 225 Ma , so the appearance of mammals in this broader sense can be given this Late Triassic date. [9] [10]

McKenna/Bell classification
In 1997, the mammals were comprehensively revised by Malcolm C. McKenna and Susan K. Bell, which has resulted in the McKenna/Bell classification. Their 1997 book, Classification of Mammals above the Species Level , [11] is a comprehensive work on the systematics, relationships and occurrences of all mammal taxa, living and extinct, down through the rank of genus, though molecular genetic data challenge several of the higher level groupings. The authors worked together as paleontologists at the American Museum of Natural History , New York . McKenna inherited the project from Simpson and, with Bell, constructed a completely updated hierarchical system, covering living and extinct taxa that reflects the historical genealogy of Mammalia. [2]
Extinct groups are represented by a dagger (†).
Class Mammalia

Molecular classification of placentals
Molecular studies based on DNA analysis have suggested new relationships among mammal families over the last few years. Most of these findings have been independently validated by retrotransposon presence/absence data . [13] Classification systems based on molecular studies reveal three major groups or lineages of placental mammals— Afrotheria , Xenarthra and Boreoeutheria —which diverged in the Cretaceous . The relationships between these three lineages is contentious, and all three possible different hypotheses have been proposed with respect to which group is basal . These hypotheses are Atlantogenata (basal Boreoeutheria), Epitheria (basal Xenarthra) and Exafroplacentalia (basal Afrotheria). [14] Boreoeutheria in turn contains two major lineages— Euarchontoglires and Laurasiatheria .
Estimates for the divergence times between these three placental groups range from 105 to 120 million years ago, depending on the type of DNA used (such as nuclear or mitochondrial ) [15] and varying interpretations of paleogeographic data. [14]
Cladogram based on Tarver et al . (2016) [16]
Group I: Superorder Afrotheria [17]
Group II: Superorder Xenarthra [17]
Group III: Magnaorder Boreoeutheria [17]

Evolution

Origins
Synapsida , a clade that contains mammals and their extinct relatives, originated during the Pennsylvanian subperiod , when they split from reptilian and avian lineages. Crown group mammals evolved from earlier mammaliaforms during the Early Jurassic . The cladogram takes Mammalia to be the crown group. [18]

Evolution from amniotes
The first fully terrestrial vertebrates were amniotes . Like their amphibious tetrapod predecessors, they had lungs and limbs. Amniotic eggs, however, have internal membranes that allow the developing embryo to breathe but keep water in. Hence, amniotes can lay eggs on dry land, while amphibians generally need to lay their eggs in water.
The first amniotes apparently arose in the Pennsylvanian subperiod of the Carboniferous . They descended from earlier reptiliomorph amphibious tetrapods, [19] which lived on land that was already inhabited by insects and other invertebrates as well as ferns , mosses and other plants. Within a few million years, two important amniote lineages became distinct: the synapsids , which would later include the common ancestor of the mammals; and the sauropsids , which now include turtles , lizards , snakes , crocodilians , dinosaurs and birds . [20] Synapsids have a single hole ( temporal fenestra ) low on each side of the skull. One synapsid group, the pelycosaurs , included the largest and fiercest animals of the early Permian . [21] Nonmammalian synapsids are sometimes called "mammal-like reptiles". [22] [23]
Therapsids , a group of synapsids, descended from pelycosaurs in the Middle Permian, about 265 million years ago, and became the dominant land vertebrates. [22] They differ from basal eupelycosaurs in several features of the skull and jaws, including: larger skulls and incisors which are equal in size in therapsids, but not for eupelycosaurs. [22] The therapsid lineage leading to mammals went through a series of stages, beginning with animals that were very similar to their pelycosaur ancestors and ending with probainognathian cynodonts , some of which could easily be mistaken for mammals. Those stages were characterized by: [24]

First mammals
The Permian–Triassic extinction event about 252 million years ago, which was a prolonged event due to the accumulation of several extinction pulses, ended the dominance of carnivorous therapsids. [25] In the early Triassic, most medium to large land carnivore niches were taken over by archosaurs [26] which, over an extended period (35 million years), came to include the crocodylomorphs , [27] the pterosaurs and the dinosaurs; [28] however, large cynodonts like Trucidocynodon and traversodontids still occupied large sized carnivorous and herbivorous niches respectively. By the Jurassic, the dinosaurs had come to dominate the large terrestrial herbivore niches as well. [29]
The first mammals (in Kemp's sense) appeared in the Late Triassic epoch (about 225 million years ago), 40 million years after the first therapsids. They expanded out of their nocturnal insectivore niche from the mid-Jurassic onwards; [30] The Jurassic Castorocauda , for example, had adaptations for swimming, digging and catching fish. [31] Most, if not all, are thought to have remained nocturnal (the Nocturnal bottleneck ), accounting for much of the typical mammalian traits. [32] The majority of the mammal species that existed in the Mesozoic Era were multituberculates, eutriconodonts and spalacotheriids . [33] The earliest known metatherian is Sinodelphys , found in 125 million-year-old Early Cretaceous shale in China's northeastern Liaoning Province . The fossil is nearly complete and includes tufts of fur and imprints of soft tissues. [34]
The oldest known fossil among the Eutheria ("true beasts") is the small shrewlike Juramaia sinensis , or "Jurassic mother from China", dated to 160 million years ago in the late Jurassic. [35] A later eutherian, Eomaia , dated to 125 million years ago in the early Cretaceous, possessed some features in common with the marsupials but not with the placentals, evidence that these features were present in the last common ancestor of the two groups but were later lost in the placental lineage. [36] In particular, the epipubic bones extend forwards from the pelvis. These are not found in any modern placental, but they are found in marsupials, monotremes, nontherian mammals and Ukhaatherium , an early Cretaceous animal in the eutherian order Asioryctitheria . This also applies to the multituberculates. [37] They are apparently an ancestral feature, which subsequently disappeared in the placental lineage. These epipubic bones seem to function by stiffening the muscles during locomotion, reducing the amount of space being presented, which placentals require to contain their fetus during gestation periods. A narrow pelvic outlet indicates that the young were very small at birth and therefore pregnancy was short, as in modern marsupials. This suggests that the placenta was a later development. [38]
The earliest known monotreme was Teinolophos , which lived about 120 million years ago in Australia. [39] Monotremes have some features which may be inherited from the original amniotes such as the same orifice to urinate, defecate and reproduce ( cloaca ) – as lizards and birds also do – [40] and they lay eggs which are leathery and uncalcified. [41]

Earliest appearances of features
Hadrocodium , whose fossils date from approximately 195 million years ago, in the early Jurassic, provides the first clear evidence of a jaw joint formed solely by the squamosal and dentary bones; there is no space in the jaw for the articular, a bone involved in the jaws of all early synapsids. [42]
The earliest clear evidence of hair or fur is in fossils of Castorocauda and Megaconus , from 164 million years ago in the mid-Jurassic. In the 1950s, it was suggested that the foramina (passages) in the maxillae and premaxillae (bones in the front of the upper jaw) of cynodonts were channels which supplied blood vessels and nerves to vibrissae ( whiskers ) and so were evidence of hair or fur; [43] [44] it was soon pointed out, however, that foramina do not necessarily show that an animal had vibrissae, as the modern lizard Tupinambis has foramina that are almost identical to those found in the nonmammalian cynodont Thrinaxodon . [23] [45] Popular sources, nevertheless, continue to attribute whiskers to Thrinaxodon . [46] Studies on Permian coprolites suggest that non-mammalian synapsids of the epoch already had fur, setting the evolution of hairs possibly as far back as dicynodonts . [47]
When endothermy first appeared in the evolution of mammals is uncertain, though it is generally agreed to have first evolved in non-mammalian therapsids . [47] [48] Modern monotremes have lower body temperatures and more variable metabolic rates than marsupials and placentals, [49] but there is evidence that some of their ancestors, perhaps including ancestors of the therians, may have had body temperatures like those of modern therians. [50] Likewise, some modern therians like afrotheres and xenarthrans have secondarily developed lower body temperatures. [51]
The evolution of erect limbs in mammals is incomplete — living and fossil monotremes have sprawling limbs. The parasagittal (nonsprawling) limb posture appeared sometime in the late Jurassic or early Cretaceous; it is found in the eutherian Eomaia and the metatherian Sinodelphys , both dated to 125 million years ago. [52] Epipubic bones, a feature that strongly influenced the reproduction of most mammal clades, are first found in Tritylodontidae , suggesting that it is a synapomorphy between them and mammaliformes . They are omnipresent in non-placental mammaliformes, though Megazostrodon and Erythrotherium appear to have lacked them. [53]
It has been suggested that the original function of lactation ( milk production) was to keep eggs moist. Much of the argument is based on monotremes, the egg-laying mammals. [54] [55]

Rise of the mammals
Therian mammals took over the medium- to large-sized ecological niches in the Cenozoic , after the Cretaceous–Paleogene extinction event approximately 66 million years ago emptied ecological space once filled by non-avian dinosaurs and other groups of reptiles, as well as various other mammal groups, [56] and underwent an exponential increase in body size ( megafauna ). [57] Then mammals diversified very quickly; both birds and mammals show an exponential rise in diversity. [56] For example, the earliest known bat dates from about 50 million years ago, only 16 million years after the extinction of the dinosaurs. [58]
Molecular phylogenetic studies initially suggested that most placental orders diverged about 100 to 85 million years ago and that modern families appeared in the period from the late Eocene through the Miocene . [59] However, no placental fossils have been found from before the end of the Cretaceous. [60] The earliest undisputed fossils of placentals comes from the early Paleocene , after the extinction of the dinosaurs. [60] In particular, scientists have identified an early Paleocene animal named Protungulatum donnae as one of the first placental mammals. [61] however it has been reclassified as a non-placental eutherian. [62] Recalibrations of genetic and morphological diversity rates have suggested a Late Cretaceous origin for placentals, and a Paleocene origin for most modern clades. [63]
The earliest known ancestor of primates is Archicebus achilles [64] from around 55 million years ago. [64] This tiny primate weighed 20–30 grams (0.7–1.1 ounce) and could fit within a human palm. [64]

Anatomy and morphology

Distinguishing features
Living mammal species can be identified by the presence of sweat glands , including those that are specialized to produce milk to nourish their young. [65] In classifying fossils, however, other features must be used, since soft tissue glands and many other features are not visible in fossils. [66]
Many traits shared by all living mammals appeared among the earliest members of the group:
For the most part, these characteristics were not present in the Triassic ancestors of the mammals. [71] Nearly all mammaliaforms possess an epipubic bone, the exception being modern placentals. [72]

Biological systems
The majority of mammals have seven cervical vertebrae (bones in the neck), including bats , giraffes , whales and humans . The exceptions are the manatee and the two-toed sloth , which have just six, and the three-toed sloth which has nine cervical vertebrae. [73] All mammalian brains possess a neocortex , a brain region unique to mammals. [74] Placental mammals have a corpus callosum , unlike monotremes and marsupials. [75]
The lungs of mammals are spongy and honeycombed. Breathing is mainly achieved with the diaphragm , which divides the thorax from the abdominal cavity, forming a dome convex to the thorax. Contraction of the diaphragm flattens the dome, increasing the volume of the lung cavity. Air enters through the oral and nasal cavities, and travels through the larynx, trachea and bronchi , and expands the alveoli . Relaxing the diaphragm has the opposite effect, decreasing the volume of the lung cavity, causing air to be pushed out of the lungs. During exercise, the abdominal wall contracts , increasing pressure on the diaphragm, which forces air out quicker and more forcefully. The rib cage is able to expand and contract the chest cavity through the action of other respiratory muscles. Consequently, air is sucked into or expelled out of the lungs, always moving down its pressure gradient. [76] [77] This type of lung is known as a bellows lung due to its resemblance to blacksmith bellows . [77]
The mammalian heart has four chambers, two upper atria , the receiving chambers, and two lower ventricles , the discharging chambers. [78] The heart has four valves, which separate its chambers and ensures blood flows in the correct direction through the heart (preventing backflow). After gas exchange in the pulmonary capillaries (blood vessels in the lungs), oxygen-rich blood returns to the left atrium via one of the four pulmonary veins . Blood flows nearly continuously back into the atrium, which acts as the receiving chamber, and from here through an opening into the left ventricle. Most blood flows passively into the heart while both the atria and ventricles are relaxed, but toward the end of the ventricular relaxation period , the left atrium will contract, pumping blood into the ventricle. The heart also requires nutrients and oxygen found in blood like other muscles, and is supplied via coronary arteries . [79]
The integumentary system is made up of three layers: the outermost epidermis , the dermis and the hypodermis . The epidermis is typically 10 to 30 cells thick; its main function is to provide a waterproof layer. Its outermost cells are constantly lost; its bottommost cells are constantly dividing and pushing upward. The middle layer, the dermis, is 15 to 40 times thicker than the epidermis. The dermis is made up of many components, such as bony structures and blood vessels. The hypodermis is made up of adipose tissue , which stores lipids and provides cushioning and insulation. The thickness of this layer varies widely from species to species; [80] :97 marine mammals require a thick hypodermis ( blubber ) for insulation, and right whales have the thickest blubber at 20 inches (51 cm). [81] Although other animals have features such as whiskers, feathers , setae , or cilia that superficially resemble it, no animals other than mammals have hair . It is a definitive characteristic of the class. Though some mammals have very little, careful examination reveals the characteristic, often in obscure parts of their bodies. [80] :61
Herbivores have developed a diverse range of physical structures to facilitate the consumption of plant material . To break up intact plant tissues, mammals have developed teeth structures that reflect their feeding preferences. For instance, frugivores (animals that feed primarily on fruit) and herbivores that feed on soft foliage have low-crowned teeth specialized for grinding foliage and seeds . Grazing animals that tend to eat hard, silica -rich grasses, have high-crowned teeth, which are capable of grinding tough plant tissues and do not wear down as quickly as low-crowned teeth. [82] Most carnivorous mammals have carnassialiforme teeth (of varying length depending on diet), long canines and similar tooth replacement patterns. [83]
The stomach of Artiodactyls is divided into four sections: the rumen , the reticulum , the omasum and the abomasum (only ruminants have a rumen). After the plant material is consumed, it is mixed with saliva in the rumen and reticulum and separates into solid and liquid material. The solids lump together to form a bolus (or cud ), and is regurgitated. When the bolus enters the mouth, the fluid is squeezed out with the tongue and swallowed again. Ingested food passes to the rumen and reticulum where cellulytic microbes ( bacteria , protozoa and fungi ) produce cellulase , which is needed to break down the cellulose in plants. [84] Perissodactyls , in contrast to the ruminants, store digested food that has left the stomach in an enlarged cecum , where it is fermented by bacteria. [85] Carnivora have a simple stomach adapted to digest primarily meat, as compared to the elaborate digestive systems of herbivorous animals, which are necessary to break down tough, complex plant fibers. The caecum is either absent or short and simple, and the large intestine is not sacculated or much wider than the small intestine. [86]
The mammalian excretory system involves many components. Like most other land animals, mammals are ureotelic , and convert ammonia into urea , which is done by the liver as part of the urea cycle . [87] Bilirubin , a waste product derived from blood cells , is passed through bile and urine with the help of enzymes excreted by the liver. [88] The passing of bilirubin via bile through the intestinal tract gives mammalian feces a distinctive brown coloration. [89] Distinctive features of the mammalian kidney include the presence of the renal pelvis and renal pyramids , and of a clearly distinguishable cortex and medulla , which is due to the presence of elongated loops of Henle . Only the mammalian kidney has a bean shape, although there are some exceptions, such as the multilobed reniculate kidneys of pinnipeds, cetaceans and bears. [90] [91] Most adult placental mammals have no remaining trace of the cloaca . In the embryo, the embryonic cloaca divides into a posterior region that becomes part of the anus, and an anterior region that has different fates depending on the sex of the individual: in females, it develops into the vestibule that receives the urethra and vagina , while in males it forms the entirety of the penile urethra . [91] However, the tenrecs , golden moles , and some shrews retain a cloaca as adults. [92] In marsupials, the genital tract is separate from the anus, but a trace of the original cloaca does remain externally. [91] Monotremes, which translates from Greek into "single hole", have a true cloaca. [93]

Sound production
As in all other tetrapods, mammals have a larynx that can quickly open and close to produce sounds, and a supralaryngeal vocal tract which filters this sound. The lungs and surrounding musculature provide the air and pressure required phonate . The larynx controls the pitch and volume of sound, but the strength the lungs exert to exhale also contributes to volume. More primitive mammals, such as the echidna, can only hiss, as sound is achieved solely through exhaling through a partially close larynx. Other mammals phonate using vocal folds , as opposed to the vocal cords seen in birds and reptiles. The movement or tenseness of the vocal folds can result in many sounds such as purring and screaming . Mammals can change the position of the larynx, allowing them to breathe through the nose while swallowing through the mouth, and to create both oral and nasal sounds; nasal sounds, such as a dog whine, are generally soft sounds, and oral sounds, such as a dog bark, are generally loud. [94]
Some mammals have a large larynx and, thus, a low-pitched voice, namely the hammer-headed bat ( Hypsignathus monstrosus ) where the larynx can take up the entirety of the thoracic cavity while pushing the lungs, heart, and trachea into the abdomen . [95] Large vocal pads can also lower the pitch, as in the low-pitched roars of big cats . [96] The production of infrasound is possible in some mammals such as the African elephant ( Loxodonta spp.) and baleen whales . [97] [98] Small mammals with small larynxes have the ability to produced ultrasound , which can be detected by modifications to the middle ear and cochlea . Ultrasound is inaudible to birds and reptiles, which might have been important during the Mesozoic, when birds and reptiles were the dominant predators. This private channel is used by some rodents in, for example, mother-to-pup communication, and by bats when echolocating. Toothed whales also use echolocation, but, as opposed to the vocal membrane that extends upward from the vocal folds, they have a melon to manipulate sounds. Some mammals, namely the primates, have air sacs attached to the larynx, which may function to increase the volume of sound. [94]
The vocal production system is controlled by the cranial nerve nucleus in the brain, and supplied by the recurrent laryngeal nerve and the superior laryngeal nerve , branches of the vagus nerve . The vocal tract is supplied by the hypoglossal nerve and facial nerves . Electrical stimulation of the periaqueductal gray (PEG) region of the mammalian midbrain elicit vocalizations. The ability to learn new vocalizations is only exemplified in humans, seals, cetaceans, and possibly bats; in humans, this is the result of a direct connection between the motor cortex , which controls movement, and the motor neurons in the spinal cord. [94]

Fur
The fur of mammals has many uses protection, sensory purposes, waterproofing, and camouflage, with the primary usage being thermoregulation. [99] The types of hair include definitive, which may be shed after reaching a certain length; vibrissae, which are sensory hairs and are most commonly whiskers; pelage , which consists of guard hairs, under-fur, and awn hair ; spines , which are a type of stiff guard hair used for defense in, for example, porcupines ; bristles, which are long hairs usually used in visual signals, such as the mane of a lion; velli, often called "down fur," which insulates newborn mammals; and wool which is a long, soft and often curly. [80] :99 Hair length is negligible in thermoregulation, as some tropical mammals, such as sloths, have the same length of fur length as some arctic mammals but with less insulation; and, conversely, other tropical mammals with short hair have the same insulating value as arctic mammals. The denseness of fur can increase an animal's insulation value, and arctic mammals especially have dense fur; for example, the musk ox has guard hairs measuring 12 inches (30 cm) as well as a dense underfur, which forms an airtight coat, allowing them to survive in temperatures of −40 °F (−40 °C). [80] :162–163 Some desert mammals, such as camels, use dense fur to prevent solar heat from reaching their skin, allowing the animal to stay cool; a camel's fur may reach 158 °F (70 °C) in the summer, but the skin stays at 104 °F (40 °C). [80] :188 Aquatic mammals , conversely, trap air in their fur to conserve heat by keeping the skin dry. [80] :162–163
Mammalian coats are colored for a variety of reasons, the major selective pressures including camouflage , sexual selection , communication and physiological processes such as temperature regulation. Camouflage is a powerful influence in a large number of mammals, as it helps to conceal individuals from predators or prey. [100] Aposematism , warning off possible predators, is the most likely explanation of the black-and-white pelage of many mammals which are able to defend themselves, such as in the foul-smelling skunk and the powerful and aggressive honey badger . [101] In arctic and subarctic mammals such as the arctic fox ( Alopex lagopus ), collared lemming ( Dicrostonyx groenlandicus ), stoat ( Mustela erminea ), and snowshoe hare ( Lepus americanus ), seasonal color change between brown in summer and white in winter is driven largely by camouflage. [102] Differences in female and male coat color may indicate nutrition and hormone levels, important in mate selection. [103] Some arboreal mammals, notably primates and marsupials, have shades of violet, green, or blue skin on parts of their bodies, indicating some distinct advantage in their largely arboreal habitat due to convergent evolution . [104] The green coloration of sloths, however, is the result of a symbiotic relationship with algae . [105] Coat color is sometimes sexually dimorphic , as in many primate species . [106] Coat color may influence the ability to retain heat, depending on how much light is reflected. Mammals with a darker colored coat can absorb more heat from solar radiation, and stay warmer, and some smaller mammals, such as voles , have darker fur in the winter. The white, pigmentless fur of arctic mammals, such as the polar bear, may reflect more solar radiation directly onto the skin. [80] :166–167 [99]

Reproductive system
Most mammals are viviparous , giving birth to live young. However, the five species of monotreme, the platypus and the four species of echidna, lay eggs. The monotremes have a sex determination system different from that of most other mammals. [107] In particular, the sex chromosomes of a platypus are more like those of a chicken than those of a therian mammal. [108]
Viviparous mammals are in the subclass Theria; those living today are in the marsupial and placental infraclasses. Marsupials have a short gestation period, typically shorter than its estrous cycle and gives birth to an undeveloped newborn that then undergoes further development; in many species, this takes place within a pouch-like sac, the marsupium , located in the front of the mother's abdomen . This is the plesiomorphic condition among viviparous mammals; the presence of epipubic bones in all non-placental mammals prevents the expansion of the torso needed for full pregnancy. [72] Even non-placental eutherians probably reproduced this way. [109] The placentals give birth to relatively complete and developed young, usually after long gestation periods. [110] They get their name from the placenta , which connects the developing fetus to the uterine wall to allow nutrient uptake. [111]
The mammary glands of mammals are specialized to produce milk, the primary source of nutrition for newborns. The monotremes branched early from other mammals and do not have the nipples seen in most mammals, but they do have mammary glands. The young lick the milk from a mammary patch on the mother's belly. [112]

Endothermy
Nearly all mammals are endothermic ("warm-blooded"). Most mammals also have hair to help keep them warm. Like birds, mammals can forage or hunt in weather and climates too cold for ectothermic ("cold-blooded") reptiles and insects. Endothermy requires plenty of food energy, so mammals eat more food per unit of body weight than most reptiles. [113] Small insectivorous mammals eat prodigious amounts for their size. A rare exception, the naked mole-rat produces little metabolic heat, so it is considered an operational poikilotherm . [114] Birds are also endothermic, so endothermy is not unique to mammals. [115]

Locomotion

Terrestrial
Most vertebrates—the amphibians, the reptiles and some mammals such as humans and bears—are plantigrade , walking on the whole of the underside of the foot. Many mammals, such as cats and dogs, are digitigrade , walking on their toes, the greater stride length allowing more speed. Digitigrade mammals are also often adept at quiet movement. [116] Some animals such as horses are unguligrade , walking on the tips of their toes. This even further increases their stride length and thus their speed. [117] A few mammals, namely the great apes, are also known to walk on their knuckles , at least for their front legs. Giant anteaters [118] and platypuses [119] are also knuckle-walkers. Some mammals are bipeds , using only two limbs for locomotion, which can be seen in, for example, humans and the great apes. Bipedal species have a larger field of vision than quadrupeds, conserve more energy and have the ability to manipulate objects with their hands, which aids in foraging. Instead of walking, some bipeds hop, such as kangaroos and kangaroo rats . [120] [121]
Animals will use different gaits for different speeds, terrain and situations. For example, horses show four natural gaits, the slowest horse gait is the walk , then there are three faster gaits which, from slowest to fastest, are the trot , the canter and the gallop . Animals may also have unusual gaits that are used occasionally, such as for moving sideways or backwards. For example, the main human gaits are bipedal walking and running , but they employ many other gaits occasionally, including a four-legged crawl in tight spaces. [122] Mammals show a vast range of gaits , the order that they place and lift their appendages in locomotion. Gaits can be grouped into categories according to their patterns of support sequence. For quadrupeds, there are three main categories: walking gaits, running gaits and leaping gaits . [123] Walking is the most common gait, where some feet are on the ground at any given time, and found in almost all legged animals. Running is considered to occur when at some points in the stride all feet are off the ground in a moment of suspension. [122]

Arboreal
Arboreal animals frequently have elongated limbs that help them cross gaps, reach fruit or other resources, test the firmness of support ahead and, in some cases, to brachiate (swing between trees). [124] Many arboreal species, such as tree porcupines, silky anteaters , spider monkeys and possums , use prehensile tails to grasp branches. In the spider monkey, the tip of the tail has either a bare patch or adhesive pad, which provides increased friction. Claws can be used to interact with rough substrates and re-orient the direction of forces the animal applies. This is what allows squirrels to climb tree trunks that are so large to be essentially flat from the perspective of such a small animal. However, claws can interfere with an animal's ability to grasp very small branches, as they may wrap too far around and prick the animal's own paw. Frictional gripping is used by primates, relying upon hairless fingertips. Squeezing the branch between the fingertips generates frictional force that holds the animal's hand to the branch. However, this type of grip depends upon the angle of the frictional force, thus upon the diameter of the branch, with larger branches resulting in reduced gripping ability. To control descent, especially down large diameter branches, some arboreal animals such as squirrels have evolved highly mobile ankle joints that permit rotating the foot into a 'reversed' posture. This allows the claws to hook into the rough surface of the bark, opposing the force of gravity. Small size provides many advantages to arboreal species: such as increasing the relative size of branches to the animal, lower center of mass, increased stability, lower mass (allowing movement on smaller branches) and the ability to move through more cluttered habitat. [124] Size relating to weight affects gliding animals such as the sugar glider . [125] Some species of primate, bat and all species of sloth achieve passive stability by hanging beneath the branch. Both pitching and tipping become irrelevant, as the only method of failure would be losing their grip. [124]

Aerial
Bats are the only mammals that can truly fly. They fly through the air at a constant speed by moving their wings up and down (usually with some fore-aft movement as well). Because the animal is in motion, there is some airflow relative to its body which, combined with the velocity of the wings, generates a faster airflow moving over the wing. This generates a lift force vector pointing forwards and upwards, and a drag force vector pointing rearwards and upwards. The upwards components of these counteract gravity, keeping the body in the air, while the forward component provides thrust to counteract both the drag from the wing and from the body as a whole. [126]
The wings of bats are much thinner and consist of more bones than that of birds, allowing bats to maneuver more accurately and fly with more lift and less drag. [127] [128] By folding the wings inwards towards their body on the upstroke, they use 35% less energy during flight than birds. [129] The membranes are delicate, ripping easily; however, the tissue of the bat's membrane is able to regrow, such that small tears can heal quickly. [130] The surface of their wings is equipped with touch-sensitive receptors on small bumps called Merkel cells , also found on human fingertips. These sensitive areas are different in bats, as each bump has a tiny hair in the center, making it even more sensitive and allowing the bat to detect and collect information about the air flowing over its wings, and to fly more efficiently by changing the shape of its wings in response. [131]

Fossorial
Fossorial creatures live in subterranean environments. Many fossorial mammals were classified under the, now obsolete, order Insectivora , such as shrews, hedgehogs and moles. Fossorial mammals have a fusiform body, thickest at the shoulders and tapering off at the tail and nose. Unable to see in the dark burrows, most have degenerated eyes, but degeneration varies between species; pocket gophers , for example, are only semi-fossorial and have very small yet functional eyes, in the fully fossorial marsupial mole the eyes are degenerated and useless, talpa moles have vestigial eyes and the cape golden mole has a layer of skin covering the eyes. External ears flaps are also very small or absent. Truly fossorial mammals have short, stout legs as strength is more important than speed to a burrowing mammal, but semi-fossorial mammals have cursorial legs. The front paws are broad and have strong claws to help in loosening dirt while excavating burrows, and the back paws have webbing, as well as claws, which aids in throwing loosened dirt backwards. Most have large incisors to prevent dirt from flying into their mouth. [132]

Aquatic
Fully aquatic mammals, the cetaceans and sirenians , have lost their legs and have a tail fin to propel themselves through the water. Flipper movement is continuous. Whales swim by moving their tail fin and lower body up and down, propelling themselves through vertical movement, while their flippers are mainly used for steering. Their skeletal anatomy allows them to be fast swimmers. Most species have a dorsal fin to prevent themselves from turning upside-down in the water. [133] [134] The flukes of sirenians are raised up and down in long strokes to move the animal forward, and can be twisted to turn. The forelimbs are paddle-like flippers which aid in turning and slowing. [135]
Semi-aquatic mammals, like pinnipeds, have two pairs of flippers on the front and back, the fore-flippers and hind-flippers. The elbows and ankles are enclosed within the body. [136] [137] Pinnipeds have several adaptions for reducing drag . In addition to their streamlined bodies, they have smooth networks of muscle bundles in their skin that may increase laminar flow and make it easier for them to slip through water. They also lack arrector pili , so their fur can be streamlined as they swim. [138] They rely on their fore-flippers for locomotion in a wing-like manner similar to penguins and sea turtles . [139] Fore-flipper movement is not continuous, and the animal glides between each stroke. [137] Compared to terrestrial carnivorans, the fore-limbs are reduced in length, which gives the locomotor muscles at the shoulder and elbow joints greater mechanical advantage; [136] the hind-flippers serve as stabilizers. [138] Other semi-aquatic mammals include beavers, hippopotamuses , otters and platypuses. [140] Hippos are very large semi-aquatic mammals, and their barrel-shaped bodies have graviportal skeletal structures, [141] adapted to carrying their enormous weight, and their specific gravity allows them to sink and move along the bottom of a river. [142]

Behavior

Communication and vocalization
Many mammals communicate by vocalizing. Vocal communication serves many purposes, including in mating rituals, as warning calls , [144] to indicate food sources, and for social purposes. Males often call during mating rituals to ward off other males and to attract females, as in the roaring of lions and red deer . [145] The songs of the humpback whale may be signals to females; [146] they have different dialects in different regions of the ocean. [147] Social vocalizations include the territorial calls of gibbons , and the use of frequency in greater spear-nosed bats to distinguish between groups. [148] The vervet monkey gives a distinct alarm call for each of at least four different predators, and the reactions of other monkeys vary according to the call. For example, if an alarm call signals a python, the monkeys climb into the trees, whereas the eagle alarm causes monkeys to seek a hiding place on the ground. [143] Prairie dogs similarly have complex calls that signal the type, size, and speed of an approaching predator. [149] Elephants communicate socially with a variety of sounds including snorting, screaming, trumpeting, roaring and rumbling. Some of the rumbling calls are infrasonic , below the hearing range of humans, and can be heard by other elephants up to 6 miles (9.7 km) away at still times near sunrise and sunset. [150]
Mammals signal by a variety of means. Many give visual anti-predator signals , as when deer and gazelle stot , honestly indicating their fit condition and their ability to escape, [151] [152] or when white-tailed deer and other prey mammals flag with conspicuous tail markings when alarmed, informing the predator that it has been detected. [153] Many mammals make use of scent-marking , sometimes possibly to help defend territory, but probably with a range of functions both within and between species. [154] Microbats and toothed whales including oceanic dolphins vocalize both socially and in echolocation . [155] [156] [157]

Feeding
To maintain a high constant body temperature is energy expensive – mammals therefore need a nutritious and plentiful diet. While the earliest mammals were probably predators, different species have since adapted to meet their dietary requirements in a variety of ways. Some eat other animals – this is a carnivorous diet (and includes insectivorous diets). Other mammals, called herbivores , eat plants, which contain complex carbohydrates such as cellulose. An herbivorous diet includes subtypes such as granivory (seed eating), folivory (leaf eating), frugivory (fruit eating), nectarivory (nectar eating), gummivory (gum eating) and mycophagy (fungus eating). The digestive tract of an herbivore is host to bacteria that ferment these complex substances, and make them available for digestion, which are either housed in the multichambered stomach or in a large cecum. [159] Some mammals are coprophagous , consuming feces to absorb the nutrients not digested when the food was first ingested. [80] :131–137 An omnivore eats both prey and plants. Carnivorous mammals have a simple digestive tract because the proteins , lipids and minerals found in meat require little in the way of specialized digestion. Exceptions to this include baleen whales who also house gut flora in a multi-chambered stomach, like terrestrial herbivores. [160]
The size of an animal is also a factor in determining diet type ( Allen's rule ). Since small mammals have a high ratio of heat-losing surface area to heat-generating volume, they tend to have high energy requirements and a high metabolic rate . Mammals that weigh less than about 18 ounces (510 g) are mostly insectivorous because they cannot tolerate the slow, complex digestive process of an herbivore. Larger animals, on the other hand, generate more heat and less of this heat is lost. They can therefore tolerate either a slower collection process (those that prey on larger vertebrates) or a slower digestive process (herbivores). [161] Furthermore, mammals that weigh more than 18 ounces (510 g) usually cannot collect enough insects during their waking hours to sustain themselves. The only large insectivorous mammals are those that feed on huge colonies of insects ( ants or termites ). [162]
Some mammals are omnivores and display varying degrees of carnivory and herbivory, generally leaning in favor of one more than the other. Since plants and meat are digested differently, there is a preference for one over the other, as in bears where some species may be mostly carnivorous and others mostly herbivorous. [164] They are grouped into three categories: mesocarnivory (50-70% meat), hypercarnivory (70% and greater of meat), and hypocarnivory (50% or less of meat). The dentition of hypocarnivores consists of dull, triangular carnassial teeth meant for grinding food. Hypercarnivores, however, have conical teeth and sharp carnassials meant for slashing, and in some cases strong jaws for bone-crushing, as in the case of hyenas , allowing them to consume bones; some extinct groups, notably the Machairodontinae , had saber-shaped canines . [163]
Some physiological carnivores consume plant matter and some physiological herbivores consuming meat. From a behavioral aspect, this would make them omnivores, but from the physiological standpoint, this may be due to zoopharmacognosy . Physiologically, animals must be able to obtain both energy and nutrients from plant and animal materials to be considered omnivorous. Thus, such animals are still able to be classified as carnivores and herbivores when they are just obtaining nutrients from materials originating from sources that do not seemingly complement their classification. [165] For example, it is well documented that some ungulates. such as giraffes, camels, and cattle, will gnaw on bones to consume particular minerals and nutrients. [166] Also, cats, which are generally regarded as obligate carnivores, occasionally eat grass to regurgitate indigestible material (such as hairballs ), aid with hemoglobin production, and as a laxative. [167]
Many mammals, in the absence of sufficient food requirements in an environment, suppress their metabolism and conserve energy in a process known as hibernation . [168] In the period preceding hibernation, larger mammals, such as bears, become polyphagic to increase fat stores, whereas smaller mammals prefer to collect and stash food. [169] The slowing of the metabolism is accompanied by a decreased heart and respiratory rate, as well as a drop in internal temperatures, which can be around ambient temperature in some cases. For example, the internal temperatures of hibernating arctic ground squirrels can drop to −2.9 °C (26.8 °F), however the head and neck always stay above 0 °C (32 °F). [170] A few mammals in hot environments aestivate in times of drought or extreme heat, namely the fat-tailed dwarf lemur ( Cheirogaleus medius ). [171]

Intelligence
In intelligent mammals, such as primates, the cerebrum is larger relative to the rest of the brain. Intelligence itself is not easy to define, but indications of intelligence include the ability to learn, matched with behavioral flexibility. Rats , for example, are considered to be highly intelligent, as they can learn and perform new tasks, an ability that may be important when they first colonize a fresh habitat . In some mammals, food gathering appears to be related to intelligence: a deer feeding on plants has a brain smaller than a cat, which must think to outwit its prey. [162]
Tool use by animals may indicate different levels of learning and cognition . The sea otter uses rocks as essential and regular parts of its foraging behaviour (smashing abalone from rocks or breaking open shells), with some populations spending 21% of their time making tools. [172] Other tool use, such as chimpanzees using twigs to "fish" for termites, may be developed by watching others use tools and may even be a true example of animal teaching. [173] Tools may even be used in solving puzzles in which the animal appears to experience a "Eureka moment" . [174] Other mammals that do not use tools, such as dogs, can also experience a Eureka moment. [175]
Brain size was previously considered a major indicator of the intelligence of an animal. Since most of the brain is used for maintaining bodily functions, greater ratios of brain to body mass may increase the amount of brain mass available for more complex cognitive tasks. Allometric analysis indicates that mammalian brain size scales at approximately the ⅔ or ¾ exponent of the body mass. Comparison of a particular animal's brain size with the expected brain size based on such allometric analysis provides an encephalisation quotient that can be used as another indication of animal intelligence. [176] Sperm whales have the largest brain mass of any animal on earth, averaging 8,000 cubic centimetres (490 in 3 ) and 7.8 kilograms (17 lb) in mature males. [177]
Self-awareness appears to be a sign of abstract thinking. Self-awareness, although not well-defined, is believed to be a precursor to more advanced processes such as metacognitive reasoning . The traditional method for measuring this is the mirror test , which determines if an animal possesses the ability of self-recognition. [178] Mammals that have 'passed' the mirror test include Asian elephants (some pass, some do not); [179] chimpanzees; [180] bonobos; [181] orangutans; [182] humans, from 18 months ( mirror stage ); [183] bottlenose dolphins [a] [184] killer whales; [185] and false killer whales. [185]

Social structure
Eusociality is the highest level of social organization. These societies have an overlap of adult generations, the division of reproductive labor and cooperative caring of young. Usually insects, such as bees , ants and termites, have eusocial behavior, but it is demonstrated in two rodent species: the naked mole-rat [186] and the Damaraland mole-rat . [187]
Presociality is when animals exhibit more than just sexual interactions with members of the same species, but fall short of qualifying as eusocial. That is, presocial animals can display communal living, cooperative care of young, or primitive division of reproductive labor, but they do not display all of the three essential traits of eusocial animals. Humans and some species of Callitrichidae ( marmosets and tamarins ) are unique among primates in their degree of cooperative care of young. [188] Harry Harlow set up an experiment with rhesus monkeys , presocial primates, in 1958; the results from this study showed that social encounters are necessary in order for the young monkeys to develop both mentally and sexually. [189]
A fission-fusion society is a society that changes frequently in its size and composition, making up a permanent social group called the "parent group". Permanent social networks consist of all individual members of a community and often varies to track changes in their environment. In a fission–fusion society, the main parent group can fracture (fission) into smaller stable subgroups or individuals to adapt to environmental or social circumstances. For example, a number of males may break off from the main group in order to hunt or forage for food during the day, but at night they may return to join (fusion) the primary group to share food and partake in other activities. Many mammals exhibit this, such as primates (for example orangutans and spider monkeys ), [190] elephants, [191] spotted hyenas , [192] lions, [193] and dolphins. [194]
Solitary animals defend a territory and avoid social interactions with the members of its species, except during breeding season. This is to avoid resource competition, as two individuals of the same species would occupy the same niche, and to prevent depletion of food. [195] A solitary animal, while foraging, can also be less conspicuous to predators or prey. [196]
In a hierarchy , individuals are either dominant or submissive. A despotic hierarchy is where one individual is dominant while the others are submissive, as in wolves and lemurs, [197] and a pecking order is a linear ranking of individuals where there is a top individual and a bottom individual. Pecking orders may also be ranked by sex, where the lowest individual of a sex has a higher ranking than the top individual of the other sex, as in hyenas. [198] Dominant individuals, or alphas, have a high chance of reproductive success, especially in harems where one or a few males (resident males) have exclusive breeding rights to females in a group. [199] Non-resident males can also be accepted in harems, but some species, such as the common vampire bat ( Desmodus rotundus ), may be more strict. [200]
Some mammals are perfectly monogamous , meaning that they mate for life and take no other partners (even after the original mate’s death), as with wolves, Eurasian beavers , and otters. [201] [202] There are three types of polygamy: either one or multiple dominant males have breeding rights ( polygyny ), multiple males that females mate with (polyandry), or multiple males have exclusive relations with multiple females (polygynandry). It is much more common for polygynous mating to happen, which, excluding leks , are estimated to occur in up to 90% of mammals. [203] Lek mating occurs in harems, wherein one or a few males protect their harem of females from other males who would otherwise mate with the females, as in elephant seals; [204] or males congregate around females and try to attract them with various courtship displays and vocalizations, as in harbor seals. [205]
All higher mammals (excluding monotremes) share two major adaptations for care of the young: live birth and lactation. These imply a group-wide choice of a degree of parental care . They may build nests and dig burrows to raise their young in, or feed and guard them often for a prolonged period of time. Many mammals are K-selectors , and invest more time and energy into their young than do r-selectors . When two animals mate, they both share an interest in the success of the offspring, though often to different extremes. Mammalian females, both r- and K-selectors, exhibit some degree of maternal aggression, another example of parental care, which may be targeted against other females of the species or the young of other females; however, some mammals may "aunt" the infants of other females, and care for them. Mammalian males may play a role in child rearing, as with tenrecs, however this varies species to species, even within the same genus. For example, the males of the southern pig-tailed macaque ( Macaca nemestrina ) do not participate in child care, whereas the males of the Japanese macaque ( M. fuscata ) do. [206]

Humans and other mammals

In human culture
Non-human mammals play a wide variety of roles in human culture. They are the most popular of pets , with tens of millions of dogs, cats and other animals including rabbits and mice kept by families around the world. [207] [208] [209] Mammals such as mammoths , horses and deer are among the earliest subjects of art, being found in Upper Paleolithic cave paintings such as at Lascaux . [210] Major artists such as Albrecht Dürer , George Stubbs and Edwin Landseer are known for their portraits of mammals. [211] Many species of mammals have been hunted for sport and for food; deer and wild boar are especially popular as game animals . [212] [213] [214] Mammals such as horses and dogs are widely raced for sport, often combined with betting on the outcome . [215] [216] There is a tension between the role of animals as companions to humans, and their existence as individuals with rights of their own . [217] Mammals further play a wide variety of roles in literature, [218] [219] [220] film, [221] mythology, and religion. [222] [223] [224]

Uses and importance
Domestic mammals form a large part of the livestock raised for meat across the world. They include (2011) around 1.4 billion cattle , 1.2 billion sheep , 1 billion domestic pigs , [225] [226] and (1985) over 700 million rabbits. [227] Working domestic animals including cattle and horses have been used for work and transport from the origins of agriculture, their numbers declining with the arrival of mechanised transport and agricultural machinery . In 2004 they still provided some 80% of the power for the mainly small farms in the third world, and some 20% of the world's transport, again mainly in rural areas. In mountainous regions unsuitable for wheeled vehicles, pack animals continue to transport goods. [228] Mammal skins provide leather for shoes , clothing and upholstery . [229] Wool from mammals including sheep, goats and alpacas has been used for centuries for clothing. [230] [231] Mammals serve a major role in science as experimental animals , both in fundamental biological research, such as in genetics, [232] and in the development of new medicines, which must be tested exhaustively to demonstrate their safety . [233] Millions of mammals, especially mice and rats, are used in experiments each year. [234] A knockout mouse is a genetically modified mouse with an inactivated gene , replaced or disrupted with an artificial piece of DNA. They enable the study of sequenced genes whose functions are unknown. [235] [236] A small percentage of the mammals are non-human primates, used in research for their similarity to humans. [237] [238] [239]
Charles Darwin , Jared Diamond and others have noted the importance of domesticated mammals in the Neolithic development of agriculture and of civilization , causing farmers to replace hunter-gatherers around the world. [b] [241] This transition from hunting and gathering to herding flocks and growing crops was a major step in human history. The new agricultural economies, based on domesticated mammals, caused "radical restructuring of human societies, worldwide alterations in biodiversity, and significant changes in the Earth's landforms and its atmosphere... momentous outcomes". [242]

Hybrids
Hybrids are offspring resulting from the breeding of two genetically distinct individuals, which usually will result in a high degree of heterozygosity, though hybrid and heterozygous are not synonymous. The deliberate or accidental hybridizing of two or more species of closely related animals through captive breeding is a human activity which has been in existence for millennia and has grown for economic purposes. [243] Hybrids between different subspecies within a species (such as between the Bengal tiger and Siberian tiger ) are known as intra-specific hybrids. Hybrids between different species within the same genus (such as between lions and tigers) are known as interspecific hybrids or crosses. Hybrids between different genera (such as between sheep and goats) are known as intergeneric hybrids. [244] Natural hybrids will occur in hybrid zones , where two populations of species within the same genera or species living in the same or adjacent areas will interbreed with each other. Some hybrids have been recognized as species, such as the red wolf (though this is controversial). [245]
Artificial selection , the deliberate selective breeding of domestic animals, is being used to breed back recently extinct animals in an attempt to achieve an animal breed with a phenotype that resembles that extinct wildtype ancestor. A breeding-back (intraspecific) hybrid may be very similar to the extinct wildtype in appearance, ecological niche and to some extent genetics, but the initial gene pool of that wild type is lost forever with its extinction . As a result, bred-back breeds are at best vague look-alikes of extinct wildtypes, as Heck cattle are of the aurochs . [246]
Purebred wild species evolved to a specific ecology can be threatened with extinction [247] through the process of genetic pollution , the uncontrolled hybridization, introgression genetic swamping which leads to homogenization or out-competition from the heterosic hybrid species. [248] When new populations are imported or selectively bred by people, or when habitat modification brings previously isolated species into contact, extinction in some species, especially rare varieties, is possible. [249] Interbreeding can swamp the rarer gene pool and create hybrids, depleting the purebred gene pool. For example, the endangered wild water buffalo is most threatened with extinction by genetic pollution from the domestic water buffalo . Such extinctions are not always apparent from a morphological standpoint. Some degree of gene flow is a normal evolutionary process, nevertheless, hybridization threatens the existence of rare species. [250] [251]

Threats
The loss of species from ecological communities, defaunation , is primarily driven by human activity. [252] This has resulted in empty forests , ecological communities depleted of large vertebrates. [253] [254] In the Quaternary extinction event , the mass die-off of megafaunal variety coincided with the appearance of humans, suggesting a human influence. One hypothesis is that humans hunted large mammals, such as the woolly mammoth , into extinction. [255] [256]
Various species are predicted to become extinct in the near future , [257] among them the rhinoceros , [258] primates , [259] pangolins , [260] and giraffes . [261] Hunting alone threatens hundreds of mammalian species around the world. [262] [263] Scientists claim that the growing demand for meat is contributing to biodiversity loss as this is a significant driver of deforestation and habitat destruction ; species-rich habitats, such as significant portions of the Amazon rainforest , are being converted to agricultural land for meat production. [264] [265] [266] According to the World Wildlife Fund 's 2016 Living Planet Index , global wildlife populations have declined 58% since 1970, primarily due to habitat destruction, over-hunting and pollution. They project that if current trends continue, 67% of wildlife could disappear by 2020. [267] [268] Another influence is over-hunting and poaching , which can reduce the overall population of game animals, [269] especially those located near villages, [270] as in the case of peccaries . [271] The effects of poaching can especially be seen in the ivory trade with African elephants. [272] Marine mammals are at risk from entanglement from fishing gear, notably cetaceans , with discard mortalities ranging from 65,000 to 86,000 individuals annually. [273]
Several courses of actions are being taken globally, notably the Convention on Biological Diversity , otherwise known as the Rio Accord, which includes 189 signatory countries that are focused on identifying endangered species and habitats. [274] Another notable conservation organization is the IUCN, which has a membership of over 1,200 governmental and non-governmental organizations. [275]
Recent extinctions can be directly attributable to human influences. [276] [252] The IUCN characterizes 'recent' extinction as those that have occurred past the cut-off point of 1500, [277] and around 80 mammal species have gone extinct since that time and 2015. [278] Some species, such as the Père David's deer [279] are extinct in the wild , and survive solely in captive populations. Other species, such as the Florida panther , are ecologically extinct , surviving in such low numbers that that they essentially have no impact on the ecosystem. [280] :318 Other populations are only locally extinct (extirpated), still existing elsewhere, but reduced in distribution, [280] :75–77 as with the extinction of gray whales in the Atlantic . [281]

Species life span
Among mammals, species maximum life span differs by at least 100-fold ( shrew 2 years , bowhead whale 211 years). [282] Although the underlying basis for these life span differences is still uncertain, numerous studies indicate that the ability to repair DNA damages is an important determinant of mammalian life span. In the early study of Hart and Setlow (1974) [283] , it was found that DNA excision repair capability increased systematically with species life span among seven mammalian species. In more recent studies, similar results were reported. Species life span was observed to be robustly correlated with the capacity to recognize DNA double-strand breaks as well as the level of the DNA repair protein Ku80 . [282] In a study of the cells from sixteen mammalian species, genes employed in DNA repair were found to be up-regulated in the longer-lived species. [284] The cellular level of the DNA repair enzyme poly ADP ribose polymerase was found to correlate with species life span in a study of 13 mammalian species. [285] Three additional studies of a variety of mammalian species also reported a correlation between species life span and DNA repair capability. [286] [287] [288]

Notes

See also
WebPage index: 00195
Cricetidae
The Cricetidae are a family of rodents in the large and complex superfamily Muroidea . It includes true hamsters , voles , lemmings , and New World rats and mice . At almost 600 species, it is the second-largest family of mammals , and has members throughout the Americas, Asia, and Europe.

Characteristics
The cricetids are small mammals, ranging from just 5–8 cm (2.0–3.1 in) in length and 7 g (0.25 oz) in weight in the New World pygmy mouse up to 41–62 cm (16–24 in) and 1.1 kg (2.4 lb) in the muskrat . The length of their tails varies greatly in relation to their bodies, and they may be either furred or sparsely haired. The fur of most species is brownish in colour, often with a white underbelly, but many other patterns exist, especially in the cricetine and arvicoline subfamilies.
Like the Old World mice , cricetids are adapted to a wide range of habitats, from the high Arctic to tropical rainforests and hot deserts . Some are arboreal, with long balancing tails and other adaptations for climbing, while others are semiaquatic , with webbed feet and small external ears . Yet others are burrowing animals, or ground-dwellers. [1]
Their diets are similarly variable, with herbivorous , omnivorous , and insectivorous species all being known. They all have large, gnawing, incisors separated from grinding molar teeth by a gap, or diastema . Although a few exceptions occur, the dental formula for the great majority of cricetids is:
Cricetids' populations can increase rapidly in times of plenty, due to a combination of short gestation periods between 15 and 50 days, and large litter sizes relative to many other mammals. The young are typically born blind, hairless, and helpless. [1]

Evolution and systematics
The cricetids first evolved in the Old World during the Miocene . They soon adapted to a wide range of habitats, and spread throughout the world. The voles and lemmings arose later, during the Pliocene , and rapidly diversified during the Pleistocene . [2]
The circumscription of Cricetidae has gone through several permutations. Some members of the family as currently defined have been placed in the family Muridae . Some muroids have historically been placed in Cricetidae, such as mouse-like hamsters (subfamily Calomyscinae , family Calomyscidae ), gerbils (subfamily Gerbillinae , family Muridae), the crested rat (subfamily Lophiomyinae , family Muridae), zokors (subfamily Myospalacinae , family Spalacidae ), the white-tailed rat (subfamily Mystromyinae , family Nesomyidae ), and spiny dormice (subfamily Platacanthomyinae , family Platacanthomyidae ). Multigene DNA sequence studies have shown the subfamilies listed below to form a monophyletic group (that is, they share a common ancestor more recently than with any other group), and other groups now considered muroids should not be included in the Cricetidae. [3]
The cricetids thus currently include one fossil and five extant subfamilies , with about 112 living genera and 580 species :
WebPage index: 00196
Binomial nomenclature
Binomial nomenclature (also called binomi n al nomenclature or binary nomenclature ) is a formal system of naming species of living things by giving each a name composed of two parts, both of which use Latin grammatical forms , although they can be based on words from other languages. Such a name is called a binomial name (which may be shortened to just "binomial"), a binomen , binominal name or a scientific name ; more informally it is also called a Latin name . The first part of the name identifies the genus to which the species belongs; the second part identifies the species within the genus. For example, humans belong to the genus Homo and within this genus to the species Homo sapiens . The formal introduction of this system of naming species is credited to Carl Linnaeus , effectively beginning with his work Species Plantarum in 1753. [1] But Gaspard Bauhin , in as early as 1623, had introduced in his book Pinax theatri botanici (English, Illustrated exposition of plants ) many names of genera that were later adopted by Linnaeus. [2]
The application of binomial nomenclature is now governed by various internationally agreed codes of rules, of which the two most important are the International Code of Zoological Nomenclature ( ICZN ) for animals and the International Code of Nomenclature for algae, fungi, and plants ( ICN ). Although the general principles underlying binomial nomenclature are common to these two codes, there are some differences, both in the terminology they use and in their precise rules.
In modern usage, the first letter of the first part of the name, the genus, is always capitalized in writing, while that of the second part is not, even when derived from a proper noun such as the name of a person or place. Similarly, both parts are italicized when a binomial name occurs in normal text. Thus the binomial name of the annual phlox (named after botanist Thomas Drummond ) is now written as Phlox drummondii .
In scientific works, the "authority" for a binomial name is usually given, at least when it is first mentioned, and the date of publication may be specified.

History
Prior to the adoption of the modern binomial system of naming species, a scientific name consisted of a generic name combined with a specific name that was from one to several words long. Together they formed a system of polynomial nomenclature. [3] These names had two separate functions. First, to designate or label the species, and second, to be a diagnosis or description; however these two goals were eventually found to be incompatible. [4] In a simple genus, containing only two species, it was easy to tell them apart with a one-word genus and a one-word specific name; but as more species were discovered the names necessarily became longer and unwieldy, for instance Plantago foliis ovato-lanceolatus pubescentibus, spica cylindrica, scapo tereti ("Plantain with pubescent ovate-lanceolate leaves, a cylindric spike and a terete scape"), which we know today as Plantago media .
Such "polynomial names" may sometimes look like binomials, but are significantly different. For example, Gerard's herbal (as amended by Johnson) describes various kinds of spiderwort: "The first is called Phalangium ramosum , Branched Spiderwort; the second, Phalangium non ramosum , Unbranched Spiderwort. The other ... is aptly termed Phalangium Ephemerum Virginianum , Soon-Fading Spiderwort of Virginia". [5] The Latin phrases are short descriptions, rather than identifying labels.
The Bauhins , in particular Caspar Bauhin (1560–1624), took some important steps towards the binomial system, by pruning the Latin descriptions, in many cases to two words. [6] The adoption by biologists of a system of strictly binomial nomenclature is due to Swedish botanist and physician Carl von Linné, more commonly known by his Latinized name Carl Linnaeus (1707–1778). It was in his 1753 Species Plantarum that he first began consistently using a one-word "trivial name" together with a generic name in a system of binomial nomenclature. [7] This trivial name is what is now known as a specific epithet ( ICN ) or specific name ( ICZN ). [7] The Bauhins' genus names were retained in many of these, but the descriptive part was reduced to a single word.
Linnaeus's trivial names introduced an important new idea, namely that the function of a name could simply be to give a species a unique label. This meant that the name no longer need be descriptive; for example both parts could be derived from the names of people. Thus Gerard's phalangium ephemerum virginianum became Tradescantia virginiana , where the genus name honoured John Tradescant the younger , [note 1] an English botanist and gardener. [8] A bird in the parrot family was named Psittacus alexandri , meaning "Alexander's parrot", after Alexander the Great whose armies introduced eastern parakeets to Greece. [9] Linnaeus' trivial names were much easier to remember and use than the parallel polynomial names and eventually replaced them. [1]

Value
The value of the binomial nomenclature system derives primarily from its economy, its widespread use, and the uniqueness and stability of names it generally favors:

Problems
Binomial nomenclature for species has the effect that when a species is moved from one genus to another, sometimes the specific name or epithet must be changed as well. This may happen because the specific name is already used in the new genus, or to agree in gender with the new genus. Some biologists have argued for the combination of the genus name and specific epithet into a single unambiguous name, or for the use of uninomials (as used in nomenclature of ranks above species). [16]
Because binomials are unique only within a kingdom, it is possible for two or more species to share the same binomial if they occur in different kingdoms. At least five instances of such binomial duplication occur. [17]

Relationship to classification and taxonomy
Nomenclature (including binomial nomenclature) is not the same as classification, although the two are related. Classification is the ordering of items into groups based on similarities or differences; in biological classification , species are one of the kinds of item to be classified. [18] In principle, the names given to species could be completely independent of their classification. This is not the case for binomial names, since the first part of a binomial is the name of the genus into which the species is placed. Above the rank of genus, binomial nomenclature and classification are partly independent; for example, a species retains its binomial name if it is moved from one family to another or from one order to another, unless it better fits a different genus in the same or different family, or it is split from its old genus and placed in a newly created genus. The independence is only partial since the names of families and other higher taxa are usually based on genera. [ citation needed ]
Taxonomy includes both nomenclature and classification. Its first stages (sometimes called " alpha taxonomy ") are concerned with finding, describing and naming species of living or fossil organisms. [19] Binomial nomenclature is thus an important part of taxonomy as it is the system by which species are named. Taxonomists are also concerned with classification, including its principles, procedures and rules. [20]

Derivation of binomial names
A complete binomial name is always treated grammatically as if it were a phrase in the Latin language (hence the common use of the term "Latin name" for a binomial name). However, the two parts of a binomial name can each be derived from a number of sources, of which Latin is only one. These include:
The first part of the name, which identifies the genus, must be a word which can be treated as a Latin singular noun in the nominative case . It must be unique within each kingdom , but can be repeated between kingdoms. Thus Huia recurvata is an extinct species of plant, found as fossils in Yunnan , China, [30] whereas Huia masonii is a species of frog found in Java , Indonesia. [31]
The second part of the name, which identifies the species within the genus, is also treated grammatically as a Latin word. It can have one of a number of forms.
Whereas the first part of a binomial name must be unique within a kingdom, the second part is quite commonly used in two or more genera (as is shown by examples of hodgsonii above). The full binomial name must be unique within a kingdom.

Codes
From the early 19th century onwards it became ever more apparent that a body of rules was necessary to govern scientific names. In the course of time these became nomenclature codes . The International Code of Zoological Nomenclature ( ICZN ) governs the naming of animals, [33] the International Code of Nomenclature for algae, fungi, and plants ( ICN ) that of plants (including cyanobacteria ), and the International Code of Nomenclature of Bacteria ( ICNB ) that of bacteria (including Archaea ). Virus names are governed by the International Committee on Taxonomy of Viruses ( ICTV ), a taxonomic code, which determines taxa as well as names. These codes differ in certain ways, e.g.:
Unifying the different codes into a single code, the " BioCode ", has been suggested, although implementation is not in sight. (There is also a code in development for a different system of classification which does not use ranks, but instead names clades . This is called the PhyloCode .)

Writing binomial names
By tradition, the binomial names of species are usually typeset in italics; for example, Homo sapiens . [38] Generally, the binomial should be printed in a font style different from that used in the normal text; for example, " Several more Homo sapiens fossils were discovered ." When handwritten, each part of a binomial name should be underlined; for example, Homo sapiens . [39]
The first part of the binomial, the genus name, is always written with an initial capital letter. In current usage, the second part is never written with an initial capital. [40] [41] Older sources, particularly botanical works published before the 1950s, use a different convention. If the second part of the name is derived from a proper noun, e.g. the name of a person or place, a capital letter was used. Thus the modern form Berberis darwinii was written as Berberis Darwinii . A capital was also used when the name is formed by two nouns in apposition, e.g. Panthera Leo or Centaurea Cyanus . [42] [note 3]
When used with a common name, the scientific name often follows in parentheses, although this varies with publication. [43] For example, "The house sparrow ( Passer domesticus ) is decreasing in Europe."
The binomial name should generally be written in full. The exception to this is when several species from the same genus are being listed or discussed in the same paper or report, or the same species is mentioned repeatedly; in which case the genus is written in full when it is first used, but may then be abbreviated to an initial (and a period/full stop). [44] For example, a list of members of the genus Canis might be written as " Canis lupus , C. aureus , C. simensis ". In rare cases, this abbreviated form has spread to more general use; for example, the bacterium Escherichia coli is often referred to as just E. coli , and Tyrannosaurus rex is perhaps even better known simply as T. rex , these two both often appearing in this form in popular writing even where the full genus name has not already been given.
The abbreviation "sp." is used when the actual specific name cannot or need not be specified. The abbreviation "spp." (plural) indicates "several species". These abbreviations are not italicised (or underlined). [45] For example: " Canis sp." means "an unspecified species of the genus Canis ", while " Canis spp." means "two or more species of the genus Canis ". (The abbreviations "sp." and "spp." can easily be confused with the abbreviations "ssp." (zoology) or "subsp." (botany), plurals "sspp." or "subspp.", referring to one or more subspecies . See trinomen (zoology) and infraspecific name .)
The abbreviation " cf. " (i.e. confer in Latin) is used to compare individuals/taxa with known/described species. Conventions for use of the "cf." qualifier vary. [46] In paleontology, it is typically used when the identification is not confirmed. [47] For example, " Corvus cf. nasicus " was used to indicate "a fossil bird similar to the Cuban crow but not certainly identified as this species". [48] In molecular systematics papers, "cf." may be used to indicate one or more undescribed species assumed related to a described species. For example, in a paper describing the phylogeny of small benthic freshwater fish called darters, five undescribed putative species (Ozark, Sheltowee, Wildcat, Ihiyo, and Mamequit darters), notable for brightly colored nuptial males with distinctive color patterns, [49] were referred to as " Etheostoma cf. spectabile " because they had been viewed as related to, but distinct from, Etheostoma spectabile (orangethroat darter). [50] This view was supported in varying degrees by DNA analysis. The somewhat informal use of taxa names with qualifying abbreviations is referred to as open nomenclature and it is not subject to strict usage codes.
In some contexts the dagger symbol ("†") may be used before or after the binomial name to indicate that the species is extinct.

Authority
In scholarly texts, at least the first or main use of the binomial name is usually followed by the "authority" – a way of designating the scientist(s) who first published the name. The authority is written in slightly different ways in zoology and botany. For names governed by the ICZN the surname is usually written in full together with the date (normally only the year) of publication. The ICZN recommends that the "original author and date of a name should be cited at least once in each work dealing with the taxon denoted by that name." [51] For names governed by the ICN the name is generally reduced to a standard abbreviation and the date omitted. The International Plant Names Index maintains an approved list of botanical author abbreviations. Historically, abbreviations were used in zoology too.
When the original name is changed, e.g. the species is moved to a different genus, both Codes use parentheses around the original authority; the ICN also requires the person who made the change to be given. Some examples:

Other ranks
Binomial nomenclature, as described here, is a system for naming species. Implicitly it includes a system for naming genera, since the first part of the name of the species is a genus name. In a classification system based on ranks there are also ways of naming ranks above the level of genus and below the level of species. Ranks above genus (e.g., family, order, class) receive one-part names, which are conventionally not written in italics. Thus the house sparrow, Passer domesticus , belongs to the family Passeridae . Family names are normally based on genus names, although the endings used differ between zoology and botany.
Ranks below species receive three-part names, conventionally written in italics like the names of species. There are significant differences between the ICZN and the ICN . In zoology, the only rank below species is subspecies and the name is written simply as three parts (a trinomen). Thus one of the subspecies of the olive-backed pipit is Anthus hodgsoni berezowskii . In botany, there are many ranks below species and although the name itself is written in three parts, a "connecting term" (not part of the name) is needed to show the rank. Thus the American black elder is Sambucus nigra subsp. canadensis ; the white-flowered form of the ivy-leaved cyclamen is Cyclamen hederifolium f. albiflorum .

See also

Notes
WebPage index: 00197
Oldfield Thomas
Michael Rogers Oldfield Thomas FRS FZS (21 February 1858 – 16 June 1929) was a British zoologist . [1] [2] [3]
Thomas worked at the Natural History Museum on mammals , describing about 2,000 new species and subspecies for the first time. He was appointed to the Museum Secretary's office in 1876, transferring to the Zoological Department in 1878. In 1891 Thomas married Mary Kane, daughter of Sir Andrew Clark , heiress to a small fortune, which gave him the finances to hire mammal collectors and present their specimens to the museum. He also did field work himself in western Europe and South America. His wife shared his interest in natural history, and accompanied him on collecting trips. [2] In 1896 when William Henry Flower took control of the department he hired Richard Lydekker to rearrange the exhibitions, [4] allowing Thomas to concentrate on these new specimens. [5] [6] Officially retired from the museum in 1923, he continued his work without interruption. Although popular rumours suggested he died by shooting himself with a handgun while sitting at his museum desk [7] , he actually died at home [8] in 1929, aged 71, about a year after the death of his wife, "a severe blow from which he never recovered". [2]
WebPage index: 00198
Synonym (taxonomy)
In scientific nomenclature , a synonym is a scientific name that applies to a taxon that (now) goes by a different scientific name, [1] although the term is used somewhat differently in the zoological code of nomenclature. [2] For example, Linnaeus was the first to give a scientific name (under the currently used system of scientific nomenclature) to the Norway spruce , which he called Pinus abies . This name is no longer in use: it is now a synonym of the current scientific name which is Picea abies .
Unlike synonyms in other contexts, in taxonomy a synonym is not interchangeable with the name of which it is a synonym. In taxonomy, synonyms are not equals, but have a different status. For any taxon with a particular circumscription , position, and rank, only one scientific name is considered to be the correct one at any given time (this correct name is to be determined by applying the relevant code of nomenclature ). A synonym cannot exist in isolation: it is always an alternative to a different scientific name. Given that the correct name of a taxon depends on the taxonomic viewpoint used (resulting in a particular circumscription, position and rank) a name that is one taxonomist's synonym may be another taxonomist's correct name (and vice versa ).
Synonyms may arise whenever the same taxon is described and named more than once, independently. They may also arise when existing taxa are changed, as when two taxa are joined to become one, a species is moved to a different genus, a variety is moved to a different species, etc. Synonyms also come about when the codes of nomenclature change, so that older names are no longer acceptable; for example, Erica herbacea L.. has been rejected in favour of Erica carnea L. and is thus its synonym. [3]

General usage
To the general user of scientific names, in fields such as agriculture, horticulture, ecology, general science, etc., a synonym is a name that was previously used as the correct scientific name (in handbooks and similar sources) but which has been displaced by another scientific name, which is now regarded as correct. Thus Oxford Dictionaries Online defines the term as "a taxonomic name which has the same application as another, especially one which has been superseded and is no longer valid." [4] In handbooks and general texts, it is useful to have synonyms mentioned as such after the current scientific name, so as to avoid confusion. For example, if the much advertised name change should go through and the scientific name of the fruit fly were changed to Sophophora melanogaster , it would be very helpful if any mention of this name was accompanied by "(syn. Drosophila melanogaster )". Synonyms used in this way may not always meet the strict definitions of the term "synonym" in the formal rules of nomenclature which govern scientific names (see below) .
Changes of scientific name have two causes: they may be taxonomic or nomenclatural. A name change may be caused by changes in the circumscription, position or rank of a taxon, representing a change in taxonomic, scientific insight (as would be the case for the fruit fly, mentioned above). A name change may be due to purely nomenclatural reasons, that is, based on the rules of nomenclature; [ citation needed ] as for example when an older name is (re)discovered which has priority over the current name. Speaking in general, name changes for nomenclatural reasons have become less frequent over time as the rules of nomenclature allow for names to be conserved, so as to promote stability of scientific names.

Zoology
In zoological nomenclature, codified in the International Code of Zoological Nomenclature , synonyms are different scientific names of the same rank that pertain to the same taxon, for example two names for the same species. The earliest such name is called the senior synonym , while the later name is the junior synonym . One basic principle of zoological nomenclature is that the earliest correctly published (and thus available ) name, the senior synonym, takes precedence and must be used for the taxon, if no other restrictions interfere. Synonyms are important because if the earliest name cannot be used (for example because the same spelling had previously been used for a name established for another taxon), then the next available junior synonym must be used for the taxon.
Objective synonyms refer to taxa with the same type and same rank (more or less the same taxon, although circumscription may vary, even widely). This may be species-group taxa of the same rank with the same type specimen , genus-group taxa of the same rank with the same type species or if their type species are themselves objective synonyms, of family-group taxa with the same type genus, etc. [5] In the case of subjective synonyms there is no such shared type, so the synonymy is open to taxonomic judgement, [6] meaning that there is room for debate: one researcher might consider the two (or more) types to refer to one and the same taxon, another might consider them to belong to different taxa. For example, John Edward Gray published the name Antilocapra anteflexa in 1855 for a species of pronghorn , based on a pair of horns. However, it is now commonly accepted that his specimen was an unusual individual of the species Antilocapra americana published by George Ord in 1815. Ord's name thus takes precedence, with Antilocapra anteflexa being a junior subjective synonym.
Objective synonyms are common at the level of genera, because for various reasons two genera may contain the same type species; these are objective synonyms. [7] In many cases researchers established new generic names because they thought this was necessary or did not know that others had previously established another genus for the same group of species. An example is the genus Pomatia Beck, 1837, [8] which was established for a group of terrestrial snails containing as its type species the Burgundy or Roman snail Helix pomatia – since Helix pomatia was already the type species for the genus Helix Linnaeus, 1758, the genus Pomatia was an objective synonym (and useless). At the same occasion Helix is also a synonym of Pomatia , but it is older and so it has precedence.
At the species level, subjective synonyms are common because of an unexpectedly large range of variation in a species, or simple ignorance about an earlier description, may lead a biologist to describe a newly discovered specimen as a new species. A common reason for objective synonyms at this level is the creation of a replacement name.
It is possible for a junior synonym to be given precedence over a senior synonym, [9] primarily when the senior name has not been used since 1899, and the junior name is in common use. The older name may be declared to be a nomen oblitum , and the junior name declared a nomen protectum . This rule exists primarily to prevent the confusion that would result if a well-known name, with a large accompanying body of literature, were to be replaced by a completely unfamiliar name. An example is the European land snail Petasina edentula (Draparnaud, 1805). In 2002, researchers found that an older name Helix depilata Draparnaud, 1801 referred to the same species, but this name had never been used after 1899 and was fixed as a nomen oblitum under this rule by Falkner et al . 2002. [10]
Such a reversal of precedence is also possible if the senior synonym was established after 1900, but only if the International Commission on Zoological Nomenclature (ICZN) approves an application. For example, the scientific name of the red imported fire ant , Solenopsis invicta was published by Buren in 1972, who did not know that this species was first named Solenopsis saevissima wagneri by Santschi in 1916; as there were thousands of publications using the name invicta before anyone discovered the synonymy, the ICZN, in 2001, ruled that invicta would be given precedence over wagneri .
To qualify as a synonym in zoology, a name must be properly published in accordance with the rules. Manuscript names and names that were mentioned without any description ( nomina nuda ) are not considered as synonyms in zoological nomenclature.

Botany
In botanical nomenclature , a synonym is a name that is not correct for the circumscription, position, and rank of the taxon as considered in the particular botanical publication. It is always "a synonym of the correct scientific name", but which name is correct depends on the taxonomic opinion of the author. In botany the various kinds of synonyms are:
In botany, although a synonym must be a formally accepted scientific name (a validly published name): a listing of "synonyms", a "synonymy", often contains designations that for some reason did not make it as a formal name, such as manuscript names, or even misidentifications (although it is now the usual practice to list misidentifications separately [11] ).

Comparison between zoology and botany
Although the basic principles are fairly similar, the treatment of synonyms in botanical nomenclature differs in detail and terminology from zoological nomenclature, where the correct name is included among synonyms, although as first among equals it is the "senior synonym":

Synonym lists
Scientific papers may include lists of taxa, synonymizing existing taxa and (in some cases) listing references to them.
The status of a synonym may be indicated by symbols, as for instance in a system proposed for use in palaeontology by Rudolf Richter. In that system a v before the year would indicate that the authors have inspected the original material; a . that they take on the responsibility for the act of synonymizing the taxa. [12]

Other usage
The traditional concept of synonymy is often expanded in taxonomic literature to include "pro parte" (or "for part") synonyms. These are caused by splits and circumscriptional changes. They are usually indicated by the abbreviation "p.p." [13] For example:

See also
